{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaff4a70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T11:24:19.575807Z",
     "iopub.status.busy": "2025-07-16T11:24:19.575573Z",
     "iopub.status.idle": "2025-07-16T11:24:19.598019Z",
     "shell.execute_reply": "2025-07-16T11:24:19.597300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Attention is All You Need\n",
       "\n",
       "[Study Notes](#study-notes)\n",
       "\n",
       "[Questions](#questions)\n",
       "\n",
       "\n",
       "\n",
       "### Key Points\n",
       "\n",
       "##### 1. üöÄ Transformer Architecture  \n",
       "- The Transformer model is based entirely on attention mechanisms, removing recurrence and convolutions.  \n",
       "- The encoder and decoder each consist of 6 identical layers in the base model.  \n",
       "- Each encoder layer has two sub-layers: multi-head self-attention and a position-wise feed-forward network.  \n",
       "- Each decoder layer has three sub-layers: masked multi-head self-attention, encoder-decoder attention, and a feed-forward network.  \n",
       "- Residual connections and layer normalization are applied around each sub-layer.\n",
       "\n",
       "##### 2. üéØ Attention Mechanism  \n",
       "- Attention maps a query and a set of key-value pairs to an output vector via weighted sums of values.  \n",
       "- Scaled Dot-Product Attention computes attention weights by dot products of queries and keys, scaled by $ \\sqrt{d_k} $, followed by softmax.  \n",
       "- Multi-Head Attention runs multiple attention layers in parallel on linearly projected queries, keys, and values, then concatenates results.  \n",
       "- The Transformer uses 8 attention heads with $ d_k = d_v = 64 $ and $ d_{model} = 512 $.\n",
       "\n",
       "##### 3. üìè Positional Encoding  \n",
       "- Since the Transformer lacks recurrence or convolution, positional encodings are added to input embeddings to provide token order information.  \n",
       "- Positional encodings use sine and cosine functions of different frequencies for each dimension.  \n",
       "- The formula for positional encoding is:  \n",
       "  $$\n",
       "  PE(pos, 2i) = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right), \\quad PE(pos, 2i+1) = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\n",
       "  $$  \n",
       "- Learned positional embeddings were tested but sinusoidal encodings performed similarly and allow extrapolation to longer sequences.\n",
       "\n",
       "##### 4. ‚öôÔ∏è Training Details  \n",
       "- Trained on WMT 2014 English-German (4.5M sentence pairs) and English-French (36M sentence pairs) datasets.  \n",
       "- Used byte-pair encoding (BPE) or word-piece tokenization with vocabularies around 32K‚Äì37K tokens.  \n",
       "- Training done on 8 NVIDIA P100 GPUs; base model trained in ~12 hours, big model in 3.5 days.  \n",
       "- Optimizer: Adam with $ \\beta_1=0.9, \\beta_2=0.98, \\epsilon=10^{-9} $.  \n",
       "- Learning rate schedule: linear warm-up for 4000 steps, then decay proportional to inverse square root of step number.  \n",
       "- Regularization: dropout rate 0.1 on sub-layers and embeddings, label smoothing with $ \\epsilon_{ls} = 0.1 $.\n",
       "\n",
       "##### 5. üìä Performance Results  \n",
       "- Transformer (big) achieves 28.4 BLEU on WMT 2014 English-to-German, outperforming previous best models and ensembles by over 2 BLEU.  \n",
       "- Transformer (big) achieves 41.0 BLEU on WMT 2014 English-to-French, surpassing previous single models at less than 1/4 training cost.  \n",
       "- Base Transformer model surpasses all previously published models and ensembles on English-German at a fraction of training cost.\n",
       "\n",
       "##### 6. üîç Advantages of Self-Attention  \n",
       "- Self-attention layers connect all positions in a sequence with a constant number of sequential operations, unlike RNNs which require $ O(n) $ sequential steps.  \n",
       "- Self-attention has lower computational complexity than RNNs when sequence length $ n $ is less than representation dimension $ d $.  \n",
       "- Self-attention allows shorter paths for learning long-range dependencies compared to convolutional or recurrent layers.  \n",
       "- Multi-head attention mitigates the loss of resolution caused by averaging attention weights.\n",
       "\n",
       "##### 7. üß© Model Variations and Ablations  \n",
       "- Single-head attention reduces BLEU by about 0.9 compared to multi-head attention.  \n",
       "- Reducing attention key size $ d_k $ hurts model quality, indicating the importance of sufficient key dimension.  \n",
       "- Larger models and dropout improve performance and reduce overfitting.  \n",
       "- Sinusoidal positional encoding and learned positional embeddings yield nearly identical results.\n",
       "\n",
       "##### 8. üß† Generalization Beyond Translation  \n",
       "- Transformer applied to English constituency parsing achieves state-of-the-art or near state-of-the-art results on the Penn Treebank WSJ dataset.  \n",
       "- Outperforms many previous discriminative parsers even with limited training data (40K sentences).  \n",
       "- Semi-supervised training with larger corpora further improves parsing performance.\n",
       "\n",
       "\n",
       "\n",
       "<br>\n",
       "\n",
       "## Study Notes\n",
       "\n",
       "### 1. üß† Introduction to the Transformer and Attention\n",
       "\n",
       "In the world of natural language processing (NLP) and sequence modeling, tasks like machine translation have traditionally relied on **recurrent neural networks (RNNs)** and **convolutional neural networks (CNNs)**. These models process sequences step-by-step (sequentially), which limits how much they can be parallelized during training and inference. This sequential nature makes training slow, especially for long sequences.\n",
       "\n",
       "The **Transformer** is a groundbreaking model architecture that **completely removes recurrence and convolutions** and instead relies solely on an **attention mechanism** to model relationships between all parts of the input and output sequences. This shift allows the Transformer to be highly parallelizable, faster to train, and more effective at capturing long-range dependencies in sequences.\n",
       "\n",
       "##### Why is this important?\n",
       "\n",
       "- Traditional RNNs process tokens one at a time, which slows down training.\n",
       "- Attention mechanisms let the model focus on relevant parts of the input regardless of their position.\n",
       "- The Transformer achieves state-of-the-art results in machine translation with less training time and computational cost.\n",
       "\n",
       "\n",
       "### 2. üîç Background: From RNNs and CNNs to Attention\n",
       "\n",
       "Before the Transformer, sequence models were mostly based on:\n",
       "\n",
       "- **Recurrent Neural Networks (RNNs)**: Process sequences step-by-step, maintaining a hidden state that depends on previous tokens. Variants like LSTMs and GRUs improved performance but still suffered from slow sequential processing.\n",
       "- **Convolutional Neural Networks (CNNs)**: Use convolutional filters to process sequences in parallel but have limitations in capturing long-range dependencies because the number of layers needed grows with sequence length.\n",
       "- **Attention Mechanisms**: Introduced as a way to let models \"attend\" to different parts of the input sequence when generating each output token, improving the ability to model dependencies regardless of distance.\n",
       "\n",
       "The Transformer takes this a step further by **using attention exclusively**, removing the need for recurrence or convolution entirely.\n",
       "\n",
       "\n",
       "### 3. üèóÔ∏è Transformer Architecture Overview\n",
       "\n",
       "The Transformer follows the classic **encoder-decoder** structure common in sequence-to-sequence models:\n",
       "\n",
       "- **Encoder**: Takes the input sequence and converts it into a continuous representation.\n",
       "- **Decoder**: Generates the output sequence one token at a time, using the encoder‚Äôs output and previously generated tokens.\n",
       "\n",
       "##### Key features of the Transformer architecture:\n",
       "\n",
       "- Both encoder and decoder are made up of **stacks of identical layers** (6 layers each in the base model).\n",
       "- Each encoder layer has two main parts:\n",
       "  1. **Multi-head self-attention**: Allows each position in the input to attend to all other positions.\n",
       "  2. **Position-wise feed-forward network**: A fully connected network applied independently to each position.\n",
       "- Each decoder layer has three parts:\n",
       "  1. **Masked multi-head self-attention**: Prevents the decoder from \"seeing\" future tokens during training (maintains autoregressive property).\n",
       "  2. **Encoder-decoder attention**: Allows the decoder to attend to the encoder‚Äôs output.\n",
       "  3. **Position-wise feed-forward network**.\n",
       "- **Residual connections** and **layer normalization** are applied around each sub-layer to stabilize training and improve gradient flow.\n",
       "- The model uses **fixed-dimensional embeddings** (512 dimensions in the base model) for inputs and outputs.\n",
       "\n",
       "\n",
       "### 4. üéØ Attention Mechanisms in Detail\n",
       "\n",
       "##### What is Attention?\n",
       "\n",
       "Attention is a way for the model to weigh different parts of the input when producing an output. It works by comparing a **query** vector to a set of **key** vectors, producing weights that are applied to corresponding **value** vectors. The output is a weighted sum of these values.\n",
       "\n",
       "##### Scaled Dot-Product Attention\n",
       "\n",
       "- Inputs: Queries (Q), Keys (K), and Values (V).\n",
       "- Compute dot products between Q and K to measure similarity.\n",
       "- Scale the dot products by dividing by the square root of the key dimension (‚àödk) to prevent large values that can cause gradients to vanish.\n",
       "- Apply a softmax to get attention weights.\n",
       "- Multiply weights by V to get the output.\n",
       "\n",
       "Mathematically:\n",
       "\n",
       "$$\n",
       "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V\n",
       "$$\n",
       "\n",
       "##### Multi-Head Attention\n",
       "\n",
       "Instead of performing one attention operation, the Transformer uses **multiple attention heads** in parallel:\n",
       "\n",
       "- The queries, keys, and values are linearly projected into multiple smaller subspaces.\n",
       "- Each head performs scaled dot-product attention independently.\n",
       "- The outputs of all heads are concatenated and projected again to form the final output.\n",
       "\n",
       "**Why multiple heads?**\n",
       "\n",
       "- Allows the model to attend to information from different representation subspaces simultaneously.\n",
       "- Helps capture different types of relationships and dependencies.\n",
       "\n",
       "\n",
       "### 5. üß© Components of the Transformer Layers\n",
       "\n",
       "##### Encoder Layer\n",
       "\n",
       "- **Multi-head self-attention**: Each token attends to all tokens in the input sequence.\n",
       "- **Feed-forward network**: Two linear transformations with a ReLU activation in between, applied independently to each position.\n",
       "- Residual connections and layer normalization wrap both sub-layers.\n",
       "\n",
       "##### Decoder Layer\n",
       "\n",
       "- **Masked multi-head self-attention**: Ensures the decoder only attends to previous tokens (no peeking ahead).\n",
       "- **Encoder-decoder attention**: Decoder attends to the encoder‚Äôs output, allowing it to focus on relevant parts of the input.\n",
       "- **Feed-forward network**: Same as encoder.\n",
       "- Residual connections and layer normalization applied similarly.\n",
       "\n",
       "\n",
       "### 6. üìè Positional Encoding: Adding Order Without Recurrence\n",
       "\n",
       "Since the Transformer has no recurrence or convolution, it needs a way to understand the order of tokens in a sequence.\n",
       "\n",
       "- **Positional encodings** are added to the input embeddings to inject information about token positions.\n",
       "- The paper uses **sinusoidal functions** of different frequencies for each dimension of the positional encoding.\n",
       "- This method allows the model to learn relative positions and generalize to longer sequences than seen during training.\n",
       "- The formula for positional encoding at position $ pos $ and dimension $ i $ is:\n",
       "\n",
       "$$\n",
       "PE(pos, 2i) = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\n",
       "$$\n",
       "$$\n",
       "PE(pos, 2i+1) = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\n",
       "$$\n",
       "\n",
       "\n",
       "### 7. ‚öôÔ∏è Training the Transformer\n",
       "\n",
       "##### Data and Setup\n",
       "\n",
       "- Trained on large datasets like WMT 2014 English-German (4.5 million sentence pairs) and English-French (36 million sentence pairs).\n",
       "- Uses **byte-pair encoding (BPE)** or word-piece tokenization to handle vocabulary efficiently.\n",
       "- Batches are formed by approximate sequence length to optimize GPU memory usage.\n",
       "\n",
       "##### Hardware and Speed\n",
       "\n",
       "- Training done on 8 NVIDIA P100 GPUs.\n",
       "- Base model trains in about 12 hours; larger models take longer but still faster than previous architectures.\n",
       "\n",
       "##### Optimizer and Learning Rate\n",
       "\n",
       "- Uses the **Adam optimizer** with specific hyperparameters.\n",
       "- Learning rate increases linearly during a warm-up phase (first 4000 steps), then decreases proportionally to the inverse square root of the step number.\n",
       "\n",
       "##### Regularization\n",
       "\n",
       "- **Dropout** applied to sub-layer outputs and embeddings to prevent overfitting.\n",
       "- **Label smoothing** used during training to improve generalization by preventing the model from becoming too confident.\n",
       "\n",
       "\n",
       "### 8. üìä Results and Impact\n",
       "\n",
       "##### Machine Translation\n",
       "\n",
       "- The Transformer achieves **state-of-the-art BLEU scores** on English-to-German and English-to-French translation tasks.\n",
       "- Outperforms previous models, including ensembles, with significantly less training time and computational cost.\n",
       "- Demonstrates that attention-only models can replace RNNs and CNNs effectively.\n",
       "\n",
       "##### Generalization to Other Tasks\n",
       "\n",
       "- Applied successfully to **English constituency parsing**, a task with structural constraints and longer outputs.\n",
       "- Outperforms many previous models even with limited training data.\n",
       "- Shows the Transformer‚Äôs flexibility beyond translation.\n",
       "\n",
       "\n",
       "### 9. üîç Why Self-Attention? Advantages Over RNNs and CNNs\n",
       "\n",
       "- **Parallelization**: Self-attention allows all positions in a sequence to be processed simultaneously, unlike RNNs which are inherently sequential.\n",
       "- **Shorter paths for long-range dependencies**: Any two positions in the sequence can directly attend to each other in one step, making it easier to learn relationships between distant tokens.\n",
       "- **Computational efficiency**: For typical sequence lengths, self-attention is faster than RNNs.\n",
       "- **Interpretability**: Attention weights can be inspected to understand what the model focuses on, revealing syntactic and semantic patterns.\n",
       "\n",
       "\n",
       "### 10. üîÆ Conclusion and Future Directions\n",
       "\n",
       "The Transformer represents a major shift in sequence modeling by relying entirely on attention mechanisms. It achieves:\n",
       "\n",
       "- Faster training and inference.\n",
       "- Better performance on translation and parsing tasks.\n",
       "- A flexible architecture that can be extended to other modalities like images, audio, and video.\n",
       "\n",
       "Future work includes exploring **local attention** for very long sequences and making generation less sequential to further speed up inference.\n",
       "\n",
       "\n",
       "### Summary\n",
       "\n",
       "The **Transformer** is a powerful, attention-only model that replaces traditional RNNs and CNNs in sequence tasks. Its key innovation is the use of **multi-head self-attention** combined with **positional encodings** to model sequences efficiently and effectively. This architecture has revolutionized NLP and laid the foundation for many subsequent advances in AI.\n",
       "\n",
       "\n",
       "\n",
       "<br>\n",
       "\n",
       "## Questions\n",
       "\n",
       "##### 1. What is the primary architectural innovation of the Transformer compared to traditional sequence models?  \n",
       "A) Use of convolutional layers instead of recurrent layers  \n",
       "B) Complete removal of recurrence and convolution, relying solely on attention  \n",
       "C) Introduction of gated recurrent units (GRUs)  \n",
       "D) Use of multi-head self-attention mechanisms  \n",
       "\n",
       "##### 2. Which of the following are advantages of self-attention over recurrent layers?  \n",
       "A) Enables parallel computation across sequence positions  \n",
       "B) Requires fewer sequential operations proportional to sequence length  \n",
       "C) Automatically encodes positional information without additional input  \n",
       "D) Shorter maximum path length between any two positions in the sequence  \n",
       "\n",
       "##### 3. In scaled dot-product attention, why is the dot product scaled by the square root of the key dimension?  \n",
       "A) To increase the magnitude of the dot products for better gradient flow  \n",
       "B) To prevent the dot products from growing too large and pushing softmax into regions with small gradients  \n",
       "C) To normalize the attention weights so they sum to one  \n",
       "D) To reduce computational complexity  \n",
       "\n",
       "##### 4. Multi-head attention improves model performance primarily because:  \n",
       "A) It increases the total dimensionality of the model  \n",
       "B) It allows the model to attend to information from different representation subspaces simultaneously  \n",
       "C) It averages attention weights to reduce noise  \n",
       "D) It enables the model to capture multiple types of relationships in parallel  \n",
       "\n",
       "##### 5. Which of the following statements about positional encoding in the Transformer are true?  \n",
       "A) Positional encodings are learned parameters updated during training  \n",
       "B) Sinusoidal positional encodings allow the model to generalize to longer sequences than seen during training  \n",
       "C) Positional encodings are added to input embeddings to inject order information  \n",
       "D) The Transformer uses recurrent positional embeddings to encode sequence order  \n",
       "\n",
       "##### 6. How does the Transformer‚Äôs decoder prevent attending to future tokens during training?  \n",
       "A) By using a masking mechanism that sets illegal attention weights to negative infinity before softmax  \n",
       "B) By limiting the attention window to previous tokens only  \n",
       "C) By using a separate recurrent network for the decoder  \n",
       "D) By offsetting output embeddings by one position  \n",
       "\n",
       "##### 7. Which of the following are components of each encoder layer in the Transformer?  \n",
       "A) Multi-head self-attention  \n",
       "B) Position-wise feed-forward network  \n",
       "C) Encoder-decoder attention  \n",
       "D) Residual connections and layer normalization  \n",
       "\n",
       "##### 8. What is the role of residual connections in the Transformer architecture?  \n",
       "A) To allow gradients to flow more easily through deep networks  \n",
       "B) To reduce the number of parameters in the model  \n",
       "C) To combine outputs of attention heads  \n",
       "D) To normalize the input embeddings  \n",
       "\n",
       "##### 9. Compared to convolutional sequence models like ConvS2S and ByteNet, self-attention layers:  \n",
       "A) Have a maximum path length between positions that grows linearly with sequence length  \n",
       "B) Have a constant maximum path length between any two positions  \n",
       "C) Require fewer operations to relate distant positions  \n",
       "D) Are less parallelizable than convolutional layers  \n",
       "\n",
       "##### 10. Why might additive attention outperform dot-product attention without scaling for large key dimensions?  \n",
       "A) Because additive attention uses a feed-forward network that better models compatibility  \n",
       "B) Because dot-product attention is computationally more expensive  \n",
       "C) Because dot-product attention‚Äôs unscaled dot products can become very large, causing small gradients  \n",
       "D) Because additive attention normalizes the keys and queries  \n",
       "\n",
       "##### 11. Which of the following describe the training regime used for the Transformer?  \n",
       "A) Use of Adam optimizer with warm-up learning rate schedule  \n",
       "B) Training on batches grouped by approximate sequence length  \n",
       "C) Use of label smoothing to improve BLEU scores despite hurting perplexity  \n",
       "D) Training exclusively on single GPUs for maximum efficiency  \n",
       "\n",
       "##### 12. In the Transformer, what is the dimensionality relationship between the number of attention heads (h), the model dimension (dmodel), and the key/value dimensions (dk, dv)?  \n",
       "A) $ d_k = d_v = \\frac{d_{model}}{h} $  \n",
       "B) $ d_k = d_v = d_{model} \\times h $  \n",
       "C) $ d_k = d_v = d_{model} $  \n",
       "D) $ d_k = d_v = \\frac{h}{d_{model}} $  \n",
       "\n",
       "##### 13. Which of the following are true about the feed-forward networks in the Transformer layers?  \n",
       "A) They are applied independently to each position in the sequence  \n",
       "B) They consist of two linear transformations with a ReLU activation in between  \n",
       "C) They share parameters across all layers  \n",
       "D) They can be interpreted as convolutions with kernel size 1  \n",
       "\n",
       "##### 14. How does the Transformer handle input and output token embeddings and the final softmax layer?  \n",
       "A) Uses separate weight matrices for input embeddings, output embeddings, and softmax  \n",
       "B) Shares the same weight matrix between input embeddings, output embeddings, and the pre-softmax linear transformation  \n",
       "C) Multiplies embeddings by the square root of the model dimension before adding positional encodings  \n",
       "D) Uses learned positional embeddings only for the decoder  \n",
       "\n",
       "##### 15. What is the main reason the Transformer can be trained faster than RNN-based models?  \n",
       "A) It uses fewer parameters overall  \n",
       "B) It allows parallelization across all positions in the sequence during training  \n",
       "C) It uses convolutional layers that are faster than recurrent layers  \n",
       "D) It requires fewer training steps to converge  \n",
       "\n",
       "##### 16. Which of the following statements about the maximum path length in different layer types is correct?  \n",
       "A) Recurrent layers have a maximum path length proportional to the sequence length $O(n)$  \n",
       "B) Self-attention layers have a maximum path length of $O(1)$  \n",
       "C) Convolutional layers with kernel size $k$ have maximum path length $O(\\log_k n)$ if dilated  \n",
       "D) Self-attention layers have longer maximum path lengths than recurrent layers  \n",
       "\n",
       "##### 17. In the context of the Transformer, what is the significance of masking in the decoder‚Äôs self-attention?  \n",
       "A) It prevents the model from attending to padding tokens  \n",
       "B) It enforces the autoregressive property by blocking attention to future tokens  \n",
       "C) It improves computational efficiency by reducing the number of keys considered  \n",
       "D) It is only applied during inference, not training  \n",
       "\n",
       "##### 18. Which of the following are challenges or limitations of self-attention that the Transformer addresses?  \n",
       "A) Reduced effective resolution due to averaging attention-weighted positions  \n",
       "B) Difficulty in learning long-range dependencies  \n",
       "C) High computational cost for very long sequences  \n",
       "D) Inability to model positional information without recurrence  \n",
       "\n",
       "##### 19. How does the Transformer generalize to tasks beyond machine translation, such as English constituency parsing?  \n",
       "A) By using the same architecture with minimal task-specific tuning  \n",
       "B) By adding task-specific recurrent layers to the decoder  \n",
       "C) By increasing the number of attention heads significantly  \n",
       "D) By training on much larger datasets only  \n",
       "\n",
       "##### 20. Which of the following statements about the learning rate schedule used in training the Transformer are true?  \n",
       "A) The learning rate increases linearly during a warm-up phase  \n",
       "B) After warm-up, the learning rate decreases proportionally to the inverse square root of the step number  \n",
       "C) The learning rate remains constant throughout training  \n",
       "D) Warm-up steps are set to 4000 in the base model\n",
       "\n",
       "\n",
       "\n",
       "<br>\n",
       "\n",
       "## Answers\n",
       "\n",
       "##### 1. What is the primary architectural innovation of the Transformer compared to traditional sequence models?  \n",
       "A) ‚úó The Transformer removes recurrence and convolution, not replaces them with convolution.  \n",
       "B) ‚úì The Transformer relies solely on attention, removing recurrence and convolution entirely.  \n",
       "C) ‚úó GRUs are a type of RNN, not part of the Transformer innovation.  \n",
       "D) ‚úì Multi-head self-attention is a key part of the Transformer‚Äôs architecture.  \n",
       "\n",
       "**Correct:** B, D\n",
       "\n",
       "\n",
       "##### 2. Which of the following are advantages of self-attention over recurrent layers?  \n",
       "A) ‚úì Self-attention allows parallel computation across all positions.  \n",
       "B) ‚úì Requires fewer sequential operations, independent of sequence length.  \n",
       "C) ‚úó Positional information is injected separately via positional encodings.  \n",
       "D) ‚úì Maximum path length between positions is constant, aiding long-range dependency learning.  \n",
       "\n",
       "**Correct:** A, B, D\n",
       "\n",
       "\n",
       "##### 3. In scaled dot-product attention, why is the dot product scaled by the square root of the key dimension?  \n",
       "A) ‚úó Scaling reduces magnitude, not increases it.  \n",
       "B) ‚úì Prevents large dot products that push softmax into regions with tiny gradients.  \n",
       "C) ‚úó Softmax normalization is independent of scaling factor.  \n",
       "D) ‚úó Scaling does not reduce computational complexity.  \n",
       "\n",
       "**Correct:** B\n",
       "\n",
       "\n",
       "##### 4. Multi-head attention improves model performance primarily because:  \n",
       "A) ‚úó It does not increase total model dimensionality; it splits it.  \n",
       "B) ‚úì Allows attending to different representation subspaces simultaneously.  \n",
       "C) ‚úó Averaging would reduce expressiveness; multi-head concatenates outputs.  \n",
       "D) ‚úì Captures multiple types of relationships in parallel.  \n",
       "\n",
       "**Correct:** B, D\n",
       "\n",
       "\n",
       "##### 5. Which of the following statements about positional encoding in the Transformer are true?  \n",
       "A) ‚úó The paper uses fixed sinusoidal encodings, not learned parameters (though learned embeddings were tested).  \n",
       "B) ‚úì Sinusoidal encodings help generalize to longer sequences.  \n",
       "C) ‚úì Positional encodings are added to embeddings to provide order information.  \n",
       "D) ‚úó No recurrent positional embeddings are used.  \n",
       "\n",
       "**Correct:** B, C\n",
       "\n",
       "\n",
       "##### 6. How does the Transformer‚Äôs decoder prevent attending to future tokens during training?  \n",
       "A) ‚úì Uses masking to set illegal attention weights to -‚àû before softmax.  \n",
       "B) ‚úó It masks rather than limits the window size.  \n",
       "C) ‚úó No recurrent network is used in the decoder.  \n",
       "D) ‚úì Output embeddings are offset by one position to prevent peeking.  \n",
       "\n",
       "**Correct:** A, D\n",
       "\n",
       "\n",
       "##### 7. Which of the following are components of each encoder layer in the Transformer?  \n",
       "A) ‚úì Multi-head self-attention is a core sub-layer.  \n",
       "B) ‚úì Position-wise feed-forward network is the second sub-layer.  \n",
       "C) ‚úó Encoder-decoder attention is only in the decoder layers.  \n",
       "D) ‚úì Residual connections and layer normalization wrap each sub-layer.  \n",
       "\n",
       "**Correct:** A, B, D\n",
       "\n",
       "\n",
       "##### 8. What is the role of residual connections in the Transformer architecture?  \n",
       "A) ‚úì Facilitate gradient flow and stabilize training in deep networks.  \n",
       "B) ‚úó They do not reduce parameter count.  \n",
       "C) ‚úó Residuals do not combine attention heads.  \n",
       "D) ‚úó Normalization is a separate step, not residual connections.  \n",
       "\n",
       "**Correct:** A\n",
       "\n",
       "\n",
       "##### 9. Compared to convolutional sequence models like ConvS2S and ByteNet, self-attention layers:  \n",
       "A) ‚úó Convolutional models have path length growing with sequence length; self-attention does not.  \n",
       "B) ‚úì Self-attention has constant maximum path length $O(1)$.  \n",
       "C) ‚úì Self-attention requires fewer operations to relate distant positions.  \n",
       "D) ‚úó Self-attention is more parallelizable than convolutional layers.  \n",
       "\n",
       "**Correct:** B, C\n",
       "\n",
       "\n",
       "##### 10. Why might additive attention outperform dot-product attention without scaling for large key dimensions?  \n",
       "A) ‚úì Additive attention uses a feed-forward network that can better model compatibility.  \n",
       "B) ‚úó Dot-product attention is faster, not more expensive.  \n",
       "C) ‚úì Unscaled dot products can become large, causing small gradients in softmax.  \n",
       "D) ‚úó Additive attention does not normalize keys and queries.  \n",
       "\n",
       "**Correct:** A, C\n",
       "\n",
       "\n",
       "##### 11. Which of the following describe the training regime used for the Transformer?  \n",
       "A) ‚úì Adam optimizer with warm-up learning rate schedule is used.  \n",
       "B) ‚úì Batches are grouped by approximate sequence length for efficiency.  \n",
       "C) ‚úì Label smoothing improves BLEU despite hurting perplexity.  \n",
       "D) ‚úó Training uses multiple GPUs, not single GPU exclusively.  \n",
       "\n",
       "**Correct:** A, B, C\n",
       "\n",
       "\n",
       "##### 12. In the Transformer, what is the dimensionality relationship between the number of attention heads (h), the model dimension (dmodel), and the key/value dimensions (dk, dv)?  \n",
       "A) ‚úì Each head‚Äôs key and value dimension is $d_{model} / h$.  \n",
       "B) ‚úó Dimensions are not multiplied by the number of heads.  \n",
       "C) ‚úó Keys and values are not full model dimension per head.  \n",
       "D) ‚úó This ratio is inverted.  \n",
       "\n",
       "**Correct:** A\n",
       "\n",
       "\n",
       "##### 13. Which of the following are true about the feed-forward networks in the Transformer layers?  \n",
       "A) ‚úì Applied independently to each position.  \n",
       "B) ‚úì Two linear layers with ReLU activation in between.  \n",
       "C) ‚úó Parameters differ between layers; not shared.  \n",
       "D) ‚úì Equivalent to convolutions with kernel size 1.  \n",
       "\n",
       "**Correct:** A, B, D\n",
       "\n",
       "\n",
       "##### 14. How does the Transformer handle input and output token embeddings and the final softmax layer?  \n",
       "A) ‚úó Weight matrices are shared, not separate.  \n",
       "B) ‚úì Shares the same weight matrix for input embeddings, output embeddings, and pre-softmax linear layer.  \n",
       "C) ‚úì Embeddings are scaled by $\\sqrt{d_{model}}$ before adding positional encodings.  \n",
       "D) ‚úó Positional encodings are used in both encoder and decoder, not only decoder.  \n",
       "\n",
       "**Correct:** B, C\n",
       "\n",
       "\n",
       "##### 15. What is the main reason the Transformer can be trained faster than RNN-based models?  \n",
       "A) ‚úó Parameter count is not the main factor.  \n",
       "B) ‚úì Parallelization across all sequence positions during training.  \n",
       "C) ‚úó Transformer does not use convolutional layers.  \n",
       "D) ‚úó Number of training steps is not necessarily fewer.  \n",
       "\n",
       "**Correct:** B\n",
       "\n",
       "\n",
       "##### 16. Which of the following statements about the maximum path length in different layer types is correct?  \n",
       "A) ‚úì Recurrent layers have path length proportional to sequence length $O(n)$.  \n",
       "B) ‚úì Self-attention layers have constant path length $O(1)$.  \n",
       "C) ‚úì Dilated convolutions have path length $O(\\log_k n)$.  \n",
       "D) ‚úó Self-attention has shorter, not longer, path lengths than recurrent layers.  \n",
       "\n",
       "**Correct:** A, B, C\n",
       "\n",
       "\n",
       "##### 17. In the context of the Transformer, what is the significance of masking in the decoder‚Äôs self-attention?  \n",
       "A) ‚úó Masking here is for future tokens, not padding tokens.  \n",
       "B) ‚úì Prevents attending to future tokens, preserving autoregressive property.  \n",
       "C) ‚úó Masking is for correctness, not computational efficiency.  \n",
       "D) ‚úó Masking is applied during both training and inference.  \n",
       "\n",
       "**Correct:** B\n",
       "\n",
       "\n",
       "##### 18. Which of the following are challenges or limitations of self-attention that the Transformer addresses?  \n",
       "A) ‚úì Averaging attention can reduce effective resolution; multi-head attention counters this.  \n",
       "B) ‚úó Self-attention improves learning of long-range dependencies.  \n",
       "C) ‚úì Computational cost grows quadratically with sequence length, challenging very long sequences.  \n",
       "D) ‚úì Positional information must be explicitly added since no recurrence or convolution exists.  \n",
       "\n",
       "**Correct:** A, C, D\n",
       "\n",
       "\n",
       "##### 19. How does the Transformer generalize to tasks beyond machine translation, such as English constituency parsing?  \n",
       "A) ‚úì Uses the same architecture with minimal task-specific tuning.  \n",
       "B) ‚úó Does not add recurrent layers for other tasks.  \n",
       "C) ‚úó Number of attention heads is not necessarily increased.  \n",
       "D) ‚úó Does not require much larger datasets to perform well.  \n",
       "\n",
       "**Correct:** A\n",
       "\n",
       "\n",
       "##### 20. Which of the following statements about the learning rate schedule used in training the Transformer are true?  \n",
       "A) ‚úì Learning rate increases linearly during warm-up.  \n",
       "B) ‚úì After warm-up, learning rate decreases proportionally to inverse square root of step number.  \n",
       "C) ‚úó Learning rate is not constant throughout training.  \n",
       "D) ‚úì Warm-up steps are set to 4000 in the base model.  \n",
       "\n",
       "**Correct:** A, B, D"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from source.render import render\n",
    "render(1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
