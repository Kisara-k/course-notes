## 1.4 Spacy

## Questions



#### 1. What is a fundamental difference between spaCy and NLTK in terms of programming style?  
A) spaCy is object-oriented, while NLTK is primarily string processing  
B) NLTK uses object-oriented programming, spaCy uses functional programming  
C) spaCy treats text as objects with properties and methods, NLTK returns strings or lists  
D) Both spaCy and NLTK are purely string processing libraries  

#### 2. Which of the following best describes spaCy’s approach to tokenization algorithms?  
A) spaCy allows users to select from multiple tokenization algorithms  
B) spaCy automatically chooses the most efficient tokenization algorithm out of the box  
C) spaCy requires manual tuning of tokenization parameters for best results  
D) spaCy’s tokenizer handles language-specific exceptions like abbreviations by default  

#### 3. When using NLTK’s sentence tokenizer on the text "Dr. Strange loves pav bhaji.", what is a likely issue?  
A) It will correctly identify the entire sentence as one  
B) It may incorrectly split "Dr." as a separate sentence  
C) It always treats abbreviations as part of the sentence  
D) It requires additional customization or data downloads to improve accuracy  

#### 4. Which of the following statements about spaCy’s token objects is true?  
A) Each token has attributes like `.is_alpha`, `.is_digit`, and `.is_currency`  
B) Tokens in spaCy are simple strings without additional properties  
C) Token objects can be indexed like Python lists within a document object  
D) spaCy tokens cannot be sliced to create spans or sub-documents  

#### 5. What is the role of the spaCy pipeline component called "sentencizer"?  
A) It performs word tokenization  
B) It splits a paragraph into sentences in a blank pipeline  
C) It is automatically included in all spaCy pipelines by default  
D) It can be manually added to a blank pipeline to enable sentence boundary detection  

#### 6. Why might you want to customize spaCy’s tokenizer with special cases?  
A) To split slang or contractions into multiple tokens  
B) To modify the original text of tokens during tokenization  
C) To handle domain-specific abbreviations or non-standard words  
D) To improve tokenization speed by skipping exceptions  

#### 7. Which of the following are true about spaCy’s pre-trained language pipelines?  
A) They include components like tagger, parser, NER, and lemmatizer  
B) They are only available for English language  
C) They provide part-of-speech tagging and named entity recognition out of the box  
D) They can be loaded using `spacy.load()` with a model name like `en_core_web_sm`  

#### 8. What is the difference between stemming and lemmatization?  
A) Stemming uses fixed heuristic rules, lemmatization uses linguistic knowledge  
B) Lemmatization always produces valid dictionary words, stemming may not  
C) Stemming requires part-of-speech tagging, lemmatization does not  
D) Lemmatization can handle irregular word forms like "ate" → "eat"  

#### 9. Why does spaCy not support stemming?  
A) Because stemming is less accurate and spaCy prefers lemmatization  
B) Because stemming requires manual algorithm selection which spaCy avoids  
C) Because stemming is computationally more expensive than lemmatization  
D) Because spaCy’s design philosophy focuses on linguistically informed processing  

#### 10. In spaCy, what does the `.lemma_` attribute of a token represent?  
A) The original text of the token  
B) The base or dictionary form of the token as a string  
C) A unique hash identifier for the token’s lemma  
D) The part-of-speech tag of the token  

#### 11. Which of the following statements about NLTK’s tokenization capabilities is correct?  
A) NLTK provides multiple tokenizers that can be selected for different use cases  
B) NLTK’s default tokenizers always outperform spaCy’s tokenizers in accuracy  
C) NLTK requires downloading additional data packages like `punkt` for sentence tokenization  
D) NLTK’s tokenizers are object-oriented and return token objects with attributes  

#### 12. How does spaCy handle tokens like currency symbols and punctuation during tokenization?  
A) It treats currency symbols as part of the adjacent word token  
B) It splits currency symbols and punctuation into separate tokens  
C) It ignores punctuation tokens by default  
D) It uses prefix and suffix rules to separate these tokens correctly  

#### 13. What is a "span" object in spaCy?  
A) A single token in a document  
B) A slice or contiguous subsequence of tokens within a document  
C) A special type of token representing named entities  
D) A pipeline component that processes sentences  

#### 14. When creating a blank spaCy pipeline with `spacy.blank("en")`, which of the following is true?  
A) The pipeline includes tokenizer, tagger, parser, and NER by default  
B) The pipeline only includes a tokenizer component initially  
C) Sentence boundary detection is enabled by default  
D) You can manually add components like sentencizer or NER to this pipeline  

#### 15. Which of the following best describes the relationship between spaCy’s pipeline components and the final `Doc` object?  
A) Pipeline components process the text sequentially and enrich the `Doc` with annotations  
B) The `Doc` object is created only after all pipeline components finish processing  
C) Pipeline components operate independently and do not affect the `Doc` object  
D) The `Doc` object contains tokens but no information about entities or part-of-speech  

#### 16. In the context of spaCy, what does the "attribute ruler" component do?  
A) It assigns or customizes token attributes like lemma or part-of-speech  
B) It controls the order of pipeline components  
C) It performs stemming on tokens  
D) It visualizes token attributes in the output  

#### 17. Which of the following are valid reasons to use cloud-based NLP APIs instead of local spaCy models?  
A) To avoid the need for high compute resources like GPUs locally  
B) To get access to pre-trained models without installation  
C) To customize tokenization and lemmatization rules extensively  
D) To quickly perform NLP tasks via HTTP calls without deep NLP knowledge  

#### 18. Consider the sentence: `"Let's go to N.Y.!"` Why is simple splitting by spaces or periods insufficient for tokenization?  
A) Because "Let's" is a contraction that should be split into two tokens  
B) Because "N.Y." is an abbreviation and should not be split into multiple sentences  
C) Because punctuation marks like exclamation points need to be separate tokens  
D) Because spaces always indicate token boundaries in English  

#### 19. Which of the following statements about NLTK’s flexibility compared to spaCy is true?  
A) NLTK allows manual selection of tokenization algorithms, spaCy does not  
B) NLTK is better suited for production applications due to its simplicity  
C) spaCy is more customizable than NLTK in terms of tokenization algorithms  
D) NLTK requires more manual tuning to achieve high accuracy in tokenization  

#### 20. When customizing spaCy’s tokenizer to split "gimme" into two tokens, which of the following is true?  
A) You can modify the original text of the token during tokenization  
B) You can only split the token into multiple tokens without changing the text  
C) Special cases can be added using the `ORTH` symbol to define token boundaries  
D) This customization affects the underlying text stored in the `Doc` object  



<br>

## Answers



#### 1. What is a fundamental difference between spaCy and NLTK in terms of programming style?  
A) ✓ spaCy is object-oriented, allowing text to be treated as objects with properties and methods.  
B) ✗ NLTK is not primarily object-oriented; it mainly processes strings and lists.  
C) ✓ spaCy treats text as objects; NLTK returns strings or lists without encapsulating text as objects.  
D) ✗ Both are not purely string processing; spaCy is object-oriented.

**Correct:** A, C


#### 2. Which of the following best describes spaCy’s approach to tokenization algorithms?  
A) ✗ spaCy does not allow user selection of tokenization algorithms.  
B) ✓ spaCy automatically selects the most efficient tokenization algorithm out of the box.  
C) ✗ spaCy does not require manual tuning for tokenization.  
D) ✓ spaCy’s tokenizer handles language-specific exceptions like abbreviations by default.

**Correct:** B, D


#### 3. When using NLTK’s sentence tokenizer on the text "Dr. Strange loves pav bhaji.", what is a likely issue?  
A) ✗ NLTK’s default tokenizer may incorrectly split "Dr." as a sentence end.  
B) ✓ It may incorrectly split "Dr." as a separate sentence.  
C) ✗ NLTK does not always treat abbreviations correctly without customization.  
D) ✓ Additional data downloads or customization (like `punkt`) are often needed for better accuracy.

**Correct:** B, D


#### 4. Which of the following statements about spaCy’s token objects is true?  
A) ✓ Tokens have attributes like `.is_alpha`, `.is_digit`, `.is_currency`.  
B) ✗ Tokens are not simple strings; they are objects with properties.  
C) ✓ Tokens can be indexed like Python lists within a document object.  
D) ✗ Tokens can be sliced to create spans (sub-documents).

**Correct:** A, C


#### 5. What is the role of the spaCy pipeline component called "sentencizer"?  
A) ✗ Sentencizer does not perform word tokenization.  
B) ✓ It splits paragraphs into sentences in a blank pipeline.  
C) ✗ It is not automatically included in all pipelines; must be added manually in blank pipelines.  
D) ✓ It can be manually added to enable sentence boundary detection.

**Correct:** B, D


#### 6. Why might you want to customize spaCy’s tokenizer with special cases?  
A) ✓ To split slang or contractions into multiple tokens.  
B) ✗ You cannot modify the original text during tokenization, only split tokens.  
C) ✓ To handle domain-specific abbreviations or non-standard words.  
D) ✗ Customization is not primarily for speed improvements.

**Correct:** A, C


#### 7. Which of the following are true about spaCy’s pre-trained language pipelines?  
A) ✓ They include components like tagger, parser, NER, and lemmatizer.  
B) ✗ They are available for many languages, not only English.  
C) ✓ They provide POS tagging and named entity recognition out of the box.  
D) ✓ They can be loaded using `spacy.load()` with model names like `en_core_web_sm`.

**Correct:** A, C, D


#### 8. What is the difference between stemming and lemmatization?  
A) ✓ Stemming uses fixed heuristic rules; lemmatization uses linguistic knowledge.  
B) ✓ Lemmatization produces valid dictionary words; stemming may produce non-words.  
C) ✗ Stemming does not require POS tagging; lemmatization often does.  
D) ✓ Lemmatization can handle irregular forms like "ate" → "eat".

**Correct:** A, B, D


#### 9. Why does spaCy not support stemming?  
A) ✓ Because stemming is less accurate and spaCy prefers lemmatization.  
B) ✓ Because stemming requires manual algorithm selection, which spaCy avoids.  
C) ✗ Stemming is not more computationally expensive than lemmatization.  
D) ✓ spaCy focuses on linguistically informed processing rather than heuristic rules.

**Correct:** A, B, D


#### 10. In spaCy, what does the `.lemma_` attribute of a token represent?  
A) ✗ `.lemma_` is not the original text.  
B) ✓ It is the base or dictionary form of the token as a string.  
C) ✗ The unique hash is `.lemma` (without underscore), not `.lemma_`.  
D) ✗ Part-of-speech tag is `.pos_`, not `.lemma_`.

**Correct:** B


#### 11. Which of the following statements about NLTK’s tokenization capabilities is correct?  
A) ✓ NLTK provides multiple tokenizers selectable for different use cases.  
B) ✗ NLTK’s default tokenizers do not always outperform spaCy’s in accuracy.  
C) ✓ NLTK requires downloading additional data packages like `punkt` for sentence tokenization.  
D) ✗ NLTK tokenizers return strings or lists, not token objects with attributes.

**Correct:** A, C


#### 12. How does spaCy handle tokens like currency symbols and punctuation during tokenization?  
A) ✗ Currency symbols are not treated as part of adjacent words.  
B) ✓ Currency symbols and punctuation are split into separate tokens.  
C) ✗ Punctuation tokens are not ignored by default; they are tokenized.  
D) ✓ Prefix and suffix rules are used to separate these tokens correctly.

**Correct:** B, D


#### 13. What is a "span" object in spaCy?  
A) ✗ A span is not a single token.  
B) ✓ A span is a slice or contiguous subsequence of tokens within a document.  
C) ✗ Spans are not special tokens but groups of tokens.  
D) ✗ Span is not a pipeline component.

**Correct:** B


#### 14. When creating a blank spaCy pipeline with `spacy.blank("en")`, which of the following is true?  
A) ✗ Blank pipeline does not include tagger, parser, or NER by default.  
B) ✓ It only includes a tokenizer component initially.  
C) ✗ Sentence boundary detection is not enabled by default.  
D) ✓ You can manually add components like sentencizer or NER.

**Correct:** B, D


#### 15. Which of the following best describes the relationship between spaCy’s pipeline components and the final `Doc` object?  
A) ✓ Pipeline components process text sequentially and enrich the `Doc` with annotations.  
B) ✗ The `Doc` object is created immediately after tokenization, before all components finish.  
C) ✗ Pipeline components affect the `Doc` object; they are not independent.  
D) ✗ The `Doc` contains tokens and annotations like entities and POS if pipeline components run.

**Correct:** A


#### 16. In the context of spaCy, what does the "attribute ruler" component do?  
A) ✓ It assigns or customizes token attributes like lemma or POS.  
B) ✗ It does not control pipeline order.  
C) ✗ It does not perform stemming.  
D) ✗ It does not visualize token attributes.

**Correct:** A


#### 17. Which of the following are valid reasons to use cloud-based NLP APIs instead of local spaCy models?  
A) ✓ To avoid needing high compute resources like GPUs locally.  
B) ✓ To access pre-trained models without local installation.  
C) ✗ Cloud APIs usually do not allow deep customization of tokenization or lemmatization.  
D) ✓ To quickly perform NLP tasks via HTTP calls without deep NLP knowledge.

**Correct:** A, B, D


#### 18. Consider the sentence: `"Let's go to N.Y.!"` Why is simple splitting by spaces or periods insufficient for tokenization?  
A) ✓ "Let's" is a contraction that should be split into two tokens.  
B) ✓ "N.Y." is an abbreviation and should not be split into multiple sentences.  
C) ✓ Punctuation marks like exclamation points need to be separate tokens.  
D) ✗ Spaces do not always indicate token boundaries (e.g., contractions).

**Correct:** A, B, C


#### 19. Which of the following statements about NLTK’s flexibility compared to spaCy is true?  
A) ✓ NLTK allows manual selection of tokenization algorithms; spaCy does not.  
B) ✗ NLTK is not necessarily better suited for production due to complexity.  
C) ✗ spaCy is less customizable in tokenization algorithms than NLTK.  
D) ✓ NLTK requires more manual tuning to achieve high accuracy.

**Correct:** A, D


#### 20. When customizing spaCy’s tokenizer to split "gimme" into two tokens, which of the following is true?  
A) ✗ You cannot modify the original text during tokenization.  
B) ✓ You can split the token into multiple tokens without changing the text.  
C) ✓ Special cases can be added using the `ORTH` symbol to define token boundaries.  
D) ✗ This customization does not change the underlying text stored in the `Doc`.

**Correct:** B, C

