## 1.4 Spacy

[Study Notes](#study-notes)

[Questions](#questions)



### Key Points



#### 1. ü§ñ spaCy vs NLTK Differences  
- spaCy is an **object-oriented** NLP library; NLTK is primarily a **string processing** library.  
- spaCy provides the **most efficient algorithm by default**; NLTK offers multiple algorithms for customization.  
- spaCy is more **user-friendly** for app developers; NLTK is preferred by researchers for flexibility.  
- spaCy has a **newer, more active community**; NLTK is older with a less active community.  
- spaCy requires downloading language-specific models (e.g., `python -m spacy download en_core_web_sm`).  
- NLTK requires downloading additional data packages (e.g., `nltk.download('punkt')`) for tokenization.

#### 2. ‚úÇÔ∏è Tokenization in spaCy  
- Tokenization splits text into **sentences** (sentence tokenization) and **words/tokens** (word tokenization).  
- spaCy‚Äôs tokenizer handles **prefixes, suffixes, and exceptions** (e.g., contractions, currency symbols).  
- spaCy‚Äôs sentence tokenizer correctly handles abbreviations like "Dr." without splitting sentences incorrectly.  
- Each token in spaCy is an **object** with attributes like `.text`, `.is_alpha`, `.is_digit`, `.is_currency`, `.lemma_`, `.pos_`.

#### 3. üõ†Ô∏è Customizing Tokenization in spaCy  
- You can add **special cases** to spaCy‚Äôs tokenizer to split or treat specific words differently (e.g., splitting "gimme" into "gim" and "me").  
- spaCy does **not allow modifying the original text** during tokenization, only splitting tokens.

#### 4. üè≠ Language Processing Pipeline in spaCy  
- A spaCy **pipeline** is a sequence of components after tokenization, such as **tagger, parser, NER, lemmatizer, attribute ruler**.  
- `spacy.blank("en")` creates a **blank pipeline** with only a tokenizer.  
- `spacy.load("en_core_web_sm")` loads a **pre-trained pipeline** with all components.  
- Named Entity Recognition (NER) identifies entities like **persons, organizations, money** in text.  
- spaCy provides **displaCy** for visualizing named entities.  
- You can **customize pipelines** by adding or removing components, including adding components from pre-trained pipelines to blank ones.

#### 5. üîÑ Stemming and Lemmatization  
- **Stemming** uses fixed heuristic rules to reduce words to base forms (e.g., removing suffixes like -ing, -able).  
- Stemming can produce **non-words** (e.g., "ability" ‚Üí "abil").  
- **Lemmatization** uses linguistic knowledge to find the correct base form (lemma) of a word (e.g., "ate" ‚Üí "eat").  
- spaCy supports **lemmatization only**, not stemming.  
- NLTK supports **both stemming and lemmatization**.  
- Stemming is faster but less accurate; lemmatization is more accurate but computationally heavier.  
- spaCy‚Äôs lemmatizer can be **customized** using the attribute ruler to handle slang or domain-specific words (e.g., mapping "bro" and "brah" to "brother").

#### 6. ‚òÅÔ∏è Cloud NLP APIs (firstlanguage.in)  
- Cloud NLP platforms allow running NLP tasks via **HTTP API calls** without local compute resources.  
- They provide **SDKs** in Python and TypeScript and offer **free tiers** for testing.  
- Tasks include **text classification, sentiment analysis, entity recognition**, etc.



<br>

## Study Notes





### 1. ü§ñ spaCy vs NLTK: Understanding the Differences

When building Natural Language Processing (NLP) applications, you often use libraries like **spaCy** and **NLTK**. Both are popular and widely used, but they have important differences that affect how you use them and what they are best suited for.

#### What is spaCy and what is NLTK?

- **spaCy** is an **object-oriented** NLP library designed for building real-world applications quickly and efficiently. It provides pre-built models and pipelines that work well out of the box.
- **NLTK (Natural Language Toolkit)** is primarily a **string processing library** that offers a wide variety of algorithms and tools for NLP research and education. It is more flexible but requires more manual setup and tuning.

#### Key Differences Explained

- **Object-Oriented vs String Processing**  
  spaCy treats text as objects (like documents, tokens, spans), allowing you to interact with these objects and their properties directly. For example, you create an NLP object, then a document object, and then you can iterate over sentences or tokens as objects.  
  NLTK, on the other hand, works mostly with strings and lists of strings. You input a string and get back lists of tokens or sentences, but there is no object encapsulating the text with properties and methods.

- **Ease of Use and Efficiency**  
  spaCy provides the **most efficient algorithm for a task by default**. It‚Äôs like using a smartphone camera that automatically adjusts settings for a good picture.  
  NLTK is like a manual DSLR camera: it offers many options and algorithms, but you have to choose and tune them yourself.

- **Customization**  
  NLTK allows you to select from many tokenizers and algorithms, making it ideal for researchers who want to experiment with different approaches.  
  spaCy does not let you pick different algorithms for tokenization; it chooses the best one for you, which is great for developers who want reliable results quickly.

- **Community and Updates**  
  spaCy is newer with an active and growing community, frequently updated with modern NLP techniques.  
  NLTK is older, well-established, but its community is less active compared to spaCy.

#### Installation and Setup

- You install both libraries using pip:  
  ```bash
  pip install nltk
  pip install spacy
  python -m spacy download en_core_web_sm  # for English models in spaCy
  ```
- For NLTK, you may need to download additional data packages (like `punkt` for tokenization) using:  
  ```python
  import nltk
  nltk.download('punkt')
  ```

#### Example: Tokenization in spaCy vs NLTK

- **spaCy**:  
  You create an NLP object, then a document object from a string. You can iterate over sentences and tokens as objects:  
  ```python
  import spacy
  nlp = spacy.load("en_core_web_sm")
  doc = nlp("Doctor Strange loves pav bhaji of Mumbai. Hulk loves Delhi chat.")
  for sent in doc.sents:
      print(sent)
      for token in sent:
          print(token.text)
  ```
  spaCy intelligently handles abbreviations like "Dr." so it doesn‚Äôt split sentences incorrectly.

- **NLTK**:  
  You use string-based tokenizers:  
  ```python
  from nltk.tokenize import sent_tokenize, word_tokenize
  text = "Doctor Strange loves pav bhaji of Mumbai. Hulk loves Delhi chat."
  sentences = sent_tokenize(text)
  for sent in sentences:
      print(sent)
      words = word_tokenize(sent)
      print(words)
  ```
  NLTK‚Äôs default sentence tokenizer may incorrectly split "Dr." as a sentence end unless you customize it.


### 2. ‚úÇÔ∏è Tokenization in spaCy: Breaking Text into Meaningful Pieces

Tokenization is the process of splitting text into smaller units called tokens. These tokens can be sentences or words, depending on the level of tokenization.

#### Why is Tokenization Important?

- Tokenization is the **first step in almost every NLP pipeline**.  
- You can‚Äôt just split text by spaces or periods because natural language has exceptions (e.g., abbreviations like "Dr.", acronyms like "N.Y.", currency symbols, punctuation).  
- A good tokenizer understands language-specific rules and exceptions to split text correctly.

#### Sentence Tokenization

- Splitting a paragraph into sentences is called **sentence tokenization**.  
- spaCy uses a **sentence boundary detection** system that understands abbreviations and punctuation to avoid incorrect splits.

#### Word Tokenization

- Splitting sentences into words or tokens is called **word tokenization**.  
- spaCy‚Äôs tokenizer handles prefixes (like quotes, dollar signs), suffixes (like punctuation), and exceptions (like contractions "let's" ‚Üí "let" + "'s").

#### How spaCy Tokenizer Works

- spaCy first splits text by prefixes (e.g., quotes, brackets).  
- Then it handles exceptions (e.g., contractions).  
- Then it splits suffixes (e.g., punctuation marks).  
- This multi-step process ensures tokens are meaningful and useful for further analysis.

#### Example of Tokenization in spaCy

```python
import spacy
nlp = spacy.load("en_core_web_sm")
doc = nlp("Let's go to N.Y.!")
for token in doc:
    print(token.text)
```

Output tokens:  
`"`, `Let`, `'s`, `go`, `to`, `N.Y.`, `!`, `"`

This shows how spaCy splits contractions and punctuation correctly.

#### Token Attributes

Each token in spaCy is an object with many useful attributes and methods, such as:

- `.text` ‚Äî the original text of the token  
- `.is_alpha` ‚Äî is the token alphabetic?  
- `.is_digit` ‚Äî is the token a digit?  
- `.is_currency` ‚Äî is the token a currency symbol?  
- `.lemma_` ‚Äî the base form of the token (lemmatization)  
- `.pos_` ‚Äî part of speech tag (noun, verb, etc.)  

These attributes help in detailed text analysis.


### 3. üõ†Ô∏è Customizing Tokenization in spaCy

Sometimes, the default tokenizer behavior may not fit your needs, especially with slang, abbreviations, or domain-specific terms.

#### Adding Special Cases

- You can add **special rules** to the tokenizer to split or treat certain words differently.  
- For example, splitting "gimme" into two tokens "gim" and "me".

Example:

```python
from spacy.symbols import ORTH
special_case = [{ORTH: "gim"}, {ORTH: "me"}]
nlp.tokenizer.add_special_case("gimme", special_case)
doc = nlp("gimme the book")
print([token.text for token in doc])
### Output: ['gim', 'me', 'the', 'book']
```

Note: You cannot change the original text, only how it is tokenized.


### 4. üè≠ Language Processing Pipeline in spaCy

spaCy uses a **pipeline architecture** to process text step-by-step. The pipeline consists of components that perform different NLP tasks.

#### What is a Pipeline?

- A pipeline is a sequence of components that process the text after tokenization.  
- Components include:  
  - **Tagger**: assigns part-of-speech tags to tokens  
  - **Parser**: analyzes grammatical structure  
  - **NER (Named Entity Recognizer)**: identifies entities like people, organizations, dates, money  
  - **Lemmatizer**: finds base forms of words  
  - **Attribute Ruler**: customizes token attributes  

#### Blank Pipeline vs Pre-trained Pipeline

- `spacy.blank("en")` creates a **blank pipeline** with only a tokenizer. No other components are included.  
- `spacy.load("en_core_web_sm")` loads a **pre-trained pipeline** with all components ready to use.

#### Example: Using a Pre-trained Pipeline

```python
import spacy
nlp = spacy.load("en_core_web_sm")
doc = nlp("Tesla is going to acquire Twitter for 45 billion dollars.")
for token in doc:
    print(token.text, token.pos_, token.lemma_)
for ent in doc.ents:
    print(ent.text, ent.label_)
```

Output shows tokens with part-of-speech tags, lemmas, and recognized entities like "Tesla" (ORG) and "45 billion dollars" (MONEY).

#### Visualizing Entities

spaCy provides a visualization tool called **displaCy** to display entities in text:

```python
from spacy import displacy
displacy.render(doc, style="ent")
```

This renders entities with color-coded highlights.

#### Customizing Pipelines

- You can add or remove components from the pipeline.  
- For example, add only the NER component from a pre-trained pipeline to a blank pipeline.


### 5. üîÑ Stemming and Lemmatization: Reducing Words to Base Forms

In NLP, it‚Äôs often useful to reduce words to their **base or root form** to treat different forms of a word as the same.

#### What is Stemming?

- Stemming uses **simple, fixed rules** to chop off prefixes or suffixes.  
- It is a **dumb** process that does not consider the meaning or language rules.  
- Example:  
  - "talking" ‚Üí "talk" (remove -ing)  
  - "adjustable" ‚Üí "adjust" (remove -able)  
- Stemming can produce non-words like "abil" from "ability".

#### What is Lemmatization?

- Lemmatization uses **linguistic knowledge** and vocabulary to find the **correct base form (lemma)** of a word.  
- It considers the context and part of speech.  
- Example:  
  - "ate" ‚Üí "eat"  
  - "better" ‚Üí "good"  
  - "talking" ‚Üí "talk"  
- Lemmatization produces valid words.

#### Why Use Both?

- Stemming is faster and simpler but less accurate.  
- Lemmatization is more accurate but computationally heavier.  
- Depending on your application, you might choose one or both.

#### spaCy vs NLTK for Stemming and Lemmatization

- spaCy **does not support stemming** because it focuses on more accurate lemmatization.  
- NLTK supports both stemming and lemmatization.

#### Example: Stemming with NLTK

```python
from nltk.stem import PorterStemmer
stemmer = PorterStemmer()
words = ["eating", "ate", "adjustable", "ability"]
for word in words:
    print(word, "->", stemmer.stem(word))
```

Output:  
```
eating -> eat  
ate -> ate  
adjustable -> adjust  
ability -> abil
```

#### Example: Lemmatization with spaCy

```python
import spacy
nlp = spacy.load("en_core_web_sm")
doc = nlp("Eating ate adjustable ability better")
for token in doc:
    print(token.text, "->", token.lemma_)
```

Output:  
```
Eating -> eat  
ate -> eat  
adjustable -> adjustable  
ability -> ability  
better -> good
```

#### Customizing Lemmatization

You can customize spaCy‚Äôs lemmatizer using the **Attribute Ruler** to handle slang or domain-specific words:

```python
ruler = nlp.get_pipe("attribute_ruler")
patterns = [{"LOWER": "bro"}, {"LOWER": "brah"}]
attrs = {"LEMMA": "brother"}
ruler.add(patterns=patterns, attrs=attrs)
doc = nlp("bro brah")
for token in doc:
    print(token.text, token.lemma_)
### Output: bro -> brother, brah -> brother
```


### 6. ‚òÅÔ∏è Using Cloud NLP APIs (Sponsored: firstlanguage.in)

- Instead of running heavy NLP models locally, you can use cloud-based NLP APIs.  
- Platforms like **firstlanguage.in** provide easy-to-use APIs for tasks like text classification, sentiment analysis, entity recognition, etc.  
- You just send your text via HTTP requests and get results without worrying about compute resources.  
- They offer free tiers, SDKs in Python and TypeScript, and easy API key management.


### Summary

- **spaCy** is an object-oriented, efficient, and user-friendly NLP library with pre-trained pipelines ideal for app developers.  
- **NLTK** is a flexible, string-processing library with many algorithms, great for research and customization.  
- **Tokenization** is a crucial first step in NLP, and spaCy‚Äôs tokenizer handles language nuances better than simple string splits.  
- **Language processing pipelines** in spaCy consist of components like tagger, parser, NER, and lemmatizer that enrich text analysis.  
- **Stemming** is a rule-based, fast but crude method to get base words; **lemmatization** is linguistically informed and more accurate.  
- spaCy supports lemmatization but not stemming; NLTK supports both.  
- You can customize spaCy‚Äôs tokenizer and lemmatizer to handle special cases and slang.  
- Cloud NLP APIs offer scalable, easy NLP solutions without local compute needs.



<br>

## Questions



#### 1. What is a fundamental difference between spaCy and NLTK in terms of programming style?  
A) spaCy is object-oriented, while NLTK is primarily string processing  
B) NLTK uses object-oriented programming, spaCy uses functional programming  
C) spaCy treats text as objects with properties and methods, NLTK returns strings or lists  
D) Both spaCy and NLTK are purely string processing libraries  

#### 2. Which of the following best describes spaCy‚Äôs approach to tokenization algorithms?  
A) spaCy allows users to select from multiple tokenization algorithms  
B) spaCy automatically chooses the most efficient tokenization algorithm out of the box  
C) spaCy requires manual tuning of tokenization parameters for best results  
D) spaCy‚Äôs tokenizer handles language-specific exceptions like abbreviations by default  

#### 3. When using NLTK‚Äôs sentence tokenizer on the text "Dr. Strange loves pav bhaji.", what is a likely issue?  
A) It will correctly identify the entire sentence as one  
B) It may incorrectly split "Dr." as a separate sentence  
C) It always treats abbreviations as part of the sentence  
D) It requires additional customization or data downloads to improve accuracy  

#### 4. Which of the following statements about spaCy‚Äôs token objects is true?  
A) Each token has attributes like `.is_alpha`, `.is_digit`, and `.is_currency`  
B) Tokens in spaCy are simple strings without additional properties  
C) Token objects can be indexed like Python lists within a document object  
D) spaCy tokens cannot be sliced to create spans or sub-documents  

#### 5. What is the role of the spaCy pipeline component called "sentencizer"?  
A) It performs word tokenization  
B) It splits a paragraph into sentences in a blank pipeline  
C) It is automatically included in all spaCy pipelines by default  
D) It can be manually added to a blank pipeline to enable sentence boundary detection  

#### 6. Why might you want to customize spaCy‚Äôs tokenizer with special cases?  
A) To split slang or contractions into multiple tokens  
B) To modify the original text of tokens during tokenization  
C) To handle domain-specific abbreviations or non-standard words  
D) To improve tokenization speed by skipping exceptions  

#### 7. Which of the following are true about spaCy‚Äôs pre-trained language pipelines?  
A) They include components like tagger, parser, NER, and lemmatizer  
B) They are only available for English language  
C) They provide part-of-speech tagging and named entity recognition out of the box  
D) They can be loaded using `spacy.load()` with a model name like `en_core_web_sm`  

#### 8. What is the difference between stemming and lemmatization?  
A) Stemming uses fixed heuristic rules, lemmatization uses linguistic knowledge  
B) Lemmatization always produces valid dictionary words, stemming may not  
C) Stemming requires part-of-speech tagging, lemmatization does not  
D) Lemmatization can handle irregular word forms like "ate" ‚Üí "eat"  

#### 9. Why does spaCy not support stemming?  
A) Because stemming is less accurate and spaCy prefers lemmatization  
B) Because stemming requires manual algorithm selection which spaCy avoids  
C) Because stemming is computationally more expensive than lemmatization  
D) Because spaCy‚Äôs design philosophy focuses on linguistically informed processing  

#### 10. In spaCy, what does the `.lemma_` attribute of a token represent?  
A) The original text of the token  
B) The base or dictionary form of the token as a string  
C) A unique hash identifier for the token‚Äôs lemma  
D) The part-of-speech tag of the token  

#### 11. Which of the following statements about NLTK‚Äôs tokenization capabilities is correct?  
A) NLTK provides multiple tokenizers that can be selected for different use cases  
B) NLTK‚Äôs default tokenizers always outperform spaCy‚Äôs tokenizers in accuracy  
C) NLTK requires downloading additional data packages like `punkt` for sentence tokenization  
D) NLTK‚Äôs tokenizers are object-oriented and return token objects with attributes  

#### 12. How does spaCy handle tokens like currency symbols and punctuation during tokenization?  
A) It treats currency symbols as part of the adjacent word token  
B) It splits currency symbols and punctuation into separate tokens  
C) It ignores punctuation tokens by default  
D) It uses prefix and suffix rules to separate these tokens correctly  

#### 13. What is a "span" object in spaCy?  
A) A single token in a document  
B) A slice or contiguous subsequence of tokens within a document  
C) A special type of token representing named entities  
D) A pipeline component that processes sentences  

#### 14. When creating a blank spaCy pipeline with `spacy.blank("en")`, which of the following is true?  
A) The pipeline includes tokenizer, tagger, parser, and NER by default  
B) The pipeline only includes a tokenizer component initially  
C) Sentence boundary detection is enabled by default  
D) You can manually add components like sentencizer or NER to this pipeline  

#### 15. Which of the following best describes the relationship between spaCy‚Äôs pipeline components and the final `Doc` object?  
A) Pipeline components process the text sequentially and enrich the `Doc` with annotations  
B) The `Doc` object is created only after all pipeline components finish processing  
C) Pipeline components operate independently and do not affect the `Doc` object  
D) The `Doc` object contains tokens but no information about entities or part-of-speech  

#### 16. In the context of spaCy, what does the "attribute ruler" component do?  
A) It assigns or customizes token attributes like lemma or part-of-speech  
B) It controls the order of pipeline components  
C) It performs stemming on tokens  
D) It visualizes token attributes in the output  

#### 17. Which of the following are valid reasons to use cloud-based NLP APIs instead of local spaCy models?  
A) To avoid the need for high compute resources like GPUs locally  
B) To get access to pre-trained models without installation  
C) To customize tokenization and lemmatization rules extensively  
D) To quickly perform NLP tasks via HTTP calls without deep NLP knowledge  

#### 18. Consider the sentence: `"Let's go to N.Y.!"` Why is simple splitting by spaces or periods insufficient for tokenization?  
A) Because "Let's" is a contraction that should be split into two tokens  
B) Because "N.Y." is an abbreviation and should not be split into multiple sentences  
C) Because punctuation marks like exclamation points need to be separate tokens  
D) Because spaces always indicate token boundaries in English  

#### 19. Which of the following statements about NLTK‚Äôs flexibility compared to spaCy is true?  
A) NLTK allows manual selection of tokenization algorithms, spaCy does not  
B) NLTK is better suited for production applications due to its simplicity  
C) spaCy is more customizable than NLTK in terms of tokenization algorithms  
D) NLTK requires more manual tuning to achieve high accuracy in tokenization  

#### 20. When customizing spaCy‚Äôs tokenizer to split "gimme" into two tokens, which of the following is true?  
A) You can modify the original text of the token during tokenization  
B) You can only split the token into multiple tokens without changing the text  
C) Special cases can be added using the `ORTH` symbol to define token boundaries  
D) This customization affects the underlying text stored in the `Doc` object  



<br>

## Answers



#### 1. What is a fundamental difference between spaCy and NLTK in terms of programming style?  
A) ‚úì spaCy is object-oriented, allowing text to be treated as objects with properties and methods.  
B) ‚úó NLTK is not primarily object-oriented; it mainly processes strings and lists.  
C) ‚úì spaCy treats text as objects; NLTK returns strings or lists without encapsulating text as objects.  
D) ‚úó Both are not purely string processing; spaCy is object-oriented.

**Correct:** A, C


#### 2. Which of the following best describes spaCy‚Äôs approach to tokenization algorithms?  
A) ‚úó spaCy does not allow user selection of tokenization algorithms.  
B) ‚úì spaCy automatically selects the most efficient tokenization algorithm out of the box.  
C) ‚úó spaCy does not require manual tuning for tokenization.  
D) ‚úì spaCy‚Äôs tokenizer handles language-specific exceptions like abbreviations by default.

**Correct:** B, D


#### 3. When using NLTK‚Äôs sentence tokenizer on the text "Dr. Strange loves pav bhaji.", what is a likely issue?  
A) ‚úó NLTK‚Äôs default tokenizer may incorrectly split "Dr." as a sentence end.  
B) ‚úì It may incorrectly split "Dr." as a separate sentence.  
C) ‚úó NLTK does not always treat abbreviations correctly without customization.  
D) ‚úì Additional data downloads or customization (like `punkt`) are often needed for better accuracy.

**Correct:** B, D


#### 4. Which of the following statements about spaCy‚Äôs token objects is true?  
A) ‚úì Tokens have attributes like `.is_alpha`, `.is_digit`, `.is_currency`.  
B) ‚úó Tokens are not simple strings; they are objects with properties.  
C) ‚úì Tokens can be indexed like Python lists within a document object.  
D) ‚úó Tokens can be sliced to create spans (sub-documents).

**Correct:** A, C


#### 5. What is the role of the spaCy pipeline component called "sentencizer"?  
A) ‚úó Sentencizer does not perform word tokenization.  
B) ‚úì It splits paragraphs into sentences in a blank pipeline.  
C) ‚úó It is not automatically included in all pipelines; must be added manually in blank pipelines.  
D) ‚úì It can be manually added to enable sentence boundary detection.

**Correct:** B, D


#### 6. Why might you want to customize spaCy‚Äôs tokenizer with special cases?  
A) ‚úì To split slang or contractions into multiple tokens.  
B) ‚úó You cannot modify the original text during tokenization, only split tokens.  
C) ‚úì To handle domain-specific abbreviations or non-standard words.  
D) ‚úó Customization is not primarily for speed improvements.

**Correct:** A, C


#### 7. Which of the following are true about spaCy‚Äôs pre-trained language pipelines?  
A) ‚úì They include components like tagger, parser, NER, and lemmatizer.  
B) ‚úó They are available for many languages, not only English.  
C) ‚úì They provide POS tagging and named entity recognition out of the box.  
D) ‚úì They can be loaded using `spacy.load()` with model names like `en_core_web_sm`.

**Correct:** A, C, D


#### 8. What is the difference between stemming and lemmatization?  
A) ‚úì Stemming uses fixed heuristic rules; lemmatization uses linguistic knowledge.  
B) ‚úì Lemmatization produces valid dictionary words; stemming may produce non-words.  
C) ‚úó Stemming does not require POS tagging; lemmatization often does.  
D) ‚úì Lemmatization can handle irregular forms like "ate" ‚Üí "eat".

**Correct:** A, B, D


#### 9. Why does spaCy not support stemming?  
A) ‚úì Because stemming is less accurate and spaCy prefers lemmatization.  
B) ‚úì Because stemming requires manual algorithm selection, which spaCy avoids.  
C) ‚úó Stemming is not more computationally expensive than lemmatization.  
D) ‚úì spaCy focuses on linguistically informed processing rather than heuristic rules.

**Correct:** A, B, D


#### 10. In spaCy, what does the `.lemma_` attribute of a token represent?  
A) ‚úó `.lemma_` is not the original text.  
B) ‚úì It is the base or dictionary form of the token as a string.  
C) ‚úó The unique hash is `.lemma` (without underscore), not `.lemma_`.  
D) ‚úó Part-of-speech tag is `.pos_`, not `.lemma_`.

**Correct:** B


#### 11. Which of the following statements about NLTK‚Äôs tokenization capabilities is correct?  
A) ‚úì NLTK provides multiple tokenizers selectable for different use cases.  
B) ‚úó NLTK‚Äôs default tokenizers do not always outperform spaCy‚Äôs in accuracy.  
C) ‚úì NLTK requires downloading additional data packages like `punkt` for sentence tokenization.  
D) ‚úó NLTK tokenizers return strings or lists, not token objects with attributes.

**Correct:** A, C


#### 12. How does spaCy handle tokens like currency symbols and punctuation during tokenization?  
A) ‚úó Currency symbols are not treated as part of adjacent words.  
B) ‚úì Currency symbols and punctuation are split into separate tokens.  
C) ‚úó Punctuation tokens are not ignored by default; they are tokenized.  
D) ‚úì Prefix and suffix rules are used to separate these tokens correctly.

**Correct:** B, D


#### 13. What is a "span" object in spaCy?  
A) ‚úó A span is not a single token.  
B) ‚úì A span is a slice or contiguous subsequence of tokens within a document.  
C) ‚úó Spans are not special tokens but groups of tokens.  
D) ‚úó Span is not a pipeline component.

**Correct:** B


#### 14. When creating a blank spaCy pipeline with `spacy.blank("en")`, which of the following is true?  
A) ‚úó Blank pipeline does not include tagger, parser, or NER by default.  
B) ‚úì It only includes a tokenizer component initially.  
C) ‚úó Sentence boundary detection is not enabled by default.  
D) ‚úì You can manually add components like sentencizer or NER.

**Correct:** B, D


#### 15. Which of the following best describes the relationship between spaCy‚Äôs pipeline components and the final `Doc` object?  
A) ‚úì Pipeline components process text sequentially and enrich the `Doc` with annotations.  
B) ‚úó The `Doc` object is created immediately after tokenization, before all components finish.  
C) ‚úó Pipeline components affect the `Doc` object; they are not independent.  
D) ‚úó The `Doc` contains tokens and annotations like entities and POS if pipeline components run.

**Correct:** A


#### 16. In the context of spaCy, what does the "attribute ruler" component do?  
A) ‚úì It assigns or customizes token attributes like lemma or POS.  
B) ‚úó It does not control pipeline order.  
C) ‚úó It does not perform stemming.  
D) ‚úó It does not visualize token attributes.

**Correct:** A


#### 17. Which of the following are valid reasons to use cloud-based NLP APIs instead of local spaCy models?  
A) ‚úì To avoid needing high compute resources like GPUs locally.  
B) ‚úì To access pre-trained models without local installation.  
C) ‚úó Cloud APIs usually do not allow deep customization of tokenization or lemmatization.  
D) ‚úì To quickly perform NLP tasks via HTTP calls without deep NLP knowledge.

**Correct:** A, B, D


#### 18. Consider the sentence: `"Let's go to N.Y.!"` Why is simple splitting by spaces or periods insufficient for tokenization?  
A) ‚úì "Let's" is a contraction that should be split into two tokens.  
B) ‚úì "N.Y." is an abbreviation and should not be split into multiple sentences.  
C) ‚úì Punctuation marks like exclamation points need to be separate tokens.  
D) ‚úó Spaces do not always indicate token boundaries (e.g., contractions).

**Correct:** A, B, C


#### 19. Which of the following statements about NLTK‚Äôs flexibility compared to spaCy is true?  
A) ‚úì NLTK allows manual selection of tokenization algorithms; spaCy does not.  
B) ‚úó NLTK is not necessarily better suited for production due to complexity.  
C) ‚úó spaCy is less customizable in tokenization algorithms than NLTK.  
D) ‚úì NLTK requires more manual tuning to achieve high accuracy.

**Correct:** A, D


#### 20. When customizing spaCy‚Äôs tokenizer to split "gimme" into two tokens, which of the following is true?  
A) ‚úó You cannot modify the original text during tokenization.  
B) ‚úì You can split the token into multiple tokens without changing the text.  
C) ‚úì Special cases can be added using the `ORTH` symbol to define token boundaries.  
D) ‚úó This customization does not change the underlying text stored in the `Doc`.

**Correct:** B, C

