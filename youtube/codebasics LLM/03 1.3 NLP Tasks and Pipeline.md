## 1.3 NLP Tasks and Pipeline

[Study Notes](#study-notes)

[Questions](#questions)



### Key Points

#### 1. üß∞ NLP Techniques Categories  
- There are three broad categories of NLP techniques: Rules and Heuristics, Machine Learning, and Deep Learning.  
- Rules and Heuristics use manual patterns like regular expressions without ML or DL.  
- Machine Learning converts text to numeric vectors (e.g., Count Vectorizer, TF-IDF) and uses classifiers like Naive Bayes.  
- Deep Learning uses embeddings (e.g., BERT) to represent sentences as vectors capturing semantic similarity measured by cosine similarity.

#### 2. üìù NLP Tasks  
- Text Classification categorizes text into classes (e.g., spam detection, ticket priority, hate speech detection).  
- Text Similarity measures how close two texts are using sentence embeddings and cosine similarity.  
- Information Extraction pulls specific data (e.g., flight details from emails) using regex and NLP techniques.  
- Information Retrieval finds relevant documents based on queries (e.g., Google Search).  
- Chatbots come in three types: FAQ (fixed answers), Flow-based (context-aware), and Open-ended (general conversation).  
- Machine Translation uses encoder-decoder RNN or transformer models to translate languages.  
- Language Modeling predicts next words in a sentence for autocomplete.  
- Text Summarization condenses long text into short summaries.  
- Topic Modeling extracts abstract topics from large document collections.  
- Voice Assistants understand and respond to spoken language.

#### 3. üõ†Ô∏è NLP Pipeline Steps  
- Data Acquisition involves collecting labeled or unlabeled data from databases, cloud storage, public datasets, or web scraping.  
- Text Extraction & Cleanup removes irrelevant fields, fixes spelling, and merges text fields.  
- Preprocessing includes sentence segmentation, word tokenization, stemming (rule-based root word extraction), and lemmatization (grammar-based root word extraction).  
- Feature Engineering converts words into numeric vectors using techniques like TF-IDF, one-hot encoding, or embeddings.  
- Model Building trains classifiers (Naive Bayes, SVM, Random Forest) on numeric features and tunes hyperparameters with Grid Search CV.  
- Model Evaluation uses confusion matrix, accuracy, precision, recall, and F1 score to measure performance.  
- Deployment involves exporting the model, creating REST APIs (FastAPI/Flask), and hosting on cloud platforms (AWS, Azure, Google Cloud).  
- Monitoring tracks model performance in production and manages concept drift by retraining with new data.

#### 4. üîë Important Concepts and Tools  
- Regular expressions are effective for rule-based information extraction.  
- Count Vectorizer counts word occurrences; TF-IDF weighs words by importance in documents.  
- Naive Bayes is a statistical classifier commonly used for text classification.  
- Sentence embeddings (e.g., from BERT) represent sentences as vectors for similarity tasks.  
- Cosine similarity measures the angle between two vectors to quantify similarity.  
- Confusion matrix compares predicted vs. true labels to identify correct and incorrect classifications.  
- Grid Search CV helps find the best model hyperparameters through systematic search.  
- Libraries like NLTK and spaCy provide tokenization and preprocessing tools.  
- Public datasets and data augmentation help when labeled data is scarce.

<br>

## Study Notes





### 1. üß∞ Three Broad Categories of NLP Techniques

When solving any Natural Language Processing (NLP) problem, there are **three main categories of techniques** that are commonly used. Understanding these categories helps you choose the right approach depending on the problem and data you have.

#### 1.1 Rules and Heuristics (Non-ML Approach)

This is the simplest approach where you use **manual rules and patterns** to extract or classify information. For example, using **regular expressions** (regex) to find specific patterns in text like flight confirmation numbers or dates in an email. This approach does **not require machine learning or deep learning** ‚Äî it‚Äôs purely based on handcrafted rules.

- Example: Gmail detecting flight details in an email by looking for keywords like "booking ref" followed by a number.
- Pros: Simple, fast, and very accurate for well-defined patterns.
- Cons: Not flexible, hard to scale, and fails when text varies a lot.

#### 1.2 Machine Learning (Statistical Approach)

Machine learning uses **statistical models** to learn patterns from labeled data. The text is first converted into numbers (because ML models understand numbers, not raw text), and then a classifier like **Naive Bayes** is trained to categorize the text.

- Example: Spam detection in emails. Words like "urgent", "business assistance", or "usd 55 million" might indicate spam.
- How it works: Convert text into vectors using techniques like **Count Vectorizer** (counting word occurrences) or **TF-IDF** (which weighs words by importance). Then feed these vectors into a classifier.
- Pros: More flexible than rules, can handle varied text.
- Cons: Requires labeled data, struggles with unseen words or phrases.

#### 1.3 Deep Learning (Advanced Representation)

Deep learning uses neural networks to create **dense vector representations** of words or sentences called **embeddings**. These embeddings capture semantic meaning, so similar sentences have similar vectors.

- Example: Using **BERT** (a transformer-based model by Google) to generate sentence embeddings that can understand the similarity between sentences like "hurry up for an offer to win cash" and "rush for this great deal to win money".
- How it works: The model converts sentences into vectors in a high-dimensional space where similar meanings are close together (measured by **cosine similarity**).
- Pros: Handles unseen text better, captures context and meaning.
- Cons: Requires more data and computational power.


### 2. üìù Common NLP Tasks

NLP is a broad field with many different tasks. Here are some of the most important ones, explained with real-world examples.

#### 2.1 Text Classification

This is about **categorizing text into predefined classes**. For example, classifying support tickets into "high", "medium", or "low" priority.

- Use case: Camtasia software support tickets. If a ticket says "Camtasia 10 won't import mp4 file", it‚Äôs high priority.
- How it works: Convert text into vectors (e.g., TF-IDF), then use a classifier like Naive Bayes or Logistic Regression.
- Other examples: Spam detection, hate speech detection on Facebook, fake profile detection on LinkedIn, healthcare document classification (prescription vs. patient record).

#### 2.2 Text Similarity

This task measures **how similar two pieces of text are**.

- Use case: Matching resumes to job descriptions. Recruiters get many resumes and want to find the best matches quickly.
- How it works: Use sentence embeddings (e.g., from BERT or Hugging Face sentence transformers) to convert both texts into vectors, then calculate cosine similarity.
- The higher the similarity score, the better the match.

#### 2.3 Information Extraction

Extracting **specific pieces of information** from unstructured text.

- Use case: Gmail extracting flight details from emails (flight time, destination, confirmation number).
- Techniques: Regular expressions, Named Entity Recognition (NER), part-of-speech tagging.
- Example: Google News extracting trending topics like "Sarah Palin" or "Israel" from news articles.

#### 2.4 Information Retrieval

Finding **relevant documents or web pages** based on a query.

- Use case: Google Search returning websites relevant to "Vada Paav places near me".
- How it works: Index documents, score relevance (e.g., using TF-IDF), and rank results.
- Different from information extraction because it deals with retrieving whole documents, not extracting specific info.

#### 2.5 Chatbots

Automated systems that **interact with users via text**.

- Types:
  - FAQ bots: Fixed answers to fixed questions.
  - Flow-based bots: Maintain context and guide conversation flow (e.g., ordering pizza).
  - Open-ended bots: General chit-chat like Siri or Alexa.

#### 2.6 Machine Translation

Automatically **translating text from one language to another**.

- Example: Google Translate.
- How it works: Uses encoder-decoder architectures with recurrent neural networks (RNNs) or transformers.

#### 2.7 Language Modeling

Predicting the **next word or sequence of words** in a sentence.

- Use case: Gmail autocomplete suggestions.
- Types: Statistical models (based on word frequencies) and neural models (deep learning).

#### 2.8 Text Summarization

Creating a **short summary** of a longer text.

- Use case: Summarizing news articles into a few sentences or a headline.
- Helps save time when reading many documents.

#### 2.9 Topic Modeling

Discovering **abstract topics** from a large collection of documents.

- Use case: Extracting topics like "clinical trial" or "drug placebo" from pharmaceutical documents.
- Helps organize and understand large text corpora.

#### 2.10 Voice Assistants

Systems like Siri, Alexa, or Google Assistant that **understand spoken language and respond**.


### 3. üõ†Ô∏è The NLP Pipeline: Building a Real-World Application

Building an NLP application involves multiple steps combined into a **pipeline**. Let‚Äôs walk through a practical example: **Camtasia support ticket classification**.

#### 3.1 Data Acquisition

First, you need data. For Camtasia, this means collecting past support tickets stored in a database (e.g., MongoDB).

- Sometimes data is moved to cloud storage (like Amazon S3) for easier access.
- If labeled data (tickets tagged as high/medium/low priority) is not available, you may need human annotators or use public datasets.
- Other data collection methods: web scraping, product instrumentation, data augmentation.

#### 3.2 Text Extraction and Cleanup

Raw data often contains irrelevant or noisy information.

- Remove unnecessary fields like creator name or timestamp if they don‚Äôt affect priority.
- Merge title and description into one text field.
- Fix spelling mistakes and remove extra line breaks.
- This step is called **text extraction and cleanup**.

#### 3.3 Preprocessing

Prepare the text for modeling by breaking it down and normalizing it.

- **Sentence segmentation**: Split text into sentences. This is tricky because of abbreviations like "Dr." or "etc." which contain periods but don‚Äôt end sentences.
- **Word tokenization**: Split sentences into individual words or tokens.
- **Stemming**: Reduce words to their root form by chopping off endings (e.g., "eating" ‚Üí "eat").
- **Lemmatization**: More advanced than stemming; uses grammar rules to find the base form (e.g., "ate" ‚Üí "eat").
- Preprocessing helps reduce complexity and improves model performance.

#### 3.4 Feature Engineering

Convert words into numbers that machine learning models can understand.

- Techniques include **TF-IDF vectorizer**, **one-hot encoding**, and **word embeddings**.
- The goal is to represent words so that similar words have similar numeric representations.
- This step is crucial because ML models only work with numbers.

#### 3.5 Model Building

Train a machine learning model to classify tickets.

- Common classifiers: Naive Bayes, Support Vector Machine (SVM), Random Forest, Logistic Regression.
- Use labeled historical data to train the model.
- Tune hyperparameters using techniques like **Grid Search CV** to find the best model settings.
- Evaluate model performance using metrics like **accuracy**, **precision**, **recall**, **F1 score**, and **confusion matrix**.
  - Confusion matrix shows how many predictions were correct or wrong for each class.

#### 3.6 Deployment and Monitoring

Once the model is trained and tested:

- Export the model and deploy it as a web service (using FastAPI, Flask, etc.) on cloud platforms like AWS, Azure, or Google Cloud.
- Integrate with existing systems (e.g., Sentry for alerts, Jira for ticket tracking).
- Monitor model performance in production to detect issues like **concept drift** (when data changes over time).
- Update the model periodically with new data to maintain accuracy.


### Summary

- NLP problems can be solved using **rules/heuristics**, **machine learning**, or **deep learning**.
- Common NLP tasks include **text classification**, **text similarity**, **information extraction**, **information retrieval**, **chatbots**, **machine translation**, **language modeling**, **text summarization**, **topic modeling**, and **voice assistants**.
- Building an NLP application involves a pipeline: **data acquisition ‚Üí text cleanup ‚Üí preprocessing ‚Üí feature engineering ‚Üí model building ‚Üí deployment ‚Üí monitoring**.
- Each step is important and requires careful attention to detail.
- Many advanced techniques and tools (like BERT, TF-IDF, sentence transformers) exist to improve NLP applications.
- Continuous learning and iteration are key to building effective NLP systems.


If you want to dive deeper into any of these topics, there are many tutorials and resources available, including the **Codebasics YouTube playlists** and the book *Practical Natural Language Processing* by Anuj Gupta.



<br>

## Questions



#### 1. Which of the following are true about the Rules and Heuristics approach in NLP?  
A) It relies on handcrafted patterns like regular expressions.  
B) It requires large labeled datasets for training.  
C) It does not involve machine learning or deep learning.  
D) It is highly flexible and adapts well to varied text inputs.  

#### 2. In the context of text classification, why is converting text into numerical vectors necessary?  
A) Because machine learning models can only process numerical data.  
B) To reduce the size of the dataset for faster processing.  
C) To capture semantic meaning of words directly.  
D) To enable statistical analysis and pattern recognition by models.  

#### 3. What is a key limitation of using simple Count Vectorizer for text representation in machine learning?  
A) It cannot handle unseen words during prediction.  
B) It captures the semantic meaning of words effectively.  
C) It ignores the order of words in a sentence.  
D) It requires deep learning models to function.  

#### 4. Which of the following statements about sentence embeddings generated by models like BERT are correct?  
A) Sentence embeddings map semantically similar sentences to similar vectors.  
B) They rely on handcrafted rules and regular expressions.  
C) Cosine similarity is used to measure closeness between sentence vectors.  
D) They cannot be used for tasks like spam detection or text similarity.  

#### 5. In the NLP pipeline, what is the primary purpose of sentence segmentation?  
A) To convert text into numerical vectors.  
B) To split a large text into meaningful sentences.  
C) To remove stop words and punctuation.  
D) To identify the language of the text.  

#### 6. Why is lemmatization generally preferred over stemming in NLP preprocessing?  
A) Lemmatization uses grammar and vocabulary knowledge to find the base form.  
B) Stemming always produces grammatically correct base words.  
C) Lemmatization can handle irregular word forms like "ate" ‚Üí "eat".  
D) Stemming is computationally more expensive than lemmatization.  

#### 7. Which of the following are typical steps involved in building an NLP model for support ticket classification?  
A) Data acquisition from databases or cloud storage.  
B) Manual tagging of tickets if labels are unavailable.  
C) Directly deploying the raw text into a deep learning model without preprocessing.  
D) Feature engineering to convert text into numerical features.  

#### 8. What is the role of a confusion matrix in evaluating an NLP classification model?  
A) To visualize the number of correct and incorrect predictions per class.  
B) To measure the similarity between two sentences.  
C) To tune hyperparameters of the model automatically.  
D) To identify which classes the model confuses most often.  

#### 9. Which of the following best describe the difference between information extraction and information retrieval?  
A) Information extraction pulls specific data points from text.  
B) Information retrieval returns entire documents relevant to a query.  
C) Both tasks always use the same algorithms and techniques.  
D) Information retrieval is primarily used in search engines like Google.  

#### 10. In the context of chatbots, what distinguishes a flow-based bot from an FAQ bot?  
A) Flow-based bots maintain context across multiple turns in a conversation.  
B) FAQ bots provide fixed answers without considering previous dialogue.  
C) Flow-based bots cannot handle open-ended conversations.  
D) FAQ bots use deep learning to generate responses dynamically.  

#### 11. Which of the following are challenges in sentence segmentation?  
A) Handling abbreviations like "Dr." or "etc." that contain periods.  
B) Splitting sentences only at every period without exceptions.  
C) Accounting for language-specific grammar rules.  
D) Ignoring punctuation marks like question marks or exclamation points.  

#### 12. Why might a machine learning model trained on historical support tickets fail when deployed in production?  
A) Concept drift, where language or terminology changes over time.  
B) The model was trained on too much data.  
C) Lack of monitoring and updating after deployment.  
D) The model was built using deep learning instead of rules.  

#### 13. Which of the following are true about TF-IDF vectorization?  
A) It weighs words based on their frequency in a document and rarity across documents.  
B) It is a method to convert text into vectors for machine learning.  
C) It captures the semantic meaning of words better than word embeddings.  
D) It can be used in both information retrieval and text classification tasks.  

#### 14. What is the main advantage of using deep learning-based sentence embeddings over traditional count-based vectorizers?  
A) They can capture context and semantic similarity between words and sentences.  
B) They require no training data at all.  
C) They produce sparse vectors with many zeros.  
D) They handle unseen words and phrases better during prediction.  

#### 15. Which of the following are common machine learning classifiers used in NLP tasks?  
A) Naive Bayes  
B) Support Vector Machine (SVM)  
C) Random Forest  
D) Regular Expressions  

#### 16. What is the purpose of feature engineering in the NLP pipeline?  
A) To extract meaningful numerical representations from text data.  
B) To manually label the dataset for supervised learning.  
C) To convert raw text into tokens and sentences.  
D) To improve model performance by selecting or creating relevant features.  

#### 17. Which of the following statements about language modeling are correct?  
A) Language models predict the probability of the next word in a sequence.  
B) They are used in autocomplete features like Gmail suggestions.  
C) Language models only use statistical methods, never neural networks.  
D) Both statistical and neural language models exist.  

#### 18. In topic modeling, what is the main goal?  
A) To extract abstract topics from a large collection of documents.  
B) To classify documents into predefined categories.  
C) To summarize documents into a few sentences.  
D) To detect hate speech in social media posts.  

#### 19. Which of the following are true about deploying an NLP model in production?  
A) The model should be wrapped in a REST API for easy access.  
B) Monitoring is unnecessary once the model is deployed.  
C) Cloud platforms like AWS, Azure, or Google Cloud can host the model.  
D) Periodic retraining may be required to handle new data or concept drift.  

#### 20. Why is it important to discard irrelevant information like creator name or timestamp when preprocessing support tickets?  
A) Because such information does not influence the priority classification.  
B) To reduce noise and improve model accuracy.  
C) Because machine learning models cannot process non-textual data.  
D) To simplify the input data and focus on meaningful text content.  



<br>

## Answers



#### 1. Which of the following are true about the Rules and Heuristics approach in NLP?  
A) ‚úì Relies on handcrafted patterns like regex, no ML involved.  
B) ‚úó Does not require large labeled datasets; it‚Äôs rule-based.  
C) ‚úì No machine learning or deep learning involved.  
D) ‚úó Not flexible; struggles with varied or unexpected text.  

**Correct:** A, C


#### 2. In the context of text classification, why is converting text into numerical vectors necessary?  
A) ‚úì ML models require numerical input, not raw text.  
B) ‚úó Vectorization is not primarily for reducing dataset size.  
C) ‚úó Count vectors do not capture semantic meaning directly.  
D) ‚úì Enables statistical pattern recognition by ML models.  

**Correct:** A, D


#### 3. What is a key limitation of using simple Count Vectorizer for text representation in machine learning?  
A) ‚úì Cannot handle unseen words well during prediction.  
B) ‚úó Does not capture semantic meaning; only counts words.  
C) ‚úì Ignores word order, treats text as a bag of words.  
D) ‚úó Does not require deep learning to function.  

**Correct:** A, C


#### 4. Which of the following statements about sentence embeddings generated by models like BERT are correct?  
A) ‚úì Embeddings map semantically similar sentences to similar vectors.  
B) ‚úó They are learned representations, not handcrafted rules.  
C) ‚úì Cosine similarity measures closeness between vectors.  
D) ‚úó They are widely used for spam detection and similarity tasks.  

**Correct:** A, C


#### 5. In the NLP pipeline, what is the primary purpose of sentence segmentation?  
A) ‚úó Vectorization happens after segmentation.  
B) ‚úì Splits large text into meaningful sentences.  
C) ‚úó Removing stop words is a different step.  
D) ‚úó Language identification is separate from segmentation.  

**Correct:** B


#### 6. Why is lemmatization generally preferred over stemming in NLP preprocessing?  
A) ‚úì Uses grammar and vocabulary to find correct base forms.  
B) ‚úó Stemming often produces incorrect or incomplete roots.  
C) ‚úì Handles irregular forms like "ate" ‚Üí "eat".  
D) ‚úó Lemmatization is usually more computationally expensive.  

**Correct:** A, C


#### 7. Which of the following are typical steps involved in building an NLP model for support ticket classification?  
A) ‚úì Data acquisition is the first step.  
B) ‚úì Manual tagging if labels are missing.  
C) ‚úó Raw text must be preprocessed before modeling.  
D) ‚úì Feature engineering converts text to numbers.  

**Correct:** A, B, D


#### 8. What is the role of a confusion matrix in evaluating an NLP classification model?  
A) ‚úì Visualizes correct and incorrect predictions per class.  
B) ‚úó It does not measure sentence similarity.  
C) ‚úó Hyperparameter tuning is a separate process.  
D) ‚úì Identifies which classes are confused by the model.  

**Correct:** A, D


#### 9. Which of the following best describe the difference between information extraction and information retrieval?  
A) ‚úì Extraction pulls specific data points from text.  
B) ‚úì Retrieval returns entire relevant documents.  
C) ‚úó They use different algorithms and techniques.  
D) ‚úì Retrieval is used in search engines like Google.  

**Correct:** A, B, D


#### 10. In the context of chatbots, what distinguishes a flow-based bot from an FAQ bot?  
A) ‚úì Flow-based bots maintain context across turns.  
B) ‚úì FAQ bots provide fixed answers without context.  
C) ‚úó Flow-based bots can handle some open-ended dialogue.  
D) ‚úó FAQ bots typically do not use deep learning for dynamic responses.  

**Correct:** A, B


#### 11. Which of the following are challenges in sentence segmentation?  
A) ‚úì Abbreviations with periods can confuse simple split rules.  
B) ‚úó Splitting at every period without exceptions causes errors.  
C) ‚úì Grammar rules must be considered for accurate splitting.  
D) ‚úó Punctuation like question marks are important sentence boundaries.  

**Correct:** A, C


#### 12. Why might a machine learning model trained on historical support tickets fail when deployed in production?  
A) ‚úì Concept drift changes language or terminology over time.  
B) ‚úó Too much training data rarely causes failure.  
C) ‚úì Lack of monitoring means issues go unnoticed.  
D) ‚úó Model type (deep learning vs rules) is not the main cause.  

**Correct:** A, C


#### 13. Which of the following are true about TF-IDF vectorization?  
A) ‚úì Weighs words by frequency in document and rarity across corpus.  
B) ‚úì Converts text into vectors for ML models.  
C) ‚úó Does not capture semantic meaning as well as embeddings.  
D) ‚úì Used in both information retrieval and classification.  

**Correct:** A, B, D


#### 14. What is the main advantage of using deep learning-based sentence embeddings over traditional count-based vectorizers?  
A) ‚úì Capture context and semantic similarity effectively.  
B) ‚úó They do require training data (pretrained or fine-tuned).  
C) ‚úó Produce dense, not sparse, vectors.  
D) ‚úì Handle unseen words and phrases better during prediction.  

**Correct:** A, D


#### 15. Which of the following are common machine learning classifiers used in NLP tasks?  
A) ‚úì Naive Bayes is widely used for text classification.  
B) ‚úì SVM is a popular classifier for NLP.  
C) ‚úì Random Forest can be used for classification.  
D) ‚úó Regular expressions are not classifiers but rule-based tools.  

**Correct:** A, B, C


#### 16. What is the purpose of feature engineering in the NLP pipeline?  
A) ‚úì Extract meaningful numerical features from text.  
B) ‚úó Labeling is a separate data preparation step.  
C) ‚úó Tokenization is preprocessing, not feature engineering.  
D) ‚úì Improves model performance by selecting relevant features.  

**Correct:** A, D


#### 17. Which of the following statements about language modeling are correct?  
A) ‚úì Predicts probability of next word(s) in a sequence.  
B) ‚úì Used in autocomplete features like Gmail.  
C) ‚úó Neural language models are common today.  
D) ‚úì Both statistical and neural models exist.  

**Correct:** A, B, D


#### 18. In topic modeling, what is the main goal?  
A) ‚úì Extract abstract topics from large document collections.  
B) ‚úó Classification assigns documents to predefined categories.  
C) ‚úó Summarization condenses text, not topics.  
D) ‚úó Hate speech detection is a classification task.  

**Correct:** A


#### 19. Which of the following are true about deploying an NLP model in production?  
A) ‚úì Wrapping the model in a REST API enables easy access.  
B) ‚úó Monitoring is essential to detect performance issues.  
C) ‚úì Cloud platforms are commonly used for hosting models.  
D) ‚úì Periodic retraining handles new data and concept drift.  

**Correct:** A, C, D


#### 20. Why is it important to discard irrelevant information like creator name or timestamp when preprocessing support tickets?  
A) ‚úì Such info usually does not affect priority classification.  
B) ‚úì Removing noise improves model accuracy.  
C) ‚úó ML models can process non-textual data if encoded properly.  
D) ‚úì Simplifies input and focuses on meaningful text content.  

**Correct:** A, B, D

