## 1.2 Custom Loss Functions

[Study Notes](#study-notes)

[Questions](#questions)



### Key Points

#### 1. üß© Loss Functions in TensorFlow/Keras  
- Loss functions measure the difference between true labels and model predictions.  
- Built-in loss functions can be specified by name (e.g., `'mse'`) or imported and passed as functions (e.g., `mean_squared_error`).  
- Loss functions are passed to `model.compile(loss=..., optimizer=...)` to guide training.

#### 2. üõ†Ô∏è Custom Loss Functions  
- Custom loss functions are Python functions taking `y_true` and `y_pred` as inputs and returning a loss tensor.  
- Custom loss functions can be passed directly to `model.compile(loss=custom_loss_function)`.  
- Custom loss functions can be parameterized using closures to accept arguments.

#### 3. üîç Huber Loss  
- Huber loss uses a threshold to switch between squared error (for small errors) and linear error (for large errors).  
- Formula:  
  - If |error| ‚â§ threshold: loss = 0.5 * error¬≤  
  - Else: loss = threshold * (|error| - 0.5 * threshold)  
- Implemented using `tf.where` to select between small and big error losses.  
- Can be implemented as a function or as a subclass of `tf.keras.losses.Loss`.

#### 4. ‚öôÔ∏è Parameterized Custom Loss Functions  
- Parameterized loss functions use closures to fix parameters like threshold before training.  
- Example: `my_huber_loss_with_threshold(threshold)` returns a loss function with that threshold fixed.

#### 5. üßë‚Äçüè´ Object-Oriented Custom Loss Functions  
- Custom loss functions can be implemented by subclassing `tf.keras.losses.Loss`.  
- The `call(self, y_true, y_pred)` method defines the loss calculation.  
- Parameters like threshold can be stored as instance variables.

#### 6. üñºÔ∏è Siamese Networks and Contrastive Loss  
- Siamese networks use two identical subnetworks with shared weights to process two inputs.  
- Contrastive loss encourages feature vectors of similar inputs to be close and dissimilar inputs to be far apart.  
- Contrastive loss formula:  
  \[
  L = Y \times D^2 + (1 - Y) \times \max(\text{margin} - D, 0)^2
  \]  
  where \(Y\) is 1 for similar pairs, 0 for dissimilar, and \(D\) is Euclidean distance.

#### 7. üßÆ Contrastive Loss Implementation  
- Contrastive loss can be implemented as a function using Keras backend operations (`K.square`, `K.maximum`).  
- The loss averages over the batch using `K.mean`.  
- Can be parameterized with a margin using closures.

#### 8. üéõÔ∏è Parameterized Contrastive Loss  
- A wrapper function can create a contrastive loss with a specified margin.  
- Usage example: `model.compile(loss=contrastive_loss_with_margin(margin=1), optimizer=...)`.

#### 9. üèóÔ∏è Object-Oriented Contrastive Loss  
- Contrastive loss can be implemented as a subclass of `tf.keras.losses.Loss`.  
- The margin is stored as an instance variable and used in the `call` method.  
- Usage example: `model.compile(loss=ContrastiveLoss(margin=1), optimizer=...)`.



<br>

## Study Notes

### 1. üß© Introduction to Loss Functions in Neural Networks

In machine learning, **loss functions** are crucial because they measure how well a model‚Äôs predictions match the true values (labels). The goal during training is to minimize this loss, which means improving the model‚Äôs accuracy.

In TensorFlow/Keras, when you compile a model, you specify a loss function that guides the training process. For example:

```python
model.compile(loss='mse', optimizer='sgd')
```

Here, `'mse'` stands for **Mean Squared Error**, a common loss function for regression tasks. Alternatively, you can import and use loss functions explicitly:

```python
from tensorflow.keras.losses import mean_squared_error
model.compile(loss=mean_squared_error, optimizer='sgd')
```

You can also customize loss functions by passing parameters:

```python
model.compile(loss=mean_squared_error(param=value), optimizer='sgd')
```


### 2. üõ†Ô∏è Creating Custom Loss Functions

Sometimes, built-in loss functions don‚Äôt fit your specific problem. In such cases, you can create **custom loss functions**. A custom loss function is a Python function that takes two arguments:

- `y_true`: the true labels
- `y_pred`: the predicted values from the model

It returns a tensor representing the loss value.

Example of a simple custom loss function:

```python
def my_loss_function(y_true, y_pred):
    # Calculate loss here
    losses = ...
    return losses
```

This function can then be passed to `model.compile()` as the loss.


### 3. üîç Example: Huber Loss

The **Huber loss** is a popular loss function that is less sensitive to outliers than Mean Squared Error. It behaves like MSE when the error is small and like Mean Absolute Error (MAE) when the error is large.

#### How Huber Loss Works:

- Define a **threshold** value.
- If the absolute error (difference between true and predicted) is less than or equal to the threshold, use squared error (MSE style).
- If the error is larger than the threshold, use a linear loss (MAE style) scaled by the threshold.

#### Code Example:

```python
import tensorflow as tf

def my_huber_loss(y_true, y_pred):
    threshold = 1
    error = y_true - y_pred
    is_small_error = tf.abs(error) <= threshold
    
    small_error_loss = tf.square(error) / 2
    big_error_loss = threshold * (tf.abs(error) - 0.5 * threshold)
    
    return tf.where(is_small_error, small_error_loss, big_error_loss)
```

- `tf.where(condition, value_if_true, value_if_false)` acts like an if-else: it chooses the small error loss if the error is small, otherwise the big error loss.
- This function can be used directly in `model.compile()`:

```python
model.compile(optimizer='sgd', loss=my_huber_loss)
```


### 4. ‚öôÔ∏è Parameterized Custom Loss Functions

You can make your custom loss functions more flexible by allowing parameters, such as the threshold in Huber loss, to be passed dynamically.

Example:

```python
def my_huber_loss_with_threshold(threshold):
    def my_huber_loss(y_true, y_pred):
        error = y_true - y_pred
        is_small_error = tf.abs(error) <= threshold
        small_error_loss = tf.square(error) / 2
        big_error_loss = threshold * (tf.abs(error) - 0.5 * threshold)
        return tf.where(is_small_error, small_error_loss, big_error_loss)
    return my_huber_loss
```

Usage:

```python
model.compile(optimizer='sgd', loss=my_huber_loss_with_threshold(threshold=1))
```

This approach uses a **closure** to create a loss function with a fixed threshold.


### 5. üßë‚Äçüè´ Object-Oriented Custom Loss Functions

TensorFlow also supports defining custom loss functions as classes by subclassing `tf.keras.losses.Loss`. This is useful for more complex losses or when you want to keep track of parameters cleanly.

Example for Huber loss:

```python
from tensorflow.keras.losses import Loss

class MyHuberLoss(Loss):
    def __init__(self, threshold):
        super().__init__()
        self.threshold = threshold

    def call(self, y_true, y_pred):
        error = y_true - y_pred
        is_small_error = tf.abs(error) <= self.threshold
        small_error_loss = tf.square(error) / 2
        big_error_loss = self.threshold * (tf.abs(error) - 0.5 * self.threshold)
        return tf.where(is_small_error, small_error_loss, big_error_loss)
```

Usage:

```python
model.compile(optimizer='sgd', loss=MyHuberLoss(threshold=1))
```

This method is more modular and integrates well with Keras‚Äôs API.


### 6. üñºÔ∏è Custom Loss Functions for Siamese Networks: Contrastive Loss

Siamese networks are a special type of neural network architecture used for tasks like **image similarity**. They take two inputs (e.g., two images) and output feature vectors. The goal is to learn a representation where:

- Similar images have feature vectors close together.
- Different images have feature vectors far apart.

#### Architecture Overview:

- Two identical subnetworks (same structure and weights) process two images separately.
- The outputs are two vectors.
- The distance between these vectors (usually Euclidean distance) is computed.
- The loss function encourages the distance to be small for similar pairs and large for dissimilar pairs.

#### Contrastive Loss Formula:

\[
L = Y \times D^2 + (1 - Y) \times \max(\text{margin} - D, 0)^2
\]

Where:

- \(Y\) is 1 if the images are similar, 0 if different.
- \(D\) is the Euclidean distance between the two output vectors.
- \(\text{margin}\) is a hyperparameter that defines how far apart dissimilar pairs should be.

Interpretation:

- For similar pairs (\(Y=1\)), the loss is the squared distance \(D^2\), encouraging the network to minimize this distance.
- For dissimilar pairs (\(Y=0\)), the loss is zero if the distance is greater than the margin, otherwise it penalizes the network to push the distance beyond the margin.


### 7. üßÆ Implementing Contrastive Loss in Keras

#### Functional Form:

```python
import tensorflow.keras.backend as K

def contrastive_loss(y_true, y_pred):
    margin = 1
    square_pred = K.square(y_pred)
    margin_square = K.square(K.maximum(margin - y_pred, 0))
    return K.mean(y_true * square_pred + (1 - y_true) * margin_square)
```

- `y_pred` here represents the distance \(D\).
- `K.maximum(margin - y_pred, 0)` ensures the loss is only applied when the distance is less than the margin for dissimilar pairs.

Usage:

```python
model.compile(loss=contrastive_loss, optimizer='RMSprop')
```


### 8. üéõÔ∏è Parameterized Contrastive Loss

Like with Huber loss, you can create a version of contrastive loss that accepts a margin parameter:

```python
def contrastive_loss_with_margin(margin):
    def contrastive_loss(y_true, y_pred):
        square_pred = K.square(y_pred)
        margin_square = K.square(K.maximum(margin - y_pred, 0))
        return K.mean(y_true * square_pred + (1 - y_true) * margin_square)
    return contrastive_loss
```

Usage:

```python
model.compile(loss=contrastive_loss_with_margin(margin=1), optimizer='rms')
```


### 9. üèóÔ∏è Object-Oriented Contrastive Loss

You can also implement contrastive loss as a class:

```python
from tensorflow.keras.losses import Loss

class ContrastiveLoss(Loss):
    def __init__(self, margin):
        super().__init__()
        self.margin = margin

    def call(self, y_true, y_pred):
        square_pred = K.square(y_pred)
        margin_square = K.square(K.maximum(self.margin - y_pred, 0))
        return K.mean(y_true * square_pred + (1 - y_true) * margin_square)
```

Usage:

```python
model.compile(loss=ContrastiveLoss(margin=1), optimizer='rms')
```


### Summary

- **Loss functions** measure how well a model performs; minimizing loss improves the model.
- You can use built-in loss functions or create **custom loss functions** for specific needs.
- Custom loss functions can be simple functions, parameterized closures, or classes.
- **Huber loss** is a robust loss function that combines MSE and MAE behavior.
- **Contrastive loss** is used in Siamese networks to learn similarity/dissimilarity between pairs.
- Both Huber and Contrastive losses can be implemented flexibly in TensorFlow/Keras.



<br>

## Questions

#### 1. Which of the following are valid ways to specify a loss function in `model.compile()` in Keras?  
A) Using a string identifier like `'mse'`  
B) Passing a built-in loss function imported from `tensorflow.keras.losses`  
C) Passing a custom Python function that takes `(y_true, y_pred)` as arguments  
D) Passing a compiled model as the loss parameter  


#### 2. What is the primary purpose of a loss function in neural network training?  
A) To generate predictions from input data  
B) To measure the difference between predicted and true values  
C) To update the model‚Äôs weights directly  
D) To guide the optimizer in minimizing prediction errors  


#### 3. In the Huber loss function, what happens when the absolute error exceeds the threshold?  
A) The loss behaves like Mean Squared Error (MSE)  
B) The loss behaves like Mean Absolute Error (MAE) scaled by the threshold  
C) The loss becomes zero  
D) The loss increases linearly with the error magnitude  


#### 4. Consider the following snippet inside a custom loss function:  
```python
is_small_error = tf.abs(error) <= threshold  
return tf.where(is_small_error, small_error_loss, big_error_loss)
```  
What does `tf.where` do in this context?  
A) It returns the mean of `small_error_loss` and `big_error_loss`  
B) It selects `small_error_loss` where the error is small, otherwise `big_error_loss`  
C) It applies a conditional mask to the loss tensor  
D) It raises an error if the condition is not met  


#### 5. Why might you want to create a parameterized custom loss function using a closure (a function returning a function)?  
A) To allow dynamic adjustment of hyperparameters like thresholds during model compilation  
B) To avoid using classes for loss functions  
C) To enable the loss function to access model weights  
D) To improve training speed  


#### 6. When subclassing `tf.keras.losses.Loss` to create a custom loss, which method must be overridden?  
A) `__init__`  
B) `call`  
C) `compile`  
D) `fit`  


#### 7. In the context of Siamese networks, what is the role of the contrastive loss function?  
A) To minimize the Euclidean distance between all pairs of feature vectors  
B) To encourage similar inputs to have close feature vectors and dissimilar inputs to have distant feature vectors  
C) To maximize the margin between all pairs of feature vectors regardless of similarity  
D) To compute the cross-entropy between predicted and true labels  


#### 8. The contrastive loss formula is:  
\[
L = Y \times D^2 + (1 - Y) \times \max(\text{margin} - D, 0)^2
\]  
What does the term \(\max(\text{margin} - D, 0)^2\) represent?  
A) The penalty for similar pairs that are too far apart  
B) The penalty for dissimilar pairs that are too close  
C) The squared distance between feature vectors  
D) The margin value squared  


#### 9. Which of the following statements about the margin parameter in contrastive loss is true?  
A) It defines the minimum distance dissimilar pairs should have  
B) It is always set to 0.5 by default  
C) It controls how strictly the model separates dissimilar pairs  
D) It affects the loss for similar pairs only  


#### 10. In the contrastive loss function implementation, why is `K.maximum(margin - y_pred, 0)` used instead of just `margin - y_pred`?  
A) To ensure the loss is never negative  
B) To allow negative distances to contribute to the loss  
C) To clip the distance at zero for similar pairs  
D) To normalize the distance values  


#### 11. Which of the following are advantages of implementing custom loss functions as classes rather than functions?  
A) Easier to manage and store parameters like thresholds or margins  
B) Better integration with Keras‚Äôs serialization and saving mechanisms  
C) Faster execution during training  
D) Ability to override multiple methods like `call` and `get_config`  


#### 12. Suppose you have a custom loss function that returns a tensor of shape `(batch_size,)` instead of a scalar. What will happen during training?  
A) Training will proceed normally without issues  
B) TensorFlow will automatically reduce the loss to a scalar by averaging  
C) An error will be raised because the loss must be scalar  
D) The loss will be interpreted as multiple outputs and cause unpredictable behavior  


#### 13. In the Huber loss, why is the loss for small errors defined as \(\frac{1}{2} \times \text{error}^2\) instead of just \(\text{error}^2\)?  
A) To make the loss differentiable at zero  
B) To match the gradient of Mean Absolute Error  
C) To simplify the derivative and stabilize training  
D) To reduce the magnitude of the loss for small errors  


#### 14. When using a custom loss function with parameters, why can‚Äôt you just pass the function with parameters directly to `model.compile()`?  
A) Because Keras expects a callable with exactly two arguments `(y_true, y_pred)`  
B) Because functions with parameters are not serializable  
C) Because the optimizer requires a fixed loss function signature  
D) Because the loss function must be a subclass of `Loss`  


#### 15. What is the main difference between Mean Squared Error (MSE) and Huber loss in terms of sensitivity to outliers?  
A) MSE is more sensitive to outliers because it squares the error  
B) Huber loss ignores outliers completely  
C) Huber loss behaves like MSE for large errors  
D) MSE behaves like Mean Absolute Error for small errors  


#### 16. In a Siamese network, why must the two subnetworks share the same weights?  
A) To ensure the feature vectors are comparable in the same space  
B) To reduce the number of parameters in the model  
C) To prevent overfitting  
D) To allow the use of different loss functions for each input  


#### 17. Which of the following are true about the use of `tf.where` in custom loss functions?  
A) It can be used to apply different loss calculations conditionally within a batch  
B) It performs element-wise selection between two tensors based on a boolean mask  
C) It can only be used with scalar tensors  
D) It helps implement piecewise loss functions like Huber loss  


#### 18. If you want to create a custom loss function that depends on a hyperparameter (e.g., margin or threshold), which of the following approaches are valid?  
A) Use a closure that returns a loss function with the hyperparameter fixed  
B) Subclass `tf.keras.losses.Loss` and store the hyperparameter as an instance variable  
C) Pass the hyperparameter as an additional argument to the loss function during training  
D) Hardcode the hyperparameter inside the loss function without flexibility  


#### 19. What is the expected output of a contrastive loss function when the model predicts a distance \(D\) exactly equal to the margin for a dissimilar pair?  
A) Zero loss because the distance meets the margin requirement  
B) Positive loss proportional to the margin squared  
C) Negative loss, which is invalid  
D) Loss equal to \(D^2\)  


#### 20. Which of the following statements about the `call` method in a custom loss class are correct?  
A) It receives `y_true` and `y_pred` as inputs and returns the computed loss  
B) It must return a scalar tensor representing the loss for the batch  
C) It can be used to implement complex conditional logic for loss calculation  
D) It is automatically called during model training when the loss is computed



<br>

## Answers

#### 1. Which of the following are valid ways to specify a loss function in `model.compile()` in Keras?  
A) ‚úì Using a string identifier like `'mse'` ‚Äî Keras supports string aliases for common losses.  
B) ‚úì Passing a built-in loss function imported from `tensorflow.keras.losses` ‚Äî This is a standard way to specify loss.  
C) ‚úì Passing a custom Python function that takes `(y_true, y_pred)` as arguments ‚Äî Custom functions are allowed.  
D) ‚úó Passing a compiled model as the loss parameter ‚Äî This is invalid; loss must be a function or string.  

**Correct:** A,B,C


#### 2. What is the primary purpose of a loss function in neural network training?  
A) ‚úó To generate predictions from input data ‚Äî This is the model‚Äôs role, not the loss function.  
B) ‚úì To measure the difference between predicted and true values ‚Äî This is the core purpose of loss.  
C) ‚úó To update the model‚Äôs weights directly ‚Äî Optimizers update weights, not loss functions.  
D) ‚úì To guide the optimizer in minimizing prediction errors ‚Äî Loss provides the signal for optimization.  

**Correct:** B,D


#### 3. In the Huber loss function, what happens when the absolute error exceeds the threshold?  
A) ‚úó The loss behaves like Mean Squared Error (MSE) ‚Äî MSE applies for small errors, not large.  
B) ‚úì The loss behaves like Mean Absolute Error (MAE) scaled by the threshold ‚Äî For large errors, Huber loss is linear (MAE style).  
C) ‚úó The loss becomes zero ‚Äî Loss never becomes zero for large errors.  
D) ‚úì The loss increases linearly with the error magnitude ‚Äî This is the MAE-like behavior for large errors.  

**Correct:** B,D


#### 4. Consider the following snippet inside a custom loss function:  
```python
is_small_error = tf.abs(error) <= threshold  
return tf.where(is_small_error, small_error_loss, big_error_loss)
```  
What does `tf.where` do in this context?  
A) ‚úó It returns the mean of `small_error_loss` and `big_error_loss` ‚Äî It does element-wise selection, not averaging.  
B) ‚úì It selects `small_error_loss` where the error is small, otherwise `big_error_loss` ‚Äî This is exactly what `tf.where` does.  
C) ‚úì It applies a conditional mask to the loss tensor ‚Äî It uses the boolean mask to choose values.  
D) ‚úó It raises an error if the condition is not met ‚Äî It does not raise errors for false conditions.  

**Correct:** B,C


#### 5. Why might you want to create a parameterized custom loss function using a closure (a function returning a function)?  
A) ‚úì To allow dynamic adjustment of hyperparameters like thresholds during model compilation ‚Äî Closures enable passing parameters flexibly.  
B) ‚úì To avoid using classes for loss functions ‚Äî Closures are an alternative to class-based losses.  
C) ‚úó To enable the loss function to access model weights ‚Äî Loss functions don‚Äôt access weights directly.  
D) ‚úó To improve training speed ‚Äî Closures don‚Äôt inherently affect speed.  

**Correct:** A,B


#### 6. When subclassing `tf.keras.losses.Loss` to create a custom loss, which method must be overridden?  
A) ‚úó `__init__` ‚Äî Optional to override, mainly for parameters.  
B) ‚úì `call` ‚Äî This method defines the loss computation and must be overridden.  
C) ‚úó `compile` ‚Äî Not part of the loss class.  
D) ‚úó `fit` ‚Äî Belongs to the model, not loss class.  

**Correct:** B


#### 7. In the context of Siamese networks, what is the role of the contrastive loss function?  
A) ‚úó To minimize the Euclidean distance between all pairs of feature vectors ‚Äî Only similar pairs should be close, not all pairs.  
B) ‚úì To encourage similar inputs to have close feature vectors and dissimilar inputs to have distant feature vectors ‚Äî This is the core idea of contrastive loss.  
C) ‚úó To maximize the margin between all pairs of feature vectors regardless of similarity ‚Äî Margin applies only to dissimilar pairs.  
D) ‚úó To compute the cross-entropy between predicted and true labels ‚Äî Contrastive loss is not cross-entropy.  

**Correct:** B


#### 8. The contrastive loss formula is:  
\[
L = Y \times D^2 + (1 - Y) \times \max(\text{margin} - D, 0)^2
\]  
What does the term \(\max(\text{margin} - D, 0)^2\) represent?  
A) ‚úó The penalty for similar pairs that are too far apart ‚Äî Similar pairs use \(D^2\), not this term.  
B) ‚úì The penalty for dissimilar pairs that are too close ‚Äî This term penalizes dissimilar pairs closer than margin.  
C) ‚úó The squared distance between feature vectors ‚Äî This is \(D^2\), not the max term.  
D) ‚úó The margin value squared ‚Äî Margin is a parameter, not squared here.  

**Correct:** B


#### 9. Which of the following statements about the margin parameter in contrastive loss is true?  
A) ‚úì It defines the minimum distance dissimilar pairs should have ‚Äî Margin sets the desired separation.  
B) ‚úó It is always set to 0.5 by default ‚Äî Margin is user-defined, no fixed default.  
C) ‚úì It controls how strictly the model separates dissimilar pairs ‚Äî Larger margin means stricter separation.  
D) ‚úó It affects the loss for similar pairs only ‚Äî Margin affects dissimilar pairs only.  

**Correct:** A,C


#### 10. In the contrastive loss function implementation, why is `K.maximum(margin - y_pred, 0)` used instead of just `margin - y_pred`?  
A) ‚úì To ensure the loss is never negative ‚Äî Negative values would not make sense as loss.  
B) ‚úó To allow negative distances to contribute to the loss ‚Äî Negative loss is invalid.  
C) ‚úó To clip the distance at zero for similar pairs ‚Äî This term applies to dissimilar pairs only.  
D) ‚úó To normalize the distance values ‚Äî It does not normalize, only clips at zero.  

**Correct:** A


#### 11. Which of the following are advantages of implementing custom loss functions as classes rather than functions?  
A) ‚úì Easier to manage and store parameters like thresholds or margins ‚Äî Classes can hold state cleanly.  
B) ‚úì Better integration with Keras‚Äôs serialization and saving mechanisms ‚Äî Classes support config and saving.  
C) ‚úó Faster execution during training ‚Äî Execution speed is similar for both.  
D) ‚úì Ability to override multiple methods like `call` and `get_config` ‚Äî Classes allow more customization.  

**Correct:** A,B,D


#### 12. Suppose you have a custom loss function that returns a tensor of shape `(batch_size,)` instead of a scalar. What will happen during training?  
A) ‚úó Training will proceed normally without issues ‚Äî Loss must be scalar per batch.  
B) ‚úì TensorFlow will automatically reduce the loss to a scalar by averaging ‚Äî Keras reduces batch losses automatically.  
C) ‚úó An error will be raised because the loss must be scalar ‚Äî Usually no error, reduction happens internally.  
D) ‚úó The loss will be interpreted as multiple outputs and cause unpredictable behavior ‚Äî Loss is reduced, so no unpredictable behavior.  

**Correct:** B


#### 13. In the Huber loss, why is the loss for small errors defined as \(\frac{1}{2} \times \text{error}^2\) instead of just \(\text{error}^2\)?  
A) ‚úó To make the loss differentiable at zero ‚Äî Both forms are differentiable at zero.  
B) ‚úó To match the gradient of Mean Absolute Error ‚Äî Huber loss blends MSE and MAE but this is not the reason.  
C) ‚úì To simplify the derivative and stabilize training ‚Äî The factor 1/2 simplifies the gradient to error.  
D) ‚úó To reduce the magnitude of the loss for small errors ‚Äî It scales loss but main reason is gradient simplicity.  

**Correct:** C


#### 14. When using a custom loss function with parameters, why can‚Äôt you just pass the function with parameters directly to `model.compile()`?  
A) ‚úì Because Keras expects a callable with exactly two arguments `(y_true, y_pred)` ‚Äî Passing a function with extra parameters breaks this signature.  
B) ‚úó Because functions with parameters are not serializable ‚Äî Serialization is separate issue.  
C) ‚úó Because the optimizer requires a fixed loss function signature ‚Äî Optimizer does not enforce loss signature.  
D) ‚úó Because the loss function must be a subclass of `Loss` ‚Äî Functions can be used without subclassing.  

**Correct:** A


#### 15. What is the main difference between Mean Squared Error (MSE) and Huber loss in terms of sensitivity to outliers?  
A) ‚úì MSE is more sensitive to outliers because it squares the error ‚Äî Squaring amplifies large errors.  
B) ‚úó Huber loss ignores outliers completely ‚Äî It reduces their influence but does not ignore.  
C) ‚úó Huber loss behaves like MSE for large errors ‚Äî It behaves like MAE for large errors.  
D) ‚úó MSE behaves like Mean Absolute Error for small errors ‚Äî MSE always squares errors.  

**Correct:** A


#### 16. In a Siamese network, why must the two subnetworks share the same weights?  
A) ‚úì To ensure the feature vectors are comparable in the same space ‚Äî Shared weights produce consistent embeddings.  
B) ‚úì To reduce the number of parameters in the model ‚Äî Weight sharing reduces parameters.  
C) ‚úó To prevent overfitting ‚Äî Weight sharing may help but is not the main reason.  
D) ‚úó To allow the use of different loss functions for each input ‚Äî Loss is computed on outputs, not per subnetwork.  

**Correct:** A,B


#### 17. Which of the following are true about the use of `tf.where` in custom loss functions?  
A) ‚úì It can be used to apply different loss calculations conditionally within a batch ‚Äî Useful for piecewise losses.  
B) ‚úì It performs element-wise selection between two tensors based on a boolean mask ‚Äî This is its core function.  
C) ‚úó It can only be used with scalar tensors ‚Äî Works element-wise on tensors of any shape.  
D) ‚úì It helps implement piecewise loss functions like Huber loss ‚Äî Perfect for conditional logic in losses.  

**Correct:** A,B,D


#### 18. If you want to create a custom loss function that depends on a hyperparameter (e.g., margin or threshold), which of the following approaches are valid?  
A) ‚úì Use a closure that returns a loss function with the hyperparameter fixed ‚Äî Common and simple approach.  
B) ‚úì Subclass `tf.keras.losses.Loss` and store the hyperparameter as an instance variable ‚Äî Clean and modular.  
C) ‚úó Pass the hyperparameter as an additional argument to the loss function during training ‚Äî Loss functions only accept `(y_true, y_pred)`.  
D) ‚úì Hardcode the hyperparameter inside the loss function without flexibility ‚Äî Valid but inflexible.  

**Correct:** A,B,D


#### 19. What is the expected output of a contrastive loss function when the model predicts a distance \(D\) exactly equal to the margin for a dissimilar pair?  
A) ‚úì Zero loss because the distance meets the margin requirement ‚Äî \(\max(\text{margin} - D, 0) = 0\) so no penalty.  
B) ‚úó Positive loss proportional to the margin squared ‚Äî Loss is zero at margin boundary.  
C) ‚úó Negative loss, which is invalid ‚Äî Loss cannot be negative.  
D) ‚úó Loss equal to \(D^2\) ‚Äî This applies only for similar pairs.  

**Correct:** A


#### 20. Which of the following statements about the `call` method in a custom loss class are correct?  
A) ‚úì It receives `y_true` and `y_pred` as inputs and returns the computed loss ‚Äî This is its purpose.  
B) ‚úì It must return a scalar tensor representing the loss for the batch ‚Äî Loss must be scalar for training.  
C) ‚úì It can be used to implement complex conditional logic for loss calculation ‚Äî Custom logic is allowed.  
D) ‚úì It is automatically called during model training when the loss is computed ‚Äî Keras calls it internally.  

**Correct:** A,B,C,D