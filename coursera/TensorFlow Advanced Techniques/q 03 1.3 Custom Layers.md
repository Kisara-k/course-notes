## 1.3 Custom Layers

## Questions

#### 1. What are the two main roles of a neural network layer?  
A) Storing weights and biases  
B) Performing forward pass computations  
C) Managing the training dataset  
D) Initializing the optimizer  

#### 2. In a custom Keras layer, which method is responsible for creating the layer’s weights?  
A) `__init__`  
B) `build`  
C) `call`  
D) `compile`  

#### 3. When subclassing `tf.keras.layers.Layer`, what must you do to define the forward pass?  
A) Override the `build` method  
B) Override the `call` method  
C) Override the `__init__` method  
D) Override the `fit` method  

#### 4. Which of the following statements about the `build` method in a custom layer is true?  
A) It is called once the input shape is known  
B) It defines the forward computation  
C) It initializes trainable variables like weights and biases  
D) It is called every time the layer processes input data  

#### 5. What is the purpose of the `tf.Variable` in a custom layer?  
A) To store non-trainable constants  
B) To store trainable parameters like weights and biases  
C) To hold input data  
D) To define the activation function  

#### 6. How can you add an activation function inside a custom layer?  
A) By applying it inside the `call` method after the linear transformation  
B) By passing it as an argument to the `build` method  
C) By defining it in the `__init__` method and applying it in `call`  
D) By using the `compile` method  

#### 7. What is the main difference between a custom layer and a Lambda layer?  
A) Lambda layers can only apply simple functions without trainable weights  
B) Custom layers cannot have trainable weights  
C) Lambda layers require subclassing `Layer`  
D) Custom layers cannot be used inside Sequential models  

#### 8. Which of the following is a valid reason to use a Lambda layer?  
A) To quickly apply a simple custom function like `tf.abs`  
B) To create a layer with trainable weights  
C) To replace the `Dense` layer entirely  
D) To implement complex recurrent computations  

#### 9. Consider the following custom activation function used in a Lambda layer:  
```python
def my_relu(x):
    return tf.keras.backend.maximum(0.5, x)
```  
What is the effect of this activation compared to standard ReLU?  
A) It clamps all values below 0.5 to 0.5 instead of 0  
B) It behaves exactly like standard ReLU  
C) It outputs zero for all negative inputs  
D) It outputs 0.5 for all inputs less than 0.5  

#### 10. In the context of training a model with a custom layer, what does the `fit` method do?  
A) Initializes the weights of the custom layer  
B) Optimizes the weights by minimizing the loss function  
C) Defines the forward pass of the model  
D) Applies the activation function  

#### 11. Why might a model with a custom dense layer initially predict poorly before training?  
A) Because weights are randomly initialized and need training to adjust  
B) Because the activation function is missing  
C) Because the model is not compiled  
D) Because the input data is not normalized  

#### 12. Which of the following layers can be used to introduce non-linearity in a model?  
A) Dense with activation='relu'  
B) Lambda layer applying `tf.abs`  
C) Dropout layer  
D) BatchNormalization layer  

#### 13. What happens if you forget to call `super().__init__()` in the `__init__` method of a custom layer?  
A) The layer will not be properly initialized and may cause errors  
B) The weights will not be trainable  
C) The model will train but with lower accuracy  
D) The `build` method will not be called  

#### 14. Which of the following statements about the `call` method in a custom layer is false?  
A) It defines the computation from inputs to outputs  
B) It can include activation functions  
C) It is called during both training and inference  
D) It is used to initialize weights  

#### 15. When using a custom layer inside a Sequential model, which of the following is true?  
A) The custom layer must have a defined input shape or be preceded by a layer that does  
B) The custom layer cannot be followed by built-in Keras layers  
C) The custom layer must always include an activation function  
D) The custom layer must override the `compile` method  

#### 16. What is the role of the `trainable=True` argument when creating variables in a custom layer?  
A) It marks the variable as a parameter to be updated during training  
B) It freezes the variable so it does not change during training  
C) It initializes the variable with zeros  
D) It applies regularization to the variable  

#### 17. Which of the following layers are NOT typically used to create trainable parameters?  
A) Dense  
B) Dropout  
C) BatchNormalization  
D) SimpleDense (custom layer)  

#### 18. How does the `Lambda` layer differ from a custom layer in terms of flexibility?  
A) Lambda layers are limited to stateless operations without trainable weights  
B) Lambda layers can have trainable weights if specified  
C) Custom layers cannot be used inside functional API models  
D) Lambda layers require more code to implement than custom layers  

#### 19. If you want to implement a custom layer that behaves like a Dense layer but with a different activation, which approach is best?  
A) Subclass `Layer`, create weights in `build`, and apply activation in `call`  
B) Use a Lambda layer with a custom activation function only  
C) Use the built-in Dense layer and ignore activation  
D) Use Dropout followed by Dense  

#### 20. Which of the following statements about training a model with a custom layer are true?  
A) The training process updates the weights defined in the custom layer  
B) The loss function must be compatible with the output of the custom layer  
C) Custom layers cannot be used with optimizers like SGD  
D) The model’s accuracy can be monitored during training regardless of custom layers



<br>

## Answers

#### 1. What are the two main roles of a neural network layer?  
A) ✓ Stores weights and biases, which are the layer’s trainable parameters.  
B) ✓ Performs forward pass computations to transform inputs to outputs.  
C) ✗ Managing the training dataset is not a layer’s responsibility.  
D) ✗ Initializing the optimizer is done outside the layer.  

**Correct:** A, B


#### 2. In a custom Keras layer, which method is responsible for creating the layer’s weights?  
A) ✗ `__init__` initializes the layer but does not create weights.  
B) ✓ `build` is called once input shape is known and creates weights.  
C) ✗ `call` defines computation, not weight creation.  
D) ✗ `compile` is a model method, unrelated to layer weights.  

**Correct:** B


#### 3. When subclassing `tf.keras.layers.Layer`, what must you do to define the forward pass?  
A) ✗ `build` creates weights, not forward pass.  
B) ✓ `call` defines the forward computation from inputs to outputs.  
C) ✗ `__init__` initializes parameters but not computation.  
D) ✗ `fit` is a model method for training, not layer computation.  

**Correct:** B


#### 4. Which of the following statements about the `build` method in a custom layer is true?  
A) ✓ It is called once the input shape is known, allowing weight creation.  
B) ✗ Forward computation is defined in `call`, not `build`.  
C) ✓ It initializes trainable variables like weights and biases.  
D) ✗ It is called only once, not every time input is processed.  

**Correct:** A, C


#### 5. What is the purpose of the `tf.Variable` in a custom layer?  
A) ✗ Variables are trainable parameters, not constants.  
B) ✓ They store trainable parameters like weights and biases.  
C) ✗ Input data is passed as tensors, not stored as variables.  
D) ✗ Activation functions are applied in `call`, not stored as variables.  

**Correct:** B


#### 6. How can you add an activation function inside a custom layer?  
A) ✓ Apply it inside `call` after the linear transformation.  
B) ✗ `build` is for weight creation, not activation.  
C) ✓ Define in `__init__` and apply in `call` is a common pattern.  
D) ✗ `compile` is unrelated to activation functions.  

**Correct:** A, C


#### 7. What is the main difference between a custom layer and a Lambda layer?  
A) ✓ Lambda layers apply simple functions without trainable weights.  
B) ✗ Custom layers can have trainable weights.  
C) ✗ Lambda layers do not require subclassing `Layer`.  
D) ✗ Custom layers can be used inside Sequential models.  

**Correct:** A


#### 8. Which of the following is a valid reason to use a Lambda layer?  
A) ✓ Quickly apply simple custom functions like `tf.abs`.  
B) ✗ Lambda layers cannot have trainable weights.  
C) ✗ Lambda layers do not replace Dense layers entirely.  
D) ✗ Lambda layers are not designed for complex recurrent computations.  

**Correct:** A


#### 9. Consider the following custom activation function used in a Lambda layer:  
```python
def my_relu(x):
    return tf.keras.backend.maximum(0.5, x)
```  
What is the effect of this activation compared to standard ReLU?  
A) ✓ Values below 0.5 are clamped to 0.5 instead of 0.  
B) ✗ It behaves differently from standard ReLU.  
C) ✗ It does not output zero for negative inputs; outputs 0.5 instead.  
D) ✓ Outputs 0.5 for all inputs less than 0.5.  

**Correct:** A, D


#### 10. In the context of training a model with a custom layer, what does the `fit` method do?  
A) ✗ Weights are initialized before training, not during `fit`.  
B) ✓ Optimizes weights by minimizing the loss function during training.  
C) ✗ Forward pass is defined in the layer, not in `fit`.  
D) ✗ Activation functions are applied during forward pass, not in `fit`.  

**Correct:** B


#### 11. Why might a model with a custom dense layer initially predict poorly before training?  
A) ✓ Weights are randomly initialized and need training to improve.  
B) ✗ Activation function absence alone does not guarantee poor initial prediction.  
C) ✗ Model compilation is necessary but unrelated to initial prediction quality.  
D) ✗ Input normalization affects training speed but not initial random predictions.  

**Correct:** A


#### 12. Which of the following layers can be used to introduce non-linearity in a model?  
A) ✓ Dense with activation='relu' applies non-linear activation.  
B) ✓ Lambda layer applying `tf.abs` introduces non-linearity.  
C) ✗ Dropout randomly zeroes inputs, not a non-linear activation.  
D) ✗ BatchNormalization normalizes inputs, not a non-linear function.  

**Correct:** A, B


#### 13. What happens if you forget to call `super().__init__()` in the `__init__` method of a custom layer?  
A) ✓ The layer may not be properly initialized, causing errors.  
B) ✗ Weights can still be trainable if created properly.  
C) ✗ Model may train but behavior is unpredictable; not guaranteed lower accuracy.  
D) ✗ `build` will still be called if the layer is added to a model.  

**Correct:** A


#### 14. Which of the following statements about the `call` method in a custom layer is false?  
A) ✗ It does define computation from inputs to outputs.  
B) ✗ It can include activation functions.  
C) ✗ It is called during both training and inference.  
D) ✓ It is NOT used to initialize weights (that’s `build`).  

**Correct:** D


#### 15. When using a custom layer inside a Sequential model, which of the following is true?  
A) ✓ The custom layer must have a defined input shape or be preceded by a layer that does.  
B) ✗ Custom layers can be followed by built-in Keras layers.  
C) ✗ Activation function is optional, not mandatory.  
D) ✗ Layers do not override `compile`; models do.  

**Correct:** A


#### 16. What is the role of the `trainable=True` argument when creating variables in a custom layer?  
A) ✓ Marks the variable as trainable, so it updates during training.  
B) ✗ Freezing variables requires `trainable=False`.  
C) ✗ Initialization value is independent of `trainable`.  
D) ✗ Regularization is separate from the `trainable` flag.  

**Correct:** A


#### 17. Which of the following layers are NOT typically used to create trainable parameters?  
A) ✗ Dense layers have trainable weights.  
B) ✓ Dropout does not have trainable parameters; it randomly drops inputs.  
C) ✗ BatchNormalization has trainable scale and shift parameters.  
D) ✗ Custom SimpleDense layers have trainable weights.  

**Correct:** B


#### 18. How does the `Lambda` layer differ from a custom layer in terms of flexibility?  
A) ✓ Lambda layers are limited to stateless operations without trainable weights.  
B) ✗ Lambda layers cannot have trainable weights.  
C) ✗ Custom layers can be used in functional API models.  
D) ✗ Lambda layers require less code, not more, than custom layers.  

**Correct:** A


#### 19. If you want to implement a custom layer that behaves like a Dense layer but with a different activation, which approach is best?  
A) ✓ Subclass `Layer`, create weights in `build`, and apply activation in `call`.  
B) ✗ Lambda layers cannot create trainable weights, so not suitable.  
C) ✗ Using built-in Dense without activation ignores the custom activation need.  
D) ✗ Dropout is unrelated to activation functions.  

**Correct:** A


#### 20. Which of the following statements about training a model with a custom layer are true?  
A) ✓ Training updates weights defined in the custom layer.  
B) ✓ Loss function must be compatible with the model’s output.  
C) ✗ Custom layers can be used with optimizers like SGD.  
D) ✓ Accuracy can be monitored regardless of custom layers.  

**Correct:** A, B, D