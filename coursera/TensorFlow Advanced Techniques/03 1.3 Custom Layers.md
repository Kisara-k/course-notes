## 1.3 Custom Layers

[Study Notes](#study-notes)

[Questions](#questions)



### Key Points

#### 1. üß± Layer Basics  
- A layer in neural networks has **state (weights)** and performs **computation (forward pass)**.  
- A Dense layer‚Äôs state includes a **kernel (weight matrix)** and a **bias vector**.  

#### 2. üõ†Ô∏è Custom Layer Creation  
- Custom layers are created by subclassing `tf.keras.layers.Layer`.  
- The `__init__` method initializes the layer and its hyperparameters.  
- The `build(input_shape)` method creates the layer‚Äôs weights (state).  
- The `call(inputs)` method defines the forward pass computation.  

#### 3. üß© Activation in Custom Layers  
- Activation functions can be added by passing an activation argument and applying it in `call()`.  
- Use `tf.keras.activations.get(activation)` to retrieve the activation function.  

#### 4. üß™ Lambda Layer Usage  
- `tf.keras.layers.Lambda` applies a custom function to inputs without writing a full layer class.  
- Lambda layers can be used for simple operations like `tf.abs` or custom activations like ReLU variants.  

#### 5. üìä Model Training with Custom Layers  
- Models with custom layers are compiled and trained the same way as standard models.  
- Training output includes **loss** and **accuracy** metrics per epoch.  

#### 6. üß† Common Layer Types Mentioned  
- Convolutional layers: `Conv1D`, `Conv2D`, `Conv3D`, `SeparableConv2D`, `DepthwiseConv2D`  
- Recurrent layers: `LSTM`, `GRU`  
- Pooling layers: `MaxPooling2D`, `AveragePooling2D`, `GlobalAveragePooling2D`  
- Activation layers: `LeakyReLU`, `PReLU`, `ELU`, `Activation`  
- Core layers: `Dense`, `Dropout`, `BatchNormalization`, `Lambda`, `Input`  

#### 7. üßÆ Example: SimpleDense Layer  
- Weights initialized with `tf.random_normal_initializer()`.  
- Biases initialized with zeros.  
- Forward pass computes `tf.matmul(inputs, w) + b`.  
- Adding activation applies it to the linear output before returning.  

#### 8. üîÑ Using Custom Layers in Models  
- Custom layers can be used inside `tf.keras.Sequential` models like built-in layers.  
- Example: `SimpleDense(128, activation='relu')` inside a sequential model.  

#### 9. üß™ Training Example  
- Training a SimpleDense model on data `xs` and `ys` with `optimizer='sgd'` and `loss='mean_squared_error'`.  
- After sufficient epochs, the model approximates the function (e.g., y = 2x - 1).



<br>

## Study Notes

### 1. üß± What is a Layer in Neural Networks?

Before diving into custom layers, it‚Äôs important to understand what a **layer** is in the context of neural networks.

A **layer** is a fundamental building block of a neural network. It performs two main roles:

- **State (Weights):** Each layer has parameters (weights and biases) that it learns during training.
- **Computation (Forward Pass):** It defines how input data is transformed into output data using these parameters.

For example, a **Dense (fully connected) layer** multiplies the input by a weight matrix and adds a bias vector, then optionally applies an activation function.


### 2. üõ†Ô∏è Creating Custom Layers in TensorFlow Keras

TensorFlow Keras allows you to create your own layers by subclassing the `Layer` class. This is useful when you want to implement a layer with custom behavior that is not available in the built-in layers.

#### Key Components of a Custom Layer

- **`__init__` method:** Initialize the layer and define any hyperparameters (e.g., number of units).
- **`build` method:** Create the layer‚Äôs weights. This method is called once the input shape is known.
- **`call` method:** Define the forward pass computation.

#### Example: Simple Dense Layer

Here‚Äôs a minimal example of a custom dense layer:

```python
class SimpleDense(tf.keras.layers.Layer):
    def __init__(self, units=32):
        super(SimpleDense, self).__init__()
        self.units = units

    def build(self, input_shape):
        # Initialize weights and biases
        w_init = tf.random_normal_initializer()
        self.w = tf.Variable(
            initial_value=w_init(shape=(input_shape[-1], self.units), dtype='float32'),
            trainable=True,
            name="kernel"
        )
        b_init = tf.zeros_initializer()
        self.b = tf.Variable(
            initial_value=b_init(shape=(self.units,), dtype='float32'),
            trainable=True,
            name="bias"
        )

    def call(self, inputs):
        # Forward pass: matrix multiply inputs by weights and add bias
        return tf.matmul(inputs, self.w) + self.b
```

- **Weights (`self.w`)** are initialized randomly.
- **Biases (`self.b`)** are initialized to zero.
- The `call` method performs the linear transformation.


### 3. üîÑ Using Custom Layers in a Model

You can use your custom layer just like any other Keras layer inside a model:

```python
model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    SimpleDense(128),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(10, activation='softmax')
])
```

This model flattens the input image, applies the custom dense layer with 128 units, applies dropout for regularization, and finally outputs class probabilities with a softmax layer.


### 4. üß™ Training a Model with a Custom Layer

You can compile and train the model as usual:

```python
model.compile(optimizer='sgd', loss='mean_squared_error')
model.fit(xs, ys, epochs=500, verbose=0)
```

Here, `xs` and `ys` are training data arrays. After training, the model learns weights that approximate the relationship between inputs and outputs.


### 5. üß© Adding Activation Functions in Custom Layers

Activation functions introduce non-linearity, which is crucial for neural networks to learn complex patterns.

You can add an activation function inside your custom layer by:

- Adding an `activation` argument in `__init__`.
- Applying the activation in the `call` method.

Example:

```python
class SimpleDense(tf.keras.layers.Layer):
    def __init__(self, units=32, activation=None):
        super(SimpleDense, self).__init__()
        self.units = units
        self.activation = tf.keras.activations.get(activation)

    def build(self, input_shape):
        w_init = tf.random_normal_initializer()
        self.w = tf.Variable(
            initial_value=w_init(shape=(input_shape[-1], self.units), dtype='float32'),
            trainable=True,
            name="kernel"
        )
        b_init = tf.zeros_initializer()
        self.b = tf.Variable(
            initial_value=b_init(shape=(self.units,), dtype='float32'),
            trainable=True,
            name="bias"
        )

    def call(self, inputs):
        z = tf.matmul(inputs, self.w) + self.b
        return self.activation(z) if self.activation else z
```

Now you can specify activation like `'relu'` when creating the layer:

```python
SimpleDense(128, activation='relu')
```


### 6. üß™ Using Lambda Layers for Quick Custom Computations

Sometimes you want to apply a simple custom function without writing a full layer class. Keras provides the `Lambda` layer for this.

Example: Applying the absolute value function to outputs:

```python
model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(128),
    tf.keras.layers.Lambda(lambda x: tf.abs(x)),
    tf.keras.layers.Dense(10, activation='softmax')
])
```

This inserts a layer that applies `tf.abs` to the output of the dense layer.

You can also define your own activation function and use it inside a Lambda layer:

```python
def my_relu(x):
    return tf.keras.backend.maximum(0.0, x)

model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(128),
    tf.keras.layers.Lambda(my_relu),
    tf.keras.layers.Dense(10, activation='softmax')
])
```


### 7. üìä Observing Model Training and Performance

When training models with custom layers or Lambda layers, you can monitor:

- **Loss:** How well the model fits the training data.
- **Accuracy:** How often the model predicts correctly.

Example training output:

```
Epoch 1/5
1875/1875 [==============================] - 4s 2ms/step - loss: 0.2591 - accuracy: 0.9262
Epoch 2/5
1875/1875 [==============================] - 4s 2ms/step - loss: 0.1157 - accuracy: 0.9662
...
Epoch 5/5
1875/1875 [==============================] - 4s 2ms/step - loss: 0.0456 - accuracy: 0.9862
```

This shows the model improving over epochs, with loss decreasing and accuracy increasing.


### 8. üß† Summary of Commonly Used Layers Mentioned

The lecture also briefly lists many common layers you might encounter or use:

- **Convolutional Layers:** `Conv1D`, `Conv2D`, `Conv3D`, `SeparableConv2D`, `DepthwiseConv2D`
- **Recurrent Layers:** `LSTM`, `GRU`
- **Pooling Layers:** `MaxPooling2D`, `AveragePooling2D`, `GlobalAveragePooling2D`
- **Activation Layers:** `LeakyReLU`, `PReLU`, `ELU`, `Activation`
- **Core Layers:** `Dense`, `Dropout`, `BatchNormalization`, `Lambda`, `Input`

Each of these layers serves a specific purpose in building neural networks for different tasks like image recognition, sequence modeling, or general function approximation.


### Final Notes

- Custom layers give you flexibility to implement exactly the behavior you want.
- The `build` method is where you create weights, and `call` is where you define the computation.
- Lambda layers are a quick way to insert simple custom functions.
- Activation functions can be integrated into custom layers or applied separately.
- Training and evaluating models with custom layers works the same as with built-in layers.



<br>

## Questions

#### 1. What are the two main roles of a neural network layer?  
A) Storing weights and biases  
B) Performing forward pass computations  
C) Managing the training dataset  
D) Initializing the optimizer  

#### 2. In a custom Keras layer, which method is responsible for creating the layer‚Äôs weights?  
A) `__init__`  
B) `build`  
C) `call`  
D) `compile`  

#### 3. When subclassing `tf.keras.layers.Layer`, what must you do to define the forward pass?  
A) Override the `build` method  
B) Override the `call` method  
C) Override the `__init__` method  
D) Override the `fit` method  

#### 4. Which of the following statements about the `build` method in a custom layer is true?  
A) It is called once the input shape is known  
B) It defines the forward computation  
C) It initializes trainable variables like weights and biases  
D) It is called every time the layer processes input data  

#### 5. What is the purpose of the `tf.Variable` in a custom layer?  
A) To store non-trainable constants  
B) To store trainable parameters like weights and biases  
C) To hold input data  
D) To define the activation function  

#### 6. How can you add an activation function inside a custom layer?  
A) By applying it inside the `call` method after the linear transformation  
B) By passing it as an argument to the `build` method  
C) By defining it in the `__init__` method and applying it in `call`  
D) By using the `compile` method  

#### 7. What is the main difference between a custom layer and a Lambda layer?  
A) Lambda layers can only apply simple functions without trainable weights  
B) Custom layers cannot have trainable weights  
C) Lambda layers require subclassing `Layer`  
D) Custom layers cannot be used inside Sequential models  

#### 8. Which of the following is a valid reason to use a Lambda layer?  
A) To quickly apply a simple custom function like `tf.abs`  
B) To create a layer with trainable weights  
C) To replace the `Dense` layer entirely  
D) To implement complex recurrent computations  

#### 9. Consider the following custom activation function used in a Lambda layer:  
```python
def my_relu(x):
    return tf.keras.backend.maximum(0.5, x)
```  
What is the effect of this activation compared to standard ReLU?  
A) It clamps all values below 0.5 to 0.5 instead of 0  
B) It behaves exactly like standard ReLU  
C) It outputs zero for all negative inputs  
D) It outputs 0.5 for all inputs less than 0.5  

#### 10. In the context of training a model with a custom layer, what does the `fit` method do?  
A) Initializes the weights of the custom layer  
B) Optimizes the weights by minimizing the loss function  
C) Defines the forward pass of the model  
D) Applies the activation function  

#### 11. Why might a model with a custom dense layer initially predict poorly before training?  
A) Because weights are randomly initialized and need training to adjust  
B) Because the activation function is missing  
C) Because the model is not compiled  
D) Because the input data is not normalized  

#### 12. Which of the following layers can be used to introduce non-linearity in a model?  
A) Dense with activation='relu'  
B) Lambda layer applying `tf.abs`  
C) Dropout layer  
D) BatchNormalization layer  

#### 13. What happens if you forget to call `super().__init__()` in the `__init__` method of a custom layer?  
A) The layer will not be properly initialized and may cause errors  
B) The weights will not be trainable  
C) The model will train but with lower accuracy  
D) The `build` method will not be called  

#### 14. Which of the following statements about the `call` method in a custom layer is false?  
A) It defines the computation from inputs to outputs  
B) It can include activation functions  
C) It is called during both training and inference  
D) It is used to initialize weights  

#### 15. When using a custom layer inside a Sequential model, which of the following is true?  
A) The custom layer must have a defined input shape or be preceded by a layer that does  
B) The custom layer cannot be followed by built-in Keras layers  
C) The custom layer must always include an activation function  
D) The custom layer must override the `compile` method  

#### 16. What is the role of the `trainable=True` argument when creating variables in a custom layer?  
A) It marks the variable as a parameter to be updated during training  
B) It freezes the variable so it does not change during training  
C) It initializes the variable with zeros  
D) It applies regularization to the variable  

#### 17. Which of the following layers are NOT typically used to create trainable parameters?  
A) Dense  
B) Dropout  
C) BatchNormalization  
D) SimpleDense (custom layer)  

#### 18. How does the `Lambda` layer differ from a custom layer in terms of flexibility?  
A) Lambda layers are limited to stateless operations without trainable weights  
B) Lambda layers can have trainable weights if specified  
C) Custom layers cannot be used inside functional API models  
D) Lambda layers require more code to implement than custom layers  

#### 19. If you want to implement a custom layer that behaves like a Dense layer but with a different activation, which approach is best?  
A) Subclass `Layer`, create weights in `build`, and apply activation in `call`  
B) Use a Lambda layer with a custom activation function only  
C) Use the built-in Dense layer and ignore activation  
D) Use Dropout followed by Dense  

#### 20. Which of the following statements about training a model with a custom layer are true?  
A) The training process updates the weights defined in the custom layer  
B) The loss function must be compatible with the output of the custom layer  
C) Custom layers cannot be used with optimizers like SGD  
D) The model‚Äôs accuracy can be monitored during training regardless of custom layers



<br>

## Answers

#### 1. What are the two main roles of a neural network layer?  
A) ‚úì Stores weights and biases, which are the layer‚Äôs trainable parameters.  
B) ‚úì Performs forward pass computations to transform inputs to outputs.  
C) ‚úó Managing the training dataset is not a layer‚Äôs responsibility.  
D) ‚úó Initializing the optimizer is done outside the layer.  

**Correct:** A, B


#### 2. In a custom Keras layer, which method is responsible for creating the layer‚Äôs weights?  
A) ‚úó `__init__` initializes the layer but does not create weights.  
B) ‚úì `build` is called once input shape is known and creates weights.  
C) ‚úó `call` defines computation, not weight creation.  
D) ‚úó `compile` is a model method, unrelated to layer weights.  

**Correct:** B


#### 3. When subclassing `tf.keras.layers.Layer`, what must you do to define the forward pass?  
A) ‚úó `build` creates weights, not forward pass.  
B) ‚úì `call` defines the forward computation from inputs to outputs.  
C) ‚úó `__init__` initializes parameters but not computation.  
D) ‚úó `fit` is a model method for training, not layer computation.  

**Correct:** B


#### 4. Which of the following statements about the `build` method in a custom layer is true?  
A) ‚úì It is called once the input shape is known, allowing weight creation.  
B) ‚úó Forward computation is defined in `call`, not `build`.  
C) ‚úì It initializes trainable variables like weights and biases.  
D) ‚úó It is called only once, not every time input is processed.  

**Correct:** A, C


#### 5. What is the purpose of the `tf.Variable` in a custom layer?  
A) ‚úó Variables are trainable parameters, not constants.  
B) ‚úì They store trainable parameters like weights and biases.  
C) ‚úó Input data is passed as tensors, not stored as variables.  
D) ‚úó Activation functions are applied in `call`, not stored as variables.  

**Correct:** B


#### 6. How can you add an activation function inside a custom layer?  
A) ‚úì Apply it inside `call` after the linear transformation.  
B) ‚úó `build` is for weight creation, not activation.  
C) ‚úì Define in `__init__` and apply in `call` is a common pattern.  
D) ‚úó `compile` is unrelated to activation functions.  

**Correct:** A, C


#### 7. What is the main difference between a custom layer and a Lambda layer?  
A) ‚úì Lambda layers apply simple functions without trainable weights.  
B) ‚úó Custom layers can have trainable weights.  
C) ‚úó Lambda layers do not require subclassing `Layer`.  
D) ‚úó Custom layers can be used inside Sequential models.  

**Correct:** A


#### 8. Which of the following is a valid reason to use a Lambda layer?  
A) ‚úì Quickly apply simple custom functions like `tf.abs`.  
B) ‚úó Lambda layers cannot have trainable weights.  
C) ‚úó Lambda layers do not replace Dense layers entirely.  
D) ‚úó Lambda layers are not designed for complex recurrent computations.  

**Correct:** A


#### 9. Consider the following custom activation function used in a Lambda layer:  
```python
def my_relu(x):
    return tf.keras.backend.maximum(0.5, x)
```  
What is the effect of this activation compared to standard ReLU?  
A) ‚úì Values below 0.5 are clamped to 0.5 instead of 0.  
B) ‚úó It behaves differently from standard ReLU.  
C) ‚úó It does not output zero for negative inputs; outputs 0.5 instead.  
D) ‚úì Outputs 0.5 for all inputs less than 0.5.  

**Correct:** A, D


#### 10. In the context of training a model with a custom layer, what does the `fit` method do?  
A) ‚úó Weights are initialized before training, not during `fit`.  
B) ‚úì Optimizes weights by minimizing the loss function during training.  
C) ‚úó Forward pass is defined in the layer, not in `fit`.  
D) ‚úó Activation functions are applied during forward pass, not in `fit`.  

**Correct:** B


#### 11. Why might a model with a custom dense layer initially predict poorly before training?  
A) ‚úì Weights are randomly initialized and need training to improve.  
B) ‚úó Activation function absence alone does not guarantee poor initial prediction.  
C) ‚úó Model compilation is necessary but unrelated to initial prediction quality.  
D) ‚úó Input normalization affects training speed but not initial random predictions.  

**Correct:** A


#### 12. Which of the following layers can be used to introduce non-linearity in a model?  
A) ‚úì Dense with activation='relu' applies non-linear activation.  
B) ‚úì Lambda layer applying `tf.abs` introduces non-linearity.  
C) ‚úó Dropout randomly zeroes inputs, not a non-linear activation.  
D) ‚úó BatchNormalization normalizes inputs, not a non-linear function.  

**Correct:** A, B


#### 13. What happens if you forget to call `super().__init__()` in the `__init__` method of a custom layer?  
A) ‚úì The layer may not be properly initialized, causing errors.  
B) ‚úó Weights can still be trainable if created properly.  
C) ‚úó Model may train but behavior is unpredictable; not guaranteed lower accuracy.  
D) ‚úó `build` will still be called if the layer is added to a model.  

**Correct:** A


#### 14. Which of the following statements about the `call` method in a custom layer is false?  
A) ‚úó It does define computation from inputs to outputs.  
B) ‚úó It can include activation functions.  
C) ‚úó It is called during both training and inference.  
D) ‚úì It is NOT used to initialize weights (that‚Äôs `build`).  

**Correct:** D


#### 15. When using a custom layer inside a Sequential model, which of the following is true?  
A) ‚úì The custom layer must have a defined input shape or be preceded by a layer that does.  
B) ‚úó Custom layers can be followed by built-in Keras layers.  
C) ‚úó Activation function is optional, not mandatory.  
D) ‚úó Layers do not override `compile`; models do.  

**Correct:** A


#### 16. What is the role of the `trainable=True` argument when creating variables in a custom layer?  
A) ‚úì Marks the variable as trainable, so it updates during training.  
B) ‚úó Freezing variables requires `trainable=False`.  
C) ‚úó Initialization value is independent of `trainable`.  
D) ‚úó Regularization is separate from the `trainable` flag.  

**Correct:** A


#### 17. Which of the following layers are NOT typically used to create trainable parameters?  
A) ‚úó Dense layers have trainable weights.  
B) ‚úì Dropout does not have trainable parameters; it randomly drops inputs.  
C) ‚úó BatchNormalization has trainable scale and shift parameters.  
D) ‚úó Custom SimpleDense layers have trainable weights.  

**Correct:** B


#### 18. How does the `Lambda` layer differ from a custom layer in terms of flexibility?  
A) ‚úì Lambda layers are limited to stateless operations without trainable weights.  
B) ‚úó Lambda layers cannot have trainable weights.  
C) ‚úó Custom layers can be used in functional API models.  
D) ‚úó Lambda layers require less code, not more, than custom layers.  

**Correct:** A


#### 19. If you want to implement a custom layer that behaves like a Dense layer but with a different activation, which approach is best?  
A) ‚úì Subclass `Layer`, create weights in `build`, and apply activation in `call`.  
B) ‚úó Lambda layers cannot create trainable weights, so not suitable.  
C) ‚úó Using built-in Dense without activation ignores the custom activation need.  
D) ‚úó Dropout is unrelated to activation functions.  

**Correct:** A


#### 20. Which of the following statements about training a model with a custom layer are true?  
A) ‚úì Training updates weights defined in the custom layer.  
B) ‚úì Loss function must be compatible with the model‚Äôs output.  
C) ‚úó Custom layers can be used with optimizers like SGD.  
D) ‚úì Accuracy can be monitored regardless of custom layers.  

**Correct:** A, B, D