## 1.5 Callbacks

[Study Notes](#study-notes)

[Questions](#questions)



### Key Points

#### 1. 🔄 Callbacks Overview  
- Callbacks allow injecting custom behavior at various stages of model training, evaluation, and prediction.  
- They are subclasses of `tf.keras.callbacks.Callback`.  
- Callbacks can access internal states and statistics like losses and metrics during training.

#### 2. 🛠️ Callback Methods  
- `on_epoch_begin(epoch, logs=None)`: Called at the start of each epoch.  
- `on_epoch_end(epoch, logs=None)`: Called at the end of each epoch.  
- `on_train_begin(logs=None)` and `on_train_end(logs=None)`: Called at the start and end of training.  
- `on_test_begin(logs=None)` and `on_test_end(logs=None)`: Called at the start and end of evaluation.  
- `on_predict_begin(logs=None)` and `on_predict_end(logs=None)`: Called at the start and end of prediction.  
- `on_train_batch_begin(batch, logs=None)` and `on_train_batch_end(batch, logs=None)`: Called before and after each training batch.  
- Similar batch-level methods exist for test and predict phases.

#### 3. 📍 Using Callbacks in Model Methods  
- Callbacks can be passed to `model.fit()`, `model.fit_generator()`, `model.evaluate()`, `model.evaluate_generator()`, `model.predict()`, and `model.predict_generator()` via the `callbacks` argument.

#### 4. 📊 TensorBoard Callback  
- Used to visualize training metrics and model graph.  
- Requires specifying a log directory (`log_dir`).  
- Example: `tf.keras.callbacks.TensorBoard(log_dir=log_dir, update_freq='epoch')`.

#### 5. 💾 ModelCheckpoint Callback  
- Saves model or weights during training.  
- Can save only the best model based on a monitored metric (e.g., `val_loss`).  
- Options include `save_best_only`, `save_weights_only`, `monitor`, and `save_freq`.  
- Supports saving in different formats like H5 or SavedModel.

#### 6. ⏹️ EarlyStopping Callback  
- Stops training when a monitored metric stops improving.  
- Key parameters: `monitor`, `patience`, `min_delta`, `restore_best_weights`.  
- Helps prevent overfitting by stopping training early.

#### 7. 📈 CSVLogger Callback  
- Logs training metrics to a CSV file for later analysis.  
- Used by passing `CSVLogger('filename.csv')` as a callback.

#### 8. 🧑‍💻 Custom Callbacks  
- Created by subclassing `tf.keras.callbacks.Callback` and overriding relevant methods.  
- Can be used to print logs, stop training based on custom conditions, or visualize data during training.

#### 9. 🎨 Visualization Callback Example  
- Custom callback can plot predictions on test samples at the end of epochs.  
- Can save plots as images and compile them into an animation (e.g., GIF) after training ends.

#### 10. 🔗 Combining Callbacks  
- Multiple callbacks can be passed together in a list to model methods to combine functionalities like checkpointing, early stopping, and logging.



<br>

## Study Notes

### 1. 🔄 What Are Callbacks? — Introduction and Purpose

When training machine learning models, especially deep learning models, it’s often useful to have a way to **monitor and interact with the training process** at different stages. This is where **callbacks** come in.

**Callbacks** are special functions or objects that let you **inject custom behavior** during training, evaluation, or prediction. They can track metrics, save models, stop training early, visualize progress, and much more.

In TensorFlow’s Keras API, callbacks are implemented as classes that inherit from `tf.keras.callbacks.Callback`. You can think of a callback as a "listener" that reacts to events during the model lifecycle — like the start or end of an epoch, or the beginning or end of a batch.


### 2. 🛠️ How Callbacks Work — The Callback Class and Its Methods

The core of callbacks is the `Callback` class. When you create a custom callback, you subclass this and override specific methods that correspond to different events during training or evaluation.

#### Key Methods in Callbacks

- **on_epoch_begin(epoch, logs=None)**  
  Called at the **start of each epoch**. Useful for initializing or resetting things before an epoch begins.

- **on_epoch_end(epoch, logs=None)**  
  Called at the **end of each epoch**. You can use this to log metrics, save models, or decide if training should stop.

- **on_train_begin(logs=None)** and **on_train_end(logs=None)**  
  Called at the start and end of the entire training process.

- **on_test_begin(logs=None)** and **on_test_end(logs=None)**  
  Called at the start and end of evaluation/testing.

- **on_predict_begin(logs=None)** and **on_predict_end(logs=None)**  
  Called at the start and end of prediction.

- **on_train_batch_begin(batch, logs=None)** and **on_train_batch_end(batch, logs=None)**  
  Called before and after processing each batch during training.

- **on_test_batch_begin(batch, logs=None)** and **on_test_batch_end(batch, logs=None)**  
  Called before and after each batch during evaluation.

- **on_predict_batch_begin(batch, logs=None)** and **on_predict_batch_end(batch, logs=None)**  
  Called before and after each batch during prediction.

The `logs` dictionary contains metrics and loss values relevant to the current step, which you can use inside these methods.


### 3. 📍 Where and How to Use Callbacks in Model Training

You can pass callbacks to various model methods in Keras:

- `model.fit(..., callbacks=[...])` — during training
- `model.fit_generator(..., callbacks=[...])` — training with generators
- `model.evaluate(..., callbacks=[...])` — during evaluation
- `model.evaluate_generator(..., callbacks=[...])` — evaluation with generators
- `model.predict(..., callbacks=[...])` — during prediction
- `model.predict_generator(..., callbacks=[...])` — prediction with generators

This flexibility allows you to monitor or modify behavior at any stage of your model’s lifecycle.


### 4. 📊 Common Built-in Callbacks and Their Uses

TensorFlow Keras provides several useful built-in callbacks that cover common needs:

#### TensorBoard Callback

- **Purpose:** Visualize training progress, metrics (like loss and accuracy), and the model graph.
- **How it works:** Logs data to a directory that TensorBoard reads.
- **Usage example:**

```python
import datetime
log_dir = "logs/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, update_freq='epoch')

model.fit(train_data, epochs=10, callbacks=[tensorboard_callback])
```

- You can then launch TensorBoard to visualize the logs.

#### ModelCheckpoint Callback

- **Purpose:** Save the model or weights at certain intervals or when performance improves.
- **Key options:**
  - `filepath`: where to save the model.
  - `monitor`: metric to watch (e.g., `'val_loss'`).
  - `save_best_only`: save only when the monitored metric improves.
  - `save_weights_only`: save only weights, not the full model.
  - `save_freq`: how often to save (e.g., every epoch).
- **Usage examples:**

Save model every epoch:

```python
checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', verbose=1)
model.fit(train_data, epochs=5, callbacks=[checkpoint])
```

Save only best model based on validation loss:

```python
checkpoint = tf.keras.callbacks.ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)
model.fit(train_data, epochs=5, validation_data=val_data, callbacks=[checkpoint])
```

#### EarlyStopping Callback

- **Purpose:** Stop training early if the model stops improving, which helps prevent overfitting and saves time.
- **Key options:**
  - `monitor`: metric to watch (e.g., `'val_loss'`).
  - `patience`: number of epochs to wait for improvement before stopping.
  - `min_delta`: minimum change to qualify as improvement.
  - `restore_best_weights`: whether to restore the best weights after stopping.
- **Usage example:**

```python
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1)
model.fit(train_data, epochs=50, validation_data=val_data, callbacks=[early_stopping])
```

#### CSVLogger Callback

- **Purpose:** Log training metrics to a CSV file for later analysis.
- **Usage example:**

```python
csv_logger = tf.keras.callbacks.CSVLogger('training_log.csv')
model.fit(train_data, epochs=10, callbacks=[csv_logger])
```


### 5. 🧑‍💻 Creating Custom Callbacks — How and Why

Sometimes built-in callbacks don’t cover your specific needs. You can create your own by subclassing `tf.keras.callbacks.Callback` and overriding the methods you want.

#### Example: Logging batch start and end times

```python
import datetime
class MyCustomCallback(tf.keras.callbacks.Callback):
    def on_train_batch_begin(self, batch, logs=None):
        print(f'Training: batch {batch} begins at {datetime.datetime.now().time()}')

    def on_train_batch_end(self, batch, logs=None):
        print(f'Training: batch {batch} ends at {datetime.datetime.now().time()}')

my_callback = MyCustomCallback()
model.fit(x_train, y_train, batch_size=64, epochs=1, callbacks=[my_callback])
```

#### Example: Detecting Overfitting and Stopping Training

You can create a callback that monitors the ratio of validation loss to training loss and stops training if it exceeds a threshold:

```python
class DetectOverfittingCallback(tf.keras.callbacks.Callback):
    def __init__(self, threshold):
        super().__init__()
        self.threshold = threshold

    def on_epoch_end(self, epoch, logs=None):
        ratio = logs["val_loss"] / logs["loss"]
        print(f"Epoch: {epoch}, Val/Train loss ratio: {ratio:.2f}")
        if ratio > self.threshold:
            print("Stopping training due to overfitting...")
            self.model.stop_training = True

model.fit(..., callbacks=[DetectOverfittingCallback(threshold=1.3)])
```


### 6. 🎨 Visualizing Training Progress with Callbacks

You can also use callbacks to create visualizations during training, such as plotting predictions or saving images.

#### Example: Visualizing predictions every few epochs

```python
import numpy as np
import matplotlib.pyplot as plt
import io
from PIL import Image
import imageio

class VisCallback(tf.keras.callbacks.Callback):
    def __init__(self, inputs, ground_truth, display_freq=10, n_samples=10):
        self.inputs = inputs
        self.ground_truth = ground_truth
        self.display_freq = display_freq
        self.n_samples = n_samples
        self.images = []

    def on_epoch_end(self, epoch, logs=None):
        indexes = np.random.choice(len(self.inputs), size=self.n_samples)
        X_test, y_test = self.inputs[indexes], self.ground_truth[indexes]
        predictions = np.argmax(self.model.predict(X_test), axis=1)

        # Plot digits with predictions and ground truth
        plt.figure(figsize=(10, 2))
        for i in range(self.n_samples):
            plt.subplot(1, self.n_samples, i+1)
            plt.imshow(X_test[i].reshape(28, 28), cmap='gray')
            plt.title(f"P:{predictions[i]}, T:{y_test[i]}")
            plt.axis('off')
        plt.suptitle(f"Epoch {epoch}")
        plt.tight_layout()

        # Save plot to buffer
        buf = io.BytesIO()
        plt.savefig(buf, format='png')
        buf.seek(0)
        image = Image.open(buf)
        self.images.append(np.array(image))
        plt.close()

        # Display every display_freq epochs
        if epoch % self.display_freq == 0:
            plt.show()

    def on_train_end(self, logs=None):
        # Create GIF from saved images
        imageio.mimsave('training_progress.gif', self.images, fps=1)

### Usage:
### model.fit(..., callbacks=[VisCallback(x_test, y_test)])
```

This callback samples test data, predicts labels, plots them, and saves images to create an animation of model progress.


### 7. 🔗 Summary and Best Practices

- **Callbacks are powerful tools** to monitor, control, and customize the training process.
- Use **built-in callbacks** like `TensorBoard`, `ModelCheckpoint`, and `EarlyStopping` for common tasks.
- Create **custom callbacks** when you need specialized behavior, such as custom logging or stopping criteria.
- Always pass callbacks as a list to `model.fit()` or other relevant methods.
- Combine multiple callbacks to get the best of all worlds (e.g., logging + checkpointing + early stopping).
- Callbacks can also be used during evaluation and prediction, not just training.


### Final Note

Understanding callbacks is essential for effective model training and experimentation. They help you **track progress**, **save important checkpoints**, **avoid overfitting**, and **visualize results** — all crucial for building robust machine learning models.



<br>

## Questions

#### 1. Which of the following methods are called at the **start and end of each epoch** during training in a Keras callback?  
A) on_epoch_begin  
B) on_epoch_end  
C) on_train_batch_begin  
D) on_train_batch_end  

#### 2. When creating a custom callback by subclassing `tf.keras.callbacks.Callback`, which method(s) would you override to execute code **before and after processing each training batch**?  
A) on_train_batch_begin  
B) on_train_batch_end  
C) on_epoch_begin  
D) on_predict_batch_end  

#### 3. Which of the following are valid ways to use callbacks in Keras model methods?  
A) model.fit(..., callbacks=[...])  
B) model.evaluate(..., callbacks=[...])  
C) model.predict(..., callbacks=[...])  
D) model.compile(..., callbacks=[...])  

#### 4. The `ModelCheckpoint` callback can be configured to:  
A) Save only the model weights instead of the full model  
B) Save the model only when a monitored metric improves  
C) Automatically stop training when validation loss stops improving  
D) Save the model every batch by default  

#### 5. What is the purpose of the `EarlyStopping` callback?  
A) To save the best model weights during training  
B) To stop training when a monitored metric stops improving  
C) To log training metrics to a CSV file  
D) To prevent overfitting by halting training early  

#### 6. Which parameters of `EarlyStopping` control how sensitive it is to improvements in the monitored metric?  
A) patience  
B) min_delta  
C) baseline  
D) save_best_only  

#### 7. When using the `TensorBoard` callback, which of the following are true?  
A) It can visualize the model graph  
B) It tracks metrics like loss and accuracy during training  
C) It automatically saves model checkpoints  
D) It requires specifying a log directory  

#### 8. Which of the following statements about the `logs` argument passed to callback methods is true?  
A) It contains the current batch number  
B) It contains metrics and loss values relevant to the current step  
C) It is always an empty dictionary  
D) It can be used to access validation metrics during `on_epoch_end`  

#### 9. Which callback method(s) would you override to perform an action **right before training starts** and **right after training ends**?  
A) on_train_begin and on_train_end  
B) on_epoch_begin and on_epoch_end  
C) on_train_batch_begin and on_train_batch_end  
D) on_test_begin and on_test_end  

#### 10. What happens if you set `restore_best_weights=True` in the `EarlyStopping` callback?  
A) The model weights are reset to the best epoch’s weights after training stops  
B) The model saves the best weights to disk automatically  
C) Training will continue indefinitely until manually stopped  
D) The callback will ignore the `patience` parameter  

#### 11. Which of the following are true about saving models with `ModelCheckpoint`?  
A) You can save models in both H5 and SavedModel formats  
B) You can include epoch number and metric values in the filename  
C) It can only save the entire model, not just weights  
D) It can save checkpoints at a specified frequency  

#### 12. In a custom callback, what is the effect of setting `self.model.stop_training = True` inside a callback method?  
A) It immediately stops the training loop after the current batch or epoch  
B) It pauses training until manually resumed  
C) It resets the model weights to initial values  
D) It triggers saving the model checkpoint  

#### 13. Which of the following callback methods are called during **evaluation** (testing) but not during training?  
A) on_test_begin  
B) on_train_batch_end  
C) on_test_batch_end  
D) on_predict_end  

#### 14. What is the main difference between `on_train_batch_end` and `on_epoch_end` callback methods?  
A) `on_train_batch_end` is called after every batch, `on_epoch_end` after every epoch  
B) `on_train_batch_end` is called only once per epoch  
C) `on_epoch_end` is called before training starts  
D) `on_train_batch_end` is called only during evaluation  

#### 15. Which of the following are valid reasons to create a **custom callback**?  
A) To log additional information not covered by built-in callbacks  
B) To stop training based on a custom condition  
C) To visualize intermediate predictions during training  
D) To automatically tune hyperparameters during training  

#### 16. Consider a callback that monitors the ratio of validation loss to training loss and stops training if the ratio exceeds a threshold. Which callback method is best suited to implement this logic?  
A) on_epoch_end  
B) on_train_batch_end  
C) on_train_begin  
D) on_predict_end  

#### 17. When using multiple callbacks in `model.fit()`, which of the following statements are true?  
A) All callbacks receive the same event notifications independently  
B) Callbacks can interfere with each other and cause errors  
C) You can combine callbacks like EarlyStopping, ModelCheckpoint, and TensorBoard together  
D) Only one callback can be active at a time  

#### 18. Which of the following are true about the `CSVLogger` callback?  
A) It saves training metrics to a CSV file after each epoch  
B) It can log both training and validation metrics  
C) It automatically stops training if validation loss increases  
D) It requires manual calls to save the CSV file  

#### 19. What is the role of the `update_freq` parameter in the `TensorBoard` callback?  
A) It controls how often logs are written (e.g., every batch or every epoch)  
B) It sets the frequency of model checkpoint saving  
C) It determines how often the training stops for evaluation  
D) It controls the frequency of early stopping checks  

#### 20. Which of the following statements about callback usage during prediction are correct?  
A) Callbacks can be passed to `model.predict()` to monitor prediction progress  
B) `on_predict_batch_begin` and `on_predict_batch_end` methods are called during prediction  
C) Callbacks can modify the predicted output values during prediction  
D) Callbacks are not supported during prediction



<br>

## Answers

#### 1. Which of the following methods are called at the **start and end of each epoch** during training in a Keras callback?  
A) ✓ Called at the start of each epoch  
B) ✓ Called at the end of each epoch  
C) ✗ Called before each batch, not epoch  
D) ✗ Called after each batch, not epoch  

**Correct:** A, B


#### 2. When creating a custom callback by subclassing `tf.keras.callbacks.Callback`, which method(s) would you override to execute code **before and after processing each training batch**?  
A) ✓ Called before processing each training batch  
B) ✓ Called after processing each training batch  
C) ✗ Called at epoch start, not batch  
D) ✗ Related to prediction batches, not training  

**Correct:** A, B


#### 3. Which of the following are valid ways to use callbacks in Keras model methods?  
A) ✓ `fit` accepts callbacks  
B) ✓ `evaluate` accepts callbacks  
C) ✓ `predict` accepts callbacks  
D) ✗ `compile` does not accept callbacks  

**Correct:** A, B, C


#### 4. The `ModelCheckpoint` callback can be configured to:  
A) ✓ Can save only weights  
B) ✓ Can save only when monitored metric improves  
C) ✗ Does not stop training, only saves models  
D) ✗ Default save frequency is per epoch, not per batch  

**Correct:** A, B


#### 5. What is the purpose of the `EarlyStopping` callback?  
A) ✗ It does not save models, only stops training  
B) ✓ Stops training when monitored metric stops improving  
C) ✗ Logging is done by CSVLogger, not EarlyStopping  
D) ✓ Helps prevent overfitting by stopping early  

**Correct:** B, D


#### 6. Which parameters of `EarlyStopping` control how sensitive it is to improvements in the monitored metric?  
A) ✓ `patience` controls how many epochs to wait  
B) ✓ `min_delta` sets minimum improvement threshold  
C) ✓ `baseline` sets minimum acceptable metric value  
D) ✗ `save_best_only` is a ModelCheckpoint parameter  

**Correct:** A, B, C


#### 7. When using the `TensorBoard` callback, which of the following are true?  
A) ✓ Can visualize model graph  
B) ✓ Tracks metrics like loss and accuracy  
C) ✗ Does not save model checkpoints  
D) ✓ Requires specifying a log directory  

**Correct:** A, B, D


#### 8. Which of the following statements about the `logs` argument passed to callback methods is true?  
A) ✗ Batch number is passed as a separate argument, not in logs  
B) ✓ Contains metrics and loss values for current step  
C) ✗ Logs is not always empty; it contains useful info  
D) ✓ Validation metrics are available in `on_epoch_end` logs  

**Correct:** B, D


#### 9. Which callback method(s) would you override to perform an action **right before training starts** and **right after training ends**?  
A) ✓ `on_train_begin` and `on_train_end` are for training start/end  
B) ✗ Called at epoch level, not training start/end  
C) ✗ Called per batch, not training start/end  
D) ✗ Related to evaluation, not training  

**Correct:** A


#### 10. What happens if you set `restore_best_weights=True` in the `EarlyStopping` callback?  
A) ✓ Model weights are restored to best epoch after stopping  
B) ✗ Does not save weights to disk automatically  
C) ✗ Training stops based on patience, not indefinite  
D) ✗ `patience` is still respected  

**Correct:** A


#### 11. Which of the following are true about saving models with `ModelCheckpoint`?  
A) ✓ Supports both H5 and SavedModel formats  
B) ✓ Filename can include epoch and metric values  
C) ✗ Can save weights only if specified  
D) ✓ Can save checkpoints at specified frequency  

**Correct:** A, B, D


#### 12. In a custom callback, what is the effect of setting `self.model.stop_training = True` inside a callback method?  
A) ✓ Immediately stops training after current batch or epoch  
B) ✗ Does not pause training, it stops it  
C) ✗ Does not reset weights  
D) ✗ Does not trigger saving checkpoint automatically  

**Correct:** A


#### 13. Which of the following callback methods are called during **evaluation** (testing) but not during training?  
A) ✓ Called at evaluation start  
B) ✗ Training batch method, not evaluation  
C) ✓ Called after each evaluation batch  
D) ✗ Related to prediction, not evaluation  

**Correct:** A, C


#### 14. What is the main difference between `on_train_batch_end` and `on_epoch_end` callback methods?  
A) ✓ `on_train_batch_end` called after every batch, `on_epoch_end` after every epoch  
B) ✗ `on_train_batch_end` is called every batch, not once per epoch  
C) ✗ `on_epoch_end` is called after epoch, not before training  
D) ✗ `on_train_batch_end` is called during training, not evaluation only  

**Correct:** A


#### 15. Which of the following are valid reasons to create a **custom callback**?  
A) ✓ To log info not covered by built-in callbacks  
B) ✓ To stop training based on custom conditions  
C) ✓ To visualize intermediate predictions during training  
D) ✗ Hyperparameter tuning is usually done outside callbacks  

**Correct:** A, B, C


#### 16. Consider a callback that monitors the ratio of validation loss to training loss and stops training if the ratio exceeds a threshold. Which callback method is best suited to implement this logic?  
A) ✓ `on_epoch_end` is best to access epoch metrics  
B) ✗ Batch end does not have full epoch metrics  
C) ✗ Called only once at training start  
D) ✗ Called after prediction, not training  

**Correct:** A


#### 17. When using multiple callbacks in `model.fit()`, which of the following statements are true?  
A) ✓ All callbacks receive event notifications independently  
B) ✗ Callbacks generally do not interfere if implemented correctly  
C) ✓ You can combine multiple callbacks like EarlyStopping and ModelCheckpoint  
D) ✗ Multiple callbacks can be active simultaneously  

**Correct:** A, C, D


#### 18. Which of the following are true about the `CSVLogger` callback?  
A) ✓ Saves metrics to CSV after each epoch  
B) ✓ Logs both training and validation metrics if available  
C) ✗ Does not stop training on metric changes  
D) ✗ Saves automatically, no manual save needed  

**Correct:** A, B


#### 19. What is the role of the `update_freq` parameter in the `TensorBoard` callback?  
A) ✓ Controls how often logs are written (e.g., per batch or epoch)  
B) ✗ Does not control checkpoint saving frequency  
C) ✗ Does not control evaluation frequency  
D) ✗ Does not control early stopping checks  

**Correct:** A


#### 20. Which of the following statements about callback usage during prediction are correct?  
A) ✓ Callbacks can be passed to `model.predict()`  
B) ✓ `on_predict_batch_begin` and `on_predict_batch_end` are called during prediction  
C) ✗ Callbacks cannot modify predicted outputs  
D) ✗ Callbacks are supported during prediction  

**Correct:** A, B, D