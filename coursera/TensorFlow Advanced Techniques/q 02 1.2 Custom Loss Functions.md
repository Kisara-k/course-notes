## 1.2 Custom Loss Functions

## Questions

#### 1. Which of the following are valid ways to specify a loss function in `model.compile()` in Keras?  
A) Using a string identifier like `'mse'`  
B) Passing a built-in loss function imported from `tensorflow.keras.losses`  
C) Passing a custom Python function that takes `(y_true, y_pred)` as arguments  
D) Passing a compiled model as the loss parameter  


#### 2. What is the primary purpose of a loss function in neural network training?  
A) To generate predictions from input data  
B) To measure the difference between predicted and true values  
C) To update the model’s weights directly  
D) To guide the optimizer in minimizing prediction errors  


#### 3. In the Huber loss function, what happens when the absolute error exceeds the threshold?  
A) The loss behaves like Mean Squared Error (MSE)  
B) The loss behaves like Mean Absolute Error (MAE) scaled by the threshold  
C) The loss becomes zero  
D) The loss increases linearly with the error magnitude  


#### 4. Consider the following snippet inside a custom loss function:  
```python
is_small_error = tf.abs(error) <= threshold  
return tf.where(is_small_error, small_error_loss, big_error_loss)
```  
What does `tf.where` do in this context?  
A) It returns the mean of `small_error_loss` and `big_error_loss`  
B) It selects `small_error_loss` where the error is small, otherwise `big_error_loss`  
C) It applies a conditional mask to the loss tensor  
D) It raises an error if the condition is not met  


#### 5. Why might you want to create a parameterized custom loss function using a closure (a function returning a function)?  
A) To allow dynamic adjustment of hyperparameters like thresholds during model compilation  
B) To avoid using classes for loss functions  
C) To enable the loss function to access model weights  
D) To improve training speed  


#### 6. When subclassing `tf.keras.losses.Loss` to create a custom loss, which method must be overridden?  
A) `__init__`  
B) `call`  
C) `compile`  
D) `fit`  


#### 7. In the context of Siamese networks, what is the role of the contrastive loss function?  
A) To minimize the Euclidean distance between all pairs of feature vectors  
B) To encourage similar inputs to have close feature vectors and dissimilar inputs to have distant feature vectors  
C) To maximize the margin between all pairs of feature vectors regardless of similarity  
D) To compute the cross-entropy between predicted and true labels  


#### 8. The contrastive loss formula is:  
\[
L = Y \times D^2 + (1 - Y) \times \max(\text{margin} - D, 0)^2
\]  
What does the term \(\max(\text{margin} - D, 0)^2\) represent?  
A) The penalty for similar pairs that are too far apart  
B) The penalty for dissimilar pairs that are too close  
C) The squared distance between feature vectors  
D) The margin value squared  


#### 9. Which of the following statements about the margin parameter in contrastive loss is true?  
A) It defines the minimum distance dissimilar pairs should have  
B) It is always set to 0.5 by default  
C) It controls how strictly the model separates dissimilar pairs  
D) It affects the loss for similar pairs only  


#### 10. In the contrastive loss function implementation, why is `K.maximum(margin - y_pred, 0)` used instead of just `margin - y_pred`?  
A) To ensure the loss is never negative  
B) To allow negative distances to contribute to the loss  
C) To clip the distance at zero for similar pairs  
D) To normalize the distance values  


#### 11. Which of the following are advantages of implementing custom loss functions as classes rather than functions?  
A) Easier to manage and store parameters like thresholds or margins  
B) Better integration with Keras’s serialization and saving mechanisms  
C) Faster execution during training  
D) Ability to override multiple methods like `call` and `get_config`  


#### 12. Suppose you have a custom loss function that returns a tensor of shape `(batch_size,)` instead of a scalar. What will happen during training?  
A) Training will proceed normally without issues  
B) TensorFlow will automatically reduce the loss to a scalar by averaging  
C) An error will be raised because the loss must be scalar  
D) The loss will be interpreted as multiple outputs and cause unpredictable behavior  


#### 13. In the Huber loss, why is the loss for small errors defined as \(\frac{1}{2} \times \text{error}^2\) instead of just \(\text{error}^2\)?  
A) To make the loss differentiable at zero  
B) To match the gradient of Mean Absolute Error  
C) To simplify the derivative and stabilize training  
D) To reduce the magnitude of the loss for small errors  


#### 14. When using a custom loss function with parameters, why can’t you just pass the function with parameters directly to `model.compile()`?  
A) Because Keras expects a callable with exactly two arguments `(y_true, y_pred)`  
B) Because functions with parameters are not serializable  
C) Because the optimizer requires a fixed loss function signature  
D) Because the loss function must be a subclass of `Loss`  


#### 15. What is the main difference between Mean Squared Error (MSE) and Huber loss in terms of sensitivity to outliers?  
A) MSE is more sensitive to outliers because it squares the error  
B) Huber loss ignores outliers completely  
C) Huber loss behaves like MSE for large errors  
D) MSE behaves like Mean Absolute Error for small errors  


#### 16. In a Siamese network, why must the two subnetworks share the same weights?  
A) To ensure the feature vectors are comparable in the same space  
B) To reduce the number of parameters in the model  
C) To prevent overfitting  
D) To allow the use of different loss functions for each input  


#### 17. Which of the following are true about the use of `tf.where` in custom loss functions?  
A) It can be used to apply different loss calculations conditionally within a batch  
B) It performs element-wise selection between two tensors based on a boolean mask  
C) It can only be used with scalar tensors  
D) It helps implement piecewise loss functions like Huber loss  


#### 18. If you want to create a custom loss function that depends on a hyperparameter (e.g., margin or threshold), which of the following approaches are valid?  
A) Use a closure that returns a loss function with the hyperparameter fixed  
B) Subclass `tf.keras.losses.Loss` and store the hyperparameter as an instance variable  
C) Pass the hyperparameter as an additional argument to the loss function during training  
D) Hardcode the hyperparameter inside the loss function without flexibility  


#### 19. What is the expected output of a contrastive loss function when the model predicts a distance \(D\) exactly equal to the margin for a dissimilar pair?  
A) Zero loss because the distance meets the margin requirement  
B) Positive loss proportional to the margin squared  
C) Negative loss, which is invalid  
D) Loss equal to \(D^2\)  


#### 20. Which of the following statements about the `call` method in a custom loss class are correct?  
A) It receives `y_true` and `y_pred` as inputs and returns the computed loss  
B) It must return a scalar tensor representing the loss for the batch  
C) It can be used to implement complex conditional logic for loss calculation  
D) It is automatically called during model training when the loss is computed



<br>

## Answers

#### 1. Which of the following are valid ways to specify a loss function in `model.compile()` in Keras?  
A) ✓ Using a string identifier like `'mse'` — Keras supports string aliases for common losses.  
B) ✓ Passing a built-in loss function imported from `tensorflow.keras.losses` — This is a standard way to specify loss.  
C) ✓ Passing a custom Python function that takes `(y_true, y_pred)` as arguments — Custom functions are allowed.  
D) ✗ Passing a compiled model as the loss parameter — This is invalid; loss must be a function or string.  

**Correct:** A,B,C


#### 2. What is the primary purpose of a loss function in neural network training?  
A) ✗ To generate predictions from input data — This is the model’s role, not the loss function.  
B) ✓ To measure the difference between predicted and true values — This is the core purpose of loss.  
C) ✗ To update the model’s weights directly — Optimizers update weights, not loss functions.  
D) ✓ To guide the optimizer in minimizing prediction errors — Loss provides the signal for optimization.  

**Correct:** B,D


#### 3. In the Huber loss function, what happens when the absolute error exceeds the threshold?  
A) ✗ The loss behaves like Mean Squared Error (MSE) — MSE applies for small errors, not large.  
B) ✓ The loss behaves like Mean Absolute Error (MAE) scaled by the threshold — For large errors, Huber loss is linear (MAE style).  
C) ✗ The loss becomes zero — Loss never becomes zero for large errors.  
D) ✓ The loss increases linearly with the error magnitude — This is the MAE-like behavior for large errors.  

**Correct:** B,D


#### 4. Consider the following snippet inside a custom loss function:  
```python
is_small_error = tf.abs(error) <= threshold  
return tf.where(is_small_error, small_error_loss, big_error_loss)
```  
What does `tf.where` do in this context?  
A) ✗ It returns the mean of `small_error_loss` and `big_error_loss` — It does element-wise selection, not averaging.  
B) ✓ It selects `small_error_loss` where the error is small, otherwise `big_error_loss` — This is exactly what `tf.where` does.  
C) ✓ It applies a conditional mask to the loss tensor — It uses the boolean mask to choose values.  
D) ✗ It raises an error if the condition is not met — It does not raise errors for false conditions.  

**Correct:** B,C


#### 5. Why might you want to create a parameterized custom loss function using a closure (a function returning a function)?  
A) ✓ To allow dynamic adjustment of hyperparameters like thresholds during model compilation — Closures enable passing parameters flexibly.  
B) ✓ To avoid using classes for loss functions — Closures are an alternative to class-based losses.  
C) ✗ To enable the loss function to access model weights — Loss functions don’t access weights directly.  
D) ✗ To improve training speed — Closures don’t inherently affect speed.  

**Correct:** A,B


#### 6. When subclassing `tf.keras.losses.Loss` to create a custom loss, which method must be overridden?  
A) ✗ `__init__` — Optional to override, mainly for parameters.  
B) ✓ `call` — This method defines the loss computation and must be overridden.  
C) ✗ `compile` — Not part of the loss class.  
D) ✗ `fit` — Belongs to the model, not loss class.  

**Correct:** B


#### 7. In the context of Siamese networks, what is the role of the contrastive loss function?  
A) ✗ To minimize the Euclidean distance between all pairs of feature vectors — Only similar pairs should be close, not all pairs.  
B) ✓ To encourage similar inputs to have close feature vectors and dissimilar inputs to have distant feature vectors — This is the core idea of contrastive loss.  
C) ✗ To maximize the margin between all pairs of feature vectors regardless of similarity — Margin applies only to dissimilar pairs.  
D) ✗ To compute the cross-entropy between predicted and true labels — Contrastive loss is not cross-entropy.  

**Correct:** B


#### 8. The contrastive loss formula is:  
\[
L = Y \times D^2 + (1 - Y) \times \max(\text{margin} - D, 0)^2
\]  
What does the term \(\max(\text{margin} - D, 0)^2\) represent?  
A) ✗ The penalty for similar pairs that are too far apart — Similar pairs use \(D^2\), not this term.  
B) ✓ The penalty for dissimilar pairs that are too close — This term penalizes dissimilar pairs closer than margin.  
C) ✗ The squared distance between feature vectors — This is \(D^2\), not the max term.  
D) ✗ The margin value squared — Margin is a parameter, not squared here.  

**Correct:** B


#### 9. Which of the following statements about the margin parameter in contrastive loss is true?  
A) ✓ It defines the minimum distance dissimilar pairs should have — Margin sets the desired separation.  
B) ✗ It is always set to 0.5 by default — Margin is user-defined, no fixed default.  
C) ✓ It controls how strictly the model separates dissimilar pairs — Larger margin means stricter separation.  
D) ✗ It affects the loss for similar pairs only — Margin affects dissimilar pairs only.  

**Correct:** A,C


#### 10. In the contrastive loss function implementation, why is `K.maximum(margin - y_pred, 0)` used instead of just `margin - y_pred`?  
A) ✓ To ensure the loss is never negative — Negative values would not make sense as loss.  
B) ✗ To allow negative distances to contribute to the loss — Negative loss is invalid.  
C) ✗ To clip the distance at zero for similar pairs — This term applies to dissimilar pairs only.  
D) ✗ To normalize the distance values — It does not normalize, only clips at zero.  

**Correct:** A


#### 11. Which of the following are advantages of implementing custom loss functions as classes rather than functions?  
A) ✓ Easier to manage and store parameters like thresholds or margins — Classes can hold state cleanly.  
B) ✓ Better integration with Keras’s serialization and saving mechanisms — Classes support config and saving.  
C) ✗ Faster execution during training — Execution speed is similar for both.  
D) ✓ Ability to override multiple methods like `call` and `get_config` — Classes allow more customization.  

**Correct:** A,B,D


#### 12. Suppose you have a custom loss function that returns a tensor of shape `(batch_size,)` instead of a scalar. What will happen during training?  
A) ✗ Training will proceed normally without issues — Loss must be scalar per batch.  
B) ✓ TensorFlow will automatically reduce the loss to a scalar by averaging — Keras reduces batch losses automatically.  
C) ✗ An error will be raised because the loss must be scalar — Usually no error, reduction happens internally.  
D) ✗ The loss will be interpreted as multiple outputs and cause unpredictable behavior — Loss is reduced, so no unpredictable behavior.  

**Correct:** B


#### 13. In the Huber loss, why is the loss for small errors defined as \(\frac{1}{2} \times \text{error}^2\) instead of just \(\text{error}^2\)?  
A) ✗ To make the loss differentiable at zero — Both forms are differentiable at zero.  
B) ✗ To match the gradient of Mean Absolute Error — Huber loss blends MSE and MAE but this is not the reason.  
C) ✓ To simplify the derivative and stabilize training — The factor 1/2 simplifies the gradient to error.  
D) ✗ To reduce the magnitude of the loss for small errors — It scales loss but main reason is gradient simplicity.  

**Correct:** C


#### 14. When using a custom loss function with parameters, why can’t you just pass the function with parameters directly to `model.compile()`?  
A) ✓ Because Keras expects a callable with exactly two arguments `(y_true, y_pred)` — Passing a function with extra parameters breaks this signature.  
B) ✗ Because functions with parameters are not serializable — Serialization is separate issue.  
C) ✗ Because the optimizer requires a fixed loss function signature — Optimizer does not enforce loss signature.  
D) ✗ Because the loss function must be a subclass of `Loss` — Functions can be used without subclassing.  

**Correct:** A


#### 15. What is the main difference between Mean Squared Error (MSE) and Huber loss in terms of sensitivity to outliers?  
A) ✓ MSE is more sensitive to outliers because it squares the error — Squaring amplifies large errors.  
B) ✗ Huber loss ignores outliers completely — It reduces their influence but does not ignore.  
C) ✗ Huber loss behaves like MSE for large errors — It behaves like MAE for large errors.  
D) ✗ MSE behaves like Mean Absolute Error for small errors — MSE always squares errors.  

**Correct:** A


#### 16. In a Siamese network, why must the two subnetworks share the same weights?  
A) ✓ To ensure the feature vectors are comparable in the same space — Shared weights produce consistent embeddings.  
B) ✓ To reduce the number of parameters in the model — Weight sharing reduces parameters.  
C) ✗ To prevent overfitting — Weight sharing may help but is not the main reason.  
D) ✗ To allow the use of different loss functions for each input — Loss is computed on outputs, not per subnetwork.  

**Correct:** A,B


#### 17. Which of the following are true about the use of `tf.where` in custom loss functions?  
A) ✓ It can be used to apply different loss calculations conditionally within a batch — Useful for piecewise losses.  
B) ✓ It performs element-wise selection between two tensors based on a boolean mask — This is its core function.  
C) ✗ It can only be used with scalar tensors — Works element-wise on tensors of any shape.  
D) ✓ It helps implement piecewise loss functions like Huber loss — Perfect for conditional logic in losses.  

**Correct:** A,B,D


#### 18. If you want to create a custom loss function that depends on a hyperparameter (e.g., margin or threshold), which of the following approaches are valid?  
A) ✓ Use a closure that returns a loss function with the hyperparameter fixed — Common and simple approach.  
B) ✓ Subclass `tf.keras.losses.Loss` and store the hyperparameter as an instance variable — Clean and modular.  
C) ✗ Pass the hyperparameter as an additional argument to the loss function during training — Loss functions only accept `(y_true, y_pred)`.  
D) ✓ Hardcode the hyperparameter inside the loss function without flexibility — Valid but inflexible.  

**Correct:** A,B,D


#### 19. What is the expected output of a contrastive loss function when the model predicts a distance \(D\) exactly equal to the margin for a dissimilar pair?  
A) ✓ Zero loss because the distance meets the margin requirement — \(\max(\text{margin} - D, 0) = 0\) so no penalty.  
B) ✗ Positive loss proportional to the margin squared — Loss is zero at margin boundary.  
C) ✗ Negative loss, which is invalid — Loss cannot be negative.  
D) ✗ Loss equal to \(D^2\) — This applies only for similar pairs.  

**Correct:** A


#### 20. Which of the following statements about the `call` method in a custom loss class are correct?  
A) ✓ It receives `y_true` and `y_pred` as inputs and returns the computed loss — This is its purpose.  
B) ✓ It must return a scalar tensor representing the loss for the batch — Loss must be scalar for training.  
C) ✓ It can be used to implement complex conditional logic for loss calculation — Custom logic is allowed.  
D) ✓ It is automatically called during model training when the loss is computed — Keras calls it internally.  

**Correct:** A,B,C,D