## 1.4 Custom Models

[Study Notes](#study-notes)

[Questions](#questions)



### Key Points

#### 1. üõ†Ô∏è Custom Models  
- Custom models subclass `tf.keras.Model` and override the `call()` method to define the forward pass.  
- Custom models allow multiple inputs and outputs, e.g., a Wide and Deep model with two inputs and two outputs.  
- Subclassed models retain built-in features like `model.fit()`, `model.evaluate()`, `model.predict()`, `model.save()`, and `model.summary()`.  

#### 2. ‚öôÔ∏è Limitations of Sequential and Functional APIs  
- Sequential and Functional APIs only support models that are Directed Acyclic Graphs (DAGs) of layers.  
- They cannot easily handle dynamic architectures or recursive networks with loops or changing computation graphs.  

#### 3. üîÑ Residual Networks (ResNets)  
- Residual blocks add the input tensor to the output of a few layers, learning a residual function \( F(x) \) such that output = \( F(x) + x \).  
- Skip connections in residual blocks help mitigate the vanishing gradient problem in deep networks.  

#### 4. üß± Residual Block Implementations  
- `CNNResidual` layer consists of multiple Conv2D layers with ReLU activations and adds the input to the output.  
- `DNNResidual` layer consists of multiple Dense layers with ReLU activations and adds the input to the output.  
- Residual blocks can be stacked and repeated multiple times inside a custom model.  

#### 5. üèóÔ∏è ResNet Architecture Components  
- ResNet starts with a 7x7 Conv2D layer, followed by batch normalization, ReLU activation, and max pooling.  
- Identity blocks in ResNet have two 3x3 Conv2D layers with batch normalization and ReLU, plus a skip connection adding the input to the output.  
- ResNet uses global average pooling before the final dense classification layer.  

#### 6. üñºÔ∏è ResNet Model Details  
- The ResNet model includes layers: Conv2D (7x7), BatchNorm, ReLU, MaxPool, multiple IdentityBlocks, GlobalAveragePooling2D, and a Dense softmax classifier.  
- IdentityBlock adds the input tensor to the output of two Conv2D + BatchNorm + ReLU layers.  
- ResNet is typically compiled with Adam optimizer, sparse categorical crossentropy loss, and accuracy metric.  

#### 7. ‚öôÔ∏è Model Training and Utilities  
- Custom and subclassed models support standard Keras training methods like `fit()`.  
- Models can be saved entirely or by weights using `model.save()` and `model.save_weights()`.  
- Model architecture can be summarized with `model.summary()` and visualized with `tf.keras.utils.plot_model()`.



<br>

## Study Notes

### 1. üõ†Ô∏è Custom Models in TensorFlow/Keras: Introduction and Basics

When building neural networks, TensorFlow/Keras offers several ways to define models. The most common are the **Sequential API** and the **Functional API**. However, these approaches have limitations when you want more flexibility or need to build complex architectures. This is where **custom models** come in, allowing you to subclass the `Model` class and define your own forward pass and architecture.

#### What is a Custom Model?

A **custom model** is a class that inherits from `tf.keras.Model`. Instead of just stacking layers, you explicitly define how data flows through the network by overriding the `call()` method. This approach gives you full control over the model‚Äôs behavior, including how inputs are processed, how layers are connected, and how outputs are generated.

#### Example: Wide and Deep Model

The lecture introduces a **Wide and Deep model**, which combines two types of inputs:

- **Wide input**: A simple input that might represent a single feature or a small set of features.
- **Deep input**: Passed through multiple dense layers to learn complex representations.

Here‚Äôs the conceptual flow:

1. Two inputs: `input_a` (wide) and `input_b` (deep).
2. `input_b` goes through two dense layers with ReLU activation.
3. The output of these dense layers is concatenated with `input_a`.
4. The concatenated vector is passed through a final dense layer to produce the main output.
5. Optionally, an auxiliary output can be created from the deep path to help with training.

This is implemented both in the Functional API and as a subclassed model:

```python
class WideAndDeepModel(Model):
    def __init__(self, units=30, activation='relu', **kwargs):
        super().__init__(**kwargs)
        self.hidden1 = Dense(units, activation=activation)
        self.hidden2 = Dense(units, activation=activation)
        self.main_output = Dense(1)
        self.aux_output = Dense(1)

    def call(self, inputs):
        input_A, input_B = inputs
        hidden1 = self.hidden1(input_B)
        hidden2 = self.hidden2(hidden1)
        concat = concatenate([input_A, hidden2])
        main_output = self.main_output(concat)
        aux_output = self.aux_output(hidden2)
        return main_output, aux_output
```

#### Why Use Custom Models?

- **Flexibility**: You can define complex architectures that are not just simple stacks or DAGs of layers.
- **Modularity**: Easily reuse parts of the model or experiment with new ideas.
- **Control**: Implement custom training loops, multiple outputs, or conditional logic inside the model.


### 2. ‚öôÔ∏è Built-in Features of the Model Class

When you subclass `Model`, you still get many useful features automatically:

- **Training, evaluation, and prediction loops**: Use `model.fit()`, `model.evaluate()`, and `model.predict()` just like with Sequential or Functional models.
- **Saving and serialization**: Save the entire model or just weights with `model.save()` and `model.save_weights()`.
- **Summarization and visualization**: Use `model.summary()` to print the model architecture and `tf.keras.utils.plot_model()` to visualize it.

These features make subclassing models powerful without losing the convenience of Keras.


### 3. üöß Limitations of Sequential and Functional APIs

While Sequential and Functional APIs are great for many models, they have some limitations:

- They only support models that can be represented as **Directed Acyclic Graphs (DAGs)** of layers. This means no loops or cycles.
- They struggle with **dynamic architectures** where the computation graph changes during execution.
- They are not well-suited for **recursive networks** or models with complex control flow.

Subclassing models overcomes these limitations by allowing you to write arbitrary Python code in the `call()` method.


### 4. üîÑ Residual Networks (ResNets): Concept and Motivation

Residual Networks, or **ResNets**, are a breakthrough architecture designed to solve the problem of training very deep neural networks. As networks get deeper, they often suffer from the **vanishing gradient problem**, making training difficult and leading to worse performance.

#### What is a Residual Block?

A **residual block** introduces a shortcut or skip connection that bypasses one or more layers. Instead of learning a direct mapping, the block learns a **residual function** ‚Äî the difference between the input and the desired output.

Mathematically, if the input is \( x \), the block learns \( F(x) \), and the output is:

\[
y = F(x) + x
\]

This helps gradients flow more easily through the network and allows training of much deeper models.


### 5. üß± Implementing Residual Blocks in TensorFlow/Keras

The lecture shows two types of residual blocks implemented as custom layers:

#### CNNResidual (for convolutional layers)

- Contains multiple Conv2D layers with ReLU activations.
- Adds the input tensor to the output of these layers.

```python
class CNNResidual(Layer):
    def __init__(self, layers, filters, **kwargs):
        super().__init__(**kwargs)
        self.hidden = [Conv2D(filters, (3, 3), activation="relu") for _ in range(layers)]

    def call(self, inputs):
        x = inputs
        for layer in self.hidden:
            x = layer(x)
        return inputs + x
```

#### DNNResidual (for dense layers)

- Similar to CNNResidual but uses Dense layers instead of Conv2D.

```python
class DNNResidual(Layer):
    def __init__(self, layers, neurons, **kwargs):
        super().__init__(**kwargs)
        self.hidden = [Dense(neurons, activation="relu") for _ in range(layers)]

    def call(self, inputs):
        x = inputs
        for layer in self.hidden:
            x = layer(x)
        return inputs + x
```

#### Combining Residual Blocks in a Model

You can stack these residual blocks inside a custom model:

```python
class MyResidual(Model):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.hidden1 = Dense(30, activation="relu")
        self.block1 = CNNResidual(2, 32)
        self.block2 = DNNResidual(2, 64)
        self.out = Dense(1)

    def call(self, inputs):
        x = self.hidden1(inputs)
        x = self.block1(x)
        for _ in range(3):  # Repeat block2 three times
            x = self.block2(x)
        return self.out(x)
```


### 6. üèóÔ∏è ResNet Architecture: Building Blocks and Structure

ResNet is a specific architecture built from residual blocks, designed primarily for image recognition tasks.

#### Key Components of ResNet:

- **7x7 Conv2D layer**: The first layer uses a large 7x7 convolution to capture broad features.
- **Batch Normalization**: Normalizes activations to speed up training and improve stability.
- **Max Pooling**: Reduces spatial dimensions while keeping important features.
- **Identity Blocks**: Residual blocks that keep the input and output dimensions the same.
- **Global Average Pooling**: Reduces each feature map to a single number by averaging, reducing parameters before the final classification layer.
- **Fully Connected Layer**: The final dense layer outputs class probabilities.

#### Identity Block Example

An identity block is a residual block where the input and output have the same shape, so the input can be added directly to the output of the convolutional layers.

```python
class IdentityBlock(tf.keras.Model):
    def __init__(self, filters, kernel_size):
        super(IdentityBlock, self).__init__()
        self.conv1 = tf.keras.layers.Conv2D(filters, kernel_size, padding='same')
        self.bn1 = tf.keras.layers.BatchNormalization()
        self.conv2 = tf.keras.layers.Conv2D(filters, kernel_size, padding='same')
        self.bn2 = tf.keras.layers.BatchNormalization()
        self.act = tf.keras.layers.Activation('relu')
        self.add = tf.keras.layers.Add()

    def call(self, input_tensor):
        x = self.conv1(input_tensor)
        x = self.bn1(x)
        x = self.act(x)
        x = self.conv2(x)
        x = self.bn2(x)
        x = self.add([x, input_tensor])  # Add skip connection
        x = self.act(x)
        return x
```


### 7. üñºÔ∏è Complete ResNet Model Example

Putting it all together, a simplified ResNet model looks like this:

```python
class ResNet(tf.keras.Model):
    def __init__(self, num_classes):
        super(ResNet, self).__init__()
        self.conv = tf.keras.layers.Conv2D(64, 7, padding='same')
        self.bn = tf.keras.layers.BatchNormalization()
        self.act = tf.keras.layers.Activation('relu')
        self.max_pool = tf.keras.layers.MaxPool2D((3, 3))
        self.id1a = IdentityBlock(64, 3)
        self.id1b = IdentityBlock(64, 3)
        self.global_pool = tf.keras.layers.GlobalAveragePooling2D()
        self.classifier = tf.keras.layers.Dense(num_classes, activation='softmax')

    def call(self, inputs):
        x = self.conv(inputs)
        x = self.bn(x)
        x = self.act(x)
        x = self.max_pool(x)
        x = self.id1a(x)
        x = self.id1b(x)
        x = self.global_pool(x)
        return self.classifier(x)
```

#### Training the ResNet

- Compile the model with an optimizer (e.g., Adam), loss function (e.g., sparse categorical crossentropy), and metrics (e.g., accuracy).
- Load and preprocess a dataset (e.g., MNIST).
- Train the model using `model.fit()`.


### Summary

- **Custom models** allow you to build flexible architectures by subclassing `tf.keras.Model`.
- You can combine multiple inputs, outputs, and complex layer connections.
- **Residual networks** solve the problem of training very deep networks by adding skip connections.
- Residual blocks can be implemented for both convolutional and dense layers.
- The **ResNet architecture** uses identity blocks, batch normalization, and pooling layers to build deep, effective models for image classification.
- TensorFlow/Keras provides tools to easily build, train, save, and visualize these models.



<br>

## Questions

#### 1. What are the main advantages of subclassing `tf.keras.Model` over using the Sequential or Functional API?  
A) Ability to implement dynamic control flow inside the model  
B) Automatic generation of model summary without any code  
C) Support for models with loops or recursive connections  
D) Simplifies saving and loading models compared to Functional API  


#### 2. In the Wide and Deep model example, why is the auxiliary output added from the deep path?  
A) To provide an additional training signal that can improve convergence  
B) To reduce the number of parameters in the model  
C) To allow the model to predict multiple targets simultaneously  
D) To replace the main output during inference  


#### 3. Which of the following statements about the `call()` method in a subclassed model are true?  
A) It defines the forward pass of the model  
B) It must always return a single tensor output  
C) It can accept multiple inputs and return multiple outputs  
D) It is automatically called during `model.fit()` and `model.predict()`  


#### 4. What is a key limitation of the Sequential API that the Functional API overcomes?  
A) Sequential API cannot handle models with multiple inputs or outputs  
B) Sequential API cannot be saved or serialized  
C) Sequential API does not support activation functions  
D) Sequential API cannot be used for convolutional layers  


#### 5. Why are residual connections important in very deep neural networks?  
A) They prevent the network from overfitting by reducing parameters  
B) They help mitigate the vanishing gradient problem by allowing gradients to flow directly  
C) They increase the depth of the network without increasing computation  
D) They allow the network to learn identity mappings more easily  


#### 6. In the `CNNResidual` layer, what does the addition of `inputs + x` represent?  
A) Concatenation of input and output features  
B) A skip connection that adds the input tensor to the output of convolutional layers  
C) A method to normalize the output features  
D) A way to reduce the dimensionality of the output  


#### 7. Which of the following are true about the Functional API in TensorFlow/Keras?  
A) It represents models as Directed Acyclic Graphs (DAGs) of layers  
B) It supports dynamic loops and recursive connections natively  
C) It allows multiple inputs and outputs in a model  
D) It requires subclassing `Model` to define the forward pass  


#### 8. What is the primary role of Batch Normalization in ResNet blocks?  
A) To add non-linearity to the model  
B) To normalize activations and stabilize training  
C) To reduce the number of parameters in the model  
D) To perform dimensionality reduction before convolution  


#### 9. In the IdentityBlock class, why is the input tensor added to the output of the convolutional layers?  
A) To implement a residual connection that helps gradient flow  
B) To concatenate features for richer representations  
C) To enforce the output to have the same shape as the input  
D) To apply dropout regularization  


#### 10. Which of the following are reasons to use Global Average Pooling before the final dense layer in ResNet?  
A) To reduce the spatial dimensions to a single value per feature map  
B) To reduce the number of parameters and prevent overfitting  
C) To increase the spatial resolution of feature maps  
D) To replace fully connected layers with convolutional layers  


#### 11. When subclassing a model, which of the following are true about the `__init__` and `call` methods?  
A) `__init__` is used to define layers and initialize variables  
B) `call` defines the computation performed on inputs  
C) `call` must be called explicitly by the user during training  
D) Layers defined in `__init__` are automatically tracked by Keras  


#### 12. What is a key difference between the `CNNResidual` and `DNNResidual` classes shown in the lecture?  
A) `CNNResidual` uses convolutional layers, while `DNNResidual` uses dense layers  
B) `CNNResidual` adds inputs and outputs by concatenation, `DNNResidual` uses addition  
C) `DNNResidual` is only used for image data, `CNNResidual` for tabular data  
D) Both implement residual connections but differ in layer types  


#### 13. Which of the following statements about the ResNet architecture are correct?  
A) It uses large kernel convolutions (e.g., 7x7) at the beginning to capture broad features  
B) It relies solely on dense layers without convolutional layers  
C) It stacks multiple residual blocks to enable very deep networks  
D) It uses max pooling to reduce spatial dimensions early in the network  


#### 14. Why might the Functional and Sequential APIs be insufficient for some advanced neural network architectures?  
A) They cannot handle models with multiple outputs  
B) They do not support models with cycles or dynamic computation graphs  
C) They cannot be used with convolutional layers  
D) They require manual implementation of training loops  


#### 15. In the Wide and Deep model example, what is the purpose of concatenating the wide input with the output of the deep layers?  
A) To combine simple features with learned complex representations  
B) To reduce the dimensionality of the input data  
C) To enforce the model to use only linear transformations  
D) To separate the training of wide and deep parts of the model  


#### 16. Which of the following are true about saving and loading subclassed models?  
A) You can save the entire model architecture and weights using `model.save()`  
B) Subclassed models cannot be saved or loaded in TensorFlow  
C) You can save only the weights using `model.save_weights()`  
D) The Functional API models cannot be saved, only subclassed models can  


#### 17. What is the effect of repeating a residual block multiple times in a model, as shown in the `MyResidual` example?  
A) It increases the model‚Äôs depth and capacity to learn complex features  
B) It reduces the number of parameters by weight sharing  
C) It helps the model learn hierarchical feature representations  
D) It prevents overfitting by limiting the number of layers  


#### 18. In the ResNet example, what is the role of the `Activation('relu')` layer after batch normalization?  
A) To introduce non-linearity so the network can learn complex functions  
B) To normalize the output of the convolutional layers  
C) To reduce the dimensionality of the feature maps  
D) To perform element-wise addition with the input tensor  


#### 19. Which of the following are true about the training process of a ResNet model on a dataset like MNIST?  
A) The model can be compiled with an optimizer like Adam and a loss like sparse categorical crossentropy  
B) The dataset must be preprocessed and batched before training  
C) ResNet cannot be trained on simple datasets like MNIST due to its complexity  
D) The model‚Äôs accuracy can be monitored using metrics during training  


#### 20. What are the benefits of using subclassing to build modular architectures in TensorFlow/Keras?  
A) It allows quick experimentation with new ideas and architectures  
B) It forces the use of only sequential layers  
C) It enables control over the forward pass and custom training logic  
D) It restricts the model to only DAG structures



<br>

## Answers

#### 1. What are the main advantages of subclassing `tf.keras.Model` over using the Sequential or Functional API?  
A) ‚úì Allows dynamic control flow inside the model, which Sequential/Functional APIs cannot handle  
B) ‚úó Model summary is available in all APIs, not unique to subclassing  
C) ‚úì Supports loops or recursive connections not possible in Sequential/Functional APIs  
D) ‚úó Saving/loading is equally supported in Functional API  

**Correct:** A, C


#### 2. In the Wide and Deep model example, why is the auxiliary output added from the deep path?  
A) ‚úì Provides an additional training signal that can improve convergence  
B) ‚úó Auxiliary output does not reduce parameters; it adds complexity  
C) ‚úì Allows predicting multiple targets or helps regularize training  
D) ‚úó Auxiliary output is not a replacement for main output during inference  

**Correct:** A, C


#### 3. Which of the following statements about the `call()` method in a subclassed model are true?  
A) ‚úì Defines the forward pass of the model  
B) ‚úó Can return multiple outputs, not limited to one  
C) ‚úì Can accept multiple inputs and return multiple outputs  
D) ‚úì Automatically called during `fit()`, `predict()`, etc.  

**Correct:** A, C, D


#### 4. What is a key limitation of the Sequential API that the Functional API overcomes?  
A) ‚úì Sequential API cannot handle multiple inputs or outputs; Functional API can  
B) ‚úó Both APIs support saving and serialization  
C) ‚úó Sequential API supports activation functions  
D) ‚úó Sequential API supports convolutional layers  

**Correct:** A


#### 5. Why are residual connections important in very deep neural networks?  
A) ‚úó They do not primarily reduce parameters or prevent overfitting  
B) ‚úì Help mitigate vanishing gradients by allowing gradients to flow directly  
C) ‚úó They increase depth but also increase computation  
D) ‚úì Allow the network to learn identity mappings more easily  

**Correct:** B, D


#### 6. In the `CNNResidual` layer, what does the addition of `inputs + x` represent?  
A) ‚úó It is addition, not concatenation  
B) ‚úì Skip connection adding input tensor to output of conv layers  
C) ‚úó Not a normalization method  
D) ‚úó Does not reduce dimensionality; shapes must match  

**Correct:** B


#### 7. Which of the following are true about the Functional API in TensorFlow/Keras?  
A) ‚úì Models are represented as DAGs of layers  
B) ‚úó Does not support dynamic loops or recursion natively  
C) ‚úì Supports multiple inputs and outputs  
D) ‚úó Does not require subclassing to define forward pass  

**Correct:** A, C


#### 8. What is the primary role of Batch Normalization in ResNet blocks?  
A) ‚úó It does not add non-linearity (activation does that)  
B) ‚úì Normalizes activations to stabilize and speed up training  
C) ‚úó Does not reduce parameters  
D) ‚úó Does not perform dimensionality reduction  

**Correct:** B


#### 9. In the IdentityBlock class, why is the input tensor added to the output of the convolutional layers?  
A) ‚úì Implements residual connection to help gradient flow  
B) ‚úó It is addition, not concatenation  
C) ‚úì Ensures output shape matches input for addition  
D) ‚úó Not related to dropout  

**Correct:** A, C


#### 10. Which of the following are reasons to use Global Average Pooling before the final dense layer in ResNet?  
A) ‚úì Reduces spatial dimensions to one value per feature map  
B) ‚úì Reduces parameters and helps prevent overfitting  
C) ‚úó Does not increase spatial resolution  
D) ‚úó Does not replace fully connected layers with conv layers  

**Correct:** A, B


#### 11. When subclassing a model, which of the following are true about the `__init__` and `call` methods?  
A) ‚úì `__init__` defines layers and initializes variables  
B) ‚úì `call` defines computation on inputs  
C) ‚úó `call` is called automatically, not manually by user  
D) ‚úì Layers defined in `__init__` are tracked by Keras automatically  

**Correct:** A, B, D


#### 12. What is a key difference between the `CNNResidual` and `DNNResidual` classes shown in the lecture?  
A) ‚úì `CNNResidual` uses Conv2D layers; `DNNResidual` uses Dense layers  
B) ‚úó Both use addition, not concatenation  
C) ‚úó CNNResidual is for images; DNNResidual is for dense data, not vice versa  
D) ‚úì Both implement residual connections but differ in layer types  

**Correct:** A, D


#### 13. Which of the following statements about the ResNet architecture are correct?  
A) ‚úì Uses large 7x7 convolutions at the start to capture broad features  
B) ‚úó Uses convolutional layers extensively, not just dense layers  
C) ‚úì Stacks multiple residual blocks to build very deep networks  
D) ‚úì Uses max pooling early to reduce spatial dimensions  

**Correct:** A, C, D


#### 14. Why might the Functional and Sequential APIs be insufficient for some advanced neural network architectures?  
A) ‚úó Both support multiple outputs  
B) ‚úì Cannot handle cycles or dynamic computation graphs  
C) ‚úó Both support convolutional layers  
D) ‚úó Training loops can be customized but are available by default  

**Correct:** B


#### 15. In the Wide and Deep model example, what is the purpose of concatenating the wide input with the output of the deep layers?  
A) ‚úì Combines simple features with complex learned representations  
B) ‚úó Does not reduce dimensionality; concatenation increases it  
C) ‚úó Does not enforce linear transformations only  
D) ‚úó Does not separate training of wide and deep parts  

**Correct:** A


#### 16. Which of the following are true about saving and loading subclassed models?  
A) ‚úì Entire model architecture and weights can be saved with `model.save()`  
B) ‚úó Subclassed models can be saved and loaded, though with some caveats  
C) ‚úì Weights can be saved separately with `model.save_weights()`  
D) ‚úó Functional API models can also be saved  

**Correct:** A, C


#### 17. What is the effect of repeating a residual block multiple times in a model, as shown in the `MyResidual` example?  
A) ‚úì Increases model depth and capacity to learn complex features  
B) ‚úó Does not reduce parameters by weight sharing (no weight sharing shown)  
C) ‚úì Helps learn hierarchical feature representations  
D) ‚úó Does not inherently prevent overfitting  

**Correct:** A, C


#### 18. In the ResNet example, what is the role of the `Activation('relu')` layer after batch normalization?  
A) ‚úì Introduces non-linearity for learning complex functions  
B) ‚úó BatchNorm normalizes, activation adds non-linearity  
C) ‚úó Does not reduce dimensionality  
D) ‚úó Does not perform addition with input tensor  

**Correct:** A


#### 19. Which of the following are true about the training process of a ResNet model on a dataset like MNIST?  
A) ‚úì Can be compiled with Adam optimizer and sparse categorical crossentropy loss  
B) ‚úì Dataset must be preprocessed and batched before training  
C) ‚úó ResNet can be trained on MNIST despite its complexity  
D) ‚úì Accuracy can be monitored using metrics during training  

**Correct:** A, B, D


#### 20. What are the benefits of using subclassing to build modular architectures in TensorFlow/Keras?  
A) ‚úì Allows quick experimentation with new ideas and architectures  
B) ‚úó Does not force use of only sequential layers; it enables arbitrary architectures  
C) ‚úì Enables control over forward pass and custom training logic  
D) ‚úó Does not restrict model to DAG structures; supports dynamic graphs  

**Correct:** A, C