## 1.2 Sentiment Analysis with Naive Bayes

## Study Notes

### 1. 🧠 Introduction to Sentiment Analysis and Naive Bayes

Sentiment analysis is the process of determining the emotional tone behind a body of text, such as tweets, reviews, or comments. It helps us classify text as positive, negative, or neutral based on the words used. One popular and simple method for sentiment analysis is the **Naive Bayes classifier**, a probabilistic model that uses Bayes’ theorem to predict the sentiment of a text.

Naive Bayes is called “naive” because it assumes that all features (words) in the text are independent of each other, which is often not true in natural language but still works surprisingly well in practice. This method is fast, easy to implement, and effective as a baseline for text classification tasks.


### 2. 📊 Understanding Probabilities and Bayes’ Rule in Sentiment Analysis

#### What are probabilities here?

- **P(Positive)**: The probability that a randomly chosen tweet is positive.
- **P(Negative)**: The probability that a randomly chosen tweet is negative.
- **P(happy)**: The probability that a tweet contains the word “happy.”
- **P(Positive | happy)**: The probability that a tweet is positive given that it contains the word “happy.”

#### Example from the corpus:

- Total tweets (N) = 20
- Positive tweets (Npos) = 13
- Tweets containing “happy” (Nhappy) = 4
- Positive tweets containing “happy” = 3

From this, we calculate:

- \( P(Positive) = \frac{13}{20} = 0.65 \)
- \( P(Negative) = 1 - 0.65 = 0.35 \)
- \( P(happy) = \frac{4}{20} = 0.2 \)
- \( P(Positive | happy) = \frac{3}{4} = 0.75 \)
- \( P(happy | Positive) = \frac{3}{13} \approx 0.231 \)

#### Bayes’ Rule

Bayes’ rule connects these probabilities:

\[
P(Positive | happy) = \frac{P(happy | Positive) \times P(Positive)}{P(happy)}
\]

This formula allows us to update our belief about the sentiment of a tweet given that it contains a specific word.


### 3. 🔍 Applying Bayes’ Rule: Example and Intuition

Imagine you know:

- 25% of positive tweets contain “happy” → \( P(happy | Positive) = 0.25 \)
- 13% of all tweets contain “happy” → \( P(happy) = 0.13 \)
- 40% of tweets are positive → \( P(Positive) = 0.40 \)

You see a tweet with the word “happy.” What is the probability it’s positive?

Using Bayes’ rule:

\[
P(Positive | happy) = \frac{0.25 \times 0.40}{0.13} \approx 0.77
\]

This means there’s a 77% chance the tweet is positive given it contains “happy.”


### 4. 📝 Naive Bayes for Sentiment Analysis: How It Works

Naive Bayes classifies a tweet by calculating the probability that the tweet belongs to each class (positive or negative) based on the words it contains.

#### Key steps:

- **Training data**: A set of tweets labeled as positive or negative.
- **Word probabilities**: Calculate \( P(word | class) \), the probability of each word appearing in positive or negative tweets.
- **Assumption**: Words are independent (naive assumption).
- **Classification**: For a new tweet, multiply the probabilities of each word given the class, then multiply by the prior probability of the class.

Example:

| Word     | P(word|Pos) | P(word|Neg) |
|----------|-------------|-------------|
| happy    | 0.24        | 0.25        |
| because  | 0.24        | 0.25        |
| learning | 0.15        | 0.08        |
| sad      | 0.08        | 0.17        |
| not      | 0.08        | 0.08        |

For the tweet “I am happy today; I am learning,” we calculate the product of probabilities for each word under each class and compare.


### 5. 🧮 Laplacian Smoothing: Handling Zero Probabilities

Sometimes, a word in a new tweet might not appear in the training data for a class, causing \( P(word | class) = 0 \). This zero probability would make the entire product zero, which is problematic.

**Laplacian smoothing** fixes this by adding 1 to the count of every word in every class before calculating probabilities. This ensures no probability is zero.

Formula:

\[
P(w_i | class) = \frac{freq(w_i, class) + 1}{N_{class} + V}
\]

- \( freq(w_i, class) \): frequency of word \( w_i \) in class
- \( N_{class} \): total number of words in class
- \( V \): number of unique words in the vocabulary

This smoothing makes the model more robust to unseen words.


### 6. 📈 Log Likelihood: Avoiding Underflow and Simplifying Calculations

When multiplying many small probabilities, the product can become extremely small (underflow), causing computational issues.

To avoid this, we use **logarithms**:

- Instead of multiplying probabilities, we sum their logarithms.
- Logarithms turn products into sums, which are easier and more stable to compute.

For a tweet with words \( w_1, w_2, ..., w_m \), the log likelihood for class \( c \) is:

\[
\log P(c) + \sum_{i=1}^m \log P(w_i | c)
\]

We calculate this for each class and pick the class with the highest log likelihood.


### 7. 🏋️ Training a Naive Bayes Model: Step-by-Step

Training involves several clear steps:

1. **Collect and annotate data**: Gather tweets labeled as positive or negative.
2. **Preprocess tweets**: Convert to lowercase, remove punctuation, URLs, stop words, and apply stemming/tokenization.
3. **Count word frequencies**: Calculate how often each word appears in positive and negative tweets.
4. **Calculate probabilities**: Compute \( P(w | pos) \) and \( P(w | neg) \) using counts and Laplacian smoothing.
5. **Calculate prior probabilities**: \( P(pos) \) and \( P(neg) \) based on the number of tweets in each class.
6. **Calculate log prior**: \( \log \frac{P(pos)}{P(neg)} \) for use in classification.


### 8. 🧪 Testing and Using Naive Bayes for Prediction

To predict the sentiment of a new tweet:

- Preprocess the tweet (tokenize, clean).
- For each word, look up \( \log P(w | pos) \) and \( \log P(w | neg) \).
- Sum these log probabilities and add the log prior.
- Compare the sums for positive and negative classes.
- The class with the higher sum is the predicted sentiment.

If a word is unseen, Laplacian smoothing ensures it has a small but non-zero probability.


### 9. 🌍 Applications of Naive Bayes

Naive Bayes is widely used because it is:

- **Simple and fast**: Good for large datasets.
- **Robust**: Performs well even with the naive independence assumption.

Common applications include:

- **Sentiment analysis**: Classifying tweets, reviews, or comments.
- **Spam filtering**: Detecting unwanted emails.
- **Author identification**: Guessing the author of a text.
- **Information retrieval**: Ranking documents by relevance.
- **Word sense disambiguation**: Determining the meaning of ambiguous words.


### 10. ⚠️ Assumptions and Limitations of Naive Bayes

#### Independence Assumption

Naive Bayes assumes that words appear independently of each other given the class. This is often false in natural language because words influence each other (e.g., “not happy” vs. “happy”).

#### Relative Frequency

The model relies heavily on the relative frequency of words and classes in the training data. If the dataset is unbalanced, predictions may be biased.


### 11. 🐞 Error Analysis: Common Sources of Mistakes

Naive Bayes can make errors due to:

- **Ignoring word order**: “I am happy because I am not sad” vs. “I am not happy because I am sad” have very different meanings but similar word sets.
- **Removing punctuation and stop words**: Sometimes punctuation or negations like “not” are crucial for meaning.
- **Adversarial attacks**: Sarcasm, irony, and euphemisms can confuse the model.
- **Preprocessing errors**: Over-aggressive cleaning can remove important words or context.

Example:

- Tweet: “My beloved grandmother :(” → After preprocessing: [belov, grandmoth] loses the sad emoticon, which carries sentiment.
- Tweet: “This is not good” → After removing “not,” the sentiment flips incorrectly.


### Summary

Naive Bayes is a foundational probabilistic method for sentiment analysis that uses Bayes’ theorem to classify text based on word probabilities. It is simple, fast, and surprisingly effective despite its naive assumptions. Key concepts include conditional probabilities, Laplacian smoothing to handle zero counts, and log likelihood to avoid computational issues. While it has limitations, especially regarding word order and context, it remains a powerful baseline for many NLP tasks.



<br>

## Key Points

#### 1. 📊 Probabilities in Sentiment Analysis  
- \( P(Positive) = \frac{\text{Number of positive tweets}}{\text{Total tweets}} \)  
- \( P(Negative) = 1 - P(Positive) \)  
- \( P(happy) = \frac{\text{Number of tweets containing "happy"}}{\text{Total tweets}} \)  
- \( P(Positive | happy) = \frac{\text{Number of positive tweets containing "happy"}}{\text{Number of tweets containing "happy"}} \)  
- \( P(happy | Positive) = \frac{\text{Number of positive tweets containing "happy"}}{\text{Number of positive tweets}} \)  

#### 2. 🧮 Bayes’ Rule  
- Bayes’ rule formula:  
  \[
  P(Positive | happy) = \frac{P(happy | Positive) \times P(Positive)}{P(happy)}
  \]  
- Bayes’ rule allows computing the probability of a class given evidence using conditional probabilities and priors.  
- The result of Bayes’ rule must be a probability between 0 and 1.  

#### 3. 📝 Naive Bayes Classifier Basics  
- Naive Bayes assumes **word independence** given the class.  
- Classification is done by calculating:  
  \[
  P(class) \times \prod_{i=1}^m P(word_i | class)
  \]  
- The class with the highest probability is chosen as the prediction.  

#### 4. ➕ Laplacian Smoothing  
- Laplacian smoothing adds 1 to word counts to avoid zero probabilities:  
  \[
  P(w_i | class) = \frac{freq(w_i, class) + 1}{N_{class} + V}
  \]  
- \( N_{class} \) = total word count in class, \( V \) = vocabulary size (unique words).  
- Prevents zero probability for unseen words during classification.  

#### 5. 📉 Log Likelihood  
- Multiplying many small probabilities can cause underflow; log likelihood solves this by summing logs:  
  \[
  \log P(class) + \sum_{i=1}^m \log P(word_i | class)
  \]  
- The class with the highest log likelihood is selected.  

#### 6. 🏋️ Training Naive Bayes Steps  
- Collect and annotate dataset with positive and negative labels.  
- Preprocess tweets: lowercase, remove punctuation, URLs, stop words, and tokenize.  
- Count word frequencies per class.  
- Calculate \( P(w | class) \) with Laplacian smoothing.  
- Calculate prior probabilities \( P(pos) \) and \( P(neg) \).  
- Compute log prior: \( \log \frac{P(pos)}{P(neg)} \).  

#### 7. 🧪 Testing Naive Bayes  
- Preprocess new tweets similarly to training data.  
- Sum log probabilities of words for each class plus log prior.  
- Predict class with higher sum.  
- Words not seen in training get smoothed probabilities, not zero.  

#### 8. 🌍 Applications of Naive Bayes  
- Sentiment analysis (e.g., tweets, reviews).  
- Spam filtering.  
- Author identification.  
- Information retrieval.  
- Word sense disambiguation.  

#### 9. ⚠️ Naive Bayes Assumptions and Limitations  
- Assumes **independence of words** given class (often false in NLP).  
- Relies on relative frequency of words and classes in training data.  
- Sensitive to unbalanced datasets.  

#### 10. 🐞 Common Sources of Errors  
- Ignoring **word order** changes meaning (e.g., negations).  
- Removing punctuation and stop words can remove important sentiment cues.  
- Sarcasm, irony, and euphemisms confuse the model.  
- Adversarial attacks can mislead classification.