## 1.2 Sentiment Analysis with Naive Bayes



### Key Points



#### 1. ğŸ“Š Probabilities in Sentiment Analysis  
- \( P(Positive) = \frac{\text{Number of positive tweets}}{\text{Total tweets}} \)  
- \( P(Negative) = 1 - P(Positive) \)  
- \( P(happy) = \frac{\text{Number of tweets containing "happy"}}{\text{Total tweets}} \)  
- \( P(Positive | happy) = \frac{\text{Number of positive tweets containing "happy"}}{\text{Number of tweets containing "happy"}} \)  
- \( P(happy | Positive) = \frac{\text{Number of positive tweets containing "happy"}}{\text{Number of positive tweets}} \)  

#### 2. ğŸ§® Bayesâ€™ Rule  
- Bayesâ€™ rule formula:  
  \[
  P(Positive | happy) = \frac{P(happy | Positive) \times P(Positive)}{P(happy)}
  \]  
- Bayesâ€™ rule allows computing the probability of a class given evidence using conditional probabilities and priors.  
- The result of Bayesâ€™ rule must be a probability between 0 and 1.  

#### 3. ğŸ“ Naive Bayes Classifier Basics  
- Naive Bayes assumes **word independence** given the class.  
- Classification is done by calculating:  
  \[
  P(class) \times \prod_{i=1}^m P(word_i | class)
  \]  
- The class with the highest probability is chosen as the prediction.  

#### 4. â• Laplacian Smoothing  
- Laplacian smoothing adds 1 to word counts to avoid zero probabilities:  
  \[
  P(w_i | class) = \frac{freq(w_i, class) + 1}{N_{class} + V}
  \]  
- \( N_{class} \) = total word count in class, \( V \) = vocabulary size (unique words).  
- Prevents zero probability for unseen words during classification.  

#### 5. ğŸ“‰ Log Likelihood  
- Multiplying many small probabilities can cause underflow; log likelihood solves this by summing logs:  
  \[
  \log P(class) + \sum_{i=1}^m \log P(word_i | class)
  \]  
- The class with the highest log likelihood is selected.  

#### 6. ğŸ‹ï¸ Training Naive Bayes Steps  
- Collect and annotate dataset with positive and negative labels.  
- Preprocess tweets: lowercase, remove punctuation, URLs, stop words, and tokenize.  
- Count word frequencies per class.  
- Calculate \( P(w | class) \) with Laplacian smoothing.  
- Calculate prior probabilities \( P(pos) \) and \( P(neg) \).  
- Compute log prior: \( \log \frac{P(pos)}{P(neg)} \).  

#### 7. ğŸ§ª Testing Naive Bayes  
- Preprocess new tweets similarly to training data.  
- Sum log probabilities of words for each class plus log prior.  
- Predict class with higher sum.  
- Words not seen in training get smoothed probabilities, not zero.  

#### 8. ğŸŒ Applications of Naive Bayes  
- Sentiment analysis (e.g., tweets, reviews).  
- Spam filtering.  
- Author identification.  
- Information retrieval.  
- Word sense disambiguation.  

#### 9. âš ï¸ Naive Bayes Assumptions and Limitations  
- Assumes **independence of words** given class (often false in NLP).  
- Relies on relative frequency of words and classes in training data.  
- Sensitive to unbalanced datasets.  

#### 10. ğŸ Common Sources of Errors  
- Ignoring **word order** changes meaning (e.g., negations).  
- Removing punctuation and stop words can remove important sentiment cues.  
- Sarcasm, irony, and euphemisms confuse the model.  
- Adversarial attacks can mislead classification.  



<br>

## Study Notes





### 1. ğŸ§  Introduction to Sentiment Analysis and Naive Bayes

Sentiment analysis is the process of determining the emotional tone behind a body of text, such as tweets, reviews, or comments. It helps us classify text as positive, negative, or neutral based on the words used. One popular and simple method for sentiment analysis is the **Naive Bayes classifier**, a probabilistic model that uses Bayesâ€™ theorem to predict the sentiment of a text.

Naive Bayes is called â€œnaiveâ€ because it assumes that all features (words) in the text are independent of each other, which is often not true in natural language but still works surprisingly well in practice. This method is fast, easy to implement, and effective as a baseline for text classification tasks.


### 2. ğŸ“Š Understanding Probabilities and Bayesâ€™ Rule in Sentiment Analysis

#### What are probabilities here?

- **P(Positive)**: The probability that a randomly chosen tweet is positive.
- **P(Negative)**: The probability that a randomly chosen tweet is negative.
- **P(happy)**: The probability that a tweet contains the word â€œhappy.â€
- **P(Positive | happy)**: The probability that a tweet is positive given that it contains the word â€œhappy.â€

#### Example from the corpus:

- Total tweets (N) = 20
- Positive tweets (Npos) = 13
- Tweets containing â€œhappyâ€ (Nhappy) = 4
- Positive tweets containing â€œhappyâ€ = 3

From this, we calculate:

- \( P(Positive) = \frac{13}{20} = 0.65 \)
- \( P(Negative) = 1 - 0.65 = 0.35 \)
- \( P(happy) = \frac{4}{20} = 0.2 \)
- \( P(Positive | happy) = \frac{3}{4} = 0.75 \)
- \( P(happy | Positive) = \frac{3}{13} \approx 0.231 \)

#### Bayesâ€™ Rule

Bayesâ€™ rule connects these probabilities:

\[
P(Positive | happy) = \frac{P(happy | Positive) \times P(Positive)}{P(happy)}
\]

This formula allows us to update our belief about the sentiment of a tweet given that it contains a specific word.


### 3. ğŸ” Applying Bayesâ€™ Rule: Example and Intuition

Imagine you know:

- 25% of positive tweets contain â€œhappyâ€ â†’ \( P(happy | Positive) = 0.25 \)
- 13% of all tweets contain â€œhappyâ€ â†’ \( P(happy) = 0.13 \)
- 40% of tweets are positive â†’ \( P(Positive) = 0.40 \)

You see a tweet with the word â€œhappy.â€ What is the probability itâ€™s positive?

Using Bayesâ€™ rule:

\[
P(Positive | happy) = \frac{0.25 \times 0.40}{0.13} \approx 0.77
\]

This means thereâ€™s a 77% chance the tweet is positive given it contains â€œhappy.â€


### 4. ğŸ“ Naive Bayes for Sentiment Analysis: How It Works

Naive Bayes classifies a tweet by calculating the probability that the tweet belongs to each class (positive or negative) based on the words it contains.

#### Key steps:

- **Training data**: A set of tweets labeled as positive or negative.
- **Word probabilities**: Calculate \( P(word | class) \), the probability of each word appearing in positive or negative tweets.
- **Assumption**: Words are independent (naive assumption).
- **Classification**: For a new tweet, multiply the probabilities of each word given the class, then multiply by the prior probability of the class.

Example:

| Word     | P(word|Pos) | P(word|Neg) |
|----------|-------------|-------------|
| happy    | 0.24        | 0.25        |
| because  | 0.24        | 0.25        |
| learning | 0.15        | 0.08        |
| sad      | 0.08        | 0.17        |
| not      | 0.08        | 0.08        |

For the tweet â€œI am happy today; I am learning,â€ we calculate the product of probabilities for each word under each class and compare.


### 5. ğŸ§® Laplacian Smoothing: Handling Zero Probabilities

Sometimes, a word in a new tweet might not appear in the training data for a class, causing \( P(word | class) = 0 \). This zero probability would make the entire product zero, which is problematic.

**Laplacian smoothing** fixes this by adding 1 to the count of every word in every class before calculating probabilities. This ensures no probability is zero.

Formula:

\[
P(w_i | class) = \frac{freq(w_i, class) + 1}{N_{class} + V}
\]

- \( freq(w_i, class) \): frequency of word \( w_i \) in class
- \( N_{class} \): total number of words in class
- \( V \): number of unique words in the vocabulary

This smoothing makes the model more robust to unseen words.


### 6. ğŸ“ˆ Log Likelihood: Avoiding Underflow and Simplifying Calculations

When multiplying many small probabilities, the product can become extremely small (underflow), causing computational issues.

To avoid this, we use **logarithms**:

- Instead of multiplying probabilities, we sum their logarithms.
- Logarithms turn products into sums, which are easier and more stable to compute.

For a tweet with words \( w_1, w_2, ..., w_m \), the log likelihood for class \( c \) is:

\[
\log P(c) + \sum_{i=1}^m \log P(w_i | c)
\]

We calculate this for each class and pick the class with the highest log likelihood.


### 7. ğŸ‹ï¸ Training a Naive Bayes Model: Step-by-Step

Training involves several clear steps:

1. **Collect and annotate data**: Gather tweets labeled as positive or negative.
2. **Preprocess tweets**: Convert to lowercase, remove punctuation, URLs, stop words, and apply stemming/tokenization.
3. **Count word frequencies**: Calculate how often each word appears in positive and negative tweets.
4. **Calculate probabilities**: Compute \( P(w | pos) \) and \( P(w | neg) \) using counts and Laplacian smoothing.
5. **Calculate prior probabilities**: \( P(pos) \) and \( P(neg) \) based on the number of tweets in each class.
6. **Calculate log prior**: \( \log \frac{P(pos)}{P(neg)} \) for use in classification.


### 8. ğŸ§ª Testing and Using Naive Bayes for Prediction

To predict the sentiment of a new tweet:

- Preprocess the tweet (tokenize, clean).
- For each word, look up \( \log P(w | pos) \) and \( \log P(w | neg) \).
- Sum these log probabilities and add the log prior.
- Compare the sums for positive and negative classes.
- The class with the higher sum is the predicted sentiment.

If a word is unseen, Laplacian smoothing ensures it has a small but non-zero probability.


### 9. ğŸŒ Applications of Naive Bayes

Naive Bayes is widely used because it is:

- **Simple and fast**: Good for large datasets.
- **Robust**: Performs well even with the naive independence assumption.

Common applications include:

- **Sentiment analysis**: Classifying tweets, reviews, or comments.
- **Spam filtering**: Detecting unwanted emails.
- **Author identification**: Guessing the author of a text.
- **Information retrieval**: Ranking documents by relevance.
- **Word sense disambiguation**: Determining the meaning of ambiguous words.


### 10. âš ï¸ Assumptions and Limitations of Naive Bayes

#### Independence Assumption

Naive Bayes assumes that words appear independently of each other given the class. This is often false in natural language because words influence each other (e.g., â€œnot happyâ€ vs. â€œhappyâ€).

#### Relative Frequency

The model relies heavily on the relative frequency of words and classes in the training data. If the dataset is unbalanced, predictions may be biased.


### 11. ğŸ Error Analysis: Common Sources of Mistakes

Naive Bayes can make errors due to:

- **Ignoring word order**: â€œI am happy because I am not sadâ€ vs. â€œI am not happy because I am sadâ€ have very different meanings but similar word sets.
- **Removing punctuation and stop words**: Sometimes punctuation or negations like â€œnotâ€ are crucial for meaning.
- **Adversarial attacks**: Sarcasm, irony, and euphemisms can confuse the model.
- **Preprocessing errors**: Over-aggressive cleaning can remove important words or context.

Example:

- Tweet: â€œMy beloved grandmother :(â€ â†’ After preprocessing: [belov, grandmoth] loses the sad emoticon, which carries sentiment.
- Tweet: â€œThis is not goodâ€ â†’ After removing â€œnot,â€ the sentiment flips incorrectly.


### Summary

Naive Bayes is a foundational probabilistic method for sentiment analysis that uses Bayesâ€™ theorem to classify text based on word probabilities. It is simple, fast, and surprisingly effective despite its naive assumptions. Key concepts include conditional probabilities, Laplacian smoothing to handle zero counts, and log likelihood to avoid computational issues. While it has limitations, especially regarding word order and context, it remains a powerful baseline for many NLP tasks.



<br>

## Questions



#### 1. What does the Naive Bayes classifier assume about the features (words) in a text?  
A) All words are dependent on each other  
B) All words are independent given the class  
C) Words have equal probability in all classes  
D) Word order is crucial for classification  

#### 2. Given a dataset with 65% positive tweets and 35% negative tweets, what is the prior probability \( P(Positive) \)?  
A) 0.35  
B) 0.65  
C) 0.50  
D) Cannot be determined without word frequencies  

#### 3. If \( P(happy | Positive) = 0.25 \), \( P(Positive) = 0.40 \), and \( P(happy) = 0.13 \), what does Bayesâ€™ rule calculate?  
A) Probability a tweet contains â€œhappyâ€ given it is positive  
B) Probability a tweet is positive given it contains â€œhappyâ€  
C) Probability a tweet is negative given it contains â€œhappyâ€  
D) Probability a tweet contains â€œhappyâ€ regardless of sentiment  

#### 4. Why is Laplacian smoothing necessary in Naive Bayes?  
A) To increase the probability of frequent words  
B) To avoid zero probabilities for unseen words  
C) To normalize the prior probabilities  
D) To remove stop words from the dataset  

#### 5. Which of the following is true about the log likelihood in Naive Bayes?  
A) It converts sums into products to simplify calculations  
B) It helps prevent numerical underflow when multiplying many probabilities  
C) It is only used when the dataset is very small  
D) It always produces probabilities greater than 1  

#### 6. When classifying a tweet, how does Naive Bayes combine the probabilities of individual words?  
A) By summing the probabilities of each word  
B) By multiplying the probabilities of each word  
C) By summing the logarithms of the probabilities of each word  
D) By taking the maximum probability among the words  

#### 7. Which of the following can cause errors in Naive Bayes sentiment classification?  
A) Ignoring word order  
B) Removing punctuation and stop words  
C) Using Laplacian smoothing  
D) Sarcasm and irony in tweets  

#### 8. What is the effect of the independence assumption in Naive Bayes on natural language processing?  
A) It perfectly models word dependencies  
B) It simplifies computation but ignores word context  
C) It improves accuracy by considering word order  
D) It makes the model unable to classify any text  

#### 9. If a word appears only in negative tweets during training, what will be its probability \( P(word | Positive) \) after Laplacian smoothing?  
A) Exactly zero  
B) Slightly greater than zero  
C) Equal to \( P(word | Negative) \)  
D) Equal to one  

#### 10. How does Naive Bayes handle words in a test tweet that were never seen during training?  
A) It assigns zero probability and rejects the tweet  
B) It ignores those words completely  
C) Laplacian smoothing assigns a small non-zero probability  
D) It treats them as stop words  

#### 11. Which of the following best describes the â€œpriorâ€ in Naive Bayes classification?  
A) The probability of a word appearing in a tweet  
B) The probability of a class before seeing any words  
C) The probability of a tweet given a class  
D) The probability of a tweet containing a specific word  

#### 12. Why might Naive Bayes perform poorly on tweets with negations like â€œnot happyâ€?  
A) Because it treats â€œnotâ€ and â€œhappyâ€ as independent words  
B) Because it removes â€œnotâ€ during preprocessing  
C) Because it cannot handle words with multiple meanings  
D) Because it always assumes positive sentiment for â€œhappyâ€  

#### 13. Which of the following is NOT a typical preprocessing step before training Naive Bayes?  
A) Lowercasing all words  
B) Removing punctuation and URLs  
C) Stemming or lemmatization  
D) Randomly shuffling word order  

#### 14. What does the term â€œlog priorâ€ refer to in Naive Bayes?  
A) The logarithm of the ratio of positive to negative class probabilities  
B) The logarithm of the probability of each word  
C) The logarithm of the total number of words in the corpus  
D) The logarithm of the smoothing parameter  

#### 15. Consider two tweets: â€œI am happy because I am learningâ€ and â€œI am not happy because I am learning.â€ Why might Naive Bayes struggle to distinguish their sentiments?  
A) Because it ignores the word â€œnotâ€  
B) Because it treats all words independently and ignores word order  
C) Because it does not use Laplacian smoothing  
D) Because it only looks at the first word in the tweet  

#### 16. Which of the following applications is NOT commonly associated with Naive Bayes?  
A) Spam filtering  
B) Image recognition  
C) Author identification  
D) Word sense disambiguation  

#### 17. What is the main reason Naive Bayes is considered a â€œbaselineâ€ model in NLP?  
A) It is the most accurate model available  
B) It is simple, fast, and provides a good starting point  
C) It requires no training data  
D) It models complex word dependencies  

#### 18. How does the relative frequency of classes in the training data affect Naive Bayes?  
A) It has no effect on classification  
B) It influences the prior probabilities and can bias predictions  
C) It only affects the smoothing parameter  
D) It changes the independence assumption  

#### 19. Which of the following statements about Bayesâ€™ rule is correct?  
A) It requires knowledge of the joint probability of two events  
B) It expresses \( P(A|B) \) in terms of \( P(B|A) \), \( P(A) \), and \( P(B) \)  
C) It can only be applied to independent events  
D) It is used to calculate the probability of a word given a class  

#### 20. Why might removing punctuation during preprocessing lead to errors in sentiment classification?  
A) Because punctuation never carries sentiment information  
B) Because punctuation can change the meaning or tone of a sentence  
C) Because it increases the vocabulary size unnecessarily  
D) Because it causes Laplacian smoothing to fail  



<br>

## Answers



#### 1. What does the Naive Bayes classifier assume about the features (words) in a text?  
A) âœ— Words are not independent; this contradicts the naive assumption.  
B) âœ“ Naive Bayes assumes words are independent given the class.  
C) âœ— Words do not have equal probability across classes; probabilities differ by class.  
D) âœ— Word order is ignored in Naive Bayes.  

**Correct:** B


#### 2. Given a dataset with 65% positive tweets and 35% negative tweets, what is the prior probability \( P(Positive) \)?  
A) âœ— 0.35 is the negative class prior.  
B) âœ“ 0.65 is the correct prior for positive tweets.  
C) âœ— 0.50 is incorrect unless classes are balanced.  
D) âœ— Prior can be determined from class counts, no word frequencies needed.  

**Correct:** B


#### 3. If \( P(happy | Positive) = 0.25 \), \( P(Positive) = 0.40 \), and \( P(happy) = 0.13 \), what does Bayesâ€™ rule calculate?  
A) âœ— This is the conditional probability given the class, not what Bayesâ€™ rule calculates here.  
B) âœ“ Bayesâ€™ rule calculates \( P(Positive | happy) \), the probability tweet is positive given â€œhappy.â€  
C) âœ— This is not the probability of negative given â€œhappy.â€  
D) âœ— \( P(happy) \) is given, not calculated by Bayesâ€™ rule here.  

**Correct:** B


#### 4. Why is Laplacian smoothing necessary in Naive Bayes?  
A) âœ— It does not increase probabilities of frequent words specifically.  
B) âœ“ It prevents zero probabilities for unseen words, avoiding zeroing out the product.  
C) âœ— It does not normalize priors.  
D) âœ— It is unrelated to stop word removal.  

**Correct:** B


#### 5. Which of the following is true about the log likelihood in Naive Bayes?  
A) âœ— Log likelihood converts products into sums, not sums into products.  
B) âœ“ It prevents numerical underflow by summing logs instead of multiplying probabilities.  
C) âœ— It is used regardless of dataset size.  
D) âœ— Log likelihood values are not probabilities and can be negative or >1 in raw form.  

**Correct:** B


#### 6. When classifying a tweet, how does Naive Bayes combine the probabilities of individual words?  
A) âœ— Probabilities are multiplied, not summed directly.  
B) âœ— Multiplying probabilities is correct but computationally unstable.  
C) âœ“ Logarithms of probabilities are summed to avoid underflow.  
D) âœ— Maximum probability is not used; all words contribute.  

**Correct:** C


#### 7. Which of the following can cause errors in Naive Bayes sentiment classification?  
A) âœ“ Ignoring word order loses important context.  
B) âœ“ Removing punctuation and stop words can remove sentiment cues.  
C) âœ— Laplacian smoothing reduces errors, does not cause them.  
D) âœ“ Sarcasm and irony confuse the model as it relies on literal word meaning.  

**Correct:** A,B,D


#### 8. What is the effect of the independence assumption in Naive Bayes on natural language processing?  
A) âœ— It does not model dependencies perfectly.  
B) âœ“ It simplifies computation but ignores word context and dependencies.  
C) âœ— It does not improve accuracy by considering word order.  
D) âœ— It does not make the model unusable.  

**Correct:** B


#### 9. If a word appears only in negative tweets during training, what will be its probability \( P(word | Positive) \) after Laplacian smoothing?  
A) âœ— It will not be zero due to smoothing.  
B) âœ“ Slightly greater than zero because smoothing adds 1 to counts.  
C) âœ— It will not equal \( P(word | Negative) \) since counts differ.  
D) âœ— It cannot be one unless it appears exclusively in positive tweets.  

**Correct:** B


#### 10. How does Naive Bayes handle words in a test tweet that were never seen during training?  
A) âœ— It does not assign zero probability due to smoothing.  
B) âœ— It does not ignore unseen words; they contribute small probability.  
C) âœ“ Laplacian smoothing assigns a small non-zero probability to unseen words.  
D) âœ— It does not treat unseen words as stop words.  

**Correct:** C


#### 11. Which of the following best describes the â€œpriorâ€ in Naive Bayes classification?  
A) âœ— Prior is about classes, not individual words.  
B) âœ“ Prior is the probability of a class before seeing any words.  
C) âœ— This is the likelihood, not the prior.  
D) âœ— This is a marginal probability of a word, not the prior.  

**Correct:** B


#### 12. Why might Naive Bayes perform poorly on tweets with negations like â€œnot happyâ€?  
A) âœ“ Because it treats â€œnotâ€ and â€œhappyâ€ independently, missing negation effect.  
B) âœ— Negations are usually kept during preprocessing; removing them is not standard.  
C) âœ— Word ambiguity is a separate issue.  
D) âœ— It does not always assume â€œhappyâ€ is positive regardless of context.  

**Correct:** A


#### 13. Which of the following is NOT a typical preprocessing step before training Naive Bayes?  
A) âœ— Lowercasing is standard.  
B) âœ— Removing punctuation and URLs is standard.  
C) âœ— Stemming or lemmatization is common.  
D) âœ“ Randomly shuffling word order is not done; word order is ignored but not shuffled.  

**Correct:** D


#### 14. What does the term â€œlog priorâ€ refer to in Naive Bayes?  
A) âœ“ Logarithm of the ratio of positive to negative class probabilities.  
B) âœ— Log prior is about classes, not individual words.  
C) âœ— This is unrelated to log prior.  
D) âœ— Smoothing parameter is unrelated to log prior.  

**Correct:** A


#### 15. Consider two tweets: â€œI am happy because I am learningâ€ and â€œI am not happy because I am learning.â€ Why might Naive Bayes struggle to distinguish their sentiments?  
A) âœ— â€œNotâ€ is usually kept during preprocessing.  
B) âœ“ Because it treats words independently and ignores word order, missing negation effect.  
C) âœ— Laplacian smoothing is unrelated to this issue.  
D) âœ— It uses all words, not just the first.  

**Correct:** B


#### 16. Which of the following applications is NOT commonly associated with Naive Bayes?  
A) âœ— Spam filtering is a common application.  
B) âœ“ Image recognition is generally not done with Naive Bayes.  
C) âœ— Author identification is a known application.  
D) âœ— Word sense disambiguation is a common use case.  

**Correct:** B


#### 17. What is the main reason Naive Bayes is considered a â€œbaselineâ€ model in NLP?  
A) âœ— It is not the most accurate model available.  
B) âœ“ It is simple, fast, and provides a good starting point.  
C) âœ— It requires training data.  
D) âœ— It does not model complex dependencies.  

**Correct:** B


#### 18. How does the relative frequency of classes in the training data affect Naive Bayes?  
A) âœ— It does affect classification through priors.  
B) âœ“ It influences prior probabilities and can bias predictions if unbalanced.  
C) âœ— It does not only affect smoothing.  
D) âœ— It does not change the independence assumption.  

**Correct:** B


#### 19. Which of the following statements about Bayesâ€™ rule is correct?  
A) âœ“ Bayesâ€™ rule uses conditional probabilities, which relate to joint probabilities.  
B) âœ“ It expresses \( P(A|B) \) in terms of \( P(B|A) \), \( P(A) \), and \( P(B) \).  
C) âœ— It can be applied to dependent events as well.  
D) âœ— Bayesâ€™ rule calculates posterior probabilities, not directly \( P(word|class) \).  

**Correct:** A,B


#### 20. Why might removing punctuation during preprocessing lead to errors in sentiment classification?  
A) âœ— Punctuation often carries sentiment or tone.  
B) âœ“ Removing punctuation can change meaning or tone, affecting sentiment.  
C) âœ— Removing punctuation reduces vocabulary size, not increases it.  
D) âœ— It does not cause Laplacian smoothing to fail.  

**Correct:** B

