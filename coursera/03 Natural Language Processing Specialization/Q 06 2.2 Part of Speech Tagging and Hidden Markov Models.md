## 2.2 Part of Speech Tagging and Hidden Markov Models

## Questions

#### 1. What is the primary goal of Part of Speech (POS) tagging in natural language processing?  
A) To assign a grammatical category to each word in a sentence  
B) To translate sentences from one language to another  
C) To identify named entities like people and places  
D) To generate new sentences based on grammar rules  

#### 2. Which of the following are typical POS tags used in tagging?  
A) Noun  
B) Verb  
C) Determiner  
D) Sentiment  

#### 3. Why is it insufficient to tag words solely based on their dictionary definitions?  
A) Because words can have multiple possible tags depending on context  
B) Because dictionaries do not contain all words  
C) Because POS tagging requires semantic understanding, not just syntax  
D) Because tagging depends on the frequency of words in a corpus  

#### 4. In a Markov chain model for POS tagging, what does the "state" represent?  
A) A word in the sentence  
B) A POS tag  
C) A transition probability  
D) An emission probability  

#### 5. What does the Markov property imply in the context of POS tagging?  
A) The probability of a tag depends only on the previous tag  
B) The probability of a word depends on all previous words  
C) The probability of a tag depends on the entire sentence  
D) The probability of a word depends only on the current tag  

#### 6. Which of the following best describes the transition matrix in POS tagging?  
A) It contains probabilities of words given tags  
B) It contains probabilities of moving from one tag to another  
C) It contains probabilities of tags starting a sentence  
D) It contains probabilities of words following other words  

#### 7. What is the role of initial probabilities (π) in a Markov model for POS tagging?  
A) To represent the likelihood of a word being a noun  
B) To represent the likelihood of a tag starting a sentence  
C) To represent the likelihood of a tag following another tag  
D) To represent the likelihood of a word being emitted by a tag  

#### 8. In a Hidden Markov Model (HMM), what is "hidden"?  
A) The observed words  
B) The POS tags  
C) The transition probabilities  
D) The emission probabilities  

#### 9. What are emission probabilities in an HMM for POS tagging?  
A) Probabilities of transitioning from one tag to another  
B) Probabilities of a tag generating a particular word  
C) Probabilities of a word starting a sentence  
D) Probabilities of a tag being the first tag in a sentence  

#### 10. Why is smoothing necessary when calculating transition and emission probabilities?  
A) To increase the probability of frequent events  
B) To avoid zero probabilities for unseen tag or word pairs  
C) To speed up the Viterbi algorithm  
D) To reduce the size of the transition matrix  

#### 11. Consider the sentence "Why not learn something?" Which of the following statements is true regarding POS tagging?  
A) "Why" is always tagged as a noun  
B) "Learn" can be tagged as a verb or noun depending on context  
C) "Something" is typically tagged as a noun  
D) "Not" is usually tagged as an adverb  

#### 12. How does the Viterbi algorithm find the most likely sequence of POS tags?  
A) By enumerating all possible tag sequences and selecting the best  
B) By using dynamic programming to efficiently compute the best path  
C) By randomly guessing tags and checking probabilities  
D) By only considering emission probabilities  

#### 13. Which of the following are steps in the Viterbi algorithm?  
A) Initialization  
B) Forward pass  
C) Backward pass  
D) Emission smoothing  

#### 14. Why are log probabilities used in implementations of the Viterbi algorithm?  
A) To simplify multiplication into addition and avoid numerical underflow  
B) To make probabilities easier to interpret  
C) To speed up matrix multiplication  
D) To avoid zero probabilities  

#### 15. If a transition probability from tag A to tag B is zero in the training data, what problem might arise without smoothing?  
A) The model will assign zero probability to any sequence containing that transition  
B) The model will ignore that transition and choose randomly  
C) The model will overestimate the probability of that transition  
D) The model will crash during decoding  

#### 16. Which of the following best describes the difference between Markov chains and Hidden Markov Models in POS tagging?  
A) Markov chains model observed words, HMMs model hidden tags  
B) Markov chains have observable states, HMMs have hidden states and observable outputs  
C) Markov chains use emission probabilities, HMMs do not  
D) HMMs assume independence between tags, Markov chains do not  

#### 17. When building the transition matrix from a corpus, what data is counted?  
A) The number of times a word appears in the corpus  
B) The number of times a tag follows another tag  
C) The number of times a word is emitted by a tag  
D) The number of sentences in the corpus  

#### 18. Which of the following statements about emission probabilities is correct?  
A) They depend on the frequency of a word given a tag in the training data  
B) They are always equal for all words under a given tag  
C) They represent the probability of a tag following another tag  
D) They are irrelevant in the Viterbi algorithm  

#### 19. In the context of POS tagging, what does the term "observable" refer to in an HMM?  
A) The POS tags  
B) The words in the sentence  
C) The transition probabilities  
D) The initial probabilities  

#### 20. Which of the following challenges can arise when using HMMs for POS tagging?  
A) Ambiguity in word tagging due to multiple possible tags  
B) Zero probabilities for unseen word-tag or tag-tag pairs  
C) Computational inefficiency in finding the best tag sequence  
D) The inability to model long-range dependencies beyond adjacent tags



<br>

## Answers

#### 1. What is the primary goal of Part of Speech (POS) tagging in natural language processing?  
A) ✓ Assign a grammatical category to each word in a sentence  
B) ✗ Translation is a different task  
C) ✗ Named entity recognition is separate from POS tagging  
D) ✗ Sentence generation is unrelated  

**Correct:** A


#### 2. Which of the following are typical POS tags used in tagging?  
A) ✓ Noun is a common POS tag  
B) ✓ Verb is a common POS tag  
C) ✓ Determiner is a common POS tag  
D) ✗ Sentiment is not a POS tag, but a separate NLP task  

**Correct:** A,B,C


#### 3. Why is it insufficient to tag words solely based on their dictionary definitions?  
A) ✓ Words can have multiple tags depending on context  
B) ✗ Dictionary coverage is not the main issue here  
C) ✗ POS tagging focuses on syntax, not deep semantics  
D) ✗ Frequency alone does not solve ambiguity  

**Correct:** A


#### 4. In a Markov chain model for POS tagging, what does the "state" represent?  
A) ✗ Words are observations, not states  
B) ✓ States correspond to POS tags  
C) ✗ Transition probabilities are parameters, not states  
D) ✗ Emission probabilities are parameters, not states  

**Correct:** B


#### 5. What does the Markov property imply in the context of POS tagging?  
A) ✓ The next tag depends only on the previous tag  
B) ✗ Words do not depend on all previous words in Markov assumption  
C) ✗ The entire sentence context is not considered in Markov models  
D) ✗ Word depends on tag, not vice versa  

**Correct:** A


#### 6. Which of the following best describes the transition matrix in POS tagging?  
A) ✗ This describes emission probabilities  
B) ✓ Transition matrix contains probabilities of tag-to-tag transitions  
C) ✗ Initial probabilities are separate from transition matrix  
D) ✗ Word-to-word probabilities are not modeled here  

**Correct:** B


#### 7. What is the role of initial probabilities (π) in a Markov model for POS tagging?  
A) ✗ Initial probabilities relate to tags, not word categories  
B) ✓ They represent likelihood of tags starting a sentence  
C) ✗ Transition probabilities relate to tag-to-tag transitions, not initial  
D) ✗ Emission probabilities relate words to tags, not initial tag likelihood  

**Correct:** B


#### 8. In a Hidden Markov Model (HMM), what is "hidden"?  
A) ✗ Observed words are visible, not hidden  
B) ✓ POS tags are hidden states  
C) ✗ Transition probabilities are parameters, not hidden states  
D) ✗ Emission probabilities are parameters, not hidden states  

**Correct:** B


#### 9. What are emission probabilities in an HMM for POS tagging?  
A) ✗ Transition probabilities describe tag-to-tag moves  
B) ✓ Emission probabilities describe likelihood of a word given a tag  
C) ✗ Starting word probabilities are initial probabilities  
D) ✗ Initial tag probabilities are separate from emission  

**Correct:** B


#### 10. Why is smoothing necessary when calculating transition and emission probabilities?  
A) ✗ Smoothing does not increase frequent event probabilities  
B) ✓ To avoid zero probabilities for unseen pairs  
C) ✗ Smoothing does not affect algorithm speed directly  
D) ✗ Smoothing does not reduce matrix size  

**Correct:** B


#### 11. Consider the sentence "Why not learn something?" Which of the following statements is true regarding POS tagging?  
A) ✗ "Why" is usually tagged as a wh-adverb (WRB), not noun  
B) ✓ "Learn" can be verb or noun depending on context  
C) ✓ "Something" is typically tagged as a noun  
D) ✓ "Not" is usually tagged as an adverb  

**Correct:** B,C,D


#### 12. How does the Viterbi algorithm find the most likely sequence of POS tags?  
A) ✗ Enumerating all sequences is computationally infeasible  
B) ✓ Uses dynamic programming to efficiently find best path  
C) ✗ It does not guess randomly  
D) ✗ It considers both transition and emission probabilities  

**Correct:** B


#### 13. Which of the following are steps in the Viterbi algorithm?  
A) ✓ Initialization is the first step  
B) ✓ Forward pass computes probabilities stepwise  
C) ✓ Backward pass recovers best path  
D) ✗ Emission smoothing is not a Viterbi step  

**Correct:** A,B,C


#### 14. Why are log probabilities used in implementations of the Viterbi algorithm?  
A) ✓ To convert multiplication into addition and avoid underflow  
B) ✗ Log probabilities are harder to interpret directly  
C) ✗ Logarithms do not speed up matrix multiplication  
D) ✗ Log probabilities do not prevent zero probabilities  

**Correct:** A


#### 15. If a transition probability from tag A to tag B is zero in the training data, what problem might arise without smoothing?  
A) ✓ The model assigns zero probability to sequences with that transition  
B) ✗ The model does not ignore transitions; zero probability is fatal  
C) ✗ Zero counts do not cause overestimation  
D) ✗ The model usually does not crash but produces zero likelihoods  

**Correct:** A


#### 16. Which of the following best describes the difference between Markov chains and Hidden Markov Models in POS tagging?  
A) ✗ Markov chains model tags, not words; HMMs model hidden tags and observed words  
B) ✓ Markov chains have observable states; HMMs have hidden states and observable outputs  
C) ✗ Both use emission probabilities; Markov chains do not use emissions explicitly  
D) ✗ HMMs assume Markov property; both model tag dependencies similarly  

**Correct:** B


#### 17. When building the transition matrix from a corpus, what data is counted?  
A) ✗ Word frequency alone is not enough  
B) ✓ Counts of tag pairs (tag following another tag)  
C) ✗ Emission counts are separate  
D) ✗ Number of sentences is unrelated to transition counts  

**Correct:** B


#### 18. Which of the following statements about emission probabilities is correct?  
A) ✓ They depend on frequency of word given tag in training data  
B) ✗ Emission probabilities vary by word, not equal for all words under a tag  
C) ✗ Tag-to-tag probabilities are transition probabilities  
D) ✗ Emission probabilities are essential in Viterbi algorithm  

**Correct:** A


#### 19. In the context of POS tagging, what does the term "observable" refer to in an HMM?  
A) ✗ POS tags are hidden states  
B) ✓ Words in the sentence are observable outputs  
C) ✗ Transition probabilities are parameters, not observations  
D) ✗ Initial probabilities are parameters, not observations  

**Correct:** B


#### 20. Which of the following challenges can arise when using HMMs for POS tagging?  
A) ✓ Ambiguity due to multiple possible tags for words  
B) ✓ Zero probabilities for unseen pairs without smoothing  
C) ✗ Viterbi algorithm is efficient, not computationally prohibitive  
D) ✓ HMMs model only adjacent tag dependencies, not long-range  

**Correct:** A,B,D