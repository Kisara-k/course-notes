## 3.1 Recurrent Neural Networks for Language Modeling

## Study Notes

### 1. üß© Introduction to Neural Networks for Sentiment Analysis

Neural networks are powerful tools used in natural language processing (NLP) to analyze and understand text data. One common application is **sentiment analysis**, where the goal is to classify text (like tweets or reviews) as expressing positive or negative sentiment.

#### How Neural Networks Work in Sentiment Analysis

- **Input Representation:** Words in a sentence or tweet are first converted into numerical form. This is often done using **word embeddings**, which are vectors representing the meaning of words in a continuous space.
- **Padding:** Since tweets or sentences vary in length, shorter ones are padded with zeros to match the length of the longest input. This ensures consistent input size for the network.
- **Forward Propagation:** The input vectors pass through layers of the network, where each layer applies mathematical operations to transform the data.
- **Output:** The network outputs a prediction, such as a probability that the sentiment is positive (1) or negative (0).

#### Example

Consider the tweet: *"This movie was almost good."*  
Each word is converted into a vector (embedding), and the sequence is padded if necessary. The network processes this sequence to predict sentiment.


### 2. ‚öôÔ∏è Neural Network Components: Dense and ReLU Layers

Understanding the building blocks of neural networks is essential.

#### Dense (Fully Connected) Layer

- A **dense layer** connects every input neuron to every output neuron.
- It performs a **dot product** between the input vector and a matrix of **trainable weights**, then adds a bias.
- This layer transforms the input into a new representation that the network can use to learn patterns.

#### ReLU Activation Function

- **ReLU (Rectified Linear Unit)** is a non-linear function applied after the dense layer.
- It outputs the input directly if it is positive; otherwise, it outputs zero.
- This non-linearity allows the network to learn complex patterns beyond simple linear relationships.

#### Summary

- Dense layers perform weighted sums of inputs.
- ReLU layers introduce non-linearity, enabling the network to model complex data.


### 3. üßÆ Embedding and Mean Layers

#### Embedding Layer

- Converts words into dense vectors (embeddings) that capture semantic meaning.
- The embedding layer has **trainable weights**, meaning the network learns the best vector representations during training.
- Each word in the vocabulary is assigned an index, and the embedding layer maps this index to a vector.

#### Mean Layer

- After embedding, a **mean layer** can be used to average the embeddings of all words in a sentence.
- This produces a single vector representing the entire sentence.
- The mean layer itself has **no trainable parameters**; it simply computes the average.

#### Example

For the sentence *"I am happy"*, the embedding layer converts each word to a vector, and the mean layer averages these vectors to get a sentence-level representation.


### 4. üìú Traditional Language Models and Their Limitations

Before RNNs, language models often used **N-grams**:

- **N-grams** are sequences of *N* words (e.g., bigrams = 2 words, trigrams = 3 words).
- They estimate the probability of a word based on the previous *N-1* words.
- However, N-grams require **large amounts of memory** and **storage** because the number of possible sequences grows exponentially with *N*.
- They struggle to capture **long-range dependencies** (relationships between words far apart in a sentence).

#### Summary

- N-gram models are simple but inefficient and limited.
- They need large RAM and storage.
- They cannot effectively model distant word relationships.


### 5. üîÑ Recurrent Neural Networks (RNNs): Modeling Sequences

RNNs are designed to handle sequential data like sentences, where the order of words matters.

#### Key Idea

- RNNs process one word at a time, maintaining a **hidden state** that carries information from previous words.
- This hidden state acts like a memory, allowing the network to consider all previous words when predicting the next word or classifying sentiment.

#### Advantages Over N-grams

- RNNs can model **long-distance dependencies** because the hidden state summarizes all past inputs.
- They **share parameters** across time steps, making them more memory-efficient than N-grams.

#### Example

Given the sentence: *"I called her but she did not __________"*, an RNN can predict the next word by considering the entire preceding context, not just the last two or three words.


### 6. üî¢ Applications of RNNs in NLP

RNNs are versatile and can be applied to various NLP tasks:

- **One-to-One:** Simple classification tasks, e.g., classifying a single word or sentence.
- **One-to-Many:** Generating sequences from a single input, e.g., image captioning where one image leads to a sequence of words.
- **Many-to-One:** Summarizing a sequence into one output, e.g., sentiment analysis of a tweet.
- **Many-to-Many:** Tasks where both input and output are sequences, e.g., machine translation (translating sentences from one language to another).

#### Summary

RNNs are flexible and can be adapted to many sequence-based problems in NLP.


### 7. üîÑ How RNNs Work: The Math Behind the Scenes

#### Information Propagation Through Time

- At each time step, the RNN takes two inputs:
  1. The current word‚Äôs embedding.
  2. The hidden state from the previous time step.
- It combines these inputs to produce a new hidden state, which summarizes all information up to that point.
- This process repeats for every word in the sequence.

#### Prediction

- The hidden state at the final time step can be used to make predictions, such as classifying sentiment or predicting the next word.


### 8. üìâ Loss Function for RNNs: Cross Entropy Loss

To train RNNs, we need a way to measure how well the model‚Äôs predictions match the true labels.

- **Cross Entropy Loss** is commonly used for classification tasks.
- It measures the difference between the predicted probability distribution and the true distribution (usually one-hot encoded).
- For sequences, the loss is averaged over all time steps to account for the entire sequence.


### 9. üíª Implementing RNNs: The scan() Function in TensorFlow

Frameworks like TensorFlow use abstractions to efficiently compute RNN forward propagation.

- The **scan()** function applies a given function repeatedly over a sequence, maintaining a state (like the hidden state in RNNs).
- This abstraction allows parallel computation and efficient use of GPUs.
- It mimics the step-by-step processing of RNNs while optimizing performance.


### 10. üõ†Ô∏è Gated Recurrent Units (GRUs): Improving RNNs

Vanilla RNNs can struggle with remembering important information over long sequences due to problems like vanishing gradients.

#### What are GRUs?

- GRUs are a type of RNN that include **gates** to control the flow of information.
- They have:
  - **Update gate:** Decides how much of the past information to keep.
  - **Reset gate:** Decides how to combine new input with past memory.
- These gates help the network **remember important information** and **forget irrelevant details**.

#### Benefits

- GRUs can better capture long-term dependencies.
- They help preserve important context, improving performance on complex tasks.


### 11. üîÑ Deep and Bidirectional RNNs: Enhancing Context Understanding

#### Deep RNNs

- Instead of a single recurrent layer, deep RNNs stack multiple layers.
- Each layer processes the sequence and passes its output to the next layer.
- This allows the network to learn more complex features and representations.

#### Bidirectional RNNs

- Standard RNNs process sequences from past to future.
- **Bidirectional RNNs** process sequences in both directions:
  - One RNN reads the sequence forward.
  - Another reads it backward.
- The outputs from both directions are combined.
- This means the model has access to **both past and future context** when making predictions.

#### Example

In the sentence: *"I was trying really hard to get a hold of __________. Louise finally answered when I was about to give up."*  
A bidirectional RNN can use information from both before and after the blank to predict the missing word.


### Summary

- Neural networks for sentiment analysis start by converting words into embeddings and processing them through dense and ReLU layers.
- Traditional N-gram models are limited by memory and inability to capture long-range dependencies.
- RNNs overcome these limitations by maintaining a hidden state that summarizes past information.
- RNNs are versatile and can be applied to many NLP tasks.
- GRUs improve RNNs by using gates to better manage information flow.
- Deep and bidirectional RNNs enhance the model‚Äôs ability to understand complex and contextual language patterns.
- Frameworks like TensorFlow use functions like `scan()` to efficiently implement RNN computations.



<br>

## Key Points

#### 1. üß† Neural Networks for Sentiment Analysis  
- Words are converted into numerical vectors called embeddings.  
- Tweets or sentences are padded with zeros to match the length of the longest input.  
- Neural networks use forward propagation through layers to predict sentiment (positive = 1, negative = 0).

#### 2. ‚öôÔ∏è Dense and ReLU Layers  
- Dense layers perform a dot product between inputs and trainable weights plus bias.  
- ReLU (Rectified Linear Unit) outputs the input if positive, otherwise zero, introducing non-linearity.  
- ReLU enables the network to learn complex patterns beyond linear relationships.

#### 3. üìö Embedding and Mean Layers  
- Embedding layers have trainable weights that map vocabulary indices to dense vectors.  
- Mean layers compute the average of word embeddings to represent an entire sentence.  
- Mean layers have no trainable parameters.

#### 4. üìú Traditional Language Models (N-grams)  
- N-grams model word sequences of length N (e.g., bigrams, trigrams).  
- They require large memory and storage due to exponential growth in possible sequences.  
- N-grams struggle to capture long-range dependencies between distant words.

#### 5. üîÑ Recurrent Neural Networks (RNNs)  
- RNNs maintain a hidden state that summarizes all previous words in a sequence.  
- They share parameters across time steps, making them more memory-efficient than N-grams.  
- RNNs can model relationships among distant words in a sequence.

#### 6. üõ†Ô∏è Applications of RNNs  
- One-to-One: Single input to single output (e.g., word classification).  
- One-to-Many: Single input to sequence output (e.g., image captioning).  
- Many-to-One: Sequence input to single output (e.g., sentiment analysis).  
- Many-to-Many: Sequence input to sequence output (e.g., machine translation).

#### 7. üî¢ RNN Computation  
- At each time step, RNNs take the current input and previous hidden state to compute a new hidden state.  
- The hidden state propagates information through time, enabling sequence modeling.

#### 8. üìâ Loss Function for RNNs  
- Cross Entropy Loss is used for classification tasks in RNNs.  
- The loss is averaged over all time steps in the sequence.

#### 9. üíª TensorFlow scan() Function  
- The scan() function applies a function repeatedly over a sequence, maintaining state.  
- It enables efficient forward propagation and parallel computation on GPUs.  
- scan() mimics the step-by-step processing of RNNs.

#### 10. üîê Gated Recurrent Units (GRUs)  
- GRUs have update and reset gates to control information flow in the hidden state.  
- Gates help preserve important information and forget irrelevant details.  
- GRUs mitigate the vanishing gradient problem better than vanilla RNNs.

#### 11. üîÑ Deep and Bidirectional RNNs  
- Deep RNNs stack multiple recurrent layers to learn complex features.  
- Bidirectional RNNs process sequences forward and backward, combining both contexts.  
- Bidirectional RNNs have access to past and future information when making predictions.