## 3.1 Recurrent Neural Networks for Language Modeling

[Study Notes](#study-notes)

[Questions](#questions)



### Key Points



#### 1. üß† Neural Networks for Sentiment Analysis  
- Words are converted into numerical vectors called embeddings.  
- Tweets or sentences are padded with zeros to match the length of the longest input.  
- Neural networks use forward propagation through layers to predict sentiment (positive = 1, negative = 0).

#### 2. ‚öôÔ∏è Dense and ReLU Layers  
- Dense layers perform a dot product between inputs and trainable weights plus bias.  
- ReLU (Rectified Linear Unit) outputs the input if positive, otherwise zero, introducing non-linearity.  
- ReLU enables the network to learn complex patterns beyond linear relationships.

#### 3. üìö Embedding and Mean Layers  
- Embedding layers have trainable weights that map vocabulary indices to dense vectors.  
- Mean layers compute the average of word embeddings to represent an entire sentence.  
- Mean layers have no trainable parameters.

#### 4. üìú Traditional Language Models (N-grams)  
- N-grams model word sequences of length N (e.g., bigrams, trigrams).  
- They require large memory and storage due to exponential growth in possible sequences.  
- N-grams struggle to capture long-range dependencies between distant words.

#### 5. üîÑ Recurrent Neural Networks (RNNs)  
- RNNs maintain a hidden state that summarizes all previous words in a sequence.  
- They share parameters across time steps, making them more memory-efficient than N-grams.  
- RNNs can model relationships among distant words in a sequence.

#### 6. üõ†Ô∏è Applications of RNNs  
- One-to-One: Single input to single output (e.g., word classification).  
- One-to-Many: Single input to sequence output (e.g., image captioning).  
- Many-to-One: Sequence input to single output (e.g., sentiment analysis).  
- Many-to-Many: Sequence input to sequence output (e.g., machine translation).

#### 7. üî¢ RNN Computation  
- At each time step, RNNs take the current input and previous hidden state to compute a new hidden state.  
- The hidden state propagates information through time, enabling sequence modeling.

#### 8. üìâ Loss Function for RNNs  
- Cross Entropy Loss is used for classification tasks in RNNs.  
- The loss is averaged over all time steps in the sequence.

#### 9. üíª TensorFlow scan() Function  
- The scan() function applies a function repeatedly over a sequence, maintaining state.  
- It enables efficient forward propagation and parallel computation on GPUs.  
- scan() mimics the step-by-step processing of RNNs.

#### 10. üîê Gated Recurrent Units (GRUs)  
- GRUs have update and reset gates to control information flow in the hidden state.  
- Gates help preserve important information and forget irrelevant details.  
- GRUs mitigate the vanishing gradient problem better than vanilla RNNs.

#### 11. üîÑ Deep and Bidirectional RNNs  
- Deep RNNs stack multiple recurrent layers to learn complex features.  
- Bidirectional RNNs process sequences forward and backward, combining both contexts.  
- Bidirectional RNNs have access to past and future information when making predictions.



<br>

## Study Notes





### 1. üß© Introduction to Neural Networks for Sentiment Analysis

Neural networks are powerful tools used in natural language processing (NLP) to analyze and understand text data. One common application is **sentiment analysis**, where the goal is to classify text (like tweets or reviews) as expressing positive or negative sentiment.

#### How Neural Networks Work in Sentiment Analysis

- **Input Representation:** Words in a sentence or tweet are first converted into numerical form. This is often done using **word embeddings**, which are vectors representing the meaning of words in a continuous space.
- **Padding:** Since tweets or sentences vary in length, shorter ones are padded with zeros to match the length of the longest input. This ensures consistent input size for the network.
- **Forward Propagation:** The input vectors pass through layers of the network, where each layer applies mathematical operations to transform the data.
- **Output:** The network outputs a prediction, such as a probability that the sentiment is positive (1) or negative (0).

#### Example

Consider the tweet: *"This movie was almost good."*  
Each word is converted into a vector (embedding), and the sequence is padded if necessary. The network processes this sequence to predict sentiment.


### 2. ‚öôÔ∏è Neural Network Components: Dense and ReLU Layers

Understanding the building blocks of neural networks is essential.

#### Dense (Fully Connected) Layer

- A **dense layer** connects every input neuron to every output neuron.
- It performs a **dot product** between the input vector and a matrix of **trainable weights**, then adds a bias.
- This layer transforms the input into a new representation that the network can use to learn patterns.

#### ReLU Activation Function

- **ReLU (Rectified Linear Unit)** is a non-linear function applied after the dense layer.
- It outputs the input directly if it is positive; otherwise, it outputs zero.
- This non-linearity allows the network to learn complex patterns beyond simple linear relationships.

#### Summary

- Dense layers perform weighted sums of inputs.
- ReLU layers introduce non-linearity, enabling the network to model complex data.


### 3. üßÆ Embedding and Mean Layers

#### Embedding Layer

- Converts words into dense vectors (embeddings) that capture semantic meaning.
- The embedding layer has **trainable weights**, meaning the network learns the best vector representations during training.
- Each word in the vocabulary is assigned an index, and the embedding layer maps this index to a vector.

#### Mean Layer

- After embedding, a **mean layer** can be used to average the embeddings of all words in a sentence.
- This produces a single vector representing the entire sentence.
- The mean layer itself has **no trainable parameters**; it simply computes the average.

#### Example

For the sentence *"I am happy"*, the embedding layer converts each word to a vector, and the mean layer averages these vectors to get a sentence-level representation.


### 4. üìú Traditional Language Models and Their Limitations

Before RNNs, language models often used **N-grams**:

- **N-grams** are sequences of *N* words (e.g., bigrams = 2 words, trigrams = 3 words).
- They estimate the probability of a word based on the previous *N-1* words.
- However, N-grams require **large amounts of memory** and **storage** because the number of possible sequences grows exponentially with *N*.
- They struggle to capture **long-range dependencies** (relationships between words far apart in a sentence).

#### Summary

- N-gram models are simple but inefficient and limited.
- They need large RAM and storage.
- They cannot effectively model distant word relationships.


### 5. üîÑ Recurrent Neural Networks (RNNs): Modeling Sequences

RNNs are designed to handle sequential data like sentences, where the order of words matters.

#### Key Idea

- RNNs process one word at a time, maintaining a **hidden state** that carries information from previous words.
- This hidden state acts like a memory, allowing the network to consider all previous words when predicting the next word or classifying sentiment.

#### Advantages Over N-grams

- RNNs can model **long-distance dependencies** because the hidden state summarizes all past inputs.
- They **share parameters** across time steps, making them more memory-efficient than N-grams.

#### Example

Given the sentence: *"I called her but she did not __________"*, an RNN can predict the next word by considering the entire preceding context, not just the last two or three words.


### 6. üî¢ Applications of RNNs in NLP

RNNs are versatile and can be applied to various NLP tasks:

- **One-to-One:** Simple classification tasks, e.g., classifying a single word or sentence.
- **One-to-Many:** Generating sequences from a single input, e.g., image captioning where one image leads to a sequence of words.
- **Many-to-One:** Summarizing a sequence into one output, e.g., sentiment analysis of a tweet.
- **Many-to-Many:** Tasks where both input and output are sequences, e.g., machine translation (translating sentences from one language to another).

#### Summary

RNNs are flexible and can be adapted to many sequence-based problems in NLP.


### 7. üîÑ How RNNs Work: The Math Behind the Scenes

#### Information Propagation Through Time

- At each time step, the RNN takes two inputs:
  1. The current word‚Äôs embedding.
  2. The hidden state from the previous time step.
- It combines these inputs to produce a new hidden state, which summarizes all information up to that point.
- This process repeats for every word in the sequence.

#### Prediction

- The hidden state at the final time step can be used to make predictions, such as classifying sentiment or predicting the next word.


### 8. üìâ Loss Function for RNNs: Cross Entropy Loss

To train RNNs, we need a way to measure how well the model‚Äôs predictions match the true labels.

- **Cross Entropy Loss** is commonly used for classification tasks.
- It measures the difference between the predicted probability distribution and the true distribution (usually one-hot encoded).
- For sequences, the loss is averaged over all time steps to account for the entire sequence.


### 9. üíª Implementing RNNs: The scan() Function in TensorFlow

Frameworks like TensorFlow use abstractions to efficiently compute RNN forward propagation.

- The **scan()** function applies a given function repeatedly over a sequence, maintaining a state (like the hidden state in RNNs).
- This abstraction allows parallel computation and efficient use of GPUs.
- It mimics the step-by-step processing of RNNs while optimizing performance.


### 10. üõ†Ô∏è Gated Recurrent Units (GRUs): Improving RNNs

Vanilla RNNs can struggle with remembering important information over long sequences due to problems like vanishing gradients.

#### What are GRUs?

- GRUs are a type of RNN that include **gates** to control the flow of information.
- They have:
  - **Update gate:** Decides how much of the past information to keep.
  - **Reset gate:** Decides how to combine new input with past memory.
- These gates help the network **remember important information** and **forget irrelevant details**.

#### Benefits

- GRUs can better capture long-term dependencies.
- They help preserve important context, improving performance on complex tasks.


### 11. üîÑ Deep and Bidirectional RNNs: Enhancing Context Understanding

#### Deep RNNs

- Instead of a single recurrent layer, deep RNNs stack multiple layers.
- Each layer processes the sequence and passes its output to the next layer.
- This allows the network to learn more complex features and representations.

#### Bidirectional RNNs

- Standard RNNs process sequences from past to future.
- **Bidirectional RNNs** process sequences in both directions:
  - One RNN reads the sequence forward.
  - Another reads it backward.
- The outputs from both directions are combined.
- This means the model has access to **both past and future context** when making predictions.

#### Example

In the sentence: *"I was trying really hard to get a hold of __________. Louise finally answered when I was about to give up."*  
A bidirectional RNN can use information from both before and after the blank to predict the missing word.


### Summary

- Neural networks for sentiment analysis start by converting words into embeddings and processing them through dense and ReLU layers.
- Traditional N-gram models are limited by memory and inability to capture long-range dependencies.
- RNNs overcome these limitations by maintaining a hidden state that summarizes past information.
- RNNs are versatile and can be applied to many NLP tasks.
- GRUs improve RNNs by using gates to better manage information flow.
- Deep and bidirectional RNNs enhance the model‚Äôs ability to understand complex and contextual language patterns.
- Frameworks like TensorFlow use functions like `scan()` to efficiently implement RNN computations.



<br>

## Questions



#### 1. What is the primary purpose of padding in neural network input sequences?  
A) To increase the vocabulary size  
B) To ensure all input sequences have the same length  
C) To improve the semantic meaning of words  
D) To reduce the number of trainable parameters  

#### 2. Which of the following best describes the role of the embedding layer in NLP models?  
A) It converts words into one-hot vectors  
B) It maps words to dense, trainable vector representations  
C) It averages word vectors to create sentence embeddings  
D) It applies non-linear activation functions to word indices  

#### 3. Why is the ReLU activation function preferred over linear functions in dense layers?  
A) It introduces non-linearity allowing the network to learn complex patterns  
B) It always outputs positive values, which speeds up training  
C) It prevents the vanishing gradient problem entirely  
D) It zeroes out negative inputs, adding sparsity to the model  

#### 4. Which of the following are limitations of traditional N-gram language models?  
A) They require large amounts of memory for large N  
B) They can easily capture long-range dependencies  
C) They struggle with sequences longer than N words  
D) They share parameters across time steps  

#### 5. How do RNNs differ fundamentally from N-gram models in handling sequences?  
A) RNNs process sequences one word at a time while maintaining a hidden state  
B) RNNs require storing all possible word combinations explicitly  
C) RNNs share parameters across time steps, reducing memory usage  
D) RNNs only consider the last N words for prediction  

#### 6. In the context of RNNs, what does the hidden state represent?  
A) The current word embedding  
B) A summary of all previous inputs up to the current time step  
C) The output prediction at each time step  
D) The trainable weights of the network  

#### 7. Which of the following are true about the cross-entropy loss function used in RNN training?  
A) It measures the difference between predicted and true probability distributions  
B) It is averaged over all time steps in a sequence  
C) It can only be used for binary classification tasks  
D) It penalizes incorrect predictions more heavily when the predicted probability is confident but wrong  

#### 8. What is the main advantage of using the tf.scan() function in TensorFlow for RNNs?  
A) It allows parallel computation of all time steps simultaneously  
B) It abstracts the recurrent computation over sequences efficiently  
C) It eliminates the need for trainable parameters  
D) It mimics the step-by-step processing of RNNs while optimizing GPU usage  

#### 9. How do Gated Recurrent Units (GRUs) improve upon vanilla RNNs?  
A) By introducing gates that control information flow and memory updates  
B) By removing the hidden state to simplify computations  
C) By using update and reset gates to preserve important information  
D) By stacking multiple layers to increase depth  

#### 10. Which statements about bidirectional RNNs are correct?  
A) They process sequences only from past to future  
B) They combine information from both past and future contexts  
C) They require twice the number of parameters compared to unidirectional RNNs  
D) They are less effective than vanilla RNNs for sentiment analysis  

#### 11. What is the primary function of the mean layer in an embedding-based model?  
A) To train word embeddings  
B) To compute the average vector of all word embeddings in a sentence  
C) To reduce the dimensionality of embeddings  
D) To apply non-linear transformations to embeddings  

#### 12. Why do deep RNNs often perform better on complex NLP tasks than shallow RNNs?  
A) They have more trainable parameters, allowing more complex feature extraction  
B) They process sequences faster due to parallelization  
C) They stack multiple recurrent layers, enabling hierarchical representation learning  
D) They eliminate the need for embedding layers  

#### 13. Which of the following are true about the trainable parameters in an embedding layer?  
A) They are updated during training to improve word representations  
B) They remain fixed and are not learned from data  
C) They map word indices to dense vectors  
D) They are the same as the parameters in the dense layer  

#### 14. In an RNN, what inputs are used at each time step to compute the new hidden state?  
A) The current word embedding and the previous hidden state  
B) Only the current word embedding  
C) The previous hidden state and the output prediction  
D) The entire sequence of previous words  

#### 15. Which of the following are challenges that vanilla RNNs face when modeling long sequences?  
A) Vanishing gradients that make learning long-term dependencies difficult  
B) Exploding gradients that destabilize training  
C) Inability to process sequences longer than a fixed length  
D) Excessive memory usage due to storing all past inputs explicitly  

#### 16. How does the update gate in a GRU function?  
A) It decides how much of the previous hidden state to keep  
B) It resets the hidden state to zero at each time step  
C) It controls how much new information to add to the hidden state  
D) It applies a non-linear activation to the input word embedding  

#### 17. Which of the following are true about the output of a bidirectional RNN at each time step?  
A) It depends only on the past context  
B) It combines information from both past and future contexts  
C) It is the concatenation or sum of forward and backward hidden states  
D) It ignores the current input word  

#### 18. Why is parameter sharing across time steps in RNNs beneficial?  
A) It reduces the total number of parameters, making the model more memory efficient  
B) It allows the model to generalize better to sequences of different lengths  
C) It prevents the model from learning time-dependent patterns  
D) It increases the model‚Äôs capacity to memorize training data  

#### 19. Which of the following are true about the use of RNNs in machine translation?  
A) They can be used in an encoder-decoder architecture  
B) They only process input sequences in one direction  
C) They can generate output sequences of different lengths than the input  
D) They require fixed-length input and output sequences  

#### 20. What is a key difference between the mean layer and the embedding layer in terms of trainable parameters?  
A) The embedding layer has trainable parameters, while the mean layer does not  
B) Both layers have trainable parameters  
C) The mean layer learns to weight words differently during training  
D) The embedding layer is fixed and not updated during training  



<br>

## Answers



#### 1. What is the primary purpose of padding in neural network input sequences?  
A) ‚úó Padding does not increase vocabulary size.  
B) ‚úì Padding ensures all input sequences have the same length for batch processing.  
C) ‚úó Padding does not improve semantic meaning; it just standardizes length.  
D) ‚úó Padding does not reduce trainable parameters.  

**Correct:** B


#### 2. Which of the following best describes the role of the embedding layer in NLP models?  
A) ‚úó Embeddings are dense vectors, not one-hot vectors.  
B) ‚úì Embedding layers map words to dense, trainable vector representations.  
C) ‚úó Averaging is done by a mean layer, not the embedding layer itself.  
D) ‚úó Embedding layers do not apply non-linear activations to indices.  

**Correct:** B


#### 3. Why is the ReLU activation function preferred over linear functions in dense layers?  
A) ‚úì ReLU introduces non-linearity, enabling learning of complex patterns.  
B) ‚úó ReLU outputs zero for negative inputs, not always positive values.  
C) ‚úó ReLU helps with vanishing gradients but does not prevent it entirely.  
D) ‚úì ReLU zeroes out negative inputs, adding sparsity which can help learning.  

**Correct:** A, D


#### 4. Which of the following are limitations of traditional N-gram language models?  
A) ‚úì Large N-grams require huge memory.  
B) ‚úó N-grams cannot easily capture long-range dependencies.  
C) ‚úì N-grams only consider fixed-length contexts, so struggle with longer sequences.  
D) ‚úó N-grams do not share parameters; each N-gram is separate.  

**Correct:** A, C


#### 5. How do RNNs differ fundamentally from N-gram models in handling sequences?  
A) ‚úì RNNs process sequences one word at a time with a hidden state.  
B) ‚úó RNNs do not store all word combinations explicitly.  
C) ‚úì RNNs share parameters across time steps, reducing memory usage.  
D) ‚úó RNNs consider all previous words, not just last N words.  

**Correct:** A, C


#### 6. In the context of RNNs, what does the hidden state represent?  
A) ‚úó Hidden state is not just the current word embedding.  
B) ‚úì It summarizes all previous inputs up to the current time step.  
C) ‚úó Output prediction is separate from the hidden state.  
D) ‚úó Hidden state is not the trainable weights themselves.  

**Correct:** B


#### 7. Which of the following are true about the cross-entropy loss function used in RNN training?  
A) ‚úì It measures difference between predicted and true distributions.  
B) ‚úì Loss is averaged over all time steps in a sequence.  
C) ‚úó Cross-entropy can be used for multi-class, not just binary classification.  
D) ‚úì It penalizes confident but wrong predictions more heavily.  

**Correct:** A, B, D


#### 8. What is the main advantage of using the tf.scan() function in TensorFlow for RNNs?  
A) ‚úó It does not compute all time steps in parallel; it mimics sequential processing.  
B) ‚úì It abstracts recurrent computation efficiently over sequences.  
C) ‚úó It does not eliminate trainable parameters.  
D) ‚úì It mimics step-by-step RNN processing while optimizing GPU usage.  

**Correct:** B, D


#### 9. How do Gated Recurrent Units (GRUs) improve upon vanilla RNNs?  
A) ‚úì By introducing gates controlling information flow and memory updates.  
B) ‚úó GRUs do not remove hidden states; they modify how they are updated.  
C) ‚úì Update and reset gates help preserve important information.  
D) ‚úó Stacking layers is a feature of deep RNNs, not specific to GRUs.  

**Correct:** A, C


#### 10. Which statements about bidirectional RNNs are correct?  
A) ‚úó They process sequences in both forward and backward directions.  
B) ‚úì They combine information from past and future contexts.  
C) ‚úì They require roughly twice the parameters due to two RNNs.  
D) ‚úó They are generally more effective than vanilla RNNs for context-dependent tasks.  

**Correct:** B, C


#### 11. What is the primary function of the mean layer in an embedding-based model?  
A) ‚úó The mean layer does not train embeddings; embedding layer does.  
B) ‚úì It computes the average vector of all word embeddings in a sentence.  
C) ‚úó It does not reduce dimensionality; it averages vectors of same dimension.  
D) ‚úó It does not apply non-linear transformations.  

**Correct:** B


#### 12. Why do deep RNNs often perform better on complex NLP tasks than shallow RNNs?  
A) ‚úì More parameters allow learning complex features.  
B) ‚úó Deep RNNs are generally slower due to sequential dependencies.  
C) ‚úì Stacking layers enables hierarchical representation learning.  
D) ‚úó Deep RNNs still require embedding layers for word representation.  

**Correct:** A, C


#### 13. Which of the following are true about the trainable parameters in an embedding layer?  
A) ‚úì They are updated during training to improve word vectors.  
B) ‚úó Embedding weights are not fixed; they are learned.  
C) ‚úì They map word indices to dense vectors.  
D) ‚úó Embedding parameters are distinct from dense layer parameters.  

**Correct:** A, C


#### 14. In an RNN, what inputs are used at each time step to compute the new hidden state?  
A) ‚úì Current word embedding and previous hidden state.  
B) ‚úó Current word embedding alone is insufficient.  
C) ‚úó Output prediction is not input to hidden state computation.  
D) ‚úó Entire previous sequence is summarized in hidden state, not fed directly.  

**Correct:** A


#### 15. Which of the following are challenges that vanilla RNNs face when modeling long sequences?  
A) ‚úì Vanishing gradients make learning long-term dependencies difficult.  
B) ‚úì Exploding gradients can destabilize training.  
C) ‚úó RNNs can process sequences of arbitrary length in theory.  
D) ‚úó RNNs do not store all past inputs explicitly; they summarize in hidden state.  

**Correct:** A, B


#### 16. How does the update gate in a GRU function?  
A) ‚úì It decides how much of the previous hidden state to keep.  
B) ‚úó It does not reset the hidden state to zero each time step.  
C) ‚úì It controls how much new information to add to the hidden state.  
D) ‚úó It does not apply activation to the input embedding directly.  

**Correct:** A, C


#### 17. Which of the following are true about the output of a bidirectional RNN at each time step?  
A) ‚úó It depends on both past and future context, not just past.  
B) ‚úì It combines information from past and future contexts.  
C) ‚úì Output is often concatenation or sum of forward and backward hidden states.  
D) ‚úó It does not ignore the current input word.  

**Correct:** B, C


#### 18. Why is parameter sharing across time steps in RNNs beneficial?  
A) ‚úì It reduces total parameters, improving memory efficiency.  
B) ‚úì It helps generalize to sequences of varying lengths.  
C) ‚úó It does not prevent learning time-dependent patterns; it models them.  
D) ‚úó It does not increase memorization capacity; it regularizes the model.  

**Correct:** A, B


#### 19. Which of the following are true about the use of RNNs in machine translation?  
A) ‚úì They can be used in encoder-decoder architectures.  
B) ‚úó Encoder and decoder can process sequences in different directions.  
C) ‚úì They can generate output sequences of different lengths than input.  
D) ‚úó They do not require fixed-length input/output sequences.  

**Correct:** A, C


#### 20. What is a key difference between the mean layer and the embedding layer in terms of trainable parameters?  
A) ‚úì Embedding layer has trainable parameters; mean layer does not.  
B) ‚úó Mean layer does not have trainable parameters.  
C) ‚úó Mean layer simply averages embeddings; it does not learn weights.  
D) ‚úó Embedding layer is updated during training, not fixed.  

**Correct:** A

