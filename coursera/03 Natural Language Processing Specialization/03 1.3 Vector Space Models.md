## 1.3 Vector Space Models



### Key Points



#### 1. üìê Vector Space Models (VSMs)  
- VSMs represent words and documents as vectors in a high-dimensional space.  
- Words are represented based on co-occurrence with other words or frequency within documents.  
- The fundamental concept: "You shall know a word by the company it keeps" (Firth, 1957).

#### 2. üî¢ Word Vector Representations  
- Word-by-word vectors count how often words co-occur within a window size \( k \).  
- Word-by-document vectors count how often words appear in different documents or categories.  
- Word-by-word captures local context; word-by-document captures global context.

#### 3. üìè Similarity Measures in Vector Spaces  
- Euclidean distance measures the straight-line distance between two vectors.  
- Euclidean distance can be misleading when comparing vectors of different magnitudes.  
- Cosine similarity measures the cosine of the angle between two vectors, ranging from 0 to 1.  
- Cosine similarity is preferred when comparing vectors of different sizes because it focuses on direction, not magnitude.

#### 4. üîÑ Vector Arithmetic and Word Relationships  
- Vector arithmetic can capture semantic relationships, e.g., Washington - USA + Russia ‚âà Moscow.  
- This property allows prediction of word relationships using vector operations.

#### 5. üëÄ Principal Component Analysis (PCA) for Visualization  
- PCA reduces high-dimensional vectors to lower dimensions (e.g., 2D or 3D) for visualization.  
- PCA finds uncorrelated features (principal components) that capture the most variance in the data.  
- Visualization helps verify if vector representations capture meaningful word relationships.

#### 6. ‚öôÔ∏è PCA Algorithm Steps  
- Mean normalize the data by subtracting the mean of each feature.  
- Compute the covariance matrix of the data.  
- Perform Singular Value Decomposition (SVD) to get eigenvectors and eigenvalues.  
- Eigenvectors represent directions of uncorrelated features; eigenvalues represent variance captured.  
- Project data onto principal components using dot product.  
- Select components that retain the highest percentage of variance.



<br>

## Study Notes



### Study Notes: Vector Space Models in Natural Language Processing (NLP) üìö


### 1. üß≠ Introduction to Vector Space Models

When working with language in computers, one of the biggest challenges is how to represent words and documents in a way that machines can understand and process. **Vector Space Models (VSMs)** provide a powerful way to do this by representing words and documents as points or vectors in a multi-dimensional space.

#### What is a Vector Space Model?

A vector space model is a mathematical framework where words, phrases, or documents are represented as vectors‚Äîessentially lists of numbers‚Äîin a high-dimensional space. Each dimension corresponds to some feature or context related to the word or document.

Why is this useful? Because it allows us to measure **similarity** and **relationships** between words or documents by comparing their vectors. For example, words with similar meanings tend to be close together in this space.

#### Why Learn Vector Space Models?

- To capture the **meaning** of words based on their context.
- To enable applications like **machine translation**, **chatbots**, and **information extraction**.
- To understand how words relate to each other beyond just their dictionary definitions.
- To build systems that can understand and generate human language more naturally.

#### Key Idea: "You shall know a word by the company it keeps" (Firth, 1957)

This famous quote means that the meaning of a word can be understood by looking at the words that appear around it. Vector space models operationalize this idea by counting and analyzing word co-occurrences.


### 2. üìä Representing Words and Documents as Vectors

#### Word-by-Word Vector Representation (Co-occurrence Vectors)

One way to create vectors is to look at how often words appear near each other within a certain window or distance (say, 2 words before or after). For example, in the sentence:

> "I like simple data"

If we set the window size \( k = 2 \), the word "simple" co-occurs with "I", "like", and "data" within that window.

Each word is represented as a vector where each dimension counts how many times it co-occurs with other words in the corpus.

#### Word-by-Document Vector Representation

Instead of looking at word co-occurrences, we can represent a word by how often it appears in different documents or categories. For example, the word "economy" might appear frequently in documents about finance but rarely in entertainment articles.

Similarly, documents can be represented as vectors where each dimension corresponds to the frequency of a particular word in that document.

#### Summary

- **Word-by-word vectors** capture local context (neighboring words).
- **Word-by-document vectors** capture global context (word distribution across documents).


### 3. üìè Measuring Similarity in Vector Spaces

Once words or documents are represented as vectors, we need ways to measure how similar or different they are.

#### Euclidean Distance

Euclidean distance is the straight-line distance between two points (vectors) in space. For example, if you have two vectors:

- Corpus A: (500, 7000)
- Corpus B: (9320, 1000)

The Euclidean distance measures how far apart these two points are in the 2D space.

**Formula:**  
\[
d(\mathbf{v}, \mathbf{w}) = \sqrt{\sum_{i=1}^n (v_i - w_i)^2}
\]

Where \( v_i \) and \( w_i \) are the components of vectors \( \mathbf{v} \) and \( \mathbf{w} \).

**Example in Python:**

```python
import numpy as np

v = np.array([1, 6, 8])
w = np.array([0, 4, 6])
d = np.linalg.norm(v - w)
print("Euclidean distance:", d)
```

This outputs the straight-line distance between vectors \( v \) and \( w \).

#### Limitations of Euclidean Distance

Euclidean distance can be misleading when comparing vectors of different lengths or magnitudes. For example, two vectors might be far apart in terms of Euclidean distance but still point in the same direction, meaning they are semantically similar.

#### Cosine Similarity

Cosine similarity measures the **angle** between two vectors rather than the distance. It calculates the cosine of the angle between vectors, which ranges from 0 (completely different) to 1 (exactly the same direction).

**Formula:**  
\[
\text{cosine similarity}(\mathbf{v}, \mathbf{w}) = \frac{\mathbf{v} \cdot \mathbf{w}}{\|\mathbf{v}\| \|\mathbf{w}\|}
\]

Where \( \mathbf{v} \cdot \mathbf{w} \) is the dot product, and \( \|\mathbf{v}\| \) is the norm (length) of vector \( \mathbf{v} \).

Cosine similarity is especially useful when comparing documents or corpora of different sizes because it focuses on the **direction** of the vectors, not their magnitude.


### 4. üîÑ Manipulating Word Vectors: Capturing Relationships

One of the most exciting aspects of vector space models is that they can capture **semantic relationships** between words through simple vector arithmetic.

For example, consider the famous analogy:

\[
\text{Washington} - \text{USA} + \text{Russia} \approx \text{Moscow}
\]

This means that the vector difference between "Washington" and "USA" is similar to the difference between "Moscow" and "Russia". This property allows us to predict or infer relationships between words.

This idea was popularized by Mikolov et al. (2013) in their work on distributed word representations.


### 5. üëÄ Visualizing Word Vectors with PCA

When vectors have many dimensions (hundreds or thousands), it‚Äôs hard to visualize or understand their relationships. **Principal Component Analysis (PCA)** is a technique used to reduce the dimensionality of the data while preserving as much information as possible.

#### What PCA Does

- Finds new axes (called **principal components**) that capture the most variance (information) in the data.
- These new axes are **uncorrelated**, meaning they represent independent features.
- Projects the original high-dimensional data onto a lower-dimensional space (usually 2D or 3D) for visualization.

#### Example

Words like "oil", "gas", "town", and "city" can be plotted in 2D space after PCA, showing clusters of related words.

#### Why Visualization Matters

Visualizing word vectors helps us see if the model captures meaningful relationships, such as grouping synonyms or related concepts close together.


### 6. ‚öôÔ∏è PCA Algorithm: How It Works

PCA involves several mathematical steps:

1. **Mean Normalize the Data:** Subtract the mean of each feature to center the data around zero.
2. **Compute the Covariance Matrix:** Measures how features vary together.
3. **Perform Singular Value Decomposition (SVD):** Decomposes the covariance matrix into eigenvectors and eigenvalues.
4. **Eigenvectors:** Directions of the new uncorrelated features (principal components).
5. **Eigenvalues:** Amount of variance (information) captured by each principal component.
6. **Project Data:** Use the dot product to project original data onto the principal components.
7. **Retain Variance:** Choose the number of components that retain most of the variance (information).

This process reduces the number of dimensions while keeping the most important information, making it easier to analyze and visualize.


### 7. üìù Summary

- **Vector Space Models** represent words and documents as vectors in a high-dimensional space.
- Words are represented based on **co-occurrence** with other words or their frequency in documents.
- **Similarity measures** like Euclidean distance and cosine similarity help compare vectors.
- **Cosine similarity** is often preferred because it focuses on the direction of vectors, handling different magnitudes better.
- Vector arithmetic can capture **semantic relationships** between words.
- **PCA** helps visualize high-dimensional vectors by reducing dimensions while preserving information.
- Understanding these concepts is fundamental for many NLP applications like machine translation, chatbots, and information extraction.



<br>

## Questions



#### 1. What is the primary purpose of representing words as vectors in a vector space model?  
A) To enable computers to perform arithmetic on words  
B) To capture the relative meaning of words based on their context  
C) To store dictionary definitions of words  
D) To measure similarity between words and documents  

#### 2. Which of the following best describes a word-by-word vector representation?  
A) Counting how many times a word appears in a document  
B) Counting how often words co-occur within a fixed window size in text  
C) Representing words as one-hot encoded vectors  
D) Representing documents as vectors of word frequencies  

#### 3. Why might cosine similarity be preferred over Euclidean distance when comparing word vectors?  
A) Cosine similarity is sensitive to vector magnitude differences  
B) Cosine similarity measures the angle between vectors, ignoring magnitude  
C) Euclidean distance can be misleading when vectors have different lengths  
D) Euclidean distance always gives values between 0 and 1  

#### 4. Which of the following statements about Euclidean distance is true?  
A) It measures the straight-line distance between two points in vector space  
B) It is always a better measure of similarity than cosine similarity  
C) It can be calculated using the norm of the difference between two vectors  
D) It is unaffected by the scale or length of the vectors  

#### 5. In the analogy "Washington - USA + Russia ‚âà Moscow," what property of word vectors does this illustrate?  
A) Vector addition and subtraction can capture semantic relationships  
B) Word vectors are always orthogonal  
C) Word vectors encode syntactic information only  
D) Vector arithmetic can be used to predict unknown word relationships  

#### 6. What does the window size \( k \) represent in co-occurrence vector models?  
A) The number of documents in the corpus  
B) The maximum distance between words to be considered co-occurring  
C) The number of dimensions in the vector space  
D) The frequency threshold for including a word in the vocabulary  

#### 7. Which of the following are true about word-by-document vector representations?  
A) They count how often a word appears in different documents or categories  
B) They capture local context by looking at neighboring words  
C) They can be used to represent documents as vectors of word frequencies  
D) They ignore the frequency of words in documents  

#### 8. What is a key limitation of using Euclidean distance for comparing vectors from corpora of different sizes?  
A) It ignores the direction of vectors  
B) It is computationally expensive  
C) It cannot handle vectors with negative values  
D) It always produces values between 0 and 1  

#### 9. Which of the following statements about cosine similarity are correct?  
A) Cosine similarity values range from -1 to 1  
B) It is calculated using the dot product and norms of vectors  
C) It is useful when comparing vectors of different magnitudes  
D) It measures the Euclidean distance between vectors  

#### 10. Principal Component Analysis (PCA) is used in vector space models primarily to:  
A) Increase the number of dimensions in the vector space  
B) Find uncorrelated features that capture the most variance in data  
C) Visualize high-dimensional word vectors in 2D or 3D  
D) Normalize word vectors to unit length  

#### 11. In PCA, what do eigenvectors represent?  
A) The amount of variance retained by each principal component  
B) Directions of new uncorrelated features in the data  
C) The original word vectors before transformation  
D) The dot product between two vectors  

#### 12. What is the role of eigenvalues in PCA?  
A) They represent the direction of principal components  
B) They quantify the variance explained by each principal component  
C) They are used to normalize the data before PCA  
D) They indicate the number of dimensions to reduce to  

#### 13. Which of the following steps are part of the PCA algorithm?  
A) Mean normalization of data  
B) Computing the covariance matrix  
C) Performing Singular Value Decomposition (SVD)  
D) Calculating cosine similarity between vectors  

#### 14. Why is it important to mean normalize data before applying PCA?  
A) To center the data around zero so variance is measured correctly  
B) To reduce the number of dimensions automatically  
C) To ensure all vectors have the same length  
D) To remove noise from the data  

#### 15. Which of the following are true about the phrase "You shall know a word by the company it keeps"?  
A) It suggests that word meaning can be inferred from surrounding words  
B) It supports the idea of co-occurrence-based vector representations  
C) It implies that words have fixed meanings regardless of context  
D) It is the fundamental concept behind vector space models  

#### 16. How does vector space modeling help in machine translation?  
A) By encoding syntactic rules explicitly  
B) By representing words and phrases as vectors to capture meaning  
C) By translating words based on their vector similarity across languages  
D) By storing bilingual dictionaries as vectors  

#### 17. Which of the following are challenges when using Euclidean distance for similarity in NLP?  
A) It is sensitive to the magnitude of vectors, which can vary widely  
B) It cannot distinguish between vectors pointing in the same direction but with different lengths  
C) It is computationally more complex than cosine similarity  
D) It always favors shorter vectors over longer ones  

#### 18. When visualizing word vectors, why might dimensionality reduction be necessary?  
A) Because word vectors are often in hundreds or thousands of dimensions  
B) To improve the accuracy of word similarity calculations  
C) To make it possible to plot vectors on 2D or 3D graphs  
D) To remove semantic information from the vectors  

#### 19. Which of the following statements about dot product in vector space models is true?  
A) It is used to calculate cosine similarity  
B) It measures the Euclidean distance between two vectors  
C) It is the sum of the products of corresponding vector components  
D) It always produces a value between 0 and 1  

#### 20. What does it mean for features to be "uncorrelated" in the context of PCA?  
A) The features have zero covariance with each other  
B) The features are independent and do not share information  
C) The features are identical in all dimensions  
D) The features represent redundant information  



<br>

## Answers



#### 1. What is the primary purpose of representing words as vectors in a vector space model?  
A) ‚úì To enable computers to perform arithmetic on words ‚Äî Vector arithmetic is a key feature of VSMs.  
B) ‚úì To capture the relative meaning of words based on their context ‚Äî Core goal of VSMs is to represent meaning via context.  
C) ‚úó To store dictionary definitions of words ‚Äî VSMs do not store explicit definitions.  
D) ‚úì To measure similarity between words and documents ‚Äî Similarity measurement is fundamental in VSMs.  

**Correct:** A, B, D


#### 2. Which of the following best describes a word-by-word vector representation?  
A) ‚úó Counting how many times a word appears in a document ‚Äî This is word-by-document, not word-by-word.  
B) ‚úì Counting how often words co-occur within a fixed window size in text ‚Äî This is the definition of word-by-word vectors.  
C) ‚úó Representing words as one-hot encoded vectors ‚Äî One-hot vectors are sparse and do not capture co-occurrence.  
D) ‚úó Representing documents as vectors of word frequencies ‚Äî This describes word-by-document vectors.  

**Correct:** B


#### 3. Why might cosine similarity be preferred over Euclidean distance when comparing word vectors?  
A) ‚úó Cosine similarity is sensitive to vector magnitude differences ‚Äî It actually ignores magnitude differences.  
B) ‚úì Cosine similarity measures the angle between vectors, ignoring magnitude ‚Äî This is the key advantage.  
C) ‚úì Euclidean distance can be misleading when vectors have different lengths ‚Äî Magnitude differences affect Euclidean distance.  
D) ‚úó Euclidean distance always gives values between 0 and 1 ‚Äî Euclidean distance can be any non-negative number.  

**Correct:** B, C


#### 4. Which of the following statements about Euclidean distance is true?  
A) ‚úì It measures the straight-line distance between two points in vector space ‚Äî This is the definition.  
B) ‚úó It is always a better measure of similarity than cosine similarity ‚Äî Not true; depends on context.  
C) ‚úì It can be calculated using the norm of the difference between two vectors ‚Äî This is the standard formula.  
D) ‚úó It is unaffected by the scale or length of the vectors ‚Äî It is affected by vector magnitude.  

**Correct:** A, C


#### 5. In the analogy "Washington - USA + Russia ‚âà Moscow," what property of word vectors does this illustrate?  
A) ‚úì Vector addition and subtraction can capture semantic relationships ‚Äî This is the key insight.  
B) ‚úó Word vectors are always orthogonal ‚Äî They are not necessarily orthogonal.  
C) ‚úó Word vectors encode syntactic information only ‚Äî They capture semantic info too.  
D) ‚úì Vector arithmetic can be used to predict unknown word relationships ‚Äî This is the practical use.  

**Correct:** A, D


#### 6. What does the window size \( k \) represent in co-occurrence vector models?  
A) ‚úó The number of documents in the corpus ‚Äî Irrelevant to window size.  
B) ‚úì The maximum distance between words to be considered co-occurring ‚Äî Correct definition of window size.  
C) ‚úó The number of dimensions in the vector space ‚Äî Dimensions depend on vocabulary size or features.  
D) ‚úó The frequency threshold for including a word in the vocabulary ‚Äî Different concept.  

**Correct:** B


#### 7. Which of the following are true about word-by-document vector representations?  
A) ‚úì They count how often a word appears in different documents or categories ‚Äî This is the definition.  
B) ‚úó They capture local context by looking at neighboring words ‚Äî This is word-by-word vectors.  
C) ‚úì They can be used to represent documents as vectors of word frequencies ‚Äî Documents are vectors of word counts.  
D) ‚úó They ignore the frequency of words in documents ‚Äî They explicitly use frequency.  

**Correct:** A, C


#### 8. What is a key limitation of using Euclidean distance for comparing vectors from corpora of different sizes?  
A) ‚úì It ignores the direction of vectors ‚Äî Euclidean distance is sensitive to magnitude, not direction.  
B) ‚úó It is computationally expensive ‚Äî Not a major limitation compared to cosine similarity.  
C) ‚úó It cannot handle vectors with negative values ‚Äî It can handle negatives.  
D) ‚úó It always produces values between 0 and 1 ‚Äî Values can be larger.  

**Correct:** A


#### 9. Which of the following statements about cosine similarity are correct?  
A) ‚úó Cosine similarity values range from -1 to 1 ‚Äî For non-negative vectors like word counts, range is 0 to 1.  
B) ‚úì It is calculated using the dot product and norms of vectors ‚Äî This is the formula.  
C) ‚úì It is useful when comparing vectors of different magnitudes ‚Äî Ignores magnitude, focuses on direction.  
D) ‚úó It measures the Euclidean distance between vectors ‚Äî Different metric entirely.  

**Correct:** B, C


#### 10. Principal Component Analysis (PCA) is used in vector space models primarily to:  
A) ‚úó Increase the number of dimensions in the vector space ‚Äî PCA reduces dimensions.  
B) ‚úì Find uncorrelated features that capture the most variance in data ‚Äî Core purpose of PCA.  
C) ‚úì Visualize high-dimensional word vectors in 2D or 3D ‚Äî Common use of PCA.  
D) ‚úó Normalize word vectors to unit length ‚Äî PCA does not normalize vectors.  

**Correct:** B, C


#### 11. In PCA, what do eigenvectors represent?  
A) ‚úó The amount of variance retained by each principal component ‚Äî This is eigenvalues.  
B) ‚úì Directions of new uncorrelated features in the data ‚Äî Eigenvectors define principal components.  
C) ‚úó The original word vectors before transformation ‚Äî They are transformed by eigenvectors.  
D) ‚úó The dot product between two vectors ‚Äî Dot product is a separate operation.  

**Correct:** B


#### 12. What is the role of eigenvalues in PCA?  
A) ‚úó They represent the direction of principal components ‚Äî This is eigenvectors.  
B) ‚úì They quantify the variance explained by each principal component ‚Äî Correct.  
C) ‚úó They are used to normalize the data before PCA ‚Äî Normalization is a separate step.  
D) ‚úó They indicate the number of dimensions to reduce to ‚Äî They help decide but do not indicate directly.  

**Correct:** B


#### 13. Which of the following steps are part of the PCA algorithm?  
A) ‚úì Mean normalization of data ‚Äî Essential first step.  
B) ‚úì Computing the covariance matrix ‚Äî Needed to find variance relationships.  
C) ‚úì Performing Singular Value Decomposition (SVD) ‚Äî Used to find eigenvectors and eigenvalues.  
D) ‚úó Calculating cosine similarity between vectors ‚Äî Not part of PCA.  

**Correct:** A, B, C


#### 14. Why is it important to mean normalize data before applying PCA?  
A) ‚úì To center the data around zero so variance is measured correctly ‚Äî Correct reason.  
B) ‚úó To reduce the number of dimensions automatically ‚Äî PCA reduces dimensions but normalization is not for this.  
C) ‚úó To ensure all vectors have the same length ‚Äî Normalization centers data, does not scale length.  
D) ‚úó To remove noise from the data ‚Äî Noise removal is a different process.  

**Correct:** A


#### 15. Which of the following are true about the phrase "You shall know a word by the company it keeps"?  
A) ‚úì It suggests that word meaning can be inferred from surrounding words ‚Äî Core idea behind co-occurrence.  
B) ‚úì It supports the idea of co-occurrence-based vector representations ‚Äî Directly related.  
C) ‚úó It implies that words have fixed meanings regardless of context ‚Äî Opposite of the idea.  
D) ‚úì It is the fundamental concept behind vector space models ‚Äî Foundational principle.  

**Correct:** A, B, D


#### 16. How does vector space modeling help in machine translation?  
A) ‚úó By encoding syntactic rules explicitly ‚Äî VSMs focus on semantics, not explicit syntax rules.  
B) ‚úì By representing words and phrases as vectors to capture meaning ‚Äî Captures semantic similarity.  
C) ‚úì By translating words based on their vector similarity across languages ‚Äî Cross-lingual embeddings use this.  
D) ‚úó By storing bilingual dictionaries as vectors ‚Äî Dictionaries are not vectors themselves.  

**Correct:** B, C


#### 17. Which of the following are challenges when using Euclidean distance for similarity in NLP?  
A) ‚úì It is sensitive to the magnitude of vectors, which can vary widely ‚Äî Magnitude affects Euclidean distance.  
B) ‚úì It cannot distinguish between vectors pointing in the same direction but with different lengths ‚Äî Actually, it *does* distinguish by length, which can be misleading.  
C) ‚úó It is computationally more complex than cosine similarity ‚Äî Both are similar in complexity.  
D) ‚úó It always favors shorter vectors over longer ones ‚Äî It favors closer points, not necessarily shorter vectors.  

**Correct:** A, B


#### 18. When visualizing word vectors, why might dimensionality reduction be necessary?  
A) ‚úì Because word vectors are often in hundreds or thousands of dimensions ‚Äî Too high to visualize directly.  
B) ‚úó To improve the accuracy of word similarity calculations ‚Äî PCA is for visualization, not accuracy improvement.  
C) ‚úì To make it possible to plot vectors on 2D or 3D graphs ‚Äî Visualization requires fewer dimensions.  
D) ‚úó To remove semantic information from the vectors ‚Äî PCA tries to preserve information.  

**Correct:** A, C


#### 19. Which of the following statements about dot product in vector space models is true?  
A) ‚úì It is used to calculate cosine similarity ‚Äî Dot product is numerator in cosine similarity.  
B) ‚úó It measures the Euclidean distance between two vectors ‚Äî Different operation.  
C) ‚úì It is the sum of the products of corresponding vector components ‚Äî Definition of dot product.  
D) ‚úó It always produces a value between 0 and 1 ‚Äî Dot product can be any real number.  

**Correct:** A, C


#### 20. What does it mean for features to be "uncorrelated" in the context of PCA?  
A) ‚úì The features have zero covariance with each other ‚Äî Definition of uncorrelated features.  
B) ‚úì The features are independent and do not share information ‚Äî Uncorrelated implies statistical independence in PCA context.  
C) ‚úó The features are identical in all dimensions ‚Äî Identical means perfectly correlated.  
D) ‚úó The features represent redundant information ‚Äî Uncorrelated means no redundancy.  

**Correct:** A, B

