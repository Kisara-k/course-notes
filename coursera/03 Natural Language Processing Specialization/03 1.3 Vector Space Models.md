## 1.3 Vector Space Models

## Study Notes

### 1. üß≠ Introduction to Vector Space Models

When working with language in computers, one of the biggest challenges is how to represent words and documents in a way that machines can understand and process. **Vector Space Models (VSMs)** provide a powerful way to do this by representing words and documents as points or vectors in a multi-dimensional space.

#### What is a Vector Space Model?

A vector space model is a mathematical framework where words, phrases, or documents are represented as vectors‚Äîessentially lists of numbers‚Äîin a high-dimensional space. Each dimension corresponds to some feature or context related to the word or document.

Why is this useful? Because it allows us to measure **similarity** and **relationships** between words or documents by comparing their vectors. For example, words with similar meanings tend to be close together in this space.

#### Why Learn Vector Space Models?

- To capture the **meaning** of words based on their context.
- To enable applications like **machine translation**, **chatbots**, and **information extraction**.
- To understand how words relate to each other beyond just their dictionary definitions.
- To build systems that can understand and generate human language more naturally.

#### Key Idea: "You shall know a word by the company it keeps" (Firth, 1957)

This famous quote means that the meaning of a word can be understood by looking at the words that appear around it. Vector space models operationalize this idea by counting and analyzing word co-occurrences.


### 2. üìä Representing Words and Documents as Vectors

#### Word-by-Word Vector Representation (Co-occurrence Vectors)

One way to create vectors is to look at how often words appear near each other within a certain window or distance (say, 2 words before or after). For example, in the sentence:

> "I like simple data"

If we set the window size \( k = 2 \), the word "simple" co-occurs with "I", "like", and "data" within that window.

Each word is represented as a vector where each dimension counts how many times it co-occurs with other words in the corpus.

#### Word-by-Document Vector Representation

Instead of looking at word co-occurrences, we can represent a word by how often it appears in different documents or categories. For example, the word "economy" might appear frequently in documents about finance but rarely in entertainment articles.

Similarly, documents can be represented as vectors where each dimension corresponds to the frequency of a particular word in that document.

#### Summary

- **Word-by-word vectors** capture local context (neighboring words).
- **Word-by-document vectors** capture global context (word distribution across documents).


### 3. üìè Measuring Similarity in Vector Spaces

Once words or documents are represented as vectors, we need ways to measure how similar or different they are.

#### Euclidean Distance

Euclidean distance is the straight-line distance between two points (vectors) in space. For example, if you have two vectors:

- Corpus A: (500, 7000)
- Corpus B: (9320, 1000)

The Euclidean distance measures how far apart these two points are in the 2D space.

**Formula:**  
\[
d(\mathbf{v}, \mathbf{w}) = \sqrt{\sum_{i=1}^n (v_i - w_i)^2}
\]

Where \( v_i \) and \( w_i \) are the components of vectors \( \mathbf{v} \) and \( \mathbf{w} \).

**Example in Python:**

```python
import numpy as np

v = np.array([1, 6, 8])
w = np.array([0, 4, 6])
d = np.linalg.norm(v - w)
print("Euclidean distance:", d)
```

This outputs the straight-line distance between vectors \( v \) and \( w \).

#### Limitations of Euclidean Distance

Euclidean distance can be misleading when comparing vectors of different lengths or magnitudes. For example, two vectors might be far apart in terms of Euclidean distance but still point in the same direction, meaning they are semantically similar.

#### Cosine Similarity

Cosine similarity measures the **angle** between two vectors rather than the distance. It calculates the cosine of the angle between vectors, which ranges from 0 (completely different) to 1 (exactly the same direction).

**Formula:**  
\[
\text{cosine similarity}(\mathbf{v}, \mathbf{w}) = \frac{\mathbf{v} \cdot \mathbf{w}}{\|\mathbf{v}\| \|\mathbf{w}\|}
\]

Where \( \mathbf{v} \cdot \mathbf{w} \) is the dot product, and \( \|\mathbf{v}\| \) is the norm (length) of vector \( \mathbf{v} \).

Cosine similarity is especially useful when comparing documents or corpora of different sizes because it focuses on the **direction** of the vectors, not their magnitude.


### 4. üîÑ Manipulating Word Vectors: Capturing Relationships

One of the most exciting aspects of vector space models is that they can capture **semantic relationships** between words through simple vector arithmetic.

For example, consider the famous analogy:

\[
\text{Washington} - \text{USA} + \text{Russia} \approx \text{Moscow}
\]

This means that the vector difference between "Washington" and "USA" is similar to the difference between "Moscow" and "Russia". This property allows us to predict or infer relationships between words.

This idea was popularized by Mikolov et al. (2013) in their work on distributed word representations.


### 5. üëÄ Visualizing Word Vectors with PCA

When vectors have many dimensions (hundreds or thousands), it‚Äôs hard to visualize or understand their relationships. **Principal Component Analysis (PCA)** is a technique used to reduce the dimensionality of the data while preserving as much information as possible.

#### What PCA Does

- Finds new axes (called **principal components**) that capture the most variance (information) in the data.
- These new axes are **uncorrelated**, meaning they represent independent features.
- Projects the original high-dimensional data onto a lower-dimensional space (usually 2D or 3D) for visualization.

#### Example

Words like "oil", "gas", "town", and "city" can be plotted in 2D space after PCA, showing clusters of related words.

#### Why Visualization Matters

Visualizing word vectors helps us see if the model captures meaningful relationships, such as grouping synonyms or related concepts close together.


### 6. ‚öôÔ∏è PCA Algorithm: How It Works

PCA involves several mathematical steps:

1. **Mean Normalize the Data:** Subtract the mean of each feature to center the data around zero.
2. **Compute the Covariance Matrix:** Measures how features vary together.
3. **Perform Singular Value Decomposition (SVD):** Decomposes the covariance matrix into eigenvectors and eigenvalues.
4. **Eigenvectors:** Directions of the new uncorrelated features (principal components).
5. **Eigenvalues:** Amount of variance (information) captured by each principal component.
6. **Project Data:** Use the dot product to project original data onto the principal components.
7. **Retain Variance:** Choose the number of components that retain most of the variance (information).

This process reduces the number of dimensions while keeping the most important information, making it easier to analyze and visualize.


### 7. üìù Summary

- **Vector Space Models** represent words and documents as vectors in a high-dimensional space.
- Words are represented based on **co-occurrence** with other words or their frequency in documents.
- **Similarity measures** like Euclidean distance and cosine similarity help compare vectors.
- **Cosine similarity** is often preferred because it focuses on the direction of vectors, handling different magnitudes better.
- Vector arithmetic can capture **semantic relationships** between words.
- **PCA** helps visualize high-dimensional vectors by reducing dimensions while preserving information.
- Understanding these concepts is fundamental for many NLP applications like machine translation, chatbots, and information extraction.



<br>

## Key Points

#### 1. üìê Vector Space Models (VSMs)  
- VSMs represent words and documents as vectors in a high-dimensional space.  
- Words are represented based on co-occurrence with other words or frequency within documents.  
- The fundamental concept: "You shall know a word by the company it keeps" (Firth, 1957).

#### 2. üî¢ Word Vector Representations  
- Word-by-word vectors count how often words co-occur within a window size \( k \).  
- Word-by-document vectors count how often words appear in different documents or categories.  
- Word-by-word captures local context; word-by-document captures global context.

#### 3. üìè Similarity Measures in Vector Spaces  
- Euclidean distance measures the straight-line distance between two vectors.  
- Euclidean distance can be misleading when comparing vectors of different magnitudes.  
- Cosine similarity measures the cosine of the angle between two vectors, ranging from 0 to 1.  
- Cosine similarity is preferred when comparing vectors of different sizes because it focuses on direction, not magnitude.

#### 4. üîÑ Vector Arithmetic and Word Relationships  
- Vector arithmetic can capture semantic relationships, e.g., Washington - USA + Russia ‚âà Moscow.  
- This property allows prediction of word relationships using vector operations.

#### 5. üëÄ Principal Component Analysis (PCA) for Visualization  
- PCA reduces high-dimensional vectors to lower dimensions (e.g., 2D or 3D) for visualization.  
- PCA finds uncorrelated features (principal components) that capture the most variance in the data.  
- Visualization helps verify if vector representations capture meaningful word relationships.

#### 6. ‚öôÔ∏è PCA Algorithm Steps  
- Mean normalize the data by subtracting the mean of each feature.  
- Compute the covariance matrix of the data.  
- Perform Singular Value Decomposition (SVD) to get eigenvectors and eigenvalues.  
- Eigenvectors represent directions of uncorrelated features; eigenvalues represent variance captured.  
- Project data onto principal components using dot product.  
- Select components that retain the highest percentage of variance.