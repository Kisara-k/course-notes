## 4.1 Seq2Seq and Attention for Neural Machine Translation

## Questions

#### 1. What is the primary limitation of the basic Seq2Seq model without attention in neural machine translation?  
A) It cannot handle variable-length input sequences  
B) It compresses the entire input sequence into a fixed-length vector, causing an information bottleneck  
C) It requires the input and output sequences to be the same length  
D) It cannot use LSTM or GRU units  


#### 2. In the Seq2Seq architecture, what role does the encoder’s final hidden state play?  
A) It initializes the decoder’s hidden state  
B) It represents the entire input sentence as a fixed-length vector  
C) It directly generates the output sequence  
D) It is discarded after encoding  


#### 3. Why are LSTMs or GRUs preferred over vanilla RNNs in Seq2Seq models?  
A) They are faster to train  
B) They avoid vanishing and exploding gradient problems  
C) They require fewer parameters  
D) They can handle variable-length sequences better  


#### 4. How does the attention mechanism improve the decoder’s ability to generate translations?  
A) By allowing the decoder to focus on all encoder hidden states dynamically  
B) By increasing the fixed-length vector size  
C) By using only the first encoder hidden state at every decoding step  
D) By ignoring the encoder hidden states and relying solely on the decoder’s previous output  


#### 5. In the attention mechanism, what are the “queries,” “keys,” and “values”?  
A) Queries come from the encoder, keys and values come from the decoder  
B) Queries come from the decoder, keys and values come from the encoder  
C) Queries, keys, and values all come from the encoder  
D) Queries, keys, and values all come from the decoder  


#### 6. What is the purpose of scaling the dot product in scaled dot-product attention?  
A) To increase the magnitude of similarity scores  
B) To stabilize gradients and prevent extremely large values when the key dimension is large  
C) To normalize the attention weights to sum to one  
D) To reduce the computational complexity of the attention mechanism  


#### 7. Which of the following statements about teacher forcing during training is true?  
A) The decoder always uses its own previous predictions as input  
B) The decoder uses the correct previous word from the target sequence as input  
C) Teacher forcing can cause errors from early steps to propagate during training  
D) Teacher forcing slows down the training process significantly  


#### 8. Why might greedy decoding produce suboptimal translations in Seq2Seq models?  
A) It selects the most probable word at each step without considering future words  
B) It samples words randomly from the probability distribution  
C) It always produces the longest possible output sequence  
D) It requires beam search to function properly  


#### 9. How does beam search differ from greedy decoding?  
A) Beam search keeps track of multiple candidate sequences simultaneously  
B) Beam search always selects the single most probable word at each step  
C) Beam search is computationally cheaper than greedy decoding  
D) Beam search requires normalization to avoid bias toward shorter sequences  


#### 10. What is a potential drawback of beam search decoding?  
A) It always produces less accurate translations than greedy decoding  
B) It penalizes longer sequences unless length normalization is applied  
C) It cannot handle variable-length sequences  
D) It ignores the attention mechanism  


#### 11. Which of the following best describes the context vector in attention-based Seq2Seq models?  
A) A fixed vector representing the entire input sentence  
B) A weighted sum of encoder hidden states based on attention weights  
C) The decoder’s hidden state at the previous time step  
D) The embedding of the current output word  


#### 12. How are attention weights computed in the attention mechanism?  
A) By applying a softmax to similarity scores between the decoder’s query and encoder’s keys  
B) By averaging all encoder hidden states equally  
C) By multiplying the decoder’s previous output with the encoder’s final hidden state  
D) By randomly assigning weights to encoder hidden states  


#### 13. What is the main advantage of using attention in languages with very different grammatical structures?  
A) Attention allows the model to reorder words flexibly during translation  
B) Attention forces the output to follow the source language word order  
C) Attention eliminates the need for tokenization  
D) Attention reduces the vocabulary size needed for translation  


#### 14. Which of the following is NOT a limitation of the BLEU score?  
A) It does not consider semantic meaning of sentences  
B) It ignores sentence structure and grammar  
C) It always rewards longer sentences  
D) It can be artificially inflated by outputting common words  


#### 15. How does ROUGE-N differ from BLEU in evaluating translations?  
A) ROUGE-N focuses on recall, measuring how many reference words appear in the candidate  
B) ROUGE-N measures precision, like BLEU  
C) ROUGE-N is only used for image captioning, not translation  
D) ROUGE-N ignores n-gram overlaps  


#### 16. What is the effect of increasing the temperature parameter during sampling in decoding?  
A) The model becomes more confident and conservative in word selection  
B) The model produces more random and diverse outputs  
C) The model always selects the most probable word  
D) The model ignores the probability distribution over words  


#### 17. In Minimum Bayes Risk (MBR) decoding, how is the best candidate translation selected?  
A) By choosing the candidate with the highest individual probability  
B) By selecting the candidate with the highest average similarity to all other candidates  
C) By picking the shortest candidate sequence  
D) By randomly sampling from the candidate set  


#### 18. Why is teacher forcing important during training of Seq2Seq models?  
A) It prevents the model from learning to generate sequences  
B) It helps the model learn faster by providing the correct previous word as input  
C) It guarantees perfect translation during training  
D) It eliminates the need for attention mechanisms  


#### 19. Which of the following statements about the encoder in Seq2Seq models is true?  
A) It produces a sequence of hidden states, one for each input token  
B) It only outputs a single hidden state at the end of the input sequence  
C) It generates the output translation directly  
D) It cannot handle variable-length input sequences  


#### 20. How does the attention mechanism affect the fixed-length memory constraint in Seq2Seq models?  
A) It removes the fixed-length memory constraint by using all encoder hidden states dynamically  
B) It increases the fixed-length vector size to accommodate longer sentences  
C) It compresses the input sequence into a smaller fixed-length vector  
D) It replaces the decoder with a feedforward network



<br>

## Answers

#### 1. What is the primary limitation of the basic Seq2Seq model without attention in neural machine translation?  
A) ✗ It can handle variable-length input sequences.  
B) ✓ The fixed-length vector causes an information bottleneck limiting performance on long sequences.  
C) ✗ Input and output sequences can have different lengths in Seq2Seq.  
D) ✗ LSTMs and GRUs are commonly used to avoid gradient issues, so this is not a limitation.  

**Correct:** B


#### 2. In the Seq2Seq architecture, what role does the encoder’s final hidden state play?  
A) ✓ It initializes the decoder’s hidden state.  
B) ✓ It represents the entire input sentence as a fixed-length vector.  
C) ✗ It does not directly generate the output sequence; the decoder does.  
D) ✗ It is not discarded; it is essential for decoding.  

**Correct:** A, B


#### 3. Why are LSTMs or GRUs preferred over vanilla RNNs in Seq2Seq models?  
A) ✗ They are not necessarily faster to train.  
B) ✓ They mitigate vanishing and exploding gradient problems.  
C) ✗ They often have more parameters due to gating mechanisms.  
D) ✗ Handling variable-length sequences is a property of RNNs in general, not specific to LSTMs/GRUs.  

**Correct:** B


#### 4. How does the attention mechanism improve the decoder’s ability to generate translations?  
A) ✓ It allows dynamic focus on all encoder hidden states at each decoding step.  
B) ✗ It does not increase the fixed-length vector size; it uses all hidden states instead.  
C) ✗ It does not use only the first hidden state; it uses all with weights.  
D) ✗ It does not ignore encoder states; it explicitly uses them.  

**Correct:** A


#### 5. In the attention mechanism, what are the “queries,” “keys,” and “values”?  
A) ✗ Queries come from the decoder, not the encoder.  
B) ✓ Queries come from the decoder; keys and values come from the encoder.  
C) ✗ Queries come from the decoder, not all from encoder.  
D) ✗ Keys and values come from encoder, queries from decoder.  

**Correct:** B


#### 6. What is the purpose of scaling the dot product in scaled dot-product attention?  
A) ✗ Scaling reduces, not increases, magnitude to stabilize training.  
B) ✓ It stabilizes gradients by preventing large dot product values when key dimension is large.  
C) ✗ Softmax normalizes weights, scaling is separate.  
D) ✗ Scaling does not reduce computational complexity.  

**Correct:** B


#### 7. Which of the following statements about teacher forcing during training is true?  
A) ✗ Teacher forcing uses the correct previous word, not the model’s own prediction.  
B) ✓ The decoder uses the correct previous word from the target sequence as input.  
C) ✗ Teacher forcing reduces error propagation during training.  
D) ✗ Teacher forcing generally speeds up training convergence.  

**Correct:** B


#### 8. Why might greedy decoding produce suboptimal translations in Seq2Seq models?  
A) ✓ It selects the most probable word at each step without considering future context.  
B) ✗ Random sampling is different from greedy decoding.  
C) ✗ Greedy decoding does not always produce the longest output.  
D) ✗ Greedy decoding works independently of beam search.  

**Correct:** A


#### 9. How does beam search differ from greedy decoding?  
A) ✓ Beam search keeps track of multiple candidate sequences simultaneously.  
B) ✗ Greedy decoding selects a single most probable word; beam search explores multiple.  
C) ✗ Beam search is more computationally expensive than greedy decoding.  
D) ✓ Beam search requires length normalization to avoid bias toward short sequences.  

**Correct:** A, D


#### 10. What is a potential drawback of beam search decoding?  
A) ✗ Beam search usually produces better, not worse, translations than greedy decoding.  
B) ✓ It penalizes longer sequences unless length normalization is applied.  
C) ✗ Beam search can handle variable-length sequences.  
D) ✗ Beam search uses attention mechanisms as usual.  

**Correct:** B


#### 11. Which of the following best describes the context vector in attention-based Seq2Seq models?  
A) ✗ It is not fixed; it changes at each decoding step.  
B) ✓ It is a weighted sum of encoder hidden states based on attention weights.  
C) ✗ The decoder’s hidden state is separate from the context vector.  
D) ✗ The context vector is not the embedding of the current output word.  

**Correct:** B


#### 12. How are attention weights computed in the attention mechanism?  
A) ✓ By applying softmax to similarity scores between decoder query and encoder keys.  
B) ✗ Encoder hidden states are not averaged equally; weights vary.  
C) ✗ Attention weights are not computed by multiplying decoder output with final encoder state.  
D) ✗ Weights are learned, not random.  

**Correct:** A


#### 13. What is the main advantage of using attention in languages with very different grammatical structures?  
A) ✓ Attention allows flexible reordering by focusing on relevant input words dynamically.  
B) ✗ Attention does not force output to follow source word order.  
C) ✗ Attention does not eliminate the need for tokenization.  
D) ✗ Attention does not reduce vocabulary size.  

**Correct:** A


#### 14. Which of the following is NOT a limitation of the BLEU score?  
A) ✗ BLEU does not consider semantic meaning.  
B) ✗ BLEU ignores sentence structure and grammar.  
C) ✓ BLEU does not always reward longer sentences; it can penalize them.  
D) ✗ BLEU can be artificially inflated by outputting common words.  

**Correct:** C


#### 15. How does ROUGE-N differ from BLEU in evaluating translations?  
A) ✓ ROUGE-N focuses on recall: how many reference words appear in the candidate.  
B) ✗ ROUGE-N is recall-oriented, not precision like BLEU.  
C) ✗ ROUGE-N is used in translation and summarization, not only image captioning.  
D) ✗ ROUGE-N measures n-gram overlap, not ignoring it.  

**Correct:** A


#### 16. What is the effect of increasing the temperature parameter during sampling in decoding?  
A) ✗ Lower temperature makes the model more confident, not higher.  
B) ✓ Higher temperature increases randomness and diversity in outputs.  
C) ✗ Model does not always select the most probable word at high temperature.  
D) ✗ Temperature scales the probability distribution; it does not ignore it.  

**Correct:** B


#### 17. In Minimum Bayes Risk (MBR) decoding, how is the best candidate translation selected?  
A) ✗ It is not based on individual probability alone.  
B) ✓ It selects the candidate with the highest average similarity to all other candidates.  
C) ✗ It does not pick the shortest sequence by default.  
D) ✗ It is not random sampling.  

**Correct:** B


#### 18. Why is teacher forcing important during training of Seq2Seq models?  
A) ✗ It helps the model learn to generate sequences, not prevent it.  
B) ✓ It speeds up learning by providing the correct previous word as input.  
C) ✗ It does not guarantee perfect translation during training.  
D) ✗ It does not eliminate the need for attention.  

**Correct:** B


#### 19. Which of the following statements about the encoder in Seq2Seq models is true?  
A) ✓ It produces a sequence of hidden states, one per input token.  
B) ✗ It does not output only a single hidden state; all hidden states are available.  
C) ✗ It does not generate the output translation directly.  
D) ✗ It can handle variable-length input sequences.  

**Correct:** A, D


#### 20. How does the attention mechanism affect the fixed-length memory constraint in Seq2Seq models?  
A) ✓ It removes the fixed-length memory constraint by dynamically using all encoder hidden states.  
B) ✗ It does not increase the fixed-length vector size.  
C) ✗ It does not compress the input into a smaller vector; it uses all hidden states.  
D) ✗ It does not replace the decoder with a feedforward network.  

**Correct:** A