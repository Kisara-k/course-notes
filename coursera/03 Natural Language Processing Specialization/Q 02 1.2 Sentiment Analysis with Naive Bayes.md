## 1.2 Sentiment Analysis with Naive Bayes

## Questions

#### 1. What does the Naive Bayes classifier assume about the features (words) in a text?  
A) All words are dependent on each other  
B) All words are independent given the class  
C) Words have equal probability in all classes  
D) Word order is crucial for classification  

#### 2. Given a dataset with 65% positive tweets and 35% negative tweets, what is the prior probability \( P(Positive) \)?  
A) 0.35  
B) 0.65  
C) 0.50  
D) Cannot be determined without word frequencies  

#### 3. If \( P(happy | Positive) = 0.25 \), \( P(Positive) = 0.40 \), and \( P(happy) = 0.13 \), what does Bayes’ rule calculate?  
A) Probability a tweet contains “happy” given it is positive  
B) Probability a tweet is positive given it contains “happy”  
C) Probability a tweet is negative given it contains “happy”  
D) Probability a tweet contains “happy” regardless of sentiment  

#### 4. Why is Laplacian smoothing necessary in Naive Bayes?  
A) To increase the probability of frequent words  
B) To avoid zero probabilities for unseen words  
C) To normalize the prior probabilities  
D) To remove stop words from the dataset  

#### 5. Which of the following is true about the log likelihood in Naive Bayes?  
A) It converts sums into products to simplify calculations  
B) It helps prevent numerical underflow when multiplying many probabilities  
C) It is only used when the dataset is very small  
D) It always produces probabilities greater than 1  

#### 6. When classifying a tweet, how does Naive Bayes combine the probabilities of individual words?  
A) By summing the probabilities of each word  
B) By multiplying the probabilities of each word  
C) By summing the logarithms of the probabilities of each word  
D) By taking the maximum probability among the words  

#### 7. Which of the following can cause errors in Naive Bayes sentiment classification?  
A) Ignoring word order  
B) Removing punctuation and stop words  
C) Using Laplacian smoothing  
D) Sarcasm and irony in tweets  

#### 8. What is the effect of the independence assumption in Naive Bayes on natural language processing?  
A) It perfectly models word dependencies  
B) It simplifies computation but ignores word context  
C) It improves accuracy by considering word order  
D) It makes the model unable to classify any text  

#### 9. If a word appears only in negative tweets during training, what will be its probability \( P(word | Positive) \) after Laplacian smoothing?  
A) Exactly zero  
B) Slightly greater than zero  
C) Equal to \( P(word | Negative) \)  
D) Equal to one  

#### 10. How does Naive Bayes handle words in a test tweet that were never seen during training?  
A) It assigns zero probability and rejects the tweet  
B) It ignores those words completely  
C) Laplacian smoothing assigns a small non-zero probability  
D) It treats them as stop words  

#### 11. Which of the following best describes the “prior” in Naive Bayes classification?  
A) The probability of a word appearing in a tweet  
B) The probability of a class before seeing any words  
C) The probability of a tweet given a class  
D) The probability of a tweet containing a specific word  

#### 12. Why might Naive Bayes perform poorly on tweets with negations like “not happy”?  
A) Because it treats “not” and “happy” as independent words  
B) Because it removes “not” during preprocessing  
C) Because it cannot handle words with multiple meanings  
D) Because it always assumes positive sentiment for “happy”  

#### 13. Which of the following is NOT a typical preprocessing step before training Naive Bayes?  
A) Lowercasing all words  
B) Removing punctuation and URLs  
C) Stemming or lemmatization  
D) Randomly shuffling word order  

#### 14. What does the term “log prior” refer to in Naive Bayes?  
A) The logarithm of the ratio of positive to negative class probabilities  
B) The logarithm of the probability of each word  
C) The logarithm of the total number of words in the corpus  
D) The logarithm of the smoothing parameter  

#### 15. Consider two tweets: “I am happy because I am learning” and “I am not happy because I am learning.” Why might Naive Bayes struggle to distinguish their sentiments?  
A) Because it ignores the word “not”  
B) Because it treats all words independently and ignores word order  
C) Because it does not use Laplacian smoothing  
D) Because it only looks at the first word in the tweet  

#### 16. Which of the following applications is NOT commonly associated with Naive Bayes?  
A) Spam filtering  
B) Image recognition  
C) Author identification  
D) Word sense disambiguation  

#### 17. What is the main reason Naive Bayes is considered a “baseline” model in NLP?  
A) It is the most accurate model available  
B) It is simple, fast, and provides a good starting point  
C) It requires no training data  
D) It models complex word dependencies  

#### 18. How does the relative frequency of classes in the training data affect Naive Bayes?  
A) It has no effect on classification  
B) It influences the prior probabilities and can bias predictions  
C) It only affects the smoothing parameter  
D) It changes the independence assumption  

#### 19. Which of the following statements about Bayes’ rule is correct?  
A) It requires knowledge of the joint probability of two events  
B) It expresses \( P(A|B) \) in terms of \( P(B|A) \), \( P(A) \), and \( P(B) \)  
C) It can only be applied to independent events  
D) It is used to calculate the probability of a word given a class  

#### 20. Why might removing punctuation during preprocessing lead to errors in sentiment classification?  
A) Because punctuation never carries sentiment information  
B) Because punctuation can change the meaning or tone of a sentence  
C) Because it increases the vocabulary size unnecessarily  
D) Because it causes Laplacian smoothing to fail



<br>

## Answers

#### 1. What does the Naive Bayes classifier assume about the features (words) in a text?  
A) ✗ Words are not independent; this contradicts the naive assumption.  
B) ✓ Naive Bayes assumes words are independent given the class.  
C) ✗ Words do not have equal probability across classes; probabilities differ by class.  
D) ✗ Word order is ignored in Naive Bayes.  

**Correct:** B


#### 2. Given a dataset with 65% positive tweets and 35% negative tweets, what is the prior probability \( P(Positive) \)?  
A) ✗ 0.35 is the negative class prior.  
B) ✓ 0.65 is the correct prior for positive tweets.  
C) ✗ 0.50 is incorrect unless classes are balanced.  
D) ✗ Prior can be determined from class counts, no word frequencies needed.  

**Correct:** B


#### 3. If \( P(happy | Positive) = 0.25 \), \( P(Positive) = 0.40 \), and \( P(happy) = 0.13 \), what does Bayes’ rule calculate?  
A) ✗ This is the conditional probability given the class, not what Bayes’ rule calculates here.  
B) ✓ Bayes’ rule calculates \( P(Positive | happy) \), the probability tweet is positive given “happy.”  
C) ✗ This is not the probability of negative given “happy.”  
D) ✗ \( P(happy) \) is given, not calculated by Bayes’ rule here.  

**Correct:** B


#### 4. Why is Laplacian smoothing necessary in Naive Bayes?  
A) ✗ It does not increase probabilities of frequent words specifically.  
B) ✓ It prevents zero probabilities for unseen words, avoiding zeroing out the product.  
C) ✗ It does not normalize priors.  
D) ✗ It is unrelated to stop word removal.  

**Correct:** B


#### 5. Which of the following is true about the log likelihood in Naive Bayes?  
A) ✗ Log likelihood converts products into sums, not sums into products.  
B) ✓ It prevents numerical underflow by summing logs instead of multiplying probabilities.  
C) ✗ It is used regardless of dataset size.  
D) ✗ Log likelihood values are not probabilities and can be negative or >1 in raw form.  

**Correct:** B


#### 6. When classifying a tweet, how does Naive Bayes combine the probabilities of individual words?  
A) ✗ Probabilities are multiplied, not summed directly.  
B) ✗ Multiplying probabilities is correct but computationally unstable.  
C) ✓ Logarithms of probabilities are summed to avoid underflow.  
D) ✗ Maximum probability is not used; all words contribute.  

**Correct:** C


#### 7. Which of the following can cause errors in Naive Bayes sentiment classification?  
A) ✓ Ignoring word order loses important context.  
B) ✓ Removing punctuation and stop words can remove sentiment cues.  
C) ✗ Laplacian smoothing reduces errors, does not cause them.  
D) ✓ Sarcasm and irony confuse the model as it relies on literal word meaning.  

**Correct:** A,B,D


#### 8. What is the effect of the independence assumption in Naive Bayes on natural language processing?  
A) ✗ It does not model dependencies perfectly.  
B) ✓ It simplifies computation but ignores word context and dependencies.  
C) ✗ It does not improve accuracy by considering word order.  
D) ✗ It does not make the model unusable.  

**Correct:** B


#### 9. If a word appears only in negative tweets during training, what will be its probability \( P(word | Positive) \) after Laplacian smoothing?  
A) ✗ It will not be zero due to smoothing.  
B) ✓ Slightly greater than zero because smoothing adds 1 to counts.  
C) ✗ It will not equal \( P(word | Negative) \) since counts differ.  
D) ✗ It cannot be one unless it appears exclusively in positive tweets.  

**Correct:** B


#### 10. How does Naive Bayes handle words in a test tweet that were never seen during training?  
A) ✗ It does not assign zero probability due to smoothing.  
B) ✗ It does not ignore unseen words; they contribute small probability.  
C) ✓ Laplacian smoothing assigns a small non-zero probability to unseen words.  
D) ✗ It does not treat unseen words as stop words.  

**Correct:** C


#### 11. Which of the following best describes the “prior” in Naive Bayes classification?  
A) ✗ Prior is about classes, not individual words.  
B) ✓ Prior is the probability of a class before seeing any words.  
C) ✗ This is the likelihood, not the prior.  
D) ✗ This is a marginal probability of a word, not the prior.  

**Correct:** B


#### 12. Why might Naive Bayes perform poorly on tweets with negations like “not happy”?  
A) ✓ Because it treats “not” and “happy” independently, missing negation effect.  
B) ✗ Negations are usually kept during preprocessing; removing them is not standard.  
C) ✗ Word ambiguity is a separate issue.  
D) ✗ It does not always assume “happy” is positive regardless of context.  

**Correct:** A


#### 13. Which of the following is NOT a typical preprocessing step before training Naive Bayes?  
A) ✗ Lowercasing is standard.  
B) ✗ Removing punctuation and URLs is standard.  
C) ✗ Stemming or lemmatization is common.  
D) ✓ Randomly shuffling word order is not done; word order is ignored but not shuffled.  

**Correct:** D


#### 14. What does the term “log prior” refer to in Naive Bayes?  
A) ✓ Logarithm of the ratio of positive to negative class probabilities.  
B) ✗ Log prior is about classes, not individual words.  
C) ✗ This is unrelated to log prior.  
D) ✗ Smoothing parameter is unrelated to log prior.  

**Correct:** A


#### 15. Consider two tweets: “I am happy because I am learning” and “I am not happy because I am learning.” Why might Naive Bayes struggle to distinguish their sentiments?  
A) ✗ “Not” is usually kept during preprocessing.  
B) ✓ Because it treats words independently and ignores word order, missing negation effect.  
C) ✗ Laplacian smoothing is unrelated to this issue.  
D) ✗ It uses all words, not just the first.  

**Correct:** B


#### 16. Which of the following applications is NOT commonly associated with Naive Bayes?  
A) ✗ Spam filtering is a common application.  
B) ✓ Image recognition is generally not done with Naive Bayes.  
C) ✗ Author identification is a known application.  
D) ✗ Word sense disambiguation is a common use case.  

**Correct:** B


#### 17. What is the main reason Naive Bayes is considered a “baseline” model in NLP?  
A) ✗ It is not the most accurate model available.  
B) ✓ It is simple, fast, and provides a good starting point.  
C) ✗ It requires training data.  
D) ✗ It does not model complex dependencies.  

**Correct:** B


#### 18. How does the relative frequency of classes in the training data affect Naive Bayes?  
A) ✗ It does affect classification through priors.  
B) ✓ It influences prior probabilities and can bias predictions if unbalanced.  
C) ✗ It does not only affect smoothing.  
D) ✗ It does not change the independence assumption.  

**Correct:** B


#### 19. Which of the following statements about Bayes’ rule is correct?  
A) ✓ Bayes’ rule uses conditional probabilities, which relate to joint probabilities.  
B) ✓ It expresses \( P(A|B) \) in terms of \( P(B|A) \), \( P(A) \), and \( P(B) \).  
C) ✗ It can be applied to dependent events as well.  
D) ✗ Bayes’ rule calculates posterior probabilities, not directly \( P(word|class) \).  

**Correct:** A,B


#### 20. Why might removing punctuation during preprocessing lead to errors in sentiment classification?  
A) ✗ Punctuation often carries sentiment or tone.  
B) ✓ Removing punctuation can change meaning or tone, affecting sentiment.  
C) ✗ Removing punctuation reduces vocabulary size, not increases it.  
D) ✗ It does not cause Laplacian smoothing to fail.  

**Correct:** B