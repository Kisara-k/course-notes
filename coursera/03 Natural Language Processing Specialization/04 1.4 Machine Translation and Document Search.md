## 1.4 Machine Translation and Document Search



### Key Points

#### 1. üîÑ Vector Transformation for Machine Translation  
- Machine translation can be performed by learning a transformation matrix \( R \) that maps English word vectors to French word vectors.  
- The transformation is a linear operation: French vector ‚âà \( R \times \) English vector.  
- The Frobenius norm measures the difference between transformed vectors and target vectors during training.  

#### 2. üîç K-Nearest Neighbors (K-NN)  
- K-NN finds the \( K \) closest vectors to a query vector based on distance metrics like Euclidean or cosine distance.  
- After transforming a word vector, K-NN is used to find the closest translated word vectors.  
- K-NN can also be applied to document search by comparing document vectors.  

#### 3. üóÉÔ∏è Hash Tables and Hash Functions  
- Hash functions map vectors to hash values that determine which bucket in a hash table the vector belongs to.  
- Searching for nearest neighbors is faster by only looking within the bucket(s) corresponding to the query‚Äôs hash value.  

#### 4. üìê Locality Sensitive Hashing (LSH)  
- LSH uses random hyperplanes to divide vector space; the sign of the dot product between a vector and a plane‚Äôs normal vector determines the vector‚Äôs side.  
- Each plane produces a bit (0 or 1) based on the sign, and multiple planes combine these bits into a binary hash value.  
- Similar vectors tend to have similar hash values, so they are grouped in the same bucket.  

#### 5. üåê Approximate Nearest Neighbors  
- Approximate nearest neighbor search uses multiple sets of random planes (multiple hash tables) to increase the chance of finding close neighbors efficiently.  
- This method trades some accuracy for much faster search in high-dimensional spaces.  

#### 6. üìÑ Document Representation and Search  
- Documents are represented as vectors by summing or averaging the vectors of the words they contain.  
- Document search uses K-NN to find documents with vectors closest to the query vector.

<br>

## Study Notes





### 1. üåç Introduction to Machine Translation and Document Search

In this lecture, we explore two important applications of deep learning and vector representations in natural language processing (NLP): **machine translation** and **document search**. Both tasks rely on representing words and documents as vectors in a high-dimensional space and then manipulating these vectors to find meaningful relationships.

- **Machine Translation** is about converting text from one language to another, for example, translating ‚Äúhello!‚Äù in English to ‚Äúbonjour!‚Äù in French.
- **Document Search** involves finding relevant documents based on a query, such as searching for ‚ÄúCan I get a refund?‚Äù and retrieving documents that talk about return policies or refunds.

To achieve these tasks, we use techniques like transforming word vectors, finding nearest neighbors in vector space, and efficient search methods like hash tables and locality sensitive hashing.


### 2. üîÑ Transforming Word Vectors for Machine Translation

#### What is Vector Transformation?

Words can be represented as vectors ‚Äî lists of numbers that capture their meaning in a mathematical space. For example, the English word ‚Äúcat‚Äù might be represented as `[1, 0, 1]`, and the French word ‚Äúchat‚Äù as `[2, 3, 2]`. To translate between languages, we want to find a **transformation** (a mathematical function) that converts English word vectors into their French equivalents.

#### How Does This Work?

- We represent words as vectors in their respective languages.
- We learn a **transformation matrix** \( R \) that, when multiplied by an English word vector, produces a vector close to the corresponding French word vector.
- This transformation is a **linear transformation**, meaning it can be represented by a matrix multiplication:  
  \[
  \text{French vector} \approx R \times \text{English vector}
  \]

#### Example:

If \( R = \begin{bmatrix} 2 & 0 \\ 0 & -2 \end{bmatrix} \) and the English vector is \([1, 1]\), then the transformed vector is:
\[
[1, 1] \times R = [2, -2]
\]

#### How Do We Find \( R \)?

- We start with a small set of known word pairs (English-French).
- We initialize \( R \) randomly.
- We use an optimization method (like gradient descent) to minimize the difference between \( R \times \text{English vector} \) and the French vector.
- The difference is measured using the **Frobenius norm**, which is a way to measure the size of a matrix or the error between matrices.

#### Frobenius Norm Explained:

- Think of it as the square root of the sum of the squares of all elements in a matrix.
- For example, for matrix \( A = \begin{bmatrix} 2 & 2 \\ 2 & 2 \end{bmatrix} \),  
  square each element: \( 2^2 = 4 \), sum all: \(4+4+4+4=16\), then take the square root: \(\sqrt{16} = 4\).

This norm helps us quantify how close our transformed vectors are to the target vectors.


### 3. üîç K-Nearest Neighbors (K-NN) for Finding Similar Words and Documents

#### What is K-Nearest Neighbors?

K-NN is a simple but powerful method to find the closest points (neighbors) to a given point in vector space. For example, if you have a vector for the word ‚Äúhello,‚Äù K-NN helps find the closest vectors, which might be ‚Äúbonjour‚Äù or ‚Äúsalut‚Äù in French.

#### How Does K-NN Work?

- Given a query vector, calculate the distance (usually Euclidean or cosine distance) to all other vectors.
- Select the top \( K \) closest vectors.
- These neighbors are considered the most similar or relevant.

#### Application in Translation:

- After transforming an English word vector using matrix \( R \), we find the nearest French word vectors.
- The closest French word vector is the translation.

#### Application in Document Search:

- Represent documents as vectors (more on this later).
- Given a query vector, find the nearest document vectors.
- Return the documents that are closest to the query.


### 4. üóÉÔ∏è Hash Tables and Hash Functions for Efficient Search

#### Why Use Hash Tables?

Searching through all vectors to find nearest neighbors can be very slow when you have millions of words or documents. Hash tables help speed up this search by grouping vectors into buckets based on a **hash value**.

#### What is a Hash Function?

- A hash function takes a vector and returns a number (hash value) that determines which bucket the vector belongs to.
- For example, a simple hash function might be:  
  \[
  \text{hash value} = \text{vector} \mod \text{number of buckets}
  \]
- Vectors with the same hash value go into the same bucket.

#### How Does This Help?

- When searching for neighbors, you only look inside the bucket(s) corresponding to the query‚Äôs hash value.
- This reduces the search space drastically.

#### Example:

If you have 3 buckets (0, 1, 2), and a vector‚Äôs hash value is 1, you only search bucket 1 for neighbors.


### 5. üìê Locality Sensitive Hashing (LSH) ‚Äî Dividing Vector Space with Planes

#### What is Locality Sensitive Hashing?

LSH is a special kind of hashing designed to keep similar vectors in the same bucket with high probability. It works by dividing the vector space using **random hyperplanes**.

#### How Does It Work?

- Imagine a plane slicing through the vector space.
- For each vector, determine which side of the plane it lies on by calculating the **dot product** between the vector and the plane‚Äôs normal vector.
- The **sign** of the dot product (+ or -) tells you the side.
- This sign acts like a bit (0 or 1) in the hash value.

#### Multiple Planes for More Precision

- Use multiple random planes.
- For each plane, get a bit (0 or 1) depending on which side the vector lies.
- Combine these bits into a binary number ‚Äî this is the hash value.
- Vectors close to each other tend to have similar signs for these planes, so they end up in the same bucket.

#### Example:

If you have 3 planes, and a vector lies on the positive side of planes 1 and 3, but negative side of plane 2, the hash might be `101` in binary, which equals 5 in decimal.


### 6. üåê Approximate Nearest Neighbors with Multiple Hash Tables

#### Why Approximate?

Exact nearest neighbor search is expensive in high dimensions. Approximate methods trade a bit of accuracy for much faster search.

#### How Does Approximate Nearest Neighbor Search Work?

- Use **multiple sets of random planes** to create multiple hash tables.
- Each hash table partitions the space differently.
- When searching, query all hash tables and combine results.
- This increases the chance of finding close neighbors without checking every vector.

#### Cultural Reference:

The lecture humorously references *Spider-Man: Into the Spider-Verse* to illustrate multiple ‚Äúuniverses‚Äù or hash tables, each providing a different perspective on the data.


### 7. üìÑ Document Representation and Search

#### How Do We Represent Documents as Vectors?

- Documents are made up of words, each with its own vector.
- To get a document vector, sum (or average) the vectors of all words in the document.
- For example, if the document is ‚ÄúI love learning!‚Äù and the word vectors are:  
  - ‚ÄúI‚Äù = [1, 0, 1]  
  - ‚Äúlove‚Äù = [-1, 0, 1]  
  - ‚Äúlearning‚Äù = [1, 0, 1]  
  Then the document vector is:  
  \[
  [1,0,1] + [-1,0,1] + [1,0,1] = [1, 0, 3]
  \]

#### Searching Documents with K-NN

- Represent all documents as vectors.
- Given a query (also converted to a vector), find the nearest document vectors using K-NN.
- Return the most relevant documents.


### 8. üìù Summary and Learning Objectives Recap

By the end of this lecture, you should understand:

- How to **transform word vectors** using a matrix to perform machine translation.
- How to use **K-nearest neighbors** to find similar words or documents.
- The role of **hash tables** and **hash functions** in speeding up nearest neighbor search.
- How **locality sensitive hashing (LSH)** divides vector space using random planes to group similar vectors.
- How to perform **approximate nearest neighbor search** using multiple hash tables.
- How to represent documents as vectors by combining word vectors.
- How to apply these techniques to **machine translation** and **document search** tasks.


If you want to practice these concepts, the lecture suggests trying out coding exercises like implementing the transformation matrix, Frobenius norm, K-NN search, hash functions, and LSH.



<br>

## Questions



#### 1. What is the primary purpose of the transformation matrix \( R \) in machine translation using word vectors?  
A) To convert French word vectors into English word vectors  
B) To linearly map English word vectors to approximate French word vectors  
C) To calculate the Frobenius norm of word vectors  
D) To cluster word vectors into buckets for faster search  

#### 2. Which of the following best describes the Frobenius norm?  
A) The sum of the absolute values of all elements in a matrix  
B) The square root of the sum of the squares of all elements in a matrix  
C) The maximum value in a matrix  
D) The dot product between two vectors  

#### 3. When using K-nearest neighbors (K-NN) for word translation, what is the role of the K parameter?  
A) It determines the number of closest vectors to consider as potential translations  
B) It sets the dimensionality of the word vectors  
C) It controls the number of hash tables used in locality sensitive hashing  
D) It defines the number of random planes used for hashing  

#### 4. Why are hash tables used in nearest neighbor search for large vocabularies?  
A) To reduce the dimensionality of vectors  
B) To speed up search by limiting comparisons to a subset of vectors  
C) To transform vectors into a new language space  
D) To calculate exact nearest neighbors without approximation  

#### 5. Which of the following statements about hash functions is true?  
A) A hash function always produces unique hash values for different vectors  
B) Hash functions map vectors to buckets based on a deterministic rule  
C) Hash functions reduce vectors to scalar values by summing their elements  
D) Hash functions are only useful for exact nearest neighbor search  

#### 6. In locality sensitive hashing (LSH), what does the sign of the dot product between a vector and a plane‚Äôs normal vector represent?  
A) The magnitude of the vector‚Äôs projection on the plane  
B) Which side of the plane the vector lies on  
C) The distance from the vector to the plane  
D) The angle between the vector and the plane  

#### 7. How does using multiple random planes in LSH improve the hashing process?  
A) It increases the dimensionality of the vectors  
B) It creates a more fine-grained partitioning of the vector space  
C) It guarantees exact nearest neighbor retrieval  
D) It reduces the number of hash buckets needed  

#### 8. What is the main advantage of approximate nearest neighbor search over exact nearest neighbor search?  
A) It always finds the exact closest vector  
B) It significantly reduces computation time with some loss in accuracy  
C) It requires fewer hash tables  
D) It eliminates the need for vector transformations  

#### 9. When representing a document as a vector by summing word vectors, which of the following is true?  
A) The document vector is always normalized to unit length  
B) The document vector captures the combined meaning of all words in the document  
C) The order of words in the document affects the resulting vector  
D) Words not in the vocabulary are ignored or treated as zero vectors  

#### 10. Which of the following is NOT a reason to use a transformation matrix for machine translation?  
A) To align word vectors from different languages into a common space  
B) To enable direct comparison of word meanings across languages  
C) To reduce the size of the vocabulary in each language  
D) To learn a linear mapping that minimizes translation error  

#### 11. What does the Frobenius norm measure when used as a loss function in learning the transformation matrix?  
A) The total squared difference between transformed English vectors and French vectors  
B) The cosine similarity between English and French word vectors  
C) The Euclidean distance between two individual word vectors  
D) The number of mismatched words in the vocabulary  

#### 12. In the context of hash tables, what is a potential downside of using a simple hash function like `hash_value = vector % number_of_buckets`?  
A) It may cause many vectors to collide in the same bucket, reducing search efficiency  
B) It guarantees perfect distribution of vectors across buckets  
C) It requires complex matrix multiplications  
D) It only works for vectors with integer values  

#### 13. Why is the sign of the dot product used as a bit in locality sensitive hashing instead of the actual dot product value?  
A) Because the sign is invariant to vector magnitude and captures relative position  
B) Because the actual dot product is always zero  
C) Because the sign encodes the exact distance to the plane  
D) Because the sign is easier to compute than the dot product  

#### 14. How does combining bits from multiple planes into a single hash value help in LSH?  
A) It creates a unique identifier for each vector in the dataset  
B) It encodes the vector‚Äôs position relative to multiple planes, improving similarity grouping  
C) It reduces the dimensionality of the vector to a scalar  
D) It guarantees that only identical vectors share the same hash value  

#### 15. Which of the following best describes the relationship between the number of hash tables and the accuracy of approximate nearest neighbor search?  
A) More hash tables generally increase accuracy but also increase computation  
B) More hash tables always decrease accuracy  
C) The number of hash tables does not affect accuracy  
D) Fewer hash tables guarantee better recall  

#### 16. When searching for documents using K-NN, what is a key limitation of simply summing word vectors to represent documents?  
A) It ignores word order and syntax, potentially losing important context  
B) It requires very large hash tables  
C) It cannot be used with locality sensitive hashing  
D) It only works for documents with a single word  

#### 17. Which of the following is true about the transformation matrix \( R \) learned for machine translation?  
A) It is always an orthogonal matrix  
B) It is learned by minimizing the Frobenius norm of the difference between transformed and target vectors  
C) It can be non-linear and learned using deep neural networks only  
D) It maps vectors from the target language back to the source language  

#### 18. What is the main challenge addressed by locality sensitive hashing in high-dimensional vector spaces?  
A) The curse of dimensionality making exact nearest neighbor search computationally expensive  
B) The inability to represent words as vectors  
C) The lack of available training data for translation  
D) The difficulty of computing dot products in low dimensions  

#### 19. In the context of document search, why might approximate nearest neighbor methods be preferred over exact methods?  
A) Because documents are always very short  
B) Because approximate methods scale better to large document collections with high-dimensional vectors  
C) Because exact methods cannot handle vector representations  
D) Because approximate methods guarantee perfect recall  

#### 20. Which of the following statements about K-nearest neighbors and hash tables is correct?  
A) K-NN always requires checking every vector in the dataset  
B) Hash tables can be used to reduce the search space for K-NN queries  
C) Hash tables eliminate the need for vector transformations  
D) K-NN is only applicable to machine translation, not document search  



<br>

## Answers



#### 1. What is the primary purpose of the transformation matrix \( R \) in machine translation using word vectors?  
A) ‚úó It maps English to French, not French to English (though inverse is possible).  
B) ‚úì Correct: It linearly maps English vectors to approximate French vectors.  
C) ‚úó Frobenius norm measures error, not the purpose of \( R \).  
D) ‚úó Hashing is unrelated to the transformation matrix‚Äôs role.  

**Correct:** B


#### 2. Which of the following best describes the Frobenius norm?  
A) ‚úó It sums squares, not absolute values.  
B) ‚úì Correct: It is the square root of the sum of squares of all matrix elements.  
C) ‚úó It‚Äôs not the maximum value.  
D) ‚úó Dot product is between vectors, not a matrix norm.  

**Correct:** B


#### 3. When using K-nearest neighbors (K-NN) for word translation, what is the role of the K parameter?  
A) ‚úì Correct: K is the number of closest vectors considered.  
B) ‚úó K does not set vector dimensionality.  
C) ‚úó K does not control hash tables.  
D) ‚úó K does not define number of planes.  

**Correct:** A


#### 4. Why are hash tables used in nearest neighbor search for large vocabularies?  
A) ‚úó Hash tables do not reduce dimensionality.  
B) ‚úì Correct: They speed up search by limiting comparisons to buckets.  
C) ‚úó Hash tables do not transform vectors between languages.  
D) ‚úó Hash tables approximate search, not exact search.  

**Correct:** B


#### 5. Which of the following statements about hash functions is true?  
A) ‚úó Hash collisions can occur; uniqueness is not guaranteed.  
B) ‚úì Correct: Hash functions deterministically map vectors to buckets.  
C) ‚úó Hash functions do not simply sum elements.  
D) ‚úó Hash functions are useful for approximate, not only exact, search.  

**Correct:** B


#### 6. In locality sensitive hashing (LSH), what does the sign of the dot product between a vector and a plane‚Äôs normal vector represent?  
A) ‚úó Magnitude is not represented by sign.  
B) ‚úì Correct: Sign indicates which side of the plane the vector lies on.  
C) ‚úó Sign does not measure distance.  
D) ‚úó Sign does not directly represent angle.  

**Correct:** B


#### 7. How does using multiple random planes in LSH improve the hashing process?  
A) ‚úó It does not increase vector dimensionality.  
B) ‚úì Correct: Multiple planes create finer partitions of space.  
C) ‚úó LSH is approximate, not exact.  
D) ‚úó More planes usually increase buckets, not reduce them.  

**Correct:** B


#### 8. What is the main advantage of approximate nearest neighbor search over exact nearest neighbor search?  
A) ‚úó Approximate does not guarantee exact closest vector.  
B) ‚úì Correct: It reduces computation time with some accuracy loss.  
C) ‚úó Number of hash tables varies, not necessarily fewer.  
D) ‚úó Approximate search still uses vector transformations.  

**Correct:** B


#### 9. When representing a document as a vector by summing word vectors, which of the following is true?  
A) ‚úó Normalization is optional, not always done.  
B) ‚úì Correct: Summing captures combined meaning of words.  
C) ‚úó Word order is ignored in simple summation.  
D) ‚úì Correct: Unknown words are ignored or zeroed.  

**Correct:** B, D


#### 10. Which of the following is NOT a reason to use a transformation matrix for machine translation?  
A) ‚úó Aligning vectors is a key reason.  
B) ‚úó Enables cross-language comparison.  
C) ‚úì Correct: It does not reduce vocabulary size.  
D) ‚úó Minimizing translation error is a goal.  

**Correct:** C


#### 11. What does the Frobenius norm measure when used as a loss function in learning the transformation matrix?  
A) ‚úì Correct: Measures total squared difference between transformed and target vectors.  
B) ‚úó It is not cosine similarity.  
C) ‚úó It is not distance between individual vectors only.  
D) ‚úó It does not count mismatched words.  

**Correct:** A


#### 12. In the context of hash tables, what is a potential downside of using a simple hash function like `hash_value = vector % number_of_buckets`?  
A) ‚úì Correct: Can cause many collisions, reducing efficiency.  
B) ‚úó Simple hash functions rarely guarantee perfect distribution.  
C) ‚úó Simple hash functions do not require matrix multiplications.  
D) ‚úó Works for numeric vectors, not limited to integers only.  

**Correct:** A


#### 13. Why is the sign of the dot product used as a bit in locality sensitive hashing instead of the actual dot product value?  
A) ‚úì Correct: Sign captures relative position, invariant to magnitude.  
B) ‚úó Dot product is not always zero.  
C) ‚úó Sign does not encode exact distance.  
D) ‚úó Sign is computed from dot product, not easier than dot product itself.  

**Correct:** A


#### 14. How does combining bits from multiple planes into a single hash value help in LSH?  
A) ‚úó Hash is not necessarily unique for every vector.  
B) ‚úì Correct: Encodes position relative to multiple planes, improving grouping.  
C) ‚úó It does not reduce dimensionality to scalar meaningfully.  
D) ‚úó Identical vectors share hash, but similar vectors can too.  

**Correct:** B


#### 15. Which of the following best describes the relationship between the number of hash tables and the accuracy of approximate nearest neighbor search?  
A) ‚úì Correct: More hash tables improve accuracy but increase computation.  
B) ‚úó More hash tables do not decrease accuracy.  
C) ‚úó Number of hash tables affects accuracy.  
D) ‚úó Fewer hash tables usually reduce recall.  

**Correct:** A


#### 16. When searching for documents using K-NN, what is a key limitation of simply summing word vectors to represent documents?  
A) ‚úì Correct: Ignores word order and syntax, losing context.  
B) ‚úó Hash tables are unrelated to this limitation.  
C) ‚úó Summation can be used with LSH.  
D) ‚úó Works for multi-word documents too.  

**Correct:** A


#### 17. Which of the following is true about the transformation matrix \( R \) learned for machine translation?  
A) ‚úó \( R \) is not necessarily orthogonal.  
B) ‚úì Correct: Learned by minimizing Frobenius norm of difference.  
C) ‚úó \( R \) here is linear, not necessarily non-linear or deep network.  
D) ‚úó \( R \) maps source to target, not the reverse.  

**Correct:** B


#### 18. What is the main challenge addressed by locality sensitive hashing in high-dimensional vector spaces?  
A) ‚úì Correct: The curse of dimensionality makes exact search expensive.  
B) ‚úó LSH does not address vector representation availability.  
C) ‚úó LSH is unrelated to training data size.  
D) ‚úó Dot products are easy to compute even in low dimensions.  

**Correct:** A


#### 19. In the context of document search, why might approximate nearest neighbor methods be preferred over exact methods?  
A) ‚úó Document length is not the main factor.  
B) ‚úì Correct: Approximate methods scale better to large, high-dimensional datasets.  
C) ‚úó Exact methods can handle vector representations.  
D) ‚úó Approximate methods do not guarantee perfect recall.  

**Correct:** B


#### 20. Which of the following statements about K-nearest neighbors and hash tables is correct?  
A) ‚úó K-NN can be sped up using hash tables, so not always exhaustive.  
B) ‚úì Correct: Hash tables reduce search space for K-NN queries.  
C) ‚úó Hash tables do not replace vector transformations.  
D) ‚úó K-NN applies to both translation and document search.  

**Correct:** B, D

