## 1.4 Machine Translation and Document Search

## Study Notes

### 1. üåç Introduction to Machine Translation and Document Search

In this lecture, we explore two important applications of deep learning and vector representations in natural language processing (NLP): **machine translation** and **document search**. Both tasks rely on representing words and documents as vectors in a high-dimensional space and then manipulating these vectors to find meaningful relationships.

- **Machine Translation** is about converting text from one language to another, for example, translating ‚Äúhello!‚Äù in English to ‚Äúbonjour!‚Äù in French.
- **Document Search** involves finding relevant documents based on a query, such as searching for ‚ÄúCan I get a refund?‚Äù and retrieving documents that talk about return policies or refunds.

To achieve these tasks, we use techniques like transforming word vectors, finding nearest neighbors in vector space, and efficient search methods like hash tables and locality sensitive hashing.


### 2. üîÑ Transforming Word Vectors for Machine Translation

#### What is Vector Transformation?

Words can be represented as vectors ‚Äî lists of numbers that capture their meaning in a mathematical space. For example, the English word ‚Äúcat‚Äù might be represented as `[1, 0, 1]`, and the French word ‚Äúchat‚Äù as `[2, 3, 2]`. To translate between languages, we want to find a **transformation** (a mathematical function) that converts English word vectors into their French equivalents.

#### How Does This Work?

- We represent words as vectors in their respective languages.
- We learn a **transformation matrix** \( R \) that, when multiplied by an English word vector, produces a vector close to the corresponding French word vector.
- This transformation is a **linear transformation**, meaning it can be represented by a matrix multiplication:  
  \[
  \text{French vector} \approx R \times \text{English vector}
  \]

#### Example:

If \( R = \begin{bmatrix} 2 & 0 \\ 0 & -2 \end{bmatrix} \) and the English vector is \([1, 1]\), then the transformed vector is:
\[
[1, 1] \times R = [2, -2]
\]

#### How Do We Find \( R \)?

- We start with a small set of known word pairs (English-French).
- We initialize \( R \) randomly.
- We use an optimization method (like gradient descent) to minimize the difference between \( R \times \text{English vector} \) and the French vector.
- The difference is measured using the **Frobenius norm**, which is a way to measure the size of a matrix or the error between matrices.

#### Frobenius Norm Explained:

- Think of it as the square root of the sum of the squares of all elements in a matrix.
- For example, for matrix \( A = \begin{bmatrix} 2 & 2 \\ 2 & 2 \end{bmatrix} \),  
  square each element: \( 2^2 = 4 \), sum all: \(4+4+4+4=16\), then take the square root: \(\sqrt{16} = 4\).

This norm helps us quantify how close our transformed vectors are to the target vectors.


### 3. üîç K-Nearest Neighbors (K-NN) for Finding Similar Words and Documents

#### What is K-Nearest Neighbors?

K-NN is a simple but powerful method to find the closest points (neighbors) to a given point in vector space. For example, if you have a vector for the word ‚Äúhello,‚Äù K-NN helps find the closest vectors, which might be ‚Äúbonjour‚Äù or ‚Äúsalut‚Äù in French.

#### How Does K-NN Work?

- Given a query vector, calculate the distance (usually Euclidean or cosine distance) to all other vectors.
- Select the top \( K \) closest vectors.
- These neighbors are considered the most similar or relevant.

#### Application in Translation:

- After transforming an English word vector using matrix \( R \), we find the nearest French word vectors.
- The closest French word vector is the translation.

#### Application in Document Search:

- Represent documents as vectors (more on this later).
- Given a query vector, find the nearest document vectors.
- Return the documents that are closest to the query.


### 4. üóÉÔ∏è Hash Tables and Hash Functions for Efficient Search

#### Why Use Hash Tables?

Searching through all vectors to find nearest neighbors can be very slow when you have millions of words or documents. Hash tables help speed up this search by grouping vectors into buckets based on a **hash value**.

#### What is a Hash Function?

- A hash function takes a vector and returns a number (hash value) that determines which bucket the vector belongs to.
- For example, a simple hash function might be:  
  \[
  \text{hash value} = \text{vector} \mod \text{number of buckets}
  \]
- Vectors with the same hash value go into the same bucket.

#### How Does This Help?

- When searching for neighbors, you only look inside the bucket(s) corresponding to the query‚Äôs hash value.
- This reduces the search space drastically.

#### Example:

If you have 3 buckets (0, 1, 2), and a vector‚Äôs hash value is 1, you only search bucket 1 for neighbors.


### 5. üìê Locality Sensitive Hashing (LSH) ‚Äî Dividing Vector Space with Planes

#### What is Locality Sensitive Hashing?

LSH is a special kind of hashing designed to keep similar vectors in the same bucket with high probability. It works by dividing the vector space using **random hyperplanes**.

#### How Does It Work?

- Imagine a plane slicing through the vector space.
- For each vector, determine which side of the plane it lies on by calculating the **dot product** between the vector and the plane‚Äôs normal vector.
- The **sign** of the dot product (+ or -) tells you the side.
- This sign acts like a bit (0 or 1) in the hash value.

#### Multiple Planes for More Precision

- Use multiple random planes.
- For each plane, get a bit (0 or 1) depending on which side the vector lies.
- Combine these bits into a binary number ‚Äî this is the hash value.
- Vectors close to each other tend to have similar signs for these planes, so they end up in the same bucket.

#### Example:

If you have 3 planes, and a vector lies on the positive side of planes 1 and 3, but negative side of plane 2, the hash might be `101` in binary, which equals 5 in decimal.


### 6. üåê Approximate Nearest Neighbors with Multiple Hash Tables

#### Why Approximate?

Exact nearest neighbor search is expensive in high dimensions. Approximate methods trade a bit of accuracy for much faster search.

#### How Does Approximate Nearest Neighbor Search Work?

- Use **multiple sets of random planes** to create multiple hash tables.
- Each hash table partitions the space differently.
- When searching, query all hash tables and combine results.
- This increases the chance of finding close neighbors without checking every vector.

#### Cultural Reference:

The lecture humorously references *Spider-Man: Into the Spider-Verse* to illustrate multiple ‚Äúuniverses‚Äù or hash tables, each providing a different perspective on the data.


### 7. üìÑ Document Representation and Search

#### How Do We Represent Documents as Vectors?

- Documents are made up of words, each with its own vector.
- To get a document vector, sum (or average) the vectors of all words in the document.
- For example, if the document is ‚ÄúI love learning!‚Äù and the word vectors are:  
  - ‚ÄúI‚Äù = [1, 0, 1]  
  - ‚Äúlove‚Äù = [-1, 0, 1]  
  - ‚Äúlearning‚Äù = [1, 0, 1]  
  Then the document vector is:  
  \[
  [1,0,1] + [-1,0,1] + [1,0,1] = [1, 0, 3]
  \]

#### Searching Documents with K-NN

- Represent all documents as vectors.
- Given a query (also converted to a vector), find the nearest document vectors using K-NN.
- Return the most relevant documents.


### 8. üìù Summary and Learning Objectives Recap

By the end of this lecture, you should understand:

- How to **transform word vectors** using a matrix to perform machine translation.
- How to use **K-nearest neighbors** to find similar words or documents.
- The role of **hash tables** and **hash functions** in speeding up nearest neighbor search.
- How **locality sensitive hashing (LSH)** divides vector space using random planes to group similar vectors.
- How to perform **approximate nearest neighbor search** using multiple hash tables.
- How to represent documents as vectors by combining word vectors.
- How to apply these techniques to **machine translation** and **document search** tasks.


If you want to practice these concepts, the lecture suggests trying out coding exercises like implementing the transformation matrix, Frobenius norm, K-NN search, hash functions, and LSH.



<br>

## Key Points

#### 1. üîÑ Vector Transformation for Machine Translation  
- Machine translation can be performed by learning a transformation matrix \( R \) that maps English word vectors to French word vectors.  
- The transformation is a linear operation: French vector ‚âà \( R \times \) English vector.  
- The Frobenius norm measures the difference between transformed vectors and target vectors during training.  

#### 2. üîç K-Nearest Neighbors (K-NN)  
- K-NN finds the \( K \) closest vectors to a query vector based on distance metrics like Euclidean or cosine distance.  
- After transforming a word vector, K-NN is used to find the closest translated word vectors.  
- K-NN can also be applied to document search by comparing document vectors.  

#### 3. üóÉÔ∏è Hash Tables and Hash Functions  
- Hash functions map vectors to hash values that determine which bucket in a hash table the vector belongs to.  
- Searching for nearest neighbors is faster by only looking within the bucket(s) corresponding to the query‚Äôs hash value.  

#### 4. üìê Locality Sensitive Hashing (LSH)  
- LSH uses random hyperplanes to divide vector space; the sign of the dot product between a vector and a plane‚Äôs normal vector determines the vector‚Äôs side.  
- Each plane produces a bit (0 or 1) based on the sign, and multiple planes combine these bits into a binary hash value.  
- Similar vectors tend to have similar hash values, so they are grouped in the same bucket.  

#### 5. üåê Approximate Nearest Neighbors  
- Approximate nearest neighbor search uses multiple sets of random planes (multiple hash tables) to increase the chance of finding close neighbors efficiently.  
- This method trades some accuracy for much faster search in high-dimensional spaces.  

#### 6. üìÑ Document Representation and Search  
- Documents are represented as vectors by summing or averaging the vectors of the words they contain.  
- Document search uses K-NN to find documents with vectors closest to the query vector.