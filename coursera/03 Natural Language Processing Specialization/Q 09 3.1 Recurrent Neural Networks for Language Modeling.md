## 3.1 Recurrent Neural Networks for Language Modeling

## Questions

#### 1. What is the primary purpose of padding in neural network input sequences?  
A) To increase the vocabulary size  
B) To ensure all input sequences have the same length  
C) To improve the semantic meaning of words  
D) To reduce the number of trainable parameters  

#### 2. Which of the following best describes the role of the embedding layer in NLP models?  
A) It converts words into one-hot vectors  
B) It maps words to dense, trainable vector representations  
C) It averages word vectors to create sentence embeddings  
D) It applies non-linear activation functions to word indices  

#### 3. Why is the ReLU activation function preferred over linear functions in dense layers?  
A) It introduces non-linearity allowing the network to learn complex patterns  
B) It always outputs positive values, which speeds up training  
C) It prevents the vanishing gradient problem entirely  
D) It zeroes out negative inputs, adding sparsity to the model  

#### 4. Which of the following are limitations of traditional N-gram language models?  
A) They require large amounts of memory for large N  
B) They can easily capture long-range dependencies  
C) They struggle with sequences longer than N words  
D) They share parameters across time steps  

#### 5. How do RNNs differ fundamentally from N-gram models in handling sequences?  
A) RNNs process sequences one word at a time while maintaining a hidden state  
B) RNNs require storing all possible word combinations explicitly  
C) RNNs share parameters across time steps, reducing memory usage  
D) RNNs only consider the last N words for prediction  

#### 6. In the context of RNNs, what does the hidden state represent?  
A) The current word embedding  
B) A summary of all previous inputs up to the current time step  
C) The output prediction at each time step  
D) The trainable weights of the network  

#### 7. Which of the following are true about the cross-entropy loss function used in RNN training?  
A) It measures the difference between predicted and true probability distributions  
B) It is averaged over all time steps in a sequence  
C) It can only be used for binary classification tasks  
D) It penalizes incorrect predictions more heavily when the predicted probability is confident but wrong  

#### 8. What is the main advantage of using the tf.scan() function in TensorFlow for RNNs?  
A) It allows parallel computation of all time steps simultaneously  
B) It abstracts the recurrent computation over sequences efficiently  
C) It eliminates the need for trainable parameters  
D) It mimics the step-by-step processing of RNNs while optimizing GPU usage  

#### 9. How do Gated Recurrent Units (GRUs) improve upon vanilla RNNs?  
A) By introducing gates that control information flow and memory updates  
B) By removing the hidden state to simplify computations  
C) By using update and reset gates to preserve important information  
D) By stacking multiple layers to increase depth  

#### 10. Which statements about bidirectional RNNs are correct?  
A) They process sequences only from past to future  
B) They combine information from both past and future contexts  
C) They require twice the number of parameters compared to unidirectional RNNs  
D) They are less effective than vanilla RNNs for sentiment analysis  

#### 11. What is the primary function of the mean layer in an embedding-based model?  
A) To train word embeddings  
B) To compute the average vector of all word embeddings in a sentence  
C) To reduce the dimensionality of embeddings  
D) To apply non-linear transformations to embeddings  

#### 12. Why do deep RNNs often perform better on complex NLP tasks than shallow RNNs?  
A) They have more trainable parameters, allowing more complex feature extraction  
B) They process sequences faster due to parallelization  
C) They stack multiple recurrent layers, enabling hierarchical representation learning  
D) They eliminate the need for embedding layers  

#### 13. Which of the following are true about the trainable parameters in an embedding layer?  
A) They are updated during training to improve word representations  
B) They remain fixed and are not learned from data  
C) They map word indices to dense vectors  
D) They are the same as the parameters in the dense layer  

#### 14. In an RNN, what inputs are used at each time step to compute the new hidden state?  
A) The current word embedding and the previous hidden state  
B) Only the current word embedding  
C) The previous hidden state and the output prediction  
D) The entire sequence of previous words  

#### 15. Which of the following are challenges that vanilla RNNs face when modeling long sequences?  
A) Vanishing gradients that make learning long-term dependencies difficult  
B) Exploding gradients that destabilize training  
C) Inability to process sequences longer than a fixed length  
D) Excessive memory usage due to storing all past inputs explicitly  

#### 16. How does the update gate in a GRU function?  
A) It decides how much of the previous hidden state to keep  
B) It resets the hidden state to zero at each time step  
C) It controls how much new information to add to the hidden state  
D) It applies a non-linear activation to the input word embedding  

#### 17. Which of the following are true about the output of a bidirectional RNN at each time step?  
A) It depends only on the past context  
B) It combines information from both past and future contexts  
C) It is the concatenation or sum of forward and backward hidden states  
D) It ignores the current input word  

#### 18. Why is parameter sharing across time steps in RNNs beneficial?  
A) It reduces the total number of parameters, making the model more memory efficient  
B) It allows the model to generalize better to sequences of different lengths  
C) It prevents the model from learning time-dependent patterns  
D) It increases the model’s capacity to memorize training data  

#### 19. Which of the following are true about the use of RNNs in machine translation?  
A) They can be used in an encoder-decoder architecture  
B) They only process input sequences in one direction  
C) They can generate output sequences of different lengths than the input  
D) They require fixed-length input and output sequences  

#### 20. What is a key difference between the mean layer and the embedding layer in terms of trainable parameters?  
A) The embedding layer has trainable parameters, while the mean layer does not  
B) Both layers have trainable parameters  
C) The mean layer learns to weight words differently during training  
D) The embedding layer is fixed and not updated during training



<br>

## Answers

#### 1. What is the primary purpose of padding in neural network input sequences?  
A) ✗ Padding does not increase vocabulary size.  
B) ✓ Padding ensures all input sequences have the same length for batch processing.  
C) ✗ Padding does not improve semantic meaning; it just standardizes length.  
D) ✗ Padding does not reduce trainable parameters.  

**Correct:** B


#### 2. Which of the following best describes the role of the embedding layer in NLP models?  
A) ✗ Embeddings are dense vectors, not one-hot vectors.  
B) ✓ Embedding layers map words to dense, trainable vector representations.  
C) ✗ Averaging is done by a mean layer, not the embedding layer itself.  
D) ✗ Embedding layers do not apply non-linear activations to indices.  

**Correct:** B


#### 3. Why is the ReLU activation function preferred over linear functions in dense layers?  
A) ✓ ReLU introduces non-linearity, enabling learning of complex patterns.  
B) ✗ ReLU outputs zero for negative inputs, not always positive values.  
C) ✗ ReLU helps with vanishing gradients but does not prevent it entirely.  
D) ✓ ReLU zeroes out negative inputs, adding sparsity which can help learning.  

**Correct:** A, D


#### 4. Which of the following are limitations of traditional N-gram language models?  
A) ✓ Large N-grams require huge memory.  
B) ✗ N-grams cannot easily capture long-range dependencies.  
C) ✓ N-grams only consider fixed-length contexts, so struggle with longer sequences.  
D) ✗ N-grams do not share parameters; each N-gram is separate.  

**Correct:** A, C


#### 5. How do RNNs differ fundamentally from N-gram models in handling sequences?  
A) ✓ RNNs process sequences one word at a time with a hidden state.  
B) ✗ RNNs do not store all word combinations explicitly.  
C) ✓ RNNs share parameters across time steps, reducing memory usage.  
D) ✗ RNNs consider all previous words, not just last N words.  

**Correct:** A, C


#### 6. In the context of RNNs, what does the hidden state represent?  
A) ✗ Hidden state is not just the current word embedding.  
B) ✓ It summarizes all previous inputs up to the current time step.  
C) ✗ Output prediction is separate from the hidden state.  
D) ✗ Hidden state is not the trainable weights themselves.  

**Correct:** B


#### 7. Which of the following are true about the cross-entropy loss function used in RNN training?  
A) ✓ It measures difference between predicted and true distributions.  
B) ✓ Loss is averaged over all time steps in a sequence.  
C) ✗ Cross-entropy can be used for multi-class, not just binary classification.  
D) ✓ It penalizes confident but wrong predictions more heavily.  

**Correct:** A, B, D


#### 8. What is the main advantage of using the tf.scan() function in TensorFlow for RNNs?  
A) ✗ It does not compute all time steps in parallel; it mimics sequential processing.  
B) ✓ It abstracts recurrent computation efficiently over sequences.  
C) ✗ It does not eliminate trainable parameters.  
D) ✓ It mimics step-by-step RNN processing while optimizing GPU usage.  

**Correct:** B, D


#### 9. How do Gated Recurrent Units (GRUs) improve upon vanilla RNNs?  
A) ✓ By introducing gates controlling information flow and memory updates.  
B) ✗ GRUs do not remove hidden states; they modify how they are updated.  
C) ✓ Update and reset gates help preserve important information.  
D) ✗ Stacking layers is a feature of deep RNNs, not specific to GRUs.  

**Correct:** A, C


#### 10. Which statements about bidirectional RNNs are correct?  
A) ✗ They process sequences in both forward and backward directions.  
B) ✓ They combine information from past and future contexts.  
C) ✓ They require roughly twice the parameters due to two RNNs.  
D) ✗ They are generally more effective than vanilla RNNs for context-dependent tasks.  

**Correct:** B, C


#### 11. What is the primary function of the mean layer in an embedding-based model?  
A) ✗ The mean layer does not train embeddings; embedding layer does.  
B) ✓ It computes the average vector of all word embeddings in a sentence.  
C) ✗ It does not reduce dimensionality; it averages vectors of same dimension.  
D) ✗ It does not apply non-linear transformations.  

**Correct:** B


#### 12. Why do deep RNNs often perform better on complex NLP tasks than shallow RNNs?  
A) ✓ More parameters allow learning complex features.  
B) ✗ Deep RNNs are generally slower due to sequential dependencies.  
C) ✓ Stacking layers enables hierarchical representation learning.  
D) ✗ Deep RNNs still require embedding layers for word representation.  

**Correct:** A, C


#### 13. Which of the following are true about the trainable parameters in an embedding layer?  
A) ✓ They are updated during training to improve word vectors.  
B) ✗ Embedding weights are not fixed; they are learned.  
C) ✓ They map word indices to dense vectors.  
D) ✗ Embedding parameters are distinct from dense layer parameters.  

**Correct:** A, C


#### 14. In an RNN, what inputs are used at each time step to compute the new hidden state?  
A) ✓ Current word embedding and previous hidden state.  
B) ✗ Current word embedding alone is insufficient.  
C) ✗ Output prediction is not input to hidden state computation.  
D) ✗ Entire previous sequence is summarized in hidden state, not fed directly.  

**Correct:** A


#### 15. Which of the following are challenges that vanilla RNNs face when modeling long sequences?  
A) ✓ Vanishing gradients make learning long-term dependencies difficult.  
B) ✓ Exploding gradients can destabilize training.  
C) ✗ RNNs can process sequences of arbitrary length in theory.  
D) ✗ RNNs do not store all past inputs explicitly; they summarize in hidden state.  

**Correct:** A, B


#### 16. How does the update gate in a GRU function?  
A) ✓ It decides how much of the previous hidden state to keep.  
B) ✗ It does not reset the hidden state to zero each time step.  
C) ✓ It controls how much new information to add to the hidden state.  
D) ✗ It does not apply activation to the input embedding directly.  

**Correct:** A, C


#### 17. Which of the following are true about the output of a bidirectional RNN at each time step?  
A) ✗ It depends on both past and future context, not just past.  
B) ✓ It combines information from past and future contexts.  
C) ✓ Output is often concatenation or sum of forward and backward hidden states.  
D) ✗ It does not ignore the current input word.  

**Correct:** B, C


#### 18. Why is parameter sharing across time steps in RNNs beneficial?  
A) ✓ It reduces total parameters, improving memory efficiency.  
B) ✓ It helps generalize to sequences of varying lengths.  
C) ✗ It does not prevent learning time-dependent patterns; it models them.  
D) ✗ It does not increase memorization capacity; it regularizes the model.  

**Correct:** A, B


#### 19. Which of the following are true about the use of RNNs in machine translation?  
A) ✓ They can be used in encoder-decoder architectures.  
B) ✗ Encoder and decoder can process sequences in different directions.  
C) ✓ They can generate output sequences of different lengths than input.  
D) ✗ They do not require fixed-length input/output sequences.  

**Correct:** A, C


#### 20. What is a key difference between the mean layer and the embedding layer in terms of trainable parameters?  
A) ✓ Embedding layer has trainable parameters; mean layer does not.  
B) ✗ Mean layer does not have trainable parameters.  
C) ✗ Mean layer simply averages embeddings; it does not learn weights.  
D) ✗ Embedding layer is updated during training, not fixed.  

**Correct:** A