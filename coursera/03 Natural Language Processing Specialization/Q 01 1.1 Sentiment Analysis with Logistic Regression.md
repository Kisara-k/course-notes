## 1.1 Sentiment Analysis with Logistic Regression

## Questions

#### 1. What is the primary goal of sentiment analysis in the context of tweets?  
A) To identify the topic of the tweet  
B) To classify the tweet as positive or negative  
C) To extract named entities from the tweet  
D) To generate a summary of the tweet  

#### 2. Which of the following best describes a "sparse representation" of text data?  
A) A vector where most elements are zero  
B) A vector where all elements are one  
C) A vector with equal numbers of zeros and ones  
D) A vector that contains only unique words  

#### 3. Why can sparse representations cause problems in training logistic regression models?  
A) They increase the dimensionality, leading to longer training times  
B) They reduce the model’s ability to generalize  
C) They cause the model to overfit the training data  
D) They make prediction slower due to many zero values  

#### 4. When building a vocabulary from tweets, which of the following is true?  
A) The vocabulary contains all words from all tweets, including duplicates  
B) The vocabulary contains only unique words from the entire corpus  
C) The vocabulary excludes stopwords and punctuation by default  
D) The vocabulary is updated dynamically during prediction  

#### 5. What is the purpose of counting positive and negative frequencies of words in the corpus?  
A) To determine the sentiment polarity of each word  
B) To remove words that appear equally in both classes  
C) To create features that reflect how often words appear in each class  
D) To normalize the length of tweets  

#### 6. Which of the following is NOT a typical preprocessing step before feature extraction?  
A) Removing punctuation  
B) Lowercasing all words  
C) Adding stopwords to the vocabulary  
D) Stemming words to their root form  

#### 7. How does stemming help improve the feature extraction process?  
A) By increasing the size of the vocabulary  
B) By grouping different forms of a word into one feature  
C) By removing all stopwords from the text  
D) By converting words into their synonyms  

#### 8. What does the bias term in the feature vector represent?  
A) The frequency of the most common word in the tweet  
B) A constant value to allow the model to fit data better  
C) The difference between positive and negative word counts  
D) The length of the tweet  

#### 9. In logistic regression, what does the sigmoid function output represent?  
A) The exact class label (0 or 1)  
B) The probability that the input belongs to the positive class  
C) The weighted sum of input features  
D) The error between predicted and actual labels  

#### 10. Which of the following statements about the logistic regression cost function is true?  
A) It penalizes predictions that strongly disagree with the true label more heavily  
B) It is minimized when the model predicts probabilities close to 0.5 for all examples  
C) It uses binary cross-entropy to measure prediction error  
D) It is always positive and decreases as the model improves  

#### 11. During gradient descent training, what happens if the learning rate is set too high?  
A) The model converges faster without any issues  
B) The cost function may oscillate or diverge  
C) The model will underfit the training data  
D) The parameters update too slowly  

#### 12. Why is it important to test the logistic regression model on a validation set?  
A) To check if the model memorized the training data  
B) To evaluate the model’s performance on unseen data  
C) To tune hyperparameters like learning rate and iterations  
D) To increase the size of the training set  

#### 13. Which of the following can improve the accuracy of a logistic regression sentiment classifier?  
A) Increasing the number of training iterations  
B) Adding regularization to prevent overfitting  
C) Using raw tweets without preprocessing  
D) Engineering new features that capture sentiment better  

#### 14. What is the role of the frequency dictionary (freqs) in feature extraction?  
A) It maps each word to its sentiment polarity score  
B) It stores the count of each word in positive and negative classes  
C) It removes stopwords from the tweets  
D) It converts tweets into sparse vectors  

#### 15. Consider the tweet: "I am not happy." Which of the following feature extraction outcomes is most likely?  
A) High positive frequency sum, low negative frequency sum  
B) High negative frequency sum, low positive frequency sum  
C) Equal positive and negative frequency sums  
D) Zero frequency sums because of stopwords  

#### 16. Which of the following is a potential drawback of removing all stopwords during preprocessing?  
A) It may remove words that carry sentiment in some contexts  
B) It always improves model accuracy  
C) It increases the size of the vocabulary unnecessarily  
D) It causes the model to ignore punctuation  

#### 17. How does logistic regression differ from linear regression in the context of sentiment analysis?  
A) Logistic regression outputs probabilities, linear regression outputs continuous values  
B) Logistic regression can only handle binary classification, linear regression cannot  
C) Logistic regression uses the sigmoid function, linear regression does not  
D) Logistic regression requires feature extraction, linear regression does not  

#### 18. What does a cost value close to zero indicate during logistic regression training?  
A) The model is making poor predictions  
B) The model’s predictions closely match the true labels  
C) The model is overfitting the training data  
D) The model has not yet started learning  

#### 19. Why might the presence of URLs and Twitter handles in tweets negatively impact sentiment analysis?  
A) They often contain sentiment words  
B) They add noise and irrelevant information to the text  
C) They increase the vocabulary size unnecessarily  
D) They always indicate negative sentiment  

#### 20. When extracting features from a tweet, why is it useful to include both the sum of positive and negative word frequencies?  
A) To capture the overall sentiment balance in the tweet  
B) To ensure the feature vector is sparse  
C) To allow the model to distinguish between positive and negative cues  
D) To reduce the dimensionality of the input data



<br>

## Answers

#### 1. What is the primary goal of sentiment analysis in the context of tweets?  
A) ✗ Identifying the topic is not the main goal here.  
B) ✓ Correct: Sentiment analysis classifies tweets as positive or negative.  
C) ✗ Named entity recognition is a different NLP task.  
D) ✗ Summarization is unrelated to sentiment classification.  

**Correct:** B


#### 2. Which of the following best describes a "sparse representation" of text data?  
A) ✓ Correct: Sparse vectors mostly contain zeros.  
B) ✗ All ones is dense, not sparse.  
C) ✗ Equal zeros and ones is not necessarily sparse.  
D) ✗ Vocabulary is a set of unique words, not a vector.  

**Correct:** A


#### 3. Why can sparse representations cause problems in training logistic regression models?  
A) ✓ Correct: High dimensionality with many zeros slows training.  
B) ✗ Sparse vectors don’t inherently reduce generalization.  
C) ✗ Overfitting is not directly caused by sparsity.  
D) ✓ Correct: Prediction time increases due to large sparse vectors.  

**Correct:** A, D


#### 4. When building a vocabulary from tweets, which of the following is true?  
A) ✗ Vocabulary contains unique words, not duplicates.  
B) ✓ Correct: Vocabulary is the set of unique words in corpus.  
C) ✗ Stopwords and punctuation removal is a separate preprocessing step.  
D) ✗ Vocabulary is fixed after training, not updated during prediction.  

**Correct:** B


#### 5. What is the purpose of counting positive and negative frequencies of words in the corpus?  
A) ✓ Correct: Helps identify word sentiment association.  
B) ✗ Words appearing equally are not necessarily removed.  
C) ✓ Correct: Frequencies create features reflecting class association.  
D) ✗ Frequency counts do not normalize tweet length.  

**Correct:** A, C


#### 6. Which of the following is NOT a typical preprocessing step before feature extraction?  
A) ✗ Removing punctuation is typical.  
B) ✗ Lowercasing is typical.  
C) ✓ Correct: Adding stopwords is not done; usually removed.  
D) ✗ Stemming is typical.  

**Correct:** C


#### 7. How does stemming help improve the feature extraction process?  
A) ✗ Stemming reduces vocabulary size, not increases it.  
B) ✓ Correct: Groups word variants into one root form.  
C) ✗ Stemming does not remove stopwords.  
D) ✗ Stemming does not convert words into synonyms.  

**Correct:** B


#### 8. What does the bias term in the feature vector represent?  
A) ✗ Bias is not related to word frequency.  
B) ✓ Correct: Bias is a constant to help model fit better.  
C) ✗ Bias is not the difference between positive and negative counts.  
D) ✗ Bias is not tweet length.  

**Correct:** B


#### 9. In logistic regression, what does the sigmoid function output represent?  
A) ✗ Sigmoid outputs probability, not exact class.  
B) ✓ Correct: Output is probability of positive class.  
C) ✗ Weighted sum is input to sigmoid, not output.  
D) ✗ Sigmoid output is not error.  

**Correct:** B


#### 10. Which of the following statements about the logistic regression cost function is true?  
A) ✓ Correct: Strong disagreement leads to high cost.  
B) ✗ Cost is minimized when predictions are close to true labels, not 0.5.  
C) ✓ Correct: Binary cross-entropy is used as cost.  
D) ✓ Correct: Cost is positive and decreases as model improves.  

**Correct:** A, C, D


#### 11. During gradient descent training, what happens if the learning rate is set too high?  
A) ✗ Too high learning rate can cause divergence, not faster convergence.  
B) ✓ Correct: Cost may oscillate or diverge.  
C) ✗ Underfitting is usually due to too low capacity or iterations.  
D) ✗ Parameters update too fast, not too slow.  

**Correct:** B


#### 12. Why is it important to test the logistic regression model on a validation set?  
A) ✓ Correct: To check if model memorized training data (overfitting).  
B) ✓ Correct: To evaluate performance on unseen data.  
C) ✓ Correct: To tune hyperparameters effectively.  
D) ✗ Validation set is not used to increase training size.  

**Correct:** A, B, C


#### 13. Which of the following can improve the accuracy of a logistic regression sentiment classifier?  
A) ✓ Correct: More iterations can improve training.  
B) ✓ Correct: Regularization helps prevent overfitting.  
C) ✗ Using raw tweets without preprocessing usually hurts accuracy.  
D) ✓ Correct: New features can capture sentiment better.  

**Correct:** A, B, D


#### 14. What is the role of the frequency dictionary (freqs) in feature extraction?  
A) ✗ It does not assign sentiment polarity scores directly.  
B) ✓ Correct: Stores counts of words in positive and negative classes.  
C) ✗ It does not remove stopwords.  
D) ✗ It does not convert tweets into sparse vectors directly.  

**Correct:** B


#### 15. Consider the tweet: "I am not happy." Which of the following feature extraction outcomes is most likely?  
A) ✗ "happy" is positive, but "not" negates it, so positive sum likely low.  
B) ✓ Correct: Negative frequency sum likely higher due to "not".  
C) ✗ Positive and negative sums unlikely equal due to negation.  
D) ✗ Stopwords like "not" are important here; sums won’t be zero.  

**Correct:** B


#### 16. Which of the following is a potential drawback of removing all stopwords during preprocessing?  
A) ✓ Correct: Some stopwords can carry sentiment in context.  
B) ✗ Removing stopwords does not always improve accuracy.  
C) ✗ Removing stopwords reduces vocabulary size, not increases it.  
D) ✗ Stopwords removal does not affect punctuation handling.  

**Correct:** A


#### 17. How does logistic regression differ from linear regression in the context of sentiment analysis?  
A) ✓ Correct: Logistic outputs probabilities; linear outputs continuous values.  
B) ✗ Linear regression can be used for binary classification but is less suitable.  
C) ✓ Correct: Logistic uses sigmoid; linear does not.  
D) ✗ Both require feature extraction; this is not a difference.  

**Correct:** A, C


#### 18. What does a cost value close to zero indicate during logistic regression training?  
A) ✗ Low cost means good predictions, not poor.  
B) ✓ Correct: Predictions closely match true labels.  
C) ✗ Overfitting is not indicated by low cost alone.  
D) ✗ Low cost means model has learned, not that it hasn’t started.  

**Correct:** B


#### 19. Why might the presence of URLs and Twitter handles in tweets negatively impact sentiment analysis?  
A) ✗ URLs and handles rarely contain sentiment words.  
B) ✓ Correct: They add noise and irrelevant info.  
C) ✓ Correct: They increase vocabulary size unnecessarily.  
D) ✗ They do not always indicate negative sentiment.  

**Correct:** B, C


#### 20. When extracting features from a tweet, why is it useful to include both the sum of positive and negative word frequencies?  
A) ✓ Correct: Captures overall sentiment balance.  
B) ✗ Including both does not ensure sparsity.  
C) ✓ Correct: Helps model distinguish positive vs negative cues.  
D) ✗ It does not reduce dimensionality; it summarizes frequencies.  

**Correct:** A, C