## 1.4 Machine Translation and Document Search

## Questions

#### 1. What is the primary purpose of the transformation matrix \( R \) in machine translation using word vectors?  
A) To convert French word vectors into English word vectors  
B) To linearly map English word vectors to approximate French word vectors  
C) To calculate the Frobenius norm of word vectors  
D) To cluster word vectors into buckets for faster search  

#### 2. Which of the following best describes the Frobenius norm?  
A) The sum of the absolute values of all elements in a matrix  
B) The square root of the sum of the squares of all elements in a matrix  
C) The maximum value in a matrix  
D) The dot product between two vectors  

#### 3. When using K-nearest neighbors (K-NN) for word translation, what is the role of the K parameter?  
A) It determines the number of closest vectors to consider as potential translations  
B) It sets the dimensionality of the word vectors  
C) It controls the number of hash tables used in locality sensitive hashing  
D) It defines the number of random planes used for hashing  

#### 4. Why are hash tables used in nearest neighbor search for large vocabularies?  
A) To reduce the dimensionality of vectors  
B) To speed up search by limiting comparisons to a subset of vectors  
C) To transform vectors into a new language space  
D) To calculate exact nearest neighbors without approximation  

#### 5. Which of the following statements about hash functions is true?  
A) A hash function always produces unique hash values for different vectors  
B) Hash functions map vectors to buckets based on a deterministic rule  
C) Hash functions reduce vectors to scalar values by summing their elements  
D) Hash functions are only useful for exact nearest neighbor search  

#### 6. In locality sensitive hashing (LSH), what does the sign of the dot product between a vector and a plane’s normal vector represent?  
A) The magnitude of the vector’s projection on the plane  
B) Which side of the plane the vector lies on  
C) The distance from the vector to the plane  
D) The angle between the vector and the plane  

#### 7. How does using multiple random planes in LSH improve the hashing process?  
A) It increases the dimensionality of the vectors  
B) It creates a more fine-grained partitioning of the vector space  
C) It guarantees exact nearest neighbor retrieval  
D) It reduces the number of hash buckets needed  

#### 8. What is the main advantage of approximate nearest neighbor search over exact nearest neighbor search?  
A) It always finds the exact closest vector  
B) It significantly reduces computation time with some loss in accuracy  
C) It requires fewer hash tables  
D) It eliminates the need for vector transformations  

#### 9. When representing a document as a vector by summing word vectors, which of the following is true?  
A) The document vector is always normalized to unit length  
B) The document vector captures the combined meaning of all words in the document  
C) The order of words in the document affects the resulting vector  
D) Words not in the vocabulary are ignored or treated as zero vectors  

#### 10. Which of the following is NOT a reason to use a transformation matrix for machine translation?  
A) To align word vectors from different languages into a common space  
B) To enable direct comparison of word meanings across languages  
C) To reduce the size of the vocabulary in each language  
D) To learn a linear mapping that minimizes translation error  

#### 11. What does the Frobenius norm measure when used as a loss function in learning the transformation matrix?  
A) The total squared difference between transformed English vectors and French vectors  
B) The cosine similarity between English and French word vectors  
C) The Euclidean distance between two individual word vectors  
D) The number of mismatched words in the vocabulary  

#### 12. In the context of hash tables, what is a potential downside of using a simple hash function like `hash_value = vector % number_of_buckets`?  
A) It may cause many vectors to collide in the same bucket, reducing search efficiency  
B) It guarantees perfect distribution of vectors across buckets  
C) It requires complex matrix multiplications  
D) It only works for vectors with integer values  

#### 13. Why is the sign of the dot product used as a bit in locality sensitive hashing instead of the actual dot product value?  
A) Because the sign is invariant to vector magnitude and captures relative position  
B) Because the actual dot product is always zero  
C) Because the sign encodes the exact distance to the plane  
D) Because the sign is easier to compute than the dot product  

#### 14. How does combining bits from multiple planes into a single hash value help in LSH?  
A) It creates a unique identifier for each vector in the dataset  
B) It encodes the vector’s position relative to multiple planes, improving similarity grouping  
C) It reduces the dimensionality of the vector to a scalar  
D) It guarantees that only identical vectors share the same hash value  

#### 15. Which of the following best describes the relationship between the number of hash tables and the accuracy of approximate nearest neighbor search?  
A) More hash tables generally increase accuracy but also increase computation  
B) More hash tables always decrease accuracy  
C) The number of hash tables does not affect accuracy  
D) Fewer hash tables guarantee better recall  

#### 16. When searching for documents using K-NN, what is a key limitation of simply summing word vectors to represent documents?  
A) It ignores word order and syntax, potentially losing important context  
B) It requires very large hash tables  
C) It cannot be used with locality sensitive hashing  
D) It only works for documents with a single word  

#### 17. Which of the following is true about the transformation matrix \( R \) learned for machine translation?  
A) It is always an orthogonal matrix  
B) It is learned by minimizing the Frobenius norm of the difference between transformed and target vectors  
C) It can be non-linear and learned using deep neural networks only  
D) It maps vectors from the target language back to the source language  

#### 18. What is the main challenge addressed by locality sensitive hashing in high-dimensional vector spaces?  
A) The curse of dimensionality making exact nearest neighbor search computationally expensive  
B) The inability to represent words as vectors  
C) The lack of available training data for translation  
D) The difficulty of computing dot products in low dimensions  

#### 19. In the context of document search, why might approximate nearest neighbor methods be preferred over exact methods?  
A) Because documents are always very short  
B) Because approximate methods scale better to large document collections with high-dimensional vectors  
C) Because exact methods cannot handle vector representations  
D) Because approximate methods guarantee perfect recall  

#### 20. Which of the following statements about K-nearest neighbors and hash tables is correct?  
A) K-NN always requires checking every vector in the dataset  
B) Hash tables can be used to reduce the search space for K-NN queries  
C) Hash tables eliminate the need for vector transformations  
D) K-NN is only applicable to machine translation, not document search



<br>

## Answers

#### 1. What is the primary purpose of the transformation matrix \( R \) in machine translation using word vectors?  
A) ✗ It maps English to French, not French to English (though inverse is possible).  
B) ✓ Correct: It linearly maps English vectors to approximate French vectors.  
C) ✗ Frobenius norm measures error, not the purpose of \( R \).  
D) ✗ Hashing is unrelated to the transformation matrix’s role.  

**Correct:** B


#### 2. Which of the following best describes the Frobenius norm?  
A) ✗ It sums squares, not absolute values.  
B) ✓ Correct: It is the square root of the sum of squares of all matrix elements.  
C) ✗ It’s not the maximum value.  
D) ✗ Dot product is between vectors, not a matrix norm.  

**Correct:** B


#### 3. When using K-nearest neighbors (K-NN) for word translation, what is the role of the K parameter?  
A) ✓ Correct: K is the number of closest vectors considered.  
B) ✗ K does not set vector dimensionality.  
C) ✗ K does not control hash tables.  
D) ✗ K does not define number of planes.  

**Correct:** A


#### 4. Why are hash tables used in nearest neighbor search for large vocabularies?  
A) ✗ Hash tables do not reduce dimensionality.  
B) ✓ Correct: They speed up search by limiting comparisons to buckets.  
C) ✗ Hash tables do not transform vectors between languages.  
D) ✗ Hash tables approximate search, not exact search.  

**Correct:** B


#### 5. Which of the following statements about hash functions is true?  
A) ✗ Hash collisions can occur; uniqueness is not guaranteed.  
B) ✓ Correct: Hash functions deterministically map vectors to buckets.  
C) ✗ Hash functions do not simply sum elements.  
D) ✗ Hash functions are useful for approximate, not only exact, search.  

**Correct:** B


#### 6. In locality sensitive hashing (LSH), what does the sign of the dot product between a vector and a plane’s normal vector represent?  
A) ✗ Magnitude is not represented by sign.  
B) ✓ Correct: Sign indicates which side of the plane the vector lies on.  
C) ✗ Sign does not measure distance.  
D) ✗ Sign does not directly represent angle.  

**Correct:** B


#### 7. How does using multiple random planes in LSH improve the hashing process?  
A) ✗ It does not increase vector dimensionality.  
B) ✓ Correct: Multiple planes create finer partitions of space.  
C) ✗ LSH is approximate, not exact.  
D) ✗ More planes usually increase buckets, not reduce them.  

**Correct:** B


#### 8. What is the main advantage of approximate nearest neighbor search over exact nearest neighbor search?  
A) ✗ Approximate does not guarantee exact closest vector.  
B) ✓ Correct: It reduces computation time with some accuracy loss.  
C) ✗ Number of hash tables varies, not necessarily fewer.  
D) ✗ Approximate search still uses vector transformations.  

**Correct:** B


#### 9. When representing a document as a vector by summing word vectors, which of the following is true?  
A) ✗ Normalization is optional, not always done.  
B) ✓ Correct: Summing captures combined meaning of words.  
C) ✗ Word order is ignored in simple summation.  
D) ✓ Correct: Unknown words are ignored or zeroed.  

**Correct:** B, D


#### 10. Which of the following is NOT a reason to use a transformation matrix for machine translation?  
A) ✗ Aligning vectors is a key reason.  
B) ✗ Enables cross-language comparison.  
C) ✓ Correct: It does not reduce vocabulary size.  
D) ✗ Minimizing translation error is a goal.  

**Correct:** C


#### 11. What does the Frobenius norm measure when used as a loss function in learning the transformation matrix?  
A) ✓ Correct: Measures total squared difference between transformed and target vectors.  
B) ✗ It is not cosine similarity.  
C) ✗ It is not distance between individual vectors only.  
D) ✗ It does not count mismatched words.  

**Correct:** A


#### 12. In the context of hash tables, what is a potential downside of using a simple hash function like `hash_value = vector % number_of_buckets`?  
A) ✓ Correct: Can cause many collisions, reducing efficiency.  
B) ✗ Simple hash functions rarely guarantee perfect distribution.  
C) ✗ Simple hash functions do not require matrix multiplications.  
D) ✗ Works for numeric vectors, not limited to integers only.  

**Correct:** A


#### 13. Why is the sign of the dot product used as a bit in locality sensitive hashing instead of the actual dot product value?  
A) ✓ Correct: Sign captures relative position, invariant to magnitude.  
B) ✗ Dot product is not always zero.  
C) ✗ Sign does not encode exact distance.  
D) ✗ Sign is computed from dot product, not easier than dot product itself.  

**Correct:** A


#### 14. How does combining bits from multiple planes into a single hash value help in LSH?  
A) ✗ Hash is not necessarily unique for every vector.  
B) ✓ Correct: Encodes position relative to multiple planes, improving grouping.  
C) ✗ It does not reduce dimensionality to scalar meaningfully.  
D) ✗ Identical vectors share hash, but similar vectors can too.  

**Correct:** B


#### 15. Which of the following best describes the relationship between the number of hash tables and the accuracy of approximate nearest neighbor search?  
A) ✓ Correct: More hash tables improve accuracy but increase computation.  
B) ✗ More hash tables do not decrease accuracy.  
C) ✗ Number of hash tables affects accuracy.  
D) ✗ Fewer hash tables usually reduce recall.  

**Correct:** A


#### 16. When searching for documents using K-NN, what is a key limitation of simply summing word vectors to represent documents?  
A) ✓ Correct: Ignores word order and syntax, losing context.  
B) ✗ Hash tables are unrelated to this limitation.  
C) ✗ Summation can be used with LSH.  
D) ✗ Works for multi-word documents too.  

**Correct:** A


#### 17. Which of the following is true about the transformation matrix \( R \) learned for machine translation?  
A) ✗ \( R \) is not necessarily orthogonal.  
B) ✓ Correct: Learned by minimizing Frobenius norm of difference.  
C) ✗ \( R \) here is linear, not necessarily non-linear or deep network.  
D) ✗ \( R \) maps source to target, not the reverse.  

**Correct:** B


#### 18. What is the main challenge addressed by locality sensitive hashing in high-dimensional vector spaces?  
A) ✓ Correct: The curse of dimensionality makes exact search expensive.  
B) ✗ LSH does not address vector representation availability.  
C) ✗ LSH is unrelated to training data size.  
D) ✗ Dot products are easy to compute even in low dimensions.  

**Correct:** A


#### 19. In the context of document search, why might approximate nearest neighbor methods be preferred over exact methods?  
A) ✗ Document length is not the main factor.  
B) ✓ Correct: Approximate methods scale better to large, high-dimensional datasets.  
C) ✗ Exact methods can handle vector representations.  
D) ✗ Approximate methods do not guarantee perfect recall.  

**Correct:** B


#### 20. Which of the following statements about K-nearest neighbors and hash tables is correct?  
A) ✗ K-NN can be sped up using hash tables, so not always exhaustive.  
B) ✓ Correct: Hash tables reduce search space for K-NN queries.  
C) ✗ Hash tables do not replace vector transformations.  
D) ✗ K-NN applies to both translation and document search.  

**Correct:** B, D