## 4.2 The Transformer Model



### Key Points



#### 1. ü§ñ Transformers vs RNNs  
- RNNs process sequences sequentially, making parallel computing difficult.  
- RNNs suffer from vanishing gradients and loss of information in long sequences.  
- Transformers process entire sequences simultaneously, enabling parallelization and better long-range dependency handling.  
- Transformers do not use recurrent units like LSTMs or GRUs.

#### 2. üß© Transformer Architecture: Encoder-Decoder  
- The encoder processes the entire input sequence at once using self-attention.  
- The decoder generates output tokens one at a time, attending to previous outputs and encoder outputs.  
- Decoder uses masked self-attention to prevent attending to future tokens.

#### 3. üîç Attention Mechanism  
- Attention uses Queries (Q), Keys (K), and Values (V) to compute weighted sums of values.  
- Scaled dot-product attention involves dot products of Q and K, scaling, softmax, and weighted sum of V.  
- Self-attention allows each input token to attend to every other token in the same sequence.  
- Masked self-attention masks future tokens to prevent the decoder from "seeing" future words during generation.  
- Encoder-decoder attention allows the decoder to attend to encoder outputs.

#### 4. üéØ Multi-Head Attention  
- Multi-head attention runs multiple attention mechanisms in parallel (heads).  
- Each head learns different representations and attends to different parts of the input.  
- Outputs of all heads are concatenated and linearly transformed.  
- Multi-head attention has similar computational cost to single-head attention but improves model expressiveness.

#### 5. üßÆ Positional Encoding  
- Transformers add positional encoding vectors to input embeddings to provide word order information.  
- Positional encoding enables the model to distinguish the position of words in a sequence.

#### 6. üèóÔ∏è Transformer Decoder Components  
- The decoder consists of masked self-attention, encoder-decoder attention, and feed-forward layers with ReLU activation.  
- Residual connections and layer normalization are used after each sub-layer.  
- The decoder outputs probabilities over the vocabulary via a linear layer and softmax.

#### 7. üìö NLP Applications of Transformers  
- Transformers are used for machine translation, text summarization, auto-complete, named entity recognition, question answering, sentiment analysis, spell checking, and more.  
- T5 treats all NLP tasks as text-to-text problems, making it highly versatile.

#### 8. üåü Popular Transformer Models  
- GPT-2 is a generative model predicting the next word in a sequence.  
- BERT uses bidirectional attention to understand context from both sides of a word.  
- T5 is a text-to-text transfer transformer capable of multiple NLP tasks.

#### 9. üìù Transformer Summarizer  
- Summarization uses tokenized input with special tokens like <EOS> to mark sequence ends.  
- Cross-entropy loss is applied only on the summary portion during training.  
- During inference, the model generates summaries word-by-word until an <EOS> token is produced.  
- Random sampling during inference can produce different summaries for the same input.



<br>

## Study Notes



### Study Notes: Understanding the Transformer Model


### 1. ü§ñ Introduction to Transformers and RNNs

When working with sequences of data‚Äîlike sentences in language translation or speech recognition‚Äîtraditional models like **Recurrent Neural Networks (RNNs)** have been widely used. RNNs process data sequentially, step-by-step, which means they look at one word at a time in order. However, this sequential nature causes several problems:

- **No parallel computing:** Because RNNs process one step after another, they cannot easily take advantage of modern hardware like GPUs that excel at parallel processing.
- **Vanishing gradients:** When sequences get long, RNNs struggle to learn dependencies from far-apart words because the gradients (used for learning) become very small and the model "forgets" earlier information.
- **Loss of information:** Important context from earlier in the sequence can be lost by the time the model reaches later steps.

Transformers were introduced to solve these problems by **removing the sequential bottleneck** and allowing the model to look at the entire sequence at once. This enables **parallel processing** and better handling of long-range dependencies.


### 2. üß© The Transformer Architecture: Encoder-Decoder Structure

The Transformer model is built around an **encoder-decoder architecture**, similar in spirit to RNN-based sequence-to-sequence models but fundamentally different in implementation.

- **Encoder:** Takes the input sequence (e.g., a sentence in English) and processes it all at once. Each word in the input "attends" to every other word, meaning the model learns how each word relates to all others in the sentence.
- **Decoder:** Generates the output sequence (e.g., the translated sentence in French) one word at a time. It attends to the previously generated words and also to the encoder‚Äôs output to produce the next word.

Unlike RNNs, Transformers **do not use recurrent units** like LSTMs or GRUs. Instead, they rely on a mechanism called **attention** to understand relationships between words.


### 3. üîç Attention Mechanism: The Heart of Transformers

The key innovation in Transformers is the **attention mechanism**, which allows the model to weigh the importance of different words when processing a sequence.

#### Scaled Dot-Product Attention

- Inputs to attention are three vectors: **Queries (Q)**, **Keys (K)**, and **Values (V)**.
- The model computes a score by taking the dot product of the query with all keys, scales it down (to stabilize gradients), and applies a softmax to get weights that sum to 1.
- These weights are then used to compute a weighted sum of the values, producing a context vector that captures relevant information for each query.

This process is efficient and can be implemented with just matrix multiplications and a softmax function.

#### Self-Attention

- In the encoder, **self-attention** means each word attends to every other word in the same input sentence.
- This provides a **contextual representation** of each word, capturing how it relates to the entire sentence.

#### Masked Self-Attention

- In the decoder, **masked self-attention** prevents the model from "looking ahead" at future words when predicting the next word.
- This is done by masking out future positions so the model only attends to previous words, preserving the autoregressive property needed for generation.

#### Encoder-Decoder Attention

- The decoder also uses attention to look at the encoder‚Äôs output, allowing it to incorporate information from the input sentence when generating each output word.


### 4. üéØ Multi-Head Attention: Multiple Perspectives

Instead of performing a single attention operation, Transformers use **multi-head attention**, which runs several attention mechanisms in parallel.

- Each "head" learns to focus on different parts or aspects of the input.
- The outputs of all heads are concatenated and linearly transformed to produce the final attention output.
- This allows the model to capture diverse relationships and nuances in the data.

Multi-head attention maintains similar computational cost to single-head attention but greatly improves the model‚Äôs ability to understand complex patterns.


### 5. üßÆ Positional Encoding: Adding Order to the Sequence

Since Transformers process all words simultaneously, they need a way to understand the **order** of words in a sequence.

- **Positional encoding** adds unique vectors to each word embedding that encode the position of the word in the sequence.
- This helps the model distinguish between "I am happy" and "Happy am I," for example.
- These encodings are added to the input embeddings before feeding them into the encoder and decoder.


### 6. üèóÔ∏è Transformer Decoder: Generating Output Step-by-Step

The decoder is responsible for generating the output sequence, such as a translated sentence or a summary.

- It takes the previously generated words (starting with a special start token) and applies **masked self-attention** to look only at past words.
- Then it applies **encoder-decoder attention** to incorporate information from the input sentence.
- A **feed-forward neural network** with ReLU activation processes the attention outputs.
- Residual connections and layer normalization are used throughout to stabilize training.
- Finally, a linear layer followed by a softmax produces probabilities over the vocabulary for the next word.

This process repeats until the decoder generates an end-of-sequence token.


### 7. üìö Applications of Transformers in NLP

Transformers have revolutionized many natural language processing (NLP) tasks due to their flexibility and power:

- **Machine Translation:** Translating sentences from one language to another.
- **Text Summarization:** Producing concise summaries of long documents.
- **Auto-Complete and Text Generation:** Predicting the next word or sentence.
- **Named Entity Recognition (NER):** Identifying names, places, dates in text.
- **Question Answering (Q&A):** Answering questions based on a given context.
- **Sentiment Analysis:** Determining the sentiment or emotion in text.
- **Spell Checking and Character Recognition:** Improving text quality and understanding.


### 8. üåü Popular Transformer Models

Several influential Transformer-based models have been developed:

- **GPT-2 (Generative Pre-trained Transformer 2):** Focuses on generating coherent text by predicting the next word.
- **BERT (Bidirectional Encoder Representations from Transformers):** Uses bidirectional attention to understand context from both left and right of a word.
- **T5 (Text-to-Text Transfer Transformer):** Treats every NLP task as a text-to-text problem, making it highly versatile.

These models have set new state-of-the-art results across many NLP benchmarks.


### 9. üìù Transformer Summarizer: How It Works

A Transformer-based summarizer takes a long article as input and generates a concise summary.

- The input text is tokenized and fed into the encoder.
- The decoder generates the summary word-by-word, using masked self-attention and encoder-decoder attention.
- During training, a **cross-entropy loss** function is used, focusing on the summary part of the output.
- At inference time, the model generates summaries by sampling the next word repeatedly until an end token is produced.
- Because of randomness in sampling, different summaries can be generated for the same input.


### Summary

Transformers represent a major leap forward in sequence modeling by replacing sequential RNNs with attention-based architectures that can process entire sequences in parallel. Their core innovation, the attention mechanism, allows models to understand complex relationships in data, making them highly effective for a wide range of NLP tasks. With multi-head attention, positional encoding, and a powerful encoder-decoder design, Transformers have become the foundation for many state-of-the-art language models like GPT, BERT, and T5.



<br>

## Questions



#### 1. What are the main limitations of RNNs that Transformers aim to solve?  
A) Difficulty in parallelizing computations  
B) Vanishing gradient problem  
C) Inability to handle variable-length sequences  
D) Loss of long-range information  

#### 2. Which of the following statements about the Transformer encoder are true?  
A) It processes input tokens sequentially, one at a time  
B) Each input token attends to every other token in the sequence  
C) It uses self-attention to create contextual embeddings  
D) It relies on recurrent units like LSTMs or GRUs  

#### 3. In the Transformer model, what is the purpose of positional encoding?  
A) To add information about the order of tokens in the sequence  
B) To replace the need for word embeddings  
C) To enable the model to distinguish between different positions in the input  
D) To normalize the input embeddings  

#### 4. Which of the following are components of scaled dot-product attention?  
A) Queries, Keys, and Values  
B) Softmax function applied to dot products  
C) Recurrent connections to previous time steps  
D) Scaling factor to prevent large dot product values  

#### 5. How does masked self-attention in the decoder differ from regular self-attention in the encoder?  
A) It prevents attending to future tokens in the sequence  
B) It allows attending to all tokens in the input sentence  
C) It uses a mask to set weights of future positions to zero  
D) It attends only to the first token in the sequence  

#### 6. What is the main advantage of multi-head attention over single-head attention?  
A) It reduces the computational cost significantly  
B) It allows the model to attend to information from different representation subspaces  
C) It concatenates outputs from multiple attention heads before a linear transformation  
D) It eliminates the need for positional encoding  

#### 7. Which of the following are true about the Transformer decoder?  
A) It uses masked self-attention to prevent future token leakage  
B) It applies encoder-decoder attention to incorporate input sequence information  
C) It uses recurrent layers to process sequences step-by-step  
D) It includes feed-forward layers with ReLU activation  

#### 8. Why is parallelization easier in Transformers compared to RNNs?  
A) Because Transformers process all tokens simultaneously rather than sequentially  
B) Because Transformers use convolutional layers instead of recurrent layers  
C) Because attention mechanisms allow simultaneous computation of relationships  
D) Because Transformers do not require any positional information  

#### 9. Which of the following are challenges that Transformers help overcome compared to RNNs?  
A) Handling very long sequences without loss of information  
B) Avoiding vanishing gradients during training  
C) Processing sequences with fixed length only  
D) Enabling efficient use of GPUs and TPUs  

#### 10. In the context of attention, what do the terms "queries," "keys," and "values" represent?  
A) Queries are the vectors we want to find relevant information for  
B) Keys are vectors against which queries are compared to compute attention scores  
C) Values are the vectors that are weighted and summed to produce the output  
D) Queries, keys, and values are always identical vectors  

#### 11. Which of the following best describes the role of the feed-forward network in the Transformer?  
A) It applies a non-linear transformation to each position independently  
B) It aggregates information across different positions in the sequence  
C) It is the main component responsible for attention calculations  
D) It uses recurrent connections to maintain sequence order  

#### 12. How does the Transformer handle the problem of "loss of information" in long sequences?  
A) By using self-attention to directly connect all tokens regardless of distance  
B) By stacking multiple recurrent layers to increase memory  
C) By using positional encoding to preserve order information  
D) By limiting the input sequence length to a fixed size  

#### 13. Which of the following are true about the training process of Transformer-based summarizers?  
A) They optimize a weighted cross-entropy loss focusing on the summary portion  
B) They generate summaries by predicting the entire summary at once  
C) They use tokenized input sequences with special end-of-sequence tokens  
D) They rely on masked self-attention to prevent future token leakage during summary generation  

#### 14. What is a key difference between BERT and GPT models?  
A) BERT uses bidirectional attention, while GPT uses unidirectional attention  
B) GPT is designed primarily for text generation, BERT for understanding tasks  
C) BERT uses masked self-attention, GPT does not use attention mechanisms  
D) GPT uses encoder-decoder architecture, BERT uses only an encoder  

#### 15. Which of the following statements about multi-head attention are correct?  
A) Each attention head learns to focus on different parts of the input  
B) Multi-head attention concatenates the outputs of all heads before a linear layer  
C) Multi-head attention requires significantly more computation than single-head attention  
D) Multi-head attention eliminates the need for feed-forward layers  

#### 16. Why is the softmax function used in the attention mechanism?  
A) To convert raw attention scores into probabilities that sum to 1  
B) To normalize the input embeddings before attention  
C) To scale the dot products to prevent large values  
D) To mask out future tokens in the decoder  

#### 17. Which of the following are true about the encoder-decoder attention in the Transformer?  
A) Queries come from the decoder, keys and values come from the encoder output  
B) It allows the decoder to attend to the entire input sequence context  
C) It is only used in the encoder, not the decoder  
D) It replaces the need for positional encoding in the decoder  

#### 18. What is the function of residual connections and layer normalization in the Transformer?  
A) To stabilize training and help gradients flow through deep networks  
B) To add positional information to embeddings  
C) To reduce the dimensionality of embeddings  
D) To prevent the model from attending to irrelevant tokens  

#### 19. Which of the following NLP tasks can Transformers be applied to?  
A) Named Entity Recognition (NER)  
B) Text summarization  
C) Spell checking  
D) Image classification  

#### 20. During inference, how does a Transformer-based language model generate text?  
A) By predicting the next word one at a time using previously generated words  
B) By generating the entire output sequence simultaneously  
C) By sampling from the probability distribution over the vocabulary at each step  
D) By using recurrent connections to remember previous outputs  



<br>

## Answers



#### 1. What are the main limitations of RNNs that Transformers aim to solve?  
A) ‚úì Difficulty in parallelizing computations ‚Äî RNNs process sequentially, limiting parallelism.  
B) ‚úì Vanishing gradient problem ‚Äî RNNs suffer from gradients shrinking over long sequences.  
C) ‚úó Inability to handle variable-length sequences ‚Äî RNNs can handle variable lengths naturally.  
D) ‚úì Loss of long-range information ‚Äî RNNs struggle to retain distant context.  

**Correct:** A, B, D


#### 2. Which of the following statements about the Transformer encoder are true?  
A) ‚úó It processes input tokens sequentially, one at a time ‚Äî Transformers process all tokens simultaneously.  
B) ‚úì Each input token attends to every other token in the sequence ‚Äî Self-attention connects all tokens.  
C) ‚úì It uses self-attention to create contextual embeddings ‚Äî Self-attention provides context-aware representations.  
D) ‚úó It relies on recurrent units like LSTMs or GRUs ‚Äî Transformers do not use recurrent units.  

**Correct:** B, C


#### 3. In the Transformer model, what is the purpose of positional encoding?  
A) ‚úì To add information about the order of tokens in the sequence ‚Äî Positional encoding encodes token positions.  
B) ‚úó To replace the need for word embeddings ‚Äî Positional encoding is added to embeddings, not a replacement.  
C) ‚úì To enable the model to distinguish between different positions in the input ‚Äî Helps model understand sequence order.  
D) ‚úó To normalize the input embeddings ‚Äî Normalization is a separate process.  

**Correct:** A, C


#### 4. Which of the following are components of scaled dot-product attention?  
A) ‚úì Queries, Keys, and Values ‚Äî Core inputs to attention.  
B) ‚úì Softmax function applied to dot products ‚Äî Converts scores to probabilities.  
C) ‚úó Recurrent connections to previous time steps ‚Äî Attention is non-recurrent.  
D) ‚úì Scaling factor to prevent large dot product values ‚Äî Stabilizes gradients.  

**Correct:** A, B, D


#### 5. How does masked self-attention in the decoder differ from regular self-attention in the encoder?  
A) ‚úì It prevents attending to future tokens in the sequence ‚Äî Ensures autoregressive generation.  
B) ‚úó It allows attending to all tokens in the input sentence ‚Äî Masked attention restricts future tokens.  
C) ‚úì It uses a mask to set weights of future positions to zero ‚Äî Masking disables future attention.  
D) ‚úó It attends only to the first token in the sequence ‚Äî It attends to all previous tokens, not just the first.  

**Correct:** A, C


#### 6. What is the main advantage of multi-head attention over single-head attention?  
A) ‚úó It reduces the computational cost significantly ‚Äî Multi-head attention has similar cost to single-head.  
B) ‚úì It allows the model to attend to information from different representation subspaces ‚Äî Multiple heads capture diverse features.  
C) ‚úì It concatenates outputs from multiple attention heads before a linear transformation ‚Äî This is how multi-head attention combines heads.  
D) ‚úó It eliminates the need for positional encoding ‚Äî Positional encoding is still required.  

**Correct:** B, C


#### 7. Which of the following are true about the Transformer decoder?  
A) ‚úì It uses masked self-attention to prevent future token leakage ‚Äî Ensures predictions depend only on past tokens.  
B) ‚úì It applies encoder-decoder attention to incorporate input sequence information ‚Äî Connects decoder to encoder outputs.  
C) ‚úó It uses recurrent layers to process sequences step-by-step ‚Äî Transformers avoid recurrence.  
D) ‚úì It includes feed-forward layers with ReLU activation ‚Äî Feed-forward layers add non-linearity.  

**Correct:** A, B, D


#### 8. Why is parallelization easier in Transformers compared to RNNs?  
A) ‚úì Because Transformers process all tokens simultaneously rather than sequentially ‚Äî Enables parallel computation.  
B) ‚úó Because Transformers use convolutional layers instead of recurrent layers ‚Äî Transformers do not use convolutions.  
C) ‚úì Because attention mechanisms allow simultaneous computation of relationships ‚Äî Attention computes all token interactions at once.  
D) ‚úó Because Transformers do not require any positional information ‚Äî Positional encoding is necessary.  

**Correct:** A, C


#### 9. Which of the following are challenges that Transformers help overcome compared to RNNs?  
A) ‚úì Handling very long sequences without loss of information ‚Äî Attention connects distant tokens directly.  
B) ‚úì Avoiding vanishing gradients during training ‚Äî Attention gradients do not vanish like in RNNs.  
C) ‚úó Processing sequences with fixed length only ‚Äî Transformers handle variable-length sequences.  
D) ‚úì Enabling efficient use of GPUs and TPUs ‚Äî Parallelism suits modern hardware.  

**Correct:** A, B, D


#### 10. In the context of attention, what do the terms "queries," "keys," and "values" represent?  
A) ‚úì Queries are the vectors we want to find relevant information for ‚Äî They represent the current token‚Äôs request.  
B) ‚úì Keys are vectors against which queries are compared to compute attention scores ‚Äî Keys represent candidate matches.  
C) ‚úì Values are the vectors that are weighted and summed to produce the output ‚Äî Values provide the actual information.  
D) ‚úó Queries, keys, and values are always identical vectors ‚Äî They are usually different linear projections.  

**Correct:** A, B, C


#### 11. Which of the following best describes the role of the feed-forward network in the Transformer?  
A) ‚úì It applies a non-linear transformation to each position independently ‚Äî Feed-forward layers process tokens separately.  
B) ‚úó It aggregates information across different positions in the sequence ‚Äî Attention handles cross-token aggregation.  
C) ‚úó It is the main component responsible for attention calculations ‚Äî Attention is separate from feed-forward layers.  
D) ‚úó It uses recurrent connections to maintain sequence order ‚Äî No recurrence in feed-forward layers.  

**Correct:** A


#### 12. How does the Transformer handle the problem of "loss of information" in long sequences?  
A) ‚úì By using self-attention to directly connect all tokens regardless of distance ‚Äî Attention links distant tokens directly.  
B) ‚úó By stacking multiple recurrent layers to increase memory ‚Äî Transformers do not use recurrence.  
C) ‚úì By using positional encoding to preserve order information ‚Äî Positional encoding helps maintain sequence structure.  
D) ‚úó By limiting the input sequence length to a fixed size ‚Äî Transformers can handle variable lengths.  

**Correct:** A, C


#### 13. Which of the following are true about the training process of Transformer-based summarizers?  
A) ‚úì They optimize a weighted cross-entropy loss focusing on the summary portion ‚Äî Loss weights focus on summary tokens.  
B) ‚úó They generate summaries by predicting the entire summary at once ‚Äî Summaries are generated word-by-word.  
C) ‚úì They use tokenized input sequences with special end-of-sequence tokens ‚Äî EOS tokens mark sequence boundaries.  
D) ‚úì They rely on masked self-attention to prevent future token leakage during summary generation ‚Äî Ensures proper autoregressive decoding.  

**Correct:** A, C, D


#### 14. What is a key difference between BERT and GPT models?  
A) ‚úì BERT uses bidirectional attention, while GPT uses unidirectional attention ‚Äî BERT attends both left and right, GPT only left.  
B) ‚úì GPT is designed primarily for text generation, BERT for understanding tasks ‚Äî GPT excels at generation, BERT at representation.  
C) ‚úó BERT uses masked self-attention, GPT does not use attention mechanisms ‚Äî Both use attention; BERT masks tokens during training.  
D) ‚úó GPT uses encoder-decoder architecture, BERT uses only an encoder ‚Äî GPT is decoder-only, BERT is encoder-only.  

**Correct:** A, B


#### 15. Which of the following statements about multi-head attention are correct?  
A) ‚úì Each attention head learns to focus on different parts of the input ‚Äî Heads capture diverse features.  
B) ‚úì Multi-head attention concatenates the outputs of all heads before a linear layer ‚Äî This is standard practice.  
C) ‚úó Multi-head attention requires significantly more computation than single-head attention ‚Äî Computational cost is similar.  
D) ‚úó Multi-head attention eliminates the need for feed-forward layers ‚Äî Feed-forward layers remain essential.  

**Correct:** A, B


#### 16. Why is the softmax function used in the attention mechanism?  
A) ‚úì To convert raw attention scores into probabilities that sum to 1 ‚Äî Softmax normalizes scores.  
B) ‚úó To normalize the input embeddings before attention ‚Äî Normalization is separate.  
C) ‚úó To scale the dot products to prevent large values ‚Äî Scaling is done by division, not softmax.  
D) ‚úó To mask out future tokens in the decoder ‚Äî Masking is done by adding large negative values before softmax.  

**Correct:** A


#### 17. Which of the following are true about the encoder-decoder attention in the Transformer?  
A) ‚úì Queries come from the decoder, keys and values come from the encoder output ‚Äî This connects decoder to encoder.  
B) ‚úì It allows the decoder to attend to the entire input sequence context ‚Äî Enables context-aware generation.  
C) ‚úó It is only used in the encoder, not the decoder ‚Äî It is used in the decoder.  
D) ‚úó It replaces the need for positional encoding in the decoder ‚Äî Positional encoding is still required.  

**Correct:** A, B


#### 18. What is the function of residual connections and layer normalization in the Transformer?  
A) ‚úì To stabilize training and help gradients flow through deep networks ‚Äî They improve training stability.  
B) ‚úó To add positional information to embeddings ‚Äî Positional encoding does this.  
C) ‚úó To reduce the dimensionality of embeddings ‚Äî Dimensionality is fixed or changed by linear layers.  
D) ‚úó To prevent the model from attending to irrelevant tokens ‚Äî Attention weights handle relevance.  

**Correct:** A


#### 19. Which of the following NLP tasks can Transformers be applied to?  
A) ‚úì Named Entity Recognition (NER) ‚Äî Transformers excel at sequence labeling.  
B) ‚úì Text summarization ‚Äî Transformers generate summaries effectively.  
C) ‚úì Spell checking ‚Äî Transformers can model character-level corrections.  
D) ‚úó Image classification ‚Äî Transformers are primarily for NLP, though vision transformers exist but not covered here.  

**Correct:** A, B, C


#### 20. During inference, how does a Transformer-based language model generate text?  
A) ‚úì By predicting the next word one at a time using previously generated words ‚Äî Autoregressive generation.  
B) ‚úó By generating the entire output sequence simultaneously ‚Äî Generation is sequential.  
C) ‚úì By sampling from the probability distribution over the vocabulary at each step ‚Äî Sampling introduces variability.  
D) ‚úó By using recurrent connections to remember previous outputs ‚Äî Transformers do not use recurrence.  

**Correct:** A, C

