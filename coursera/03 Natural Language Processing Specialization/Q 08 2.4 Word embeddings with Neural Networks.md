## 2.4 Word embeddings with Neural Networks

## Questions

#### 1. What are the main limitations of representing words as integers in NLP tasks?  
A) Integers imply an arbitrary ordering that does not reflect semantic relationships  
B) Integers require very large storage space for vocabulary  
C) Integers do not capture similarity between words  
D) Integers can only represent a limited vocabulary size  


#### 2. Which of the following statements about one-hot vectors are true?  
A) One-hot vectors are sparse and high-dimensional  
B) One-hot vectors inherently encode semantic similarity between words  
C) One-hot vectors have exactly one element set to 1 and the rest 0  
D) One-hot vectors are efficient for large vocabularies  


#### 3. Word embeddings differ from one-hot vectors because they:  
A) Are dense, low-dimensional vectors  
B) Capture semantic relationships between words  
C) Are always binary vectors  
D) Can be used to perform vector arithmetic for analogies  


#### 4. Which of the following are common word embedding methods?  
A) Continuous Bag-of-Words (CBOW)  
B) Skip-gram with Negative Sampling (SGNS)  
C) Principal Component Analysis (PCA)  
D) Global Vectors (GloVe)  


#### 5. What is the primary goal of the Continuous Bag-of-Words (CBOW) model?  
A) Predict the context words given the center word  
B) Predict the center word given the context words  
C) Generate one-hot vectors for words  
D) Cluster words based on their frequency  


#### 6. In the CBOW model, what does the "window size" hyperparameter control?  
A) The number of words predicted at once  
B) The number of context words considered on each side of the center word  
C) The dimensionality of the word embeddings  
D) The size of the vocabulary  


#### 7. Why is it important to clean and tokenize text before training word embeddings?  
A) To reduce vocabulary size by merging case variants  
B) To remove noise such as punctuation and special characters  
C) To ensure consistent representation of numbers and emojis  
D) To increase the dimensionality of one-hot vectors  


#### 8. Which of the following are typical steps in cleaning and tokenizing text for word embeddings?  
A) Converting all text to lowercase  
B) Removing or replacing punctuation with a standard token  
C) Keeping all numbers as they are without any replacement  
D) Removing special characters like currency symbols and emojis  


#### 9. When using a sliding window to generate training data for CBOW, what is true?  
A) The center word is always the first word in the window  
B) Context words are the words surrounding the center word within the window  
C) The window size determines how many context words are used in total  
D) The sliding window moves one word at a time through the corpus  


#### 10. How are context words represented as input to the CBOW model?  
A) As the sum of their one-hot vectors  
B) As the average of their one-hot vectors  
C) As concatenated one-hot vectors  
D) As the one-hot vector of the center word  


#### 11. Which activation functions are used in the CBOW neural network architecture?  
A) Sigmoid in the hidden layer  
B) ReLU in the hidden layer  
C) Softmax in the output layer  
D) Tanh in the output layer  


#### 12. What is the role of the softmax function in the CBOW model?  
A) To normalize the output into a probability distribution over the vocabulary  
B) To select the most frequent word in the corpus  
C) To compute the loss function directly  
D) To convert word embeddings into one-hot vectors  


#### 13. Cross-entropy loss in CBOW training is used to:  
A) Measure the difference between predicted and actual center word distributions  
B) Maximize the similarity between context and center word vectors  
C) Penalize incorrect predictions more heavily than correct ones  
D) Calculate the Euclidean distance between word embeddings  


#### 14. During backpropagation in CBOW training, what is updated?  
A) The input word vectors only  
B) The weights and biases of the neural network  
C) The one-hot vectors of the words  
D) The corpus text itself  


#### 15. Which of the following statements about extracting word embeddings after training are correct?  
A) The embedding vectors are stored in the weight matrix between input and hidden layers  
B) Embeddings can be averaged from both input-to-hidden and hidden-to-output weight matrices  
C) Embeddings are the output of the softmax layer  
D) Embeddings are one-hot vectors transformed by the ReLU function  


#### 16. Intrinsic evaluation of word embeddings involves:  
A) Testing embeddings on external NLP tasks like sentiment analysis  
B) Measuring how well embeddings capture word analogies and semantic relationships  
C) Clustering similar words and visualizing their vectors  
D) Measuring training time and computational efficiency  


#### 17. Extrinsic evaluation of word embeddings is:  
A) Faster and easier to perform than intrinsic evaluation  
B) More focused on the usefulness of embeddings in real-world NLP tasks  
C) Less reliable because it doesn’t test semantic relationships directly  
D) Time-consuming and harder to troubleshoot  


#### 18. Which of the following are advantages of using word embeddings over one-hot vectors?  
A) Reduced dimensionality and computational efficiency  
B) Ability to capture semantic similarity between words  
C) Simpler to implement and interpret  
D) Avoidance of the curse of dimensionality  


#### 19. What challenges arise if text cleaning and tokenization are not properly performed before training embeddings?  
A) Increased vocabulary size due to case and punctuation variants  
B) Poor quality embeddings due to noisy or inconsistent input  
C) Faster training due to more diverse tokens  
D) Difficulty in handling out-of-vocabulary words  


#### 20. Which of the following statements about advanced word embedding methods like BERT and ELMo are true?  
A) They generate static embeddings independent of context  
B) They produce contextual embeddings that vary depending on sentence meaning  
C) They require large pre-trained models and fine-tuning for specific tasks  
D) They are less effective than CBOW for capturing semantic analogies



<br>

## Answers

#### 1. What are the main limitations of representing words as integers in NLP tasks?  
A) ✓ Integers imply an arbitrary ordering that does not reflect semantic relationships  
B) ✗ Integers themselves do not require large storage; the issue is semantic meaning, not storage  
C) ✓ Integers do not capture similarity between words  
D) ✗ Integers can represent any vocabulary size, limited only by integer range  

**Correct:** A, C


#### 2. Which of the following statements about one-hot vectors are true?  
A) ✓ One-hot vectors are sparse and high-dimensional  
B) ✗ One-hot vectors do not encode semantic similarity; all vectors are orthogonal  
C) ✓ One-hot vectors have exactly one element set to 1 and the rest 0  
D) ✗ One-hot vectors are inefficient for large vocabularies due to sparsity  

**Correct:** A, C


#### 3. Word embeddings differ from one-hot vectors because they:  
A) ✓ Are dense, low-dimensional vectors  
B) ✓ Capture semantic relationships between words  
C) ✗ Word embeddings are continuous-valued, not binary  
D) ✓ Can be used to perform vector arithmetic for analogies  

**Correct:** A, B, D


#### 4. Which of the following are common word embedding methods?  
A) ✓ Continuous Bag-of-Words (CBOW) is a word2vec method  
B) ✓ Skip-gram with Negative Sampling (SGNS) is a word2vec method  
C) ✗ PCA is a dimensionality reduction technique, not a word embedding method  
D) ✓ Global Vectors (GloVe) is a popular embedding method  

**Correct:** A, B, D


#### 5. What is the primary goal of the Continuous Bag-of-Words (CBOW) model?  
A) ✗ This describes Skip-gram, not CBOW  
B) ✓ CBOW predicts the center word given the context words  
C) ✗ CBOW does not generate one-hot vectors; it uses them as input/output representations  
D) ✗ CBOW does not cluster words based on frequency  

**Correct:** B


#### 6. In the CBOW model, what does the "window size" hyperparameter control?  
A) ✗ CBOW predicts one center word at a time  
B) ✓ Window size controls how many context words are considered on each side of the center word  
C) ✗ Embedding dimensionality is a separate hyperparameter  
D) ✗ Vocabulary size is independent of window size  

**Correct:** B


#### 7. Why is it important to clean and tokenize text before training word embeddings?  
A) ✓ Lowercasing merges case variants, reducing vocabulary size  
B) ✓ Removing punctuation reduces noise and inconsistencies  
C) ✓ Consistent handling of numbers and emojis improves quality  
D) ✗ Cleaning does not increase one-hot vector dimensionality; it reduces noise  

**Correct:** A, B, C


#### 8. Which of the following are typical steps in cleaning and tokenizing text for word embeddings?  
A) ✓ Lowercasing is standard practice  
B) ✓ Punctuation is often replaced or removed for consistency  
C) ✗ Numbers are usually replaced or normalized, not kept as-is  
D) ✗ Special characters like emojis can be removed or converted, not always removed  

**Correct:** A, B


#### 9. When using a sliding window to generate training data for CBOW, what is true?  
A) ✗ The center word is in the middle, not the first word  
B) ✓ Context words are the surrounding words within the window  
C) ✗ Window size is half the total context size on each side, total context words = 2*C  
D) ✓ The window moves one word at a time through the corpus  

**Correct:** B, D


#### 10. How are context words represented as input to the CBOW model?  
A) ✗ The sum is not normalized; averaging is preferred  
B) ✓ The average of one-hot vectors is used as input  
C) ✗ Concatenation would increase input size and is not used in CBOW  
D) ✗ The center word is the output, not input  

**Correct:** B


#### 11. Which activation functions are used in the CBOW neural network architecture?  
A) ✗ Sigmoid is not typically used in CBOW hidden layers  
B) ✓ ReLU is used in the hidden layer for non-linearity  
C) ✓ Softmax is used in the output layer to produce probabilities  
D) ✗ Tanh is not used in the output layer  

**Correct:** B, C


#### 12. What is the role of the softmax function in the CBOW model?  
A) ✓ Softmax normalizes outputs into a probability distribution over vocabulary  
B) ✗ Softmax does not select the most frequent word  
C) ✗ Softmax does not compute loss directly; loss is computed separately  
D) ✗ Softmax does not convert embeddings into one-hot vectors  

**Correct:** A


#### 13. Cross-entropy loss in CBOW training is used to:  
A) ✓ Measure difference between predicted and actual word distributions  
B) ✗ It does not directly maximize similarity but penalizes prediction errors  
C) ✓ Penalizes incorrect predictions more than correct ones  
D) ✗ Cross-entropy is not Euclidean distance  

**Correct:** A, C


#### 14. During backpropagation in CBOW training, what is updated?  
A) ✗ Input word vectors are part of weights, but not the only parameters updated  
B) ✓ Weights and biases of the neural network are updated  
C) ✗ One-hot vectors are fixed representations, not updated  
D) ✗ The corpus text is never changed during training  

**Correct:** B


#### 15. Which of the following statements about extracting word embeddings after training are correct?  
A) ✓ Embeddings are stored in the input-to-hidden weight matrix  
B) ✓ Embeddings can be averaged from input and output weight matrices for better quality  
C) ✗ Embeddings are not outputs of the softmax layer  
D) ✗ Embeddings are continuous vectors, not one-hot transformed by ReLU  

**Correct:** A, B


#### 16. Intrinsic evaluation of word embeddings involves:  
A) ✗ Testing on external tasks is extrinsic evaluation  
B) ✓ Testing analogies and semantic relationships is intrinsic evaluation  
C) ✓ Clustering and visualization are intrinsic evaluation methods  
D) ✗ Training time is unrelated to intrinsic evaluation  

**Correct:** B, C


#### 17. Extrinsic evaluation of word embeddings is:  
A) ✗ Extrinsic evaluation is slower and more complex than intrinsic  
B) ✓ Focused on usefulness in real NLP tasks  
C) ✗ It is reliable for measuring practical effectiveness  
D) ✓ Time-consuming and harder to troubleshoot  

**Correct:** B, D


#### 18. Which of the following are advantages of using word embeddings over one-hot vectors?  
A) ✓ Reduced dimensionality and computational efficiency  
B) ✓ Capture semantic similarity between words  
C) ✗ Word embeddings are more complex to implement and interpret  
D) ✓ Help mitigate the curse of dimensionality by using dense vectors  

**Correct:** A, B, D


#### 19. What challenges arise if text cleaning and tokenization are not properly performed before training embeddings?  
A) ✓ Vocabulary size inflates due to case and punctuation variants  
B) ✓ Noisy input leads to poor quality embeddings  
C) ✗ Training is usually slower with noisy data, not faster  
D) ✓ Difficulties handling out-of-vocabulary words increase  

**Correct:** A, B, D


#### 20. Which of the following statements about advanced word embedding methods like BERT and ELMo are true?  
A) ✗ They produce contextual embeddings, not static ones  
B) ✓ Embeddings vary depending on sentence context  
C) ✓ Require large pre-trained models and fine-tuning  
D) ✗ They are generally more effective than CBOW for capturing complex semantics  

**Correct:** B, C