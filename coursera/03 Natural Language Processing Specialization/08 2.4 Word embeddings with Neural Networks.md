## 2.4 Word embeddings with Neural Networks

## Study Notes

### 1. üß© Introduction to Word Embeddings

When working with language in machine learning, computers need a way to understand words. But words are not numbers, and computers only understand numbers. So, how do we represent words in a way that machines can process?

**Word embeddings** are a powerful solution. They transform words into numerical vectors that capture their meanings and relationships. Unlike simple methods like assigning each word a unique number or a one-hot vector, embeddings place words in a continuous vector space where similar words are close together.

#### Why do we need word embeddings?

- **Integers**: Assigning each word a unique integer (e.g., "happy" = 5) is simple but meaningless because the numbers don‚Äôt reflect any relationship between words.
- **One-hot vectors**: These are large, sparse vectors with a single 1 and the rest 0s. For example, if you have 10,000 words, each word is a 10,000-dimensional vector with one 1. This is inefficient and doesn‚Äôt capture any semantic similarity.
- **Word embeddings**: Dense, low-dimensional vectors (e.g., 100-1000 dimensions) that encode semantic meaning. For example, "king" and "queen" will have vectors close to each other, reflecting their related meanings.


### 2. üåê Understanding Word Representations

#### Integers and One-hot Vectors

- **Integers**: Words are mapped to unique numbers. This is easy but doesn‚Äôt capture meaning or similarity.
- **One-hot vectors**: Each word is represented as a vector with all zeros except a 1 in the position corresponding to that word. This creates very high-dimensional vectors with no semantic information.

#### Word Embeddings as Vectors

Word embeddings represent words as vectors in a continuous space. These vectors capture relationships such as:

- **Semantic similarity**: Words with similar meanings have vectors close to each other (e.g., "happy" and "joyful").
- **Analogies**: Vector arithmetic can solve analogies like "Paris is to France as Rome is to ___" by vector operations.

For example, the vector for "Paris" minus "France" plus "Italy" should be close to the vector for "Rome."


### 3. üõ†Ô∏è How to Create Word Embeddings

Creating word embeddings involves:

- **Corpus**: A large collection of text (e.g., Wikipedia).
- **Embedding method**: Algorithms that learn word vectors from the corpus.
- **Training process**: The model learns to predict words based on context or vice versa.
- **Hyperparameters**: Settings like embedding size (dimensions of vectors), window size (context range), learning rate, etc.

#### Types of Word Embedding Methods

- **Basic methods**:
  - **word2vec (Google, 2013)**: Includes Continuous Bag-of-Words (CBOW) and Skip-gram models.
  - **GloVe (Stanford, 2014)**: Uses global word co-occurrence statistics.
  - **fastText (Facebook, 2016)**: Handles out-of-vocabulary words by using subword information.

- **Advanced methods**:
  - **Contextual embeddings** like BERT, ELMo, GPT-2, which generate word vectors depending on the sentence context.


### 4. üîÑ The Continuous Bag-of-Words (CBOW) Model

CBOW is a popular word2vec model that predicts a **center word** based on its surrounding **context words**.

#### How CBOW works:

- Given a window of words around a center word, the model tries to predict the center word from the context.
- For example, in the sentence "I am happy because I am learning," if the window size is 2, the context for "happy" is ["I", "am", "because", "I"].
- The model averages the vectors of the context words and uses this to predict the center word.

#### Preparing training data for CBOW:

- Slide a window over the corpus.
- For each position, extract context words and the center word.
- Convert words to vectors (initially one-hot vectors).
- Average context vectors to form input.
- The output is the one-hot vector of the center word.


### 5. üßπ Cleaning and Tokenization of Text

Before training, text must be cleaned and tokenized to ensure consistency and quality.

#### Key steps:

- **Lowercasing**: Convert all text to lowercase so "The," "the," and "THE" are treated the same.
- **Punctuation**: Replace or remove punctuation marks (e.g., commas, exclamation points) to avoid noise.
- **Numbers**: Remove or replace numbers with a placeholder like `<NUMBER>`.
- **Special characters**: Remove symbols like $, ‚Ç¨, ¬ß, emojis can be optionally converted to text tokens.
- **Tokenization**: Split text into individual words or tokens.

Example in Python uses libraries like `nltk` and `emoji` to clean and tokenize text properly.


### 6. üîç Sliding Window Technique for Context Extraction

The sliding window is a method to generate training pairs for CBOW:

- Move a window of size `2C + 1` across the text.
- The center word is the word in the middle.
- The context words are the `C` words before and after the center word.
- This creates many (context, center word) pairs for training.

Example: For the sentence "I am happy because I am learning" with `C=2`, the context for "happy" is ["I", "am", "because", "I"].


### 7. üî¢ Transforming Words into Vectors for CBOW

- **Center word**: Represented as a one-hot vector.
- **Context words**: Each context word is a one-hot vector; average these vectors to get the input vector.
- This averaged vector is fed into the neural network to predict the center word.

Example:

| Context Words | Vector (average)          | Center Word | Vector (one-hot)     |
|---------------|--------------------------|-------------|----------------------|
| I am because  | [0.25, 0.25, 0, 0.5, 0] | happy       | [0, 0, 1, 0, 0]      |


### 8. üèóÔ∏è Architecture of the CBOW Neural Network

The CBOW model is a simple feedforward neural network with three layers:

- **Input layer**: Takes the averaged context word vector.
- **Hidden layer**: Learns a dense representation (embedding) of words. Uses ReLU activation (`max(0, x)`).
- **Output layer**: Predicts the center word using a softmax function, which outputs probabilities for each word in the vocabulary.

#### Dimensions:

- Vocabulary size = V
- Embedding size = N
- Input vector: V-dimensional (one-hot or averaged)
- Hidden layer: N-dimensional
- Output layer: V-dimensional (probabilities for each word)


### 9. ‚öôÔ∏è Training the CBOW Model

#### Forward propagation:

- Input vector ‚Üí hidden layer (linear transformation + ReLU)
- Hidden layer ‚Üí output layer (linear transformation + softmax)
- Output is a probability distribution over vocabulary words.

#### Loss function:

- **Cross-entropy loss** measures how well the predicted probabilities match the actual center word.
- The goal is to minimize this loss during training.

#### Backpropagation and gradient descent:

- Calculate gradients of loss with respect to weights and biases.
- Update parameters to reduce loss.
- Repeat over many training examples.


### 10. üéØ Extracting Word Embeddings

After training, the learned weights in the hidden layer represent the word embeddings.

- The weight matrix between input and hidden layer (`W1`) contains the embeddings.
- Alternatively, embeddings can be averaged from input and output weight matrices.
- These embeddings can then be used for various NLP tasks.


### 11. üìä Evaluating Word Embeddings

#### Intrinsic evaluation:

- Tests the quality of embeddings by checking word relationships.
- Examples:
  - **Analogies**: "France" is to "Paris" as "Italy" is to "Rome."
  - **Clustering**: Grouping similar words together (e.g., "village," "town," "city").
- Fast and easy but doesn‚Äôt always reflect real-world usefulness.

#### Extrinsic evaluation:

- Tests embeddings on actual NLP tasks like named entity recognition or sentiment analysis.
- More time-consuming but shows practical effectiveness.
- Helps understand how embeddings improve downstream applications.


### 12. üßë‚Äçüíª Practical Considerations and Libraries

- Popular libraries like **Keras** and **PyTorch** provide embedding layers to easily create and use word embeddings.
- Example in Keras: `Embedding(10000, 400)` creates embeddings for a vocabulary of 10,000 words with 400-dimensional vectors.
- Pre-trained embeddings and models (e.g., BERT, GPT) can be fine-tuned for specific tasks.


### Summary

Word embeddings are a foundational technique in NLP that convert words into meaningful numerical vectors. The Continuous Bag-of-Words (CBOW) model is a simple yet effective way to learn these embeddings by predicting a word from its context. Proper text cleaning, tokenization, and training are essential for good embeddings. Evaluations help us understand their quality, and modern libraries make it easy to implement and use embeddings in real applications.



<br>

## Key Points

#### 1. üßÆ Word Representations  
- Words can be represented as integers, one-hot vectors, or word embeddings.  
- Integers assign unique numbers to words but have no semantic meaning.  
- One-hot vectors are high-dimensional, sparse vectors with a single 1 and no embedded meaning.  
- Word embeddings are dense, low-dimensional vectors that capture semantic relationships between words.

#### 2. üåê Word Embeddings  
- Word embeddings represent words as vectors in a continuous space (~100-1000 dimensions).  
- Similar words have vectors close to each other (semantic similarity).  
- Word embeddings enable solving analogies via vector arithmetic (e.g., Paris:France :: Rome:?).

#### 3. üõ†Ô∏è Word Embedding Methods  
- Basic methods include word2vec (CBOW and Skip-gram), GloVe, and fastText.  
- Advanced methods include contextual embeddings like BERT, ELMo, and GPT-2.  
- fastText supports out-of-vocabulary (OOV) words by using subword information.

#### 4. üîÑ Continuous Bag-of-Words (CBOW) Model  
- CBOW predicts the center word from surrounding context words within a sliding window.  
- Context words are averaged into a single vector input to the neural network.  
- The model outputs a probability distribution over the vocabulary predicting the center word.

#### 5. üßπ Text Cleaning and Tokenization  
- Text is lowercased to treat "The," "the," and "THE" as the same word.  
- Punctuation is replaced or removed (e.g., commas, exclamation marks ‚Üí periods or removed).  
- Numbers can be removed or replaced with a placeholder like `<NUMBER>`.  
- Special characters and emojis are either removed or converted to tokens.

#### 6. üîç Sliding Window Technique  
- A window of size `2C + 1` slides over the text to generate (context, center word) pairs.  
- The center word is the middle word; context words are the `C` words before and after it.

#### 7. üî¢ Vector Transformation in CBOW  
- Context words are converted to one-hot vectors and averaged to form the input vector.  
- The center word is represented as a one-hot vector as the target output.

#### 8. üèóÔ∏è CBOW Neural Network Architecture  
- Input layer size = vocabulary size (V).  
- Hidden layer size = embedding dimension (N).  
- Output layer size = vocabulary size (V).  
- Hidden layer uses ReLU activation; output layer uses softmax to produce probabilities.

#### 9. ‚öôÔ∏è Training CBOW  
- Uses forward propagation: input ‚Üí hidden (ReLU) ‚Üí output (softmax).  
- Loss function is cross-entropy loss comparing predicted and actual center word.  
- Backpropagation computes gradients; gradient descent updates weights and biases.  
- Learning rate (Œ±) is a key hyperparameter controlling update size.

#### 10. üéØ Extracting Word Embeddings  
- Word embeddings are the learned weights in the hidden layer weight matrix (W1).  
- Embeddings can also be averaged from input and output weight matrices.

#### 11. üìä Evaluating Word Embeddings  
- Intrinsic evaluation tests word relationships via analogies and clustering.  
- Extrinsic evaluation tests embeddings on real NLP tasks like named entity recognition.  
- Intrinsic evaluation is faster but less indicative of real-world usefulness.  
- Extrinsic evaluation is more time-consuming but measures practical effectiveness.

#### 12. üßë‚Äçüíª Practical Implementation  
- Keras and PyTorch provide embedding layers (e.g., `Embedding(10000, 400)`).  
- Pre-trained embeddings and models can be fine-tuned for specific tasks.