## 2.4 Word embeddings with Neural Networks



### Key Points



#### 1. üßÆ Word Representations  
- Words can be represented as integers, one-hot vectors, or word embeddings.  
- Integers assign unique numbers to words but have no semantic meaning.  
- One-hot vectors are high-dimensional, sparse vectors with a single 1 and no embedded meaning.  
- Word embeddings are dense, low-dimensional vectors that capture semantic relationships between words.

#### 2. üåê Word Embeddings  
- Word embeddings represent words as vectors in a continuous space (~100-1000 dimensions).  
- Similar words have vectors close to each other (semantic similarity).  
- Word embeddings enable solving analogies via vector arithmetic (e.g., Paris:France :: Rome:?).

#### 3. üõ†Ô∏è Word Embedding Methods  
- Basic methods include word2vec (CBOW and Skip-gram), GloVe, and fastText.  
- Advanced methods include contextual embeddings like BERT, ELMo, and GPT-2.  
- fastText supports out-of-vocabulary (OOV) words by using subword information.

#### 4. üîÑ Continuous Bag-of-Words (CBOW) Model  
- CBOW predicts the center word from surrounding context words within a sliding window.  
- Context words are averaged into a single vector input to the neural network.  
- The model outputs a probability distribution over the vocabulary predicting the center word.

#### 5. üßπ Text Cleaning and Tokenization  
- Text is lowercased to treat "The," "the," and "THE" as the same word.  
- Punctuation is replaced or removed (e.g., commas, exclamation marks ‚Üí periods or removed).  
- Numbers can be removed or replaced with a placeholder like `<NUMBER>`.  
- Special characters and emojis are either removed or converted to tokens.

#### 6. üîç Sliding Window Technique  
- A window of size `2C + 1` slides over the text to generate (context, center word) pairs.  
- The center word is the middle word; context words are the `C` words before and after it.

#### 7. üî¢ Vector Transformation in CBOW  
- Context words are converted to one-hot vectors and averaged to form the input vector.  
- The center word is represented as a one-hot vector as the target output.

#### 8. üèóÔ∏è CBOW Neural Network Architecture  
- Input layer size = vocabulary size (V).  
- Hidden layer size = embedding dimension (N).  
- Output layer size = vocabulary size (V).  
- Hidden layer uses ReLU activation; output layer uses softmax to produce probabilities.

#### 9. ‚öôÔ∏è Training CBOW  
- Uses forward propagation: input ‚Üí hidden (ReLU) ‚Üí output (softmax).  
- Loss function is cross-entropy loss comparing predicted and actual center word.  
- Backpropagation computes gradients; gradient descent updates weights and biases.  
- Learning rate (Œ±) is a key hyperparameter controlling update size.

#### 10. üéØ Extracting Word Embeddings  
- Word embeddings are the learned weights in the hidden layer weight matrix (W1).  
- Embeddings can also be averaged from input and output weight matrices.

#### 11. üìä Evaluating Word Embeddings  
- Intrinsic evaluation tests word relationships via analogies and clustering.  
- Extrinsic evaluation tests embeddings on real NLP tasks like named entity recognition.  
- Intrinsic evaluation is faster but less indicative of real-world usefulness.  
- Extrinsic evaluation is more time-consuming but measures practical effectiveness.

#### 12. üßë‚Äçüíª Practical Implementation  
- Keras and PyTorch provide embedding layers (e.g., `Embedding(10000, 400)`).  
- Pre-trained embeddings and models can be fine-tuned for specific tasks.



<br>

## Study Notes





### 1. üß© Introduction to Word Embeddings

When working with language in machine learning, computers need a way to understand words. But words are not numbers, and computers only understand numbers. So, how do we represent words in a way that machines can process?

**Word embeddings** are a powerful solution. They transform words into numerical vectors that capture their meanings and relationships. Unlike simple methods like assigning each word a unique number or a one-hot vector, embeddings place words in a continuous vector space where similar words are close together.

#### Why do we need word embeddings?

- **Integers**: Assigning each word a unique integer (e.g., "happy" = 5) is simple but meaningless because the numbers don‚Äôt reflect any relationship between words.
- **One-hot vectors**: These are large, sparse vectors with a single 1 and the rest 0s. For example, if you have 10,000 words, each word is a 10,000-dimensional vector with one 1. This is inefficient and doesn‚Äôt capture any semantic similarity.
- **Word embeddings**: Dense, low-dimensional vectors (e.g., 100-1000 dimensions) that encode semantic meaning. For example, "king" and "queen" will have vectors close to each other, reflecting their related meanings.


### 2. üåê Understanding Word Representations

#### Integers and One-hot Vectors

- **Integers**: Words are mapped to unique numbers. This is easy but doesn‚Äôt capture meaning or similarity.
- **One-hot vectors**: Each word is represented as a vector with all zeros except a 1 in the position corresponding to that word. This creates very high-dimensional vectors with no semantic information.

#### Word Embeddings as Vectors

Word embeddings represent words as vectors in a continuous space. These vectors capture relationships such as:

- **Semantic similarity**: Words with similar meanings have vectors close to each other (e.g., "happy" and "joyful").
- **Analogies**: Vector arithmetic can solve analogies like "Paris is to France as Rome is to ___" by vector operations.

For example, the vector for "Paris" minus "France" plus "Italy" should be close to the vector for "Rome."


### 3. üõ†Ô∏è How to Create Word Embeddings

Creating word embeddings involves:

- **Corpus**: A large collection of text (e.g., Wikipedia).
- **Embedding method**: Algorithms that learn word vectors from the corpus.
- **Training process**: The model learns to predict words based on context or vice versa.
- **Hyperparameters**: Settings like embedding size (dimensions of vectors), window size (context range), learning rate, etc.

#### Types of Word Embedding Methods

- **Basic methods**:
  - **word2vec (Google, 2013)**: Includes Continuous Bag-of-Words (CBOW) and Skip-gram models.
  - **GloVe (Stanford, 2014)**: Uses global word co-occurrence statistics.
  - **fastText (Facebook, 2016)**: Handles out-of-vocabulary words by using subword information.

- **Advanced methods**:
  - **Contextual embeddings** like BERT, ELMo, GPT-2, which generate word vectors depending on the sentence context.


### 4. üîÑ The Continuous Bag-of-Words (CBOW) Model

CBOW is a popular word2vec model that predicts a **center word** based on its surrounding **context words**.

#### How CBOW works:

- Given a window of words around a center word, the model tries to predict the center word from the context.
- For example, in the sentence "I am happy because I am learning," if the window size is 2, the context for "happy" is ["I", "am", "because", "I"].
- The model averages the vectors of the context words and uses this to predict the center word.

#### Preparing training data for CBOW:

- Slide a window over the corpus.
- For each position, extract context words and the center word.
- Convert words to vectors (initially one-hot vectors).
- Average context vectors to form input.
- The output is the one-hot vector of the center word.


### 5. üßπ Cleaning and Tokenization of Text

Before training, text must be cleaned and tokenized to ensure consistency and quality.

#### Key steps:

- **Lowercasing**: Convert all text to lowercase so "The," "the," and "THE" are treated the same.
- **Punctuation**: Replace or remove punctuation marks (e.g., commas, exclamation points) to avoid noise.
- **Numbers**: Remove or replace numbers with a placeholder like `<NUMBER>`.
- **Special characters**: Remove symbols like $, ‚Ç¨, ¬ß, emojis can be optionally converted to text tokens.
- **Tokenization**: Split text into individual words or tokens.

Example in Python uses libraries like `nltk` and `emoji` to clean and tokenize text properly.


### 6. üîç Sliding Window Technique for Context Extraction

The sliding window is a method to generate training pairs for CBOW:

- Move a window of size `2C + 1` across the text.
- The center word is the word in the middle.
- The context words are the `C` words before and after the center word.
- This creates many (context, center word) pairs for training.

Example: For the sentence "I am happy because I am learning" with `C=2`, the context for "happy" is ["I", "am", "because", "I"].


### 7. üî¢ Transforming Words into Vectors for CBOW

- **Center word**: Represented as a one-hot vector.
- **Context words**: Each context word is a one-hot vector; average these vectors to get the input vector.
- This averaged vector is fed into the neural network to predict the center word.

Example:

| Context Words | Vector (average)          | Center Word | Vector (one-hot)     |
|---------------|--------------------------|-------------|----------------------|
| I am because  | [0.25, 0.25, 0, 0.5, 0] | happy       | [0, 0, 1, 0, 0]      |


### 8. üèóÔ∏è Architecture of the CBOW Neural Network

The CBOW model is a simple feedforward neural network with three layers:

- **Input layer**: Takes the averaged context word vector.
- **Hidden layer**: Learns a dense representation (embedding) of words. Uses ReLU activation (`max(0, x)`).
- **Output layer**: Predicts the center word using a softmax function, which outputs probabilities for each word in the vocabulary.

#### Dimensions:

- Vocabulary size = V
- Embedding size = N
- Input vector: V-dimensional (one-hot or averaged)
- Hidden layer: N-dimensional
- Output layer: V-dimensional (probabilities for each word)


### 9. ‚öôÔ∏è Training the CBOW Model

#### Forward propagation:

- Input vector ‚Üí hidden layer (linear transformation + ReLU)
- Hidden layer ‚Üí output layer (linear transformation + softmax)
- Output is a probability distribution over vocabulary words.

#### Loss function:

- **Cross-entropy loss** measures how well the predicted probabilities match the actual center word.
- The goal is to minimize this loss during training.

#### Backpropagation and gradient descent:

- Calculate gradients of loss with respect to weights and biases.
- Update parameters to reduce loss.
- Repeat over many training examples.


### 10. üéØ Extracting Word Embeddings

After training, the learned weights in the hidden layer represent the word embeddings.

- The weight matrix between input and hidden layer (`W1`) contains the embeddings.
- Alternatively, embeddings can be averaged from input and output weight matrices.
- These embeddings can then be used for various NLP tasks.


### 11. üìä Evaluating Word Embeddings

#### Intrinsic evaluation:

- Tests the quality of embeddings by checking word relationships.
- Examples:
  - **Analogies**: "France" is to "Paris" as "Italy" is to "Rome."
  - **Clustering**: Grouping similar words together (e.g., "village," "town," "city").
- Fast and easy but doesn‚Äôt always reflect real-world usefulness.

#### Extrinsic evaluation:

- Tests embeddings on actual NLP tasks like named entity recognition or sentiment analysis.
- More time-consuming but shows practical effectiveness.
- Helps understand how embeddings improve downstream applications.


### 12. üßë‚Äçüíª Practical Considerations and Libraries

- Popular libraries like **Keras** and **PyTorch** provide embedding layers to easily create and use word embeddings.
- Example in Keras: `Embedding(10000, 400)` creates embeddings for a vocabulary of 10,000 words with 400-dimensional vectors.
- Pre-trained embeddings and models (e.g., BERT, GPT) can be fine-tuned for specific tasks.


### Summary

Word embeddings are a foundational technique in NLP that convert words into meaningful numerical vectors. The Continuous Bag-of-Words (CBOW) model is a simple yet effective way to learn these embeddings by predicting a word from its context. Proper text cleaning, tokenization, and training are essential for good embeddings. Evaluations help us understand their quality, and modern libraries make it easy to implement and use embeddings in real applications.



<br>

## Questions



#### 1. What are the main limitations of representing words as integers in NLP tasks?  
A) Integers imply an arbitrary ordering that does not reflect semantic relationships  
B) Integers require very large storage space for vocabulary  
C) Integers do not capture similarity between words  
D) Integers can only represent a limited vocabulary size  


#### 2. Which of the following statements about one-hot vectors are true?  
A) One-hot vectors are sparse and high-dimensional  
B) One-hot vectors inherently encode semantic similarity between words  
C) One-hot vectors have exactly one element set to 1 and the rest 0  
D) One-hot vectors are efficient for large vocabularies  


#### 3. Word embeddings differ from one-hot vectors because they:  
A) Are dense, low-dimensional vectors  
B) Capture semantic relationships between words  
C) Are always binary vectors  
D) Can be used to perform vector arithmetic for analogies  


#### 4. Which of the following are common word embedding methods?  
A) Continuous Bag-of-Words (CBOW)  
B) Skip-gram with Negative Sampling (SGNS)  
C) Principal Component Analysis (PCA)  
D) Global Vectors (GloVe)  


#### 5. What is the primary goal of the Continuous Bag-of-Words (CBOW) model?  
A) Predict the context words given the center word  
B) Predict the center word given the context words  
C) Generate one-hot vectors for words  
D) Cluster words based on their frequency  


#### 6. In the CBOW model, what does the "window size" hyperparameter control?  
A) The number of words predicted at once  
B) The number of context words considered on each side of the center word  
C) The dimensionality of the word embeddings  
D) The size of the vocabulary  


#### 7. Why is it important to clean and tokenize text before training word embeddings?  
A) To reduce vocabulary size by merging case variants  
B) To remove noise such as punctuation and special characters  
C) To ensure consistent representation of numbers and emojis  
D) To increase the dimensionality of one-hot vectors  


#### 8. Which of the following are typical steps in cleaning and tokenizing text for word embeddings?  
A) Converting all text to lowercase  
B) Removing or replacing punctuation with a standard token  
C) Keeping all numbers as they are without any replacement  
D) Removing special characters like currency symbols and emojis  


#### 9. When using a sliding window to generate training data for CBOW, what is true?  
A) The center word is always the first word in the window  
B) Context words are the words surrounding the center word within the window  
C) The window size determines how many context words are used in total  
D) The sliding window moves one word at a time through the corpus  


#### 10. How are context words represented as input to the CBOW model?  
A) As the sum of their one-hot vectors  
B) As the average of their one-hot vectors  
C) As concatenated one-hot vectors  
D) As the one-hot vector of the center word  


#### 11. Which activation functions are used in the CBOW neural network architecture?  
A) Sigmoid in the hidden layer  
B) ReLU in the hidden layer  
C) Softmax in the output layer  
D) Tanh in the output layer  


#### 12. What is the role of the softmax function in the CBOW model?  
A) To normalize the output into a probability distribution over the vocabulary  
B) To select the most frequent word in the corpus  
C) To compute the loss function directly  
D) To convert word embeddings into one-hot vectors  


#### 13. Cross-entropy loss in CBOW training is used to:  
A) Measure the difference between predicted and actual center word distributions  
B) Maximize the similarity between context and center word vectors  
C) Penalize incorrect predictions more heavily than correct ones  
D) Calculate the Euclidean distance between word embeddings  


#### 14. During backpropagation in CBOW training, what is updated?  
A) The input word vectors only  
B) The weights and biases of the neural network  
C) The one-hot vectors of the words  
D) The corpus text itself  


#### 15. Which of the following statements about extracting word embeddings after training are correct?  
A) The embedding vectors are stored in the weight matrix between input and hidden layers  
B) Embeddings can be averaged from both input-to-hidden and hidden-to-output weight matrices  
C) Embeddings are the output of the softmax layer  
D) Embeddings are one-hot vectors transformed by the ReLU function  


#### 16. Intrinsic evaluation of word embeddings involves:  
A) Testing embeddings on external NLP tasks like sentiment analysis  
B) Measuring how well embeddings capture word analogies and semantic relationships  
C) Clustering similar words and visualizing their vectors  
D) Measuring training time and computational efficiency  


#### 17. Extrinsic evaluation of word embeddings is:  
A) Faster and easier to perform than intrinsic evaluation  
B) More focused on the usefulness of embeddings in real-world NLP tasks  
C) Less reliable because it doesn‚Äôt test semantic relationships directly  
D) Time-consuming and harder to troubleshoot  


#### 18. Which of the following are advantages of using word embeddings over one-hot vectors?  
A) Reduced dimensionality and computational efficiency  
B) Ability to capture semantic similarity between words  
C) Simpler to implement and interpret  
D) Avoidance of the curse of dimensionality  


#### 19. What challenges arise if text cleaning and tokenization are not properly performed before training embeddings?  
A) Increased vocabulary size due to case and punctuation variants  
B) Poor quality embeddings due to noisy or inconsistent input  
C) Faster training due to more diverse tokens  
D) Difficulty in handling out-of-vocabulary words  


#### 20. Which of the following statements about advanced word embedding methods like BERT and ELMo are true?  
A) They generate static embeddings independent of context  
B) They produce contextual embeddings that vary depending on sentence meaning  
C) They require large pre-trained models and fine-tuning for specific tasks  
D) They are less effective than CBOW for capturing semantic analogies  



<br>

## Answers



#### 1. What are the main limitations of representing words as integers in NLP tasks?  
A) ‚úì Integers imply an arbitrary ordering that does not reflect semantic relationships  
B) ‚úó Integers themselves do not require large storage; the issue is semantic meaning, not storage  
C) ‚úì Integers do not capture similarity between words  
D) ‚úó Integers can represent any vocabulary size, limited only by integer range  

**Correct:** A, C


#### 2. Which of the following statements about one-hot vectors are true?  
A) ‚úì One-hot vectors are sparse and high-dimensional  
B) ‚úó One-hot vectors do not encode semantic similarity; all vectors are orthogonal  
C) ‚úì One-hot vectors have exactly one element set to 1 and the rest 0  
D) ‚úó One-hot vectors are inefficient for large vocabularies due to sparsity  

**Correct:** A, C


#### 3. Word embeddings differ from one-hot vectors because they:  
A) ‚úì Are dense, low-dimensional vectors  
B) ‚úì Capture semantic relationships between words  
C) ‚úó Word embeddings are continuous-valued, not binary  
D) ‚úì Can be used to perform vector arithmetic for analogies  

**Correct:** A, B, D


#### 4. Which of the following are common word embedding methods?  
A) ‚úì Continuous Bag-of-Words (CBOW) is a word2vec method  
B) ‚úì Skip-gram with Negative Sampling (SGNS) is a word2vec method  
C) ‚úó PCA is a dimensionality reduction technique, not a word embedding method  
D) ‚úì Global Vectors (GloVe) is a popular embedding method  

**Correct:** A, B, D


#### 5. What is the primary goal of the Continuous Bag-of-Words (CBOW) model?  
A) ‚úó This describes Skip-gram, not CBOW  
B) ‚úì CBOW predicts the center word given the context words  
C) ‚úó CBOW does not generate one-hot vectors; it uses them as input/output representations  
D) ‚úó CBOW does not cluster words based on frequency  

**Correct:** B


#### 6. In the CBOW model, what does the "window size" hyperparameter control?  
A) ‚úó CBOW predicts one center word at a time  
B) ‚úì Window size controls how many context words are considered on each side of the center word  
C) ‚úó Embedding dimensionality is a separate hyperparameter  
D) ‚úó Vocabulary size is independent of window size  

**Correct:** B


#### 7. Why is it important to clean and tokenize text before training word embeddings?  
A) ‚úì Lowercasing merges case variants, reducing vocabulary size  
B) ‚úì Removing punctuation reduces noise and inconsistencies  
C) ‚úì Consistent handling of numbers and emojis improves quality  
D) ‚úó Cleaning does not increase one-hot vector dimensionality; it reduces noise  

**Correct:** A, B, C


#### 8. Which of the following are typical steps in cleaning and tokenizing text for word embeddings?  
A) ‚úì Lowercasing is standard practice  
B) ‚úì Punctuation is often replaced or removed for consistency  
C) ‚úó Numbers are usually replaced or normalized, not kept as-is  
D) ‚úó Special characters like emojis can be removed or converted, not always removed  

**Correct:** A, B


#### 9. When using a sliding window to generate training data for CBOW, what is true?  
A) ‚úó The center word is in the middle, not the first word  
B) ‚úì Context words are the surrounding words within the window  
C) ‚úó Window size is half the total context size on each side, total context words = 2*C  
D) ‚úì The window moves one word at a time through the corpus  

**Correct:** B, D


#### 10. How are context words represented as input to the CBOW model?  
A) ‚úó The sum is not normalized; averaging is preferred  
B) ‚úì The average of one-hot vectors is used as input  
C) ‚úó Concatenation would increase input size and is not used in CBOW  
D) ‚úó The center word is the output, not input  

**Correct:** B


#### 11. Which activation functions are used in the CBOW neural network architecture?  
A) ‚úó Sigmoid is not typically used in CBOW hidden layers  
B) ‚úì ReLU is used in the hidden layer for non-linearity  
C) ‚úì Softmax is used in the output layer to produce probabilities  
D) ‚úó Tanh is not used in the output layer  

**Correct:** B, C


#### 12. What is the role of the softmax function in the CBOW model?  
A) ‚úì Softmax normalizes outputs into a probability distribution over vocabulary  
B) ‚úó Softmax does not select the most frequent word  
C) ‚úó Softmax does not compute loss directly; loss is computed separately  
D) ‚úó Softmax does not convert embeddings into one-hot vectors  

**Correct:** A


#### 13. Cross-entropy loss in CBOW training is used to:  
A) ‚úì Measure difference between predicted and actual word distributions  
B) ‚úó It does not directly maximize similarity but penalizes prediction errors  
C) ‚úì Penalizes incorrect predictions more than correct ones  
D) ‚úó Cross-entropy is not Euclidean distance  

**Correct:** A, C


#### 14. During backpropagation in CBOW training, what is updated?  
A) ‚úó Input word vectors are part of weights, but not the only parameters updated  
B) ‚úì Weights and biases of the neural network are updated  
C) ‚úó One-hot vectors are fixed representations, not updated  
D) ‚úó The corpus text is never changed during training  

**Correct:** B


#### 15. Which of the following statements about extracting word embeddings after training are correct?  
A) ‚úì Embeddings are stored in the input-to-hidden weight matrix  
B) ‚úì Embeddings can be averaged from input and output weight matrices for better quality  
C) ‚úó Embeddings are not outputs of the softmax layer  
D) ‚úó Embeddings are continuous vectors, not one-hot transformed by ReLU  

**Correct:** A, B


#### 16. Intrinsic evaluation of word embeddings involves:  
A) ‚úó Testing on external tasks is extrinsic evaluation  
B) ‚úì Testing analogies and semantic relationships is intrinsic evaluation  
C) ‚úì Clustering and visualization are intrinsic evaluation methods  
D) ‚úó Training time is unrelated to intrinsic evaluation  

**Correct:** B, C


#### 17. Extrinsic evaluation of word embeddings is:  
A) ‚úó Extrinsic evaluation is slower and more complex than intrinsic  
B) ‚úì Focused on usefulness in real NLP tasks  
C) ‚úó It is reliable for measuring practical effectiveness  
D) ‚úì Time-consuming and harder to troubleshoot  

**Correct:** B, D


#### 18. Which of the following are advantages of using word embeddings over one-hot vectors?  
A) ‚úì Reduced dimensionality and computational efficiency  
B) ‚úì Capture semantic similarity between words  
C) ‚úó Word embeddings are more complex to implement and interpret  
D) ‚úì Help mitigate the curse of dimensionality by using dense vectors  

**Correct:** A, B, D


#### 19. What challenges arise if text cleaning and tokenization are not properly performed before training embeddings?  
A) ‚úì Vocabulary size inflates due to case and punctuation variants  
B) ‚úì Noisy input leads to poor quality embeddings  
C) ‚úó Training is usually slower with noisy data, not faster  
D) ‚úì Difficulties handling out-of-vocabulary words increase  

**Correct:** A, B, D


#### 20. Which of the following statements about advanced word embedding methods like BERT and ELMo are true?  
A) ‚úó They produce contextual embeddings, not static ones  
B) ‚úì Embeddings vary depending on sentence context  
C) ‚úì Require large pre-trained models and fine-tuning  
D) ‚úó They are generally more effective than CBOW for capturing complex semantics  

**Correct:** B, C

