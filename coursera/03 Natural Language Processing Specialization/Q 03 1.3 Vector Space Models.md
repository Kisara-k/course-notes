## 1.3 Vector Space Models

## Questions

#### 1. What is the primary purpose of representing words as vectors in a vector space model?  
A) To enable computers to perform arithmetic on words  
B) To capture the relative meaning of words based on their context  
C) To store dictionary definitions of words  
D) To measure similarity between words and documents  

#### 2. Which of the following best describes a word-by-word vector representation?  
A) Counting how many times a word appears in a document  
B) Counting how often words co-occur within a fixed window size in text  
C) Representing words as one-hot encoded vectors  
D) Representing documents as vectors of word frequencies  

#### 3. Why might cosine similarity be preferred over Euclidean distance when comparing word vectors?  
A) Cosine similarity is sensitive to vector magnitude differences  
B) Cosine similarity measures the angle between vectors, ignoring magnitude  
C) Euclidean distance can be misleading when vectors have different lengths  
D) Euclidean distance always gives values between 0 and 1  

#### 4. Which of the following statements about Euclidean distance is true?  
A) It measures the straight-line distance between two points in vector space  
B) It is always a better measure of similarity than cosine similarity  
C) It can be calculated using the norm of the difference between two vectors  
D) It is unaffected by the scale or length of the vectors  

#### 5. In the analogy "Washington - USA + Russia ≈ Moscow," what property of word vectors does this illustrate?  
A) Vector addition and subtraction can capture semantic relationships  
B) Word vectors are always orthogonal  
C) Word vectors encode syntactic information only  
D) Vector arithmetic can be used to predict unknown word relationships  

#### 6. What does the window size \( k \) represent in co-occurrence vector models?  
A) The number of documents in the corpus  
B) The maximum distance between words to be considered co-occurring  
C) The number of dimensions in the vector space  
D) The frequency threshold for including a word in the vocabulary  

#### 7. Which of the following are true about word-by-document vector representations?  
A) They count how often a word appears in different documents or categories  
B) They capture local context by looking at neighboring words  
C) They can be used to represent documents as vectors of word frequencies  
D) They ignore the frequency of words in documents  

#### 8. What is a key limitation of using Euclidean distance for comparing vectors from corpora of different sizes?  
A) It ignores the direction of vectors  
B) It is computationally expensive  
C) It cannot handle vectors with negative values  
D) It always produces values between 0 and 1  

#### 9. Which of the following statements about cosine similarity are correct?  
A) Cosine similarity values range from -1 to 1  
B) It is calculated using the dot product and norms of vectors  
C) It is useful when comparing vectors of different magnitudes  
D) It measures the Euclidean distance between vectors  

#### 10. Principal Component Analysis (PCA) is used in vector space models primarily to:  
A) Increase the number of dimensions in the vector space  
B) Find uncorrelated features that capture the most variance in data  
C) Visualize high-dimensional word vectors in 2D or 3D  
D) Normalize word vectors to unit length  

#### 11. In PCA, what do eigenvectors represent?  
A) The amount of variance retained by each principal component  
B) Directions of new uncorrelated features in the data  
C) The original word vectors before transformation  
D) The dot product between two vectors  

#### 12. What is the role of eigenvalues in PCA?  
A) They represent the direction of principal components  
B) They quantify the variance explained by each principal component  
C) They are used to normalize the data before PCA  
D) They indicate the number of dimensions to reduce to  

#### 13. Which of the following steps are part of the PCA algorithm?  
A) Mean normalization of data  
B) Computing the covariance matrix  
C) Performing Singular Value Decomposition (SVD)  
D) Calculating cosine similarity between vectors  

#### 14. Why is it important to mean normalize data before applying PCA?  
A) To center the data around zero so variance is measured correctly  
B) To reduce the number of dimensions automatically  
C) To ensure all vectors have the same length  
D) To remove noise from the data  

#### 15. Which of the following are true about the phrase "You shall know a word by the company it keeps"?  
A) It suggests that word meaning can be inferred from surrounding words  
B) It supports the idea of co-occurrence-based vector representations  
C) It implies that words have fixed meanings regardless of context  
D) It is the fundamental concept behind vector space models  

#### 16. How does vector space modeling help in machine translation?  
A) By encoding syntactic rules explicitly  
B) By representing words and phrases as vectors to capture meaning  
C) By translating words based on their vector similarity across languages  
D) By storing bilingual dictionaries as vectors  

#### 17. Which of the following are challenges when using Euclidean distance for similarity in NLP?  
A) It is sensitive to the magnitude of vectors, which can vary widely  
B) It cannot distinguish between vectors pointing in the same direction but with different lengths  
C) It is computationally more complex than cosine similarity  
D) It always favors shorter vectors over longer ones  

#### 18. When visualizing word vectors, why might dimensionality reduction be necessary?  
A) Because word vectors are often in hundreds or thousands of dimensions  
B) To improve the accuracy of word similarity calculations  
C) To make it possible to plot vectors on 2D or 3D graphs  
D) To remove semantic information from the vectors  

#### 19. Which of the following statements about dot product in vector space models is true?  
A) It is used to calculate cosine similarity  
B) It measures the Euclidean distance between two vectors  
C) It is the sum of the products of corresponding vector components  
D) It always produces a value between 0 and 1  

#### 20. What does it mean for features to be "uncorrelated" in the context of PCA?  
A) The features have zero covariance with each other  
B) The features are independent and do not share information  
C) The features are identical in all dimensions  
D) The features represent redundant information



<br>

## Answers

#### 1. What is the primary purpose of representing words as vectors in a vector space model?  
A) ✓ To enable computers to perform arithmetic on words — Vector arithmetic is a key feature of VSMs.  
B) ✓ To capture the relative meaning of words based on their context — Core goal of VSMs is to represent meaning via context.  
C) ✗ To store dictionary definitions of words — VSMs do not store explicit definitions.  
D) ✓ To measure similarity between words and documents — Similarity measurement is fundamental in VSMs.  

**Correct:** A, B, D


#### 2. Which of the following best describes a word-by-word vector representation?  
A) ✗ Counting how many times a word appears in a document — This is word-by-document, not word-by-word.  
B) ✓ Counting how often words co-occur within a fixed window size in text — This is the definition of word-by-word vectors.  
C) ✗ Representing words as one-hot encoded vectors — One-hot vectors are sparse and do not capture co-occurrence.  
D) ✗ Representing documents as vectors of word frequencies — This describes word-by-document vectors.  

**Correct:** B


#### 3. Why might cosine similarity be preferred over Euclidean distance when comparing word vectors?  
A) ✗ Cosine similarity is sensitive to vector magnitude differences — It actually ignores magnitude differences.  
B) ✓ Cosine similarity measures the angle between vectors, ignoring magnitude — This is the key advantage.  
C) ✓ Euclidean distance can be misleading when vectors have different lengths — Magnitude differences affect Euclidean distance.  
D) ✗ Euclidean distance always gives values between 0 and 1 — Euclidean distance can be any non-negative number.  

**Correct:** B, C


#### 4. Which of the following statements about Euclidean distance is true?  
A) ✓ It measures the straight-line distance between two points in vector space — This is the definition.  
B) ✗ It is always a better measure of similarity than cosine similarity — Not true; depends on context.  
C) ✓ It can be calculated using the norm of the difference between two vectors — This is the standard formula.  
D) ✗ It is unaffected by the scale or length of the vectors — It is affected by vector magnitude.  

**Correct:** A, C


#### 5. In the analogy "Washington - USA + Russia ≈ Moscow," what property of word vectors does this illustrate?  
A) ✓ Vector addition and subtraction can capture semantic relationships — This is the key insight.  
B) ✗ Word vectors are always orthogonal — They are not necessarily orthogonal.  
C) ✗ Word vectors encode syntactic information only — They capture semantic info too.  
D) ✓ Vector arithmetic can be used to predict unknown word relationships — This is the practical use.  

**Correct:** A, D


#### 6. What does the window size \( k \) represent in co-occurrence vector models?  
A) ✗ The number of documents in the corpus — Irrelevant to window size.  
B) ✓ The maximum distance between words to be considered co-occurring — Correct definition of window size.  
C) ✗ The number of dimensions in the vector space — Dimensions depend on vocabulary size or features.  
D) ✗ The frequency threshold for including a word in the vocabulary — Different concept.  

**Correct:** B


#### 7. Which of the following are true about word-by-document vector representations?  
A) ✓ They count how often a word appears in different documents or categories — This is the definition.  
B) ✗ They capture local context by looking at neighboring words — This is word-by-word vectors.  
C) ✓ They can be used to represent documents as vectors of word frequencies — Documents are vectors of word counts.  
D) ✗ They ignore the frequency of words in documents — They explicitly use frequency.  

**Correct:** A, C


#### 8. What is a key limitation of using Euclidean distance for comparing vectors from corpora of different sizes?  
A) ✓ It ignores the direction of vectors — Euclidean distance is sensitive to magnitude, not direction.  
B) ✗ It is computationally expensive — Not a major limitation compared to cosine similarity.  
C) ✗ It cannot handle vectors with negative values — It can handle negatives.  
D) ✗ It always produces values between 0 and 1 — Values can be larger.  

**Correct:** A


#### 9. Which of the following statements about cosine similarity are correct?  
A) ✗ Cosine similarity values range from -1 to 1 — For non-negative vectors like word counts, range is 0 to 1.  
B) ✓ It is calculated using the dot product and norms of vectors — This is the formula.  
C) ✓ It is useful when comparing vectors of different magnitudes — Ignores magnitude, focuses on direction.  
D) ✗ It measures the Euclidean distance between vectors — Different metric entirely.  

**Correct:** B, C


#### 10. Principal Component Analysis (PCA) is used in vector space models primarily to:  
A) ✗ Increase the number of dimensions in the vector space — PCA reduces dimensions.  
B) ✓ Find uncorrelated features that capture the most variance in data — Core purpose of PCA.  
C) ✓ Visualize high-dimensional word vectors in 2D or 3D — Common use of PCA.  
D) ✗ Normalize word vectors to unit length — PCA does not normalize vectors.  

**Correct:** B, C


#### 11. In PCA, what do eigenvectors represent?  
A) ✗ The amount of variance retained by each principal component — This is eigenvalues.  
B) ✓ Directions of new uncorrelated features in the data — Eigenvectors define principal components.  
C) ✗ The original word vectors before transformation — They are transformed by eigenvectors.  
D) ✗ The dot product between two vectors — Dot product is a separate operation.  

**Correct:** B


#### 12. What is the role of eigenvalues in PCA?  
A) ✗ They represent the direction of principal components — This is eigenvectors.  
B) ✓ They quantify the variance explained by each principal component — Correct.  
C) ✗ They are used to normalize the data before PCA — Normalization is a separate step.  
D) ✗ They indicate the number of dimensions to reduce to — They help decide but do not indicate directly.  

**Correct:** B


#### 13. Which of the following steps are part of the PCA algorithm?  
A) ✓ Mean normalization of data — Essential first step.  
B) ✓ Computing the covariance matrix — Needed to find variance relationships.  
C) ✓ Performing Singular Value Decomposition (SVD) — Used to find eigenvectors and eigenvalues.  
D) ✗ Calculating cosine similarity between vectors — Not part of PCA.  

**Correct:** A, B, C


#### 14. Why is it important to mean normalize data before applying PCA?  
A) ✓ To center the data around zero so variance is measured correctly — Correct reason.  
B) ✗ To reduce the number of dimensions automatically — PCA reduces dimensions but normalization is not for this.  
C) ✗ To ensure all vectors have the same length — Normalization centers data, does not scale length.  
D) ✗ To remove noise from the data — Noise removal is a different process.  

**Correct:** A


#### 15. Which of the following are true about the phrase "You shall know a word by the company it keeps"?  
A) ✓ It suggests that word meaning can be inferred from surrounding words — Core idea behind co-occurrence.  
B) ✓ It supports the idea of co-occurrence-based vector representations — Directly related.  
C) ✗ It implies that words have fixed meanings regardless of context — Opposite of the idea.  
D) ✓ It is the fundamental concept behind vector space models — Foundational principle.  

**Correct:** A, B, D


#### 16. How does vector space modeling help in machine translation?  
A) ✗ By encoding syntactic rules explicitly — VSMs focus on semantics, not explicit syntax rules.  
B) ✓ By representing words and phrases as vectors to capture meaning — Captures semantic similarity.  
C) ✓ By translating words based on their vector similarity across languages — Cross-lingual embeddings use this.  
D) ✗ By storing bilingual dictionaries as vectors — Dictionaries are not vectors themselves.  

**Correct:** B, C


#### 17. Which of the following are challenges when using Euclidean distance for similarity in NLP?  
A) ✓ It is sensitive to the magnitude of vectors, which can vary widely — Magnitude affects Euclidean distance.  
B) ✓ It cannot distinguish between vectors pointing in the same direction but with different lengths — Actually, it *does* distinguish by length, which can be misleading.  
C) ✗ It is computationally more complex than cosine similarity — Both are similar in complexity.  
D) ✗ It always favors shorter vectors over longer ones — It favors closer points, not necessarily shorter vectors.  

**Correct:** A, B


#### 18. When visualizing word vectors, why might dimensionality reduction be necessary?  
A) ✓ Because word vectors are often in hundreds or thousands of dimensions — Too high to visualize directly.  
B) ✗ To improve the accuracy of word similarity calculations — PCA is for visualization, not accuracy improvement.  
C) ✓ To make it possible to plot vectors on 2D or 3D graphs — Visualization requires fewer dimensions.  
D) ✗ To remove semantic information from the vectors — PCA tries to preserve information.  

**Correct:** A, C


#### 19. Which of the following statements about dot product in vector space models is true?  
A) ✓ It is used to calculate cosine similarity — Dot product is numerator in cosine similarity.  
B) ✗ It measures the Euclidean distance between two vectors — Different operation.  
C) ✓ It is the sum of the products of corresponding vector components — Definition of dot product.  
D) ✗ It always produces a value between 0 and 1 — Dot product can be any real number.  

**Correct:** A, C


#### 20. What does it mean for features to be "uncorrelated" in the context of PCA?  
A) ✓ The features have zero covariance with each other — Definition of uncorrelated features.  
B) ✓ The features are independent and do not share information — Uncorrelated implies statistical independence in PCA context.  
C) ✗ The features are identical in all dimensions — Identical means perfectly correlated.  
D) ✗ The features represent redundant information — Uncorrelated means no redundancy.  

**Correct:** A, B