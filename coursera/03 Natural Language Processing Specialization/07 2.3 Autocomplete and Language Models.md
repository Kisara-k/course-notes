## 2.3 Autocomplete and Language Models

## Study Notes

### 1. ðŸ“š Introduction to Language Models and Autocomplete

Language models (LMs) are fundamental tools in natural language processing (NLP) that help computers understand and generate human language. At their core, language models estimate the probability of sequences of words. This means they can predict how likely a particular word is to follow a given sequence of words.

For example, if you have the phrase **"Lyn is eating ..."**, a language model can suggest the most probable next words like **"chocolate"**, **"eggs"**, or **"toast"** based on patterns learned from a large collection of text (called a **corpus**).

#### Why are language models important?

- **Autocomplete:** Suggest the next word or phrase as you type.
- **Spelling correction:** Detect and correct errors by comparing probabilities of word sequences (e.g., "ship" vs. "shop").
- **Speech recognition:** Choose the most probable word sequence from sounds (e.g., "I saw a van" vs. "eyes awe of an").
- **Augmentative communication:** Help people who cannot speak by predicting likely words from a menu.


### 2. ðŸ”¢ Understanding N-Grams and Their Probabilities

#### What are N-grams?

An **N-gram** is a sequence of **N** words. They are the building blocks of many language models.

- **Unigrams:** Single words (N=1). Example: {I, am, happy}
- **Bigrams:** Sequences of two words (N=2). Example: {I am, am happy}
- **Trigrams:** Sequences of three words (N=3). Example: {I am happy, am happy because}

#### How do we use N-grams?

We use N-grams to estimate the probability of a word given the previous N-1 words. For example, the probability of the word "happy" following "I am" is a **trigram probability**: P(happy | I am).

#### Calculating N-gram probabilities

- **Unigram probability:** Probability of a single word appearing in the corpus.
- **Bigram probability:** Probability of a word given the previous word.
- **Trigram probability:** Probability of a word given the previous two words.

These probabilities are calculated by counting how often the N-gram appears in the corpus and dividing by the count of the preceding (N-1)-gram.


### 3. ðŸ”„ Sequence Probability and the Markov Assumption

#### What is sequence probability?

The probability of a whole sentence or sequence of words is the product of the probabilities of each word given the previous words. However, calculating this exactly is difficult because most sentences or long sequences do not appear exactly in the training corpus.

#### The Markov assumption

To simplify, language models assume that the probability of a word depends only on the previous **N-1** words, not the entire history. This is called the **Markov assumption**.

- For bigrams, the next word depends only on the immediately preceding word.
- For trigrams, the next word depends on the two preceding words.

This assumption allows us to approximate the probability of a sentence by multiplying the conditional probabilities of each word given the previous N-1 words.


### 4. ðŸš¦ Start and End Tokens in Sentences

When modeling sentences, itâ€™s important to mark where sentences begin and end.

- **Start token `<s>`:** Added at the beginning of a sentence to indicate its start.
- **End token `</s>`:** Added at the end of a sentence to indicate its end.

For example, the sentence "the teacher drinks tea" becomes:

`<s> the teacher drinks tea </s>`

For N-grams, we add **N-1** start tokens. For a trigram model, two start tokens are added:

`<s> <s> the teacher drinks tea </s>`

These tokens help the model learn when sentences start and stop, which is crucial for accurate probability estimation and generation.


### 5. ðŸ“Š Building the N-gram Language Model

#### Count matrix and probability matrix

- **Count matrix:** A table that counts how many times each N-gram appears in the corpus.
- **Probability matrix:** Created by dividing each count by the total counts of the preceding (N-1)-gram, converting counts into probabilities.

For example, in a bigram model, the row corresponds to the first word, and the columns correspond to possible next words. Each cell contains the probability of the next word given the first word.

#### Log probabilities

Multiplying many small probabilities can lead to numerical underflow (very small numbers that computers struggle to represent). To avoid this, we use **logarithms** of probabilities.

- Multiplication of probabilities becomes addition of log probabilities.
- This makes calculations more stable and efficient.


### 6. ðŸ§ª Evaluating Language Models: Train/Test Split and Perplexity

#### Train/Validation/Test split

To evaluate a language model, the corpus is split into:

- **Training set:** Used to build the model.
- **Validation set:** Used to tune parameters.
- **Test set:** Used to evaluate the final modelâ€™s performance.

Typical splits:

- Small corpora: 80% train, 10% validation, 10% test.
- Large corpora: 98% train, 1% validation, 1% test.

#### Perplexity

Perplexity measures how well a language model predicts a test set. It is the exponentiated average negative log probability of the test data.

- **Lower perplexity means a better model.**
- It can be thought of as the modelâ€™s uncertainty: lower perplexity means less uncertainty.


### 7. ðŸš« Handling Out-of-Vocabulary (OOV) Words

#### What are OOV words?

Words that appear in the input but were never seen in the training corpus are called **out-of-vocabulary (OOV)** words.

#### How to handle OOV words?

- Introduce a special token `<UNK>` to represent all unknown words.
- Replace rare or unseen words in the corpus with `<UNK>`.
- This allows the model to assign some probability to unknown words instead of zero.

#### Vocabulary creation

- Define a minimum frequency threshold.
- Words below this frequency are replaced with `<UNK>`.
- This helps keep the vocabulary manageable and improves model robustness.


### 8. ðŸ§‚ Smoothing Techniques for Missing N-grams

#### Why smoothing?

Even with a large corpus, some valid N-grams may never appear in the training data, leading to zero probabilities. This is problematic because zero probability means the model thinks the sequence is impossible.

#### Common smoothing methods

- **Add-one smoothing (Laplacian smoothing):** Add 1 to all counts to avoid zeros.
- **Add-k smoothing:** Add a small constant k instead of 1.
- **Advanced methods:** Kneser-Ney smoothing, Good-Turing smoothing.

#### Backoff and interpolation

- **Backoff:** If an N-gram is missing, back off to a lower-order N-gram (e.g., from trigram to bigram).
- **Interpolation:** Combine probabilities from different N-gram orders weighted by some factors.

These methods help assign reasonable probabilities to unseen sequences.


### 9. ðŸ§© Summary and Practical Applications

- **N-grams** are sequences of words used to estimate probabilities in language models.
- **Language models** predict the next word in a sequence, enabling autocomplete, spelling correction, speech recognition, and assistive communication.
- **Start and end tokens** help models understand sentence boundaries.
- **Log probabilities** prevent numerical underflow in calculations.
- **Perplexity** is a key metric to evaluate model quality.
- **Out-of-vocabulary words** are handled with `<UNK>` tokens.
- **Smoothing** techniques fix zero probabilities for unseen N-grams.
- Language models are built by counting N-grams in a corpus, converting counts to probabilities, and applying smoothing and backoff as needed.


This detailed understanding of N-gram language models provides a solid foundation for building and evaluating models that can predict text sequences, enabling many practical NLP applications like autocomplete and speech recognition.



<br>

## Key Points

#### 1. ðŸ”¢ N-Grams and Probabilities  
- An N-gram is a sequence of N words (e.g., unigram = 1 word, bigram = 2 words, trigram = 3 words).  
- N-gram probability is calculated as the count of the N-gram divided by the count of the preceding (N-1)-gram.  
- Example: P(papers | it in the) = C(it in the papers) / C(it in the).

#### 2. ðŸ”„ Sequence Probability and Markov Assumption  
- The probability of a sentence is approximated by the product of conditional probabilities of each word given the previous N-1 words.  
- Markov assumption: only the last N-1 words affect the probability of the next word.  
- Bigram model uses only the previous word to predict the next word.

#### 3. ðŸš¦ Start and End Tokens  
- Sentences are prepended with N-1 start tokens `<s>` and appended with an end token `</s>`.  
- For a trigram model, two `<s>` tokens are added at the start of each sentence.  
- These tokens help the model learn sentence boundaries and improve probability estimation.

#### 4. ðŸ“Š Language Model Construction  
- Count matrix records frequencies of N-grams in the corpus.  
- Probability matrix is created by normalizing counts row-wise (dividing by the sum of counts for each (N-1)-gram).  
- Log probabilities are used to avoid numerical underflow when multiplying many small probabilities.

#### 5. ðŸ§ª Model Evaluation: Train/Test Split and Perplexity  
- Typical corpus split: small corpora (80% train, 10% validation, 10% test), large corpora (98% train, 1% validation, 1% test).  
- Perplexity measures how well a language model predicts a test set; lower perplexity indicates a better model.  
- Perplexity is calculated as the exponentiated negative average log probability of the test data.

#### 6. ðŸš« Out-of-Vocabulary (OOV) Words  
- Words not in the training vocabulary are called out-of-vocabulary (OOV).  
- OOV words are replaced with a special token `<UNK>`.  
- Vocabulary is created by including words above a minimum frequency threshold; rare words are replaced by `<UNK>`.

#### 7. ðŸ§‚ Smoothing and Backoff  
- Smoothing addresses zero probabilities for unseen N-grams in the training corpus.  
- Add-one (Laplacian) smoothing adds 1 to all counts to avoid zero probabilities.  
- Backoff uses lower-order N-gram probabilities when higher-order N-grams are missing.  
- Interpolation combines probabilities from multiple N-gram orders weighted appropriately.

#### 8. ðŸ§® Log Probability Calculations  
- Logarithms convert multiplication of probabilities into addition of log probabilities.  
- This prevents underflow and simplifies calculations in language models.  
- Example: log P(sentence) = sum of log conditional probabilities of each word.