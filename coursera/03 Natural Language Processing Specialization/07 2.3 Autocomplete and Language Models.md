## 2.3 Autocomplete and Language Models



### Key Points



#### 1. 🔢 N-Grams and Probabilities  
- An N-gram is a sequence of N words (e.g., unigram = 1 word, bigram = 2 words, trigram = 3 words).  
- N-gram probability is calculated as the count of the N-gram divided by the count of the preceding (N-1)-gram.  
- Example: P(papers | it in the) = C(it in the papers) / C(it in the).

#### 2. 🔄 Sequence Probability and Markov Assumption  
- The probability of a sentence is approximated by the product of conditional probabilities of each word given the previous N-1 words.  
- Markov assumption: only the last N-1 words affect the probability of the next word.  
- Bigram model uses only the previous word to predict the next word.

#### 3. 🚦 Start and End Tokens  
- Sentences are prepended with N-1 start tokens `<s>` and appended with an end token `</s>`.  
- For a trigram model, two `<s>` tokens are added at the start of each sentence.  
- These tokens help the model learn sentence boundaries and improve probability estimation.

#### 4. 📊 Language Model Construction  
- Count matrix records frequencies of N-grams in the corpus.  
- Probability matrix is created by normalizing counts row-wise (dividing by the sum of counts for each (N-1)-gram).  
- Log probabilities are used to avoid numerical underflow when multiplying many small probabilities.

#### 5. 🧪 Model Evaluation: Train/Test Split and Perplexity  
- Typical corpus split: small corpora (80% train, 10% validation, 10% test), large corpora (98% train, 1% validation, 1% test).  
- Perplexity measures how well a language model predicts a test set; lower perplexity indicates a better model.  
- Perplexity is calculated as the exponentiated negative average log probability of the test data.

#### 6. 🚫 Out-of-Vocabulary (OOV) Words  
- Words not in the training vocabulary are called out-of-vocabulary (OOV).  
- OOV words are replaced with a special token `<UNK>`.  
- Vocabulary is created by including words above a minimum frequency threshold; rare words are replaced by `<UNK>`.

#### 7. 🧂 Smoothing and Backoff  
- Smoothing addresses zero probabilities for unseen N-grams in the training corpus.  
- Add-one (Laplacian) smoothing adds 1 to all counts to avoid zero probabilities.  
- Backoff uses lower-order N-gram probabilities when higher-order N-grams are missing.  
- Interpolation combines probabilities from multiple N-gram orders weighted appropriately.

#### 8. 🧮 Log Probability Calculations  
- Logarithms convert multiplication of probabilities into addition of log probabilities.  
- This prevents underflow and simplifies calculations in language models.  
- Example: log P(sentence) = sum of log conditional probabilities of each word.



<br>

## Study Notes





### 1. 📚 Introduction to Language Models and Autocomplete

Language models (LMs) are fundamental tools in natural language processing (NLP) that help computers understand and generate human language. At their core, language models estimate the probability of sequences of words. This means they can predict how likely a particular word is to follow a given sequence of words.

For example, if you have the phrase **"Lyn is eating ..."**, a language model can suggest the most probable next words like **"chocolate"**, **"eggs"**, or **"toast"** based on patterns learned from a large collection of text (called a **corpus**).

#### Why are language models important?

- **Autocomplete:** Suggest the next word or phrase as you type.
- **Spelling correction:** Detect and correct errors by comparing probabilities of word sequences (e.g., "ship" vs. "shop").
- **Speech recognition:** Choose the most probable word sequence from sounds (e.g., "I saw a van" vs. "eyes awe of an").
- **Augmentative communication:** Help people who cannot speak by predicting likely words from a menu.


### 2. 🔢 Understanding N-Grams and Their Probabilities

#### What are N-grams?

An **N-gram** is a sequence of **N** words. They are the building blocks of many language models.

- **Unigrams:** Single words (N=1). Example: {I, am, happy}
- **Bigrams:** Sequences of two words (N=2). Example: {I am, am happy}
- **Trigrams:** Sequences of three words (N=3). Example: {I am happy, am happy because}

#### How do we use N-grams?

We use N-grams to estimate the probability of a word given the previous N-1 words. For example, the probability of the word "happy" following "I am" is a **trigram probability**: P(happy | I am).

#### Calculating N-gram probabilities

- **Unigram probability:** Probability of a single word appearing in the corpus.
- **Bigram probability:** Probability of a word given the previous word.
- **Trigram probability:** Probability of a word given the previous two words.

These probabilities are calculated by counting how often the N-gram appears in the corpus and dividing by the count of the preceding (N-1)-gram.


### 3. 🔄 Sequence Probability and the Markov Assumption

#### What is sequence probability?

The probability of a whole sentence or sequence of words is the product of the probabilities of each word given the previous words. However, calculating this exactly is difficult because most sentences or long sequences do not appear exactly in the training corpus.

#### The Markov assumption

To simplify, language models assume that the probability of a word depends only on the previous **N-1** words, not the entire history. This is called the **Markov assumption**.

- For bigrams, the next word depends only on the immediately preceding word.
- For trigrams, the next word depends on the two preceding words.

This assumption allows us to approximate the probability of a sentence by multiplying the conditional probabilities of each word given the previous N-1 words.


### 4. 🚦 Start and End Tokens in Sentences

When modeling sentences, it’s important to mark where sentences begin and end.

- **Start token `<s>`:** Added at the beginning of a sentence to indicate its start.
- **End token `</s>`:** Added at the end of a sentence to indicate its end.

For example, the sentence "the teacher drinks tea" becomes:

`<s> the teacher drinks tea </s>`

For N-grams, we add **N-1** start tokens. For a trigram model, two start tokens are added:

`<s> <s> the teacher drinks tea </s>`

These tokens help the model learn when sentences start and stop, which is crucial for accurate probability estimation and generation.


### 5. 📊 Building the N-gram Language Model

#### Count matrix and probability matrix

- **Count matrix:** A table that counts how many times each N-gram appears in the corpus.
- **Probability matrix:** Created by dividing each count by the total counts of the preceding (N-1)-gram, converting counts into probabilities.

For example, in a bigram model, the row corresponds to the first word, and the columns correspond to possible next words. Each cell contains the probability of the next word given the first word.

#### Log probabilities

Multiplying many small probabilities can lead to numerical underflow (very small numbers that computers struggle to represent). To avoid this, we use **logarithms** of probabilities.

- Multiplication of probabilities becomes addition of log probabilities.
- This makes calculations more stable and efficient.


### 6. 🧪 Evaluating Language Models: Train/Test Split and Perplexity

#### Train/Validation/Test split

To evaluate a language model, the corpus is split into:

- **Training set:** Used to build the model.
- **Validation set:** Used to tune parameters.
- **Test set:** Used to evaluate the final model’s performance.

Typical splits:

- Small corpora: 80% train, 10% validation, 10% test.
- Large corpora: 98% train, 1% validation, 1% test.

#### Perplexity

Perplexity measures how well a language model predicts a test set. It is the exponentiated average negative log probability of the test data.

- **Lower perplexity means a better model.**
- It can be thought of as the model’s uncertainty: lower perplexity means less uncertainty.


### 7. 🚫 Handling Out-of-Vocabulary (OOV) Words

#### What are OOV words?

Words that appear in the input but were never seen in the training corpus are called **out-of-vocabulary (OOV)** words.

#### How to handle OOV words?

- Introduce a special token `<UNK>` to represent all unknown words.
- Replace rare or unseen words in the corpus with `<UNK>`.
- This allows the model to assign some probability to unknown words instead of zero.

#### Vocabulary creation

- Define a minimum frequency threshold.
- Words below this frequency are replaced with `<UNK>`.
- This helps keep the vocabulary manageable and improves model robustness.


### 8. 🧂 Smoothing Techniques for Missing N-grams

#### Why smoothing?

Even with a large corpus, some valid N-grams may never appear in the training data, leading to zero probabilities. This is problematic because zero probability means the model thinks the sequence is impossible.

#### Common smoothing methods

- **Add-one smoothing (Laplacian smoothing):** Add 1 to all counts to avoid zeros.
- **Add-k smoothing:** Add a small constant k instead of 1.
- **Advanced methods:** Kneser-Ney smoothing, Good-Turing smoothing.

#### Backoff and interpolation

- **Backoff:** If an N-gram is missing, back off to a lower-order N-gram (e.g., from trigram to bigram).
- **Interpolation:** Combine probabilities from different N-gram orders weighted by some factors.

These methods help assign reasonable probabilities to unseen sequences.


### 9. 🧩 Summary and Practical Applications

- **N-grams** are sequences of words used to estimate probabilities in language models.
- **Language models** predict the next word in a sequence, enabling autocomplete, spelling correction, speech recognition, and assistive communication.
- **Start and end tokens** help models understand sentence boundaries.
- **Log probabilities** prevent numerical underflow in calculations.
- **Perplexity** is a key metric to evaluate model quality.
- **Out-of-vocabulary words** are handled with `<UNK>` tokens.
- **Smoothing** techniques fix zero probabilities for unseen N-grams.
- Language models are built by counting N-grams in a corpus, converting counts to probabilities, and applying smoothing and backoff as needed.


This detailed understanding of N-gram language models provides a solid foundation for building and evaluating models that can predict text sequences, enabling many practical NLP applications like autocomplete and speech recognition.



<br>

## Questions



#### 1. What does an N-gram language model primarily estimate?  
A) The grammatical correctness of a sentence  
B) The probability of a word given the previous N-1 words  
C) The semantic meaning of a sentence  
D) The probability of a sequence of words  

#### 2. Which of the following are true about the Markov assumption in N-gram models?  
A) It assumes the next word depends only on the previous N-1 words  
B) It allows modeling entire sentences without approximation  
C) It simplifies sequence probability calculation  
D) It assumes all words in a sentence are independent  

#### 3. Why are start `<s>` and end `</s>` tokens added to sentences in N-gram models?  
A) To mark sentence boundaries explicitly  
B) To increase the vocabulary size  
C) To help the model learn when sentences begin and end  
D) To improve smoothing performance  

#### 4. Given a corpus with the sentence “I am happy”, which of the following are valid bigrams?  
A) (I, am)  
B) (am, happy)  
C) (happy, I)  
D) (I, happy)  

#### 5. When calculating the probability of a sentence using a bigram model, which of the following is true?  
A) Multiply the probabilities of each word given the previous word  
B) Use the probability of each word independently  
C) Use the probability of each word given the entire previous sentence  
D) Add the probabilities of each bigram  

#### 6. What is the main reason for using log probabilities in language models?  
A) To make probabilities larger  
B) To avoid numerical underflow when multiplying many small probabilities  
C) To simplify the calculation of perplexity  
D) To convert probabilities into percentages  

#### 7. Which of the following statements about perplexity are correct?  
A) Lower perplexity indicates a better language model  
B) Perplexity measures how well a model predicts unseen data  
C) Perplexity is always greater than 1  
D) Perplexity can be directly interpreted as accuracy  

#### 8. How does the `<UNK>` token help in language modeling?  
A) It replaces all rare or unseen words in the corpus  
B) It increases the size of the vocabulary indefinitely  
C) It allows the model to assign non-zero probability to unknown words  
D) It is only used during testing, not training  

#### 9. Which of the following are challenges when estimating N-gram probabilities from a corpus?  
A) Some valid N-grams may never appear in the training data  
B) The corpus may contain out-of-vocabulary words  
C) The corpus always contains every possible N-gram  
D) Multiplying many probabilities can cause underflow  

#### 10. What is the effect of add-one (Laplacian) smoothing on N-gram probabilities?  
A) It assigns zero probability to unseen N-grams  
B) It adds one to all N-gram counts to avoid zero probabilities  
C) It can overly smooth probabilities, reducing model accuracy  
D) It only affects unigrams, not higher-order N-grams  

#### 11. In backoff smoothing, if a trigram is missing from the corpus, what happens?  
A) The model uses the corresponding bigram probability instead  
B) The model assigns zero probability to the trigram  
C) The model interpolates trigram and bigram probabilities equally  
D) The model ignores the missing trigram and moves on  

#### 12. When splitting a corpus into training, validation, and test sets, which of the following are best practices?  
A) Use 80% training, 10% validation, 10% test for small corpora  
B) Use 98% training, 1% validation, 1% test for large corpora  
C) Use the same data for training and testing to maximize data usage  
D) Ensure test data is unseen during training  

#### 13. Which of the following statements about N-gram count matrices are true?  
A) Rows correspond to (N-1)-grams and columns correspond to possible next words  
B) Each cell contains the probability of the N-gram  
C) Counts are converted to probabilities by dividing by the row sum  
D) The count matrix is always square  

#### 14. Why might a trigram model have lower perplexity than a bigram model?  
A) It considers more context, leading to better predictions  
B) It always has more data to train on  
C) It ignores start and end tokens  
D) It uses fewer parameters than a bigram model  

#### 15. Which of the following are true about out-of-vocabulary (OOV) words?  
A) They are words not seen in the training corpus  
B) They are replaced by `<UNK>` during preprocessing  
C) They always cause the model to assign zero probability to sentences containing them  
D) They can be handled by smoothing techniques  

#### 16. What is the primary purpose of interpolation in smoothing?  
A) To combine probabilities from different N-gram orders  
B) To discard lower-order N-gram probabilities  
C) To assign zero probability to unseen N-grams  
D) To increase the vocabulary size  

#### 17. Consider the sentence probability approximation using bigrams: P(Mary likes cats) = P(Mary|<s>) * P(likes|Mary) * P(cats|likes) * P(</s>|cats). Which of the following is true?  
A) The start token `<s>` is necessary to model sentence beginning  
B) The end token `</s>` is necessary to model sentence ending  
C) The probability of the first word is unconditional  
D) The model assumes independence between words  

#### 18. Which of the following statements about smoothing methods are correct?  
A) Add-k smoothing generalizes add-one smoothing by adding a constant k  
B) Kneser-Ney smoothing is an advanced method that improves probability estimates for rare N-grams  
C) Good-Turing smoothing redistributes probability mass from seen to unseen N-grams  
D) Smoothing always increases the probability of frequent N-grams  

#### 19. When evaluating a language model, why is it important to use the same vocabulary across models?  
A) To ensure perplexity comparisons are fair and meaningful  
B) Because vocabulary size does not affect model performance  
C) To avoid introducing `<UNK>` inconsistencies  
D) Because different vocabularies always produce the same probabilities  

#### 20. Which of the following best describe the limitations of N-gram language models?  
A) They cannot capture long-range dependencies beyond N-1 words  
B) They require very large corpora to estimate probabilities accurately  
C) They always produce grammatically correct sentences  
D) They assign zero probability to unseen N-grams without smoothing  



<br>

## Answers



#### 1. What does an N-gram language model primarily estimate?  
A) ✗ It does not directly assess grammatical correctness.  
B) ✓ It estimates the probability of a word given the previous N-1 words.  
C) ✗ Semantic meaning is beyond simple N-gram models.  
D) ✓ It estimates the probability of a sequence of words via chain rule approximation.  

**Correct:** B, D


#### 2. Which of the following are true about the Markov assumption in N-gram models?  
A) ✓ Correct, it limits dependency to previous N-1 words.  
B) ✗ It is an approximation, not exact modeling of entire sentences.  
C) ✓ Simplifies probability calculations by reducing context.  
D) ✗ Words are not independent; they depend on previous words.  

**Correct:** A, C


#### 3. Why are start `<s>` and end `</s>` tokens added to sentences in N-gram models?  
A) ✓ They explicitly mark sentence boundaries.  
B) ✗ They do not increase vocabulary size significantly.  
C) ✓ Help model learn sentence start and end positions.  
D) ✗ Smoothing is unrelated to start/end tokens.  

**Correct:** A, C


#### 4. Given a corpus with the sentence “I am happy”, which of the following are valid bigrams?  
A) ✓ (I, am) appears consecutively.  
B) ✓ (am, happy) appears consecutively.  
C) ✗ (happy, I) does not appear in order.  
D) ✗ (I, happy) skips a word, not a bigram.  

**Correct:** A, B


#### 5. When calculating the probability of a sentence using a bigram model, which of the following is true?  
A) ✓ Multiply conditional probabilities of each word given previous word.  
B) ✗ Words are not independent; context matters.  
C) ✗ Bigram model only considers previous one word, not entire sentence.  
D) ✗ Probabilities multiply, not add.  

**Correct:** A


#### 6. What is the main reason for using log probabilities in language models?  
A) ✗ Logarithms do not make probabilities larger.  
B) ✓ Avoid numerical underflow from multiplying many small probabilities.  
C) ✗ Log probabilities simplify multiplication, not perplexity calculation directly.  
D) ✗ Log probabilities are not percentages.  

**Correct:** B


#### 7. Which of the following statements about perplexity are correct?  
A) ✓ Lower perplexity means better predictive performance.  
B) ✓ Measures how well model predicts unseen data.  
C) ✗ Perplexity can be less than 1 in rare cases (e.g., degenerate distributions).  
D) ✗ Perplexity is not accuracy; it measures uncertainty.  

**Correct:** A, B


#### 8. How does the `<UNK>` token help in language modeling?  
A) ✓ Replaces rare or unseen words to handle OOV.  
B) ✗ It limits vocabulary size by grouping unknowns.  
C) ✓ Allows assigning non-zero probability to unknown words.  
D) ✗ Used during both training and testing preprocessing.  

**Correct:** A, C


#### 9. Which of the following are challenges when estimating N-gram probabilities from a corpus?  
A) ✓ Some valid N-grams may be missing in training data.  
B) ✓ OOV words appear in input but not in training corpus.  
C) ✗ Corpus rarely contains every possible N-gram.  
D) ✓ Multiplying many probabilities can cause underflow.  

**Correct:** A, B, D


#### 10. What is the effect of add-one (Laplacian) smoothing on N-gram probabilities?  
A) ✗ It prevents zero probabilities for unseen N-grams.  
B) ✓ Adds one to all counts to avoid zeros.  
C) ✓ Can overly smooth and reduce accuracy by inflating rare counts.  
D) ✗ Affects all N-grams, not just unigrams.  

**Correct:** B, C


#### 11. In backoff smoothing, if a trigram is missing from the corpus, what happens?  
A) ✓ The model backs off to bigram probability.  
B) ✗ It does not assign zero probability directly.  
C) ✗ Katz backoff discounts but does not interpolate equally.  
D) ✗ The model does not ignore missing N-grams.  

**Correct:** A


#### 12. When splitting a corpus into training, validation, and test sets, which of the following are best practices?  
A) ✓ 80/10/10 split for small corpora is standard.  
B) ✓ 98/1/1 split for large corpora is common.  
C) ✗ Using same data for training and testing causes overfitting.  
D) ✓ Test data must be unseen during training for valid evaluation.  

**Correct:** A, B, D


#### 13. Which of the following statements about N-gram count matrices are true?  
A) ✓ Rows correspond to (N-1)-grams, columns to next words.  
B) ✗ Count matrix contains raw counts, not probabilities.  
C) ✓ Probabilities are computed by dividing counts by row sums.  
D) ✗ Count matrix is not necessarily square; depends on vocabulary.  

**Correct:** A, C


#### 14. Why might a trigram model have lower perplexity than a bigram model?  
A) ✓ More context leads to better predictions.  
B) ✗ Trigram models often have less data per N-gram, not more.  
C) ✗ Start/end tokens are used in both models.  
D) ✗ Trigram models have more parameters, not fewer.  

**Correct:** A


#### 15. Which of the following are true about out-of-vocabulary (OOV) words?  
A) ✓ Words not seen in training corpus.  
B) ✓ Replaced by `<UNK>` during preprocessing.  
C) ✗ With `<UNK>`, model does not assign zero probability.  
D) ✗ Smoothing does not directly handle OOV; `<UNK>` does.  

**Correct:** A, B


#### 16. What is the primary purpose of interpolation in smoothing?  
A) ✓ Combine probabilities from different N-gram orders.  
B) ✗ It does not discard lower-order probabilities.  
C) ✗ It prevents zero probabilities rather than assigning them.  
D) ✗ It does not increase vocabulary size.  

**Correct:** A


#### 17. Consider the sentence probability approximation using bigrams: P(Mary likes cats) = P(Mary|<s>) * P(likes|Mary) * P(cats|likes) * P(</s>|cats). Which of the following is true?  
A) ✓ `<s>` models sentence start explicitly.  
B) ✓ `</s>` models sentence end explicitly.  
C) ✗ The first word’s probability is conditional on `<s>`, not unconditional.  
D) ✗ Model assumes dependence on previous word, not independence.  

**Correct:** A, B


#### 18. Which of the following statements about smoothing methods are correct?  
A) ✓ Add-k smoothing generalizes add-one by adding any constant k.  
B) ✓ Kneser-Ney is an advanced smoothing method improving rare N-gram estimates.  
C) ✓ Good-Turing redistributes probability mass to unseen N-grams.  
D) ✗ Smoothing reduces probabilities of frequent N-grams to allocate mass to unseen ones.  

**Correct:** A, B, C


#### 19. When evaluating a language model, why is it important to use the same vocabulary across models?  
A) ✓ Ensures fair perplexity comparison.  
B) ✗ Vocabulary size affects model performance.  
C) ✓ Avoids inconsistencies in `<UNK>` handling.  
D) ✗ Different vocabularies produce different probabilities, so not comparable.  

**Correct:** A, C


#### 20. Which of the following best describe the limitations of N-gram language models?  
A) ✓ Cannot capture dependencies beyond N-1 words.  
B) ✓ Require large corpora for accurate probability estimation.  
C) ✗ Do not guarantee grammatical correctness.  
D) ✓ Assign zero probability to unseen N-grams without smoothing.  

**Correct:** A, B, D

