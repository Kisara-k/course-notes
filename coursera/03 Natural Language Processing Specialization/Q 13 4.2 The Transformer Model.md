## 4.2 The Transformer Model

## Questions

#### 1. What are the main limitations of RNNs that Transformers aim to solve?  
A) Difficulty in parallelizing computations  
B) Vanishing gradient problem  
C) Inability to handle variable-length sequences  
D) Loss of long-range information  

#### 2. Which of the following statements about the Transformer encoder are true?  
A) It processes input tokens sequentially, one at a time  
B) Each input token attends to every other token in the sequence  
C) It uses self-attention to create contextual embeddings  
D) It relies on recurrent units like LSTMs or GRUs  

#### 3. In the Transformer model, what is the purpose of positional encoding?  
A) To add information about the order of tokens in the sequence  
B) To replace the need for word embeddings  
C) To enable the model to distinguish between different positions in the input  
D) To normalize the input embeddings  

#### 4. Which of the following are components of scaled dot-product attention?  
A) Queries, Keys, and Values  
B) Softmax function applied to dot products  
C) Recurrent connections to previous time steps  
D) Scaling factor to prevent large dot product values  

#### 5. How does masked self-attention in the decoder differ from regular self-attention in the encoder?  
A) It prevents attending to future tokens in the sequence  
B) It allows attending to all tokens in the input sentence  
C) It uses a mask to set weights of future positions to zero  
D) It attends only to the first token in the sequence  

#### 6. What is the main advantage of multi-head attention over single-head attention?  
A) It reduces the computational cost significantly  
B) It allows the model to attend to information from different representation subspaces  
C) It concatenates outputs from multiple attention heads before a linear transformation  
D) It eliminates the need for positional encoding  

#### 7. Which of the following are true about the Transformer decoder?  
A) It uses masked self-attention to prevent future token leakage  
B) It applies encoder-decoder attention to incorporate input sequence information  
C) It uses recurrent layers to process sequences step-by-step  
D) It includes feed-forward layers with ReLU activation  

#### 8. Why is parallelization easier in Transformers compared to RNNs?  
A) Because Transformers process all tokens simultaneously rather than sequentially  
B) Because Transformers use convolutional layers instead of recurrent layers  
C) Because attention mechanisms allow simultaneous computation of relationships  
D) Because Transformers do not require any positional information  

#### 9. Which of the following are challenges that Transformers help overcome compared to RNNs?  
A) Handling very long sequences without loss of information  
B) Avoiding vanishing gradients during training  
C) Processing sequences with fixed length only  
D) Enabling efficient use of GPUs and TPUs  

#### 10. In the context of attention, what do the terms "queries," "keys," and "values" represent?  
A) Queries are the vectors we want to find relevant information for  
B) Keys are vectors against which queries are compared to compute attention scores  
C) Values are the vectors that are weighted and summed to produce the output  
D) Queries, keys, and values are always identical vectors  

#### 11. Which of the following best describes the role of the feed-forward network in the Transformer?  
A) It applies a non-linear transformation to each position independently  
B) It aggregates information across different positions in the sequence  
C) It is the main component responsible for attention calculations  
D) It uses recurrent connections to maintain sequence order  

#### 12. How does the Transformer handle the problem of "loss of information" in long sequences?  
A) By using self-attention to directly connect all tokens regardless of distance  
B) By stacking multiple recurrent layers to increase memory  
C) By using positional encoding to preserve order information  
D) By limiting the input sequence length to a fixed size  

#### 13. Which of the following are true about the training process of Transformer-based summarizers?  
A) They optimize a weighted cross-entropy loss focusing on the summary portion  
B) They generate summaries by predicting the entire summary at once  
C) They use tokenized input sequences with special end-of-sequence tokens  
D) They rely on masked self-attention to prevent future token leakage during summary generation  

#### 14. What is a key difference between BERT and GPT models?  
A) BERT uses bidirectional attention, while GPT uses unidirectional attention  
B) GPT is designed primarily for text generation, BERT for understanding tasks  
C) BERT uses masked self-attention, GPT does not use attention mechanisms  
D) GPT uses encoder-decoder architecture, BERT uses only an encoder  

#### 15. Which of the following statements about multi-head attention are correct?  
A) Each attention head learns to focus on different parts of the input  
B) Multi-head attention concatenates the outputs of all heads before a linear layer  
C) Multi-head attention requires significantly more computation than single-head attention  
D) Multi-head attention eliminates the need for feed-forward layers  

#### 16. Why is the softmax function used in the attention mechanism?  
A) To convert raw attention scores into probabilities that sum to 1  
B) To normalize the input embeddings before attention  
C) To scale the dot products to prevent large values  
D) To mask out future tokens in the decoder  

#### 17. Which of the following are true about the encoder-decoder attention in the Transformer?  
A) Queries come from the decoder, keys and values come from the encoder output  
B) It allows the decoder to attend to the entire input sequence context  
C) It is only used in the encoder, not the decoder  
D) It replaces the need for positional encoding in the decoder  

#### 18. What is the function of residual connections and layer normalization in the Transformer?  
A) To stabilize training and help gradients flow through deep networks  
B) To add positional information to embeddings  
C) To reduce the dimensionality of embeddings  
D) To prevent the model from attending to irrelevant tokens  

#### 19. Which of the following NLP tasks can Transformers be applied to?  
A) Named Entity Recognition (NER)  
B) Text summarization  
C) Spell checking  
D) Image classification  

#### 20. During inference, how does a Transformer-based language model generate text?  
A) By predicting the next word one at a time using previously generated words  
B) By generating the entire output sequence simultaneously  
C) By sampling from the probability distribution over the vocabulary at each step  
D) By using recurrent connections to remember previous outputs



<br>

## Answers

#### 1. What are the main limitations of RNNs that Transformers aim to solve?  
A) ✓ Difficulty in parallelizing computations — RNNs process sequentially, limiting parallelism.  
B) ✓ Vanishing gradient problem — RNNs suffer from gradients shrinking over long sequences.  
C) ✗ Inability to handle variable-length sequences — RNNs can handle variable lengths naturally.  
D) ✓ Loss of long-range information — RNNs struggle to retain distant context.  

**Correct:** A, B, D


#### 2. Which of the following statements about the Transformer encoder are true?  
A) ✗ It processes input tokens sequentially, one at a time — Transformers process all tokens simultaneously.  
B) ✓ Each input token attends to every other token in the sequence — Self-attention connects all tokens.  
C) ✓ It uses self-attention to create contextual embeddings — Self-attention provides context-aware representations.  
D) ✗ It relies on recurrent units like LSTMs or GRUs — Transformers do not use recurrent units.  

**Correct:** B, C


#### 3. In the Transformer model, what is the purpose of positional encoding?  
A) ✓ To add information about the order of tokens in the sequence — Positional encoding encodes token positions.  
B) ✗ To replace the need for word embeddings — Positional encoding is added to embeddings, not a replacement.  
C) ✓ To enable the model to distinguish between different positions in the input — Helps model understand sequence order.  
D) ✗ To normalize the input embeddings — Normalization is a separate process.  

**Correct:** A, C


#### 4. Which of the following are components of scaled dot-product attention?  
A) ✓ Queries, Keys, and Values — Core inputs to attention.  
B) ✓ Softmax function applied to dot products — Converts scores to probabilities.  
C) ✗ Recurrent connections to previous time steps — Attention is non-recurrent.  
D) ✓ Scaling factor to prevent large dot product values — Stabilizes gradients.  

**Correct:** A, B, D


#### 5. How does masked self-attention in the decoder differ from regular self-attention in the encoder?  
A) ✓ It prevents attending to future tokens in the sequence — Ensures autoregressive generation.  
B) ✗ It allows attending to all tokens in the input sentence — Masked attention restricts future tokens.  
C) ✓ It uses a mask to set weights of future positions to zero — Masking disables future attention.  
D) ✗ It attends only to the first token in the sequence — It attends to all previous tokens, not just the first.  

**Correct:** A, C


#### 6. What is the main advantage of multi-head attention over single-head attention?  
A) ✗ It reduces the computational cost significantly — Multi-head attention has similar cost to single-head.  
B) ✓ It allows the model to attend to information from different representation subspaces — Multiple heads capture diverse features.  
C) ✓ It concatenates outputs from multiple attention heads before a linear transformation — This is how multi-head attention combines heads.  
D) ✗ It eliminates the need for positional encoding — Positional encoding is still required.  

**Correct:** B, C


#### 7. Which of the following are true about the Transformer decoder?  
A) ✓ It uses masked self-attention to prevent future token leakage — Ensures predictions depend only on past tokens.  
B) ✓ It applies encoder-decoder attention to incorporate input sequence information — Connects decoder to encoder outputs.  
C) ✗ It uses recurrent layers to process sequences step-by-step — Transformers avoid recurrence.  
D) ✓ It includes feed-forward layers with ReLU activation — Feed-forward layers add non-linearity.  

**Correct:** A, B, D


#### 8. Why is parallelization easier in Transformers compared to RNNs?  
A) ✓ Because Transformers process all tokens simultaneously rather than sequentially — Enables parallel computation.  
B) ✗ Because Transformers use convolutional layers instead of recurrent layers — Transformers do not use convolutions.  
C) ✓ Because attention mechanisms allow simultaneous computation of relationships — Attention computes all token interactions at once.  
D) ✗ Because Transformers do not require any positional information — Positional encoding is necessary.  

**Correct:** A, C


#### 9. Which of the following are challenges that Transformers help overcome compared to RNNs?  
A) ✓ Handling very long sequences without loss of information — Attention connects distant tokens directly.  
B) ✓ Avoiding vanishing gradients during training — Attention gradients do not vanish like in RNNs.  
C) ✗ Processing sequences with fixed length only — Transformers handle variable-length sequences.  
D) ✓ Enabling efficient use of GPUs and TPUs — Parallelism suits modern hardware.  

**Correct:** A, B, D


#### 10. In the context of attention, what do the terms "queries," "keys," and "values" represent?  
A) ✓ Queries are the vectors we want to find relevant information for — They represent the current token’s request.  
B) ✓ Keys are vectors against which queries are compared to compute attention scores — Keys represent candidate matches.  
C) ✓ Values are the vectors that are weighted and summed to produce the output — Values provide the actual information.  
D) ✗ Queries, keys, and values are always identical vectors — They are usually different linear projections.  

**Correct:** A, B, C


#### 11. Which of the following best describes the role of the feed-forward network in the Transformer?  
A) ✓ It applies a non-linear transformation to each position independently — Feed-forward layers process tokens separately.  
B) ✗ It aggregates information across different positions in the sequence — Attention handles cross-token aggregation.  
C) ✗ It is the main component responsible for attention calculations — Attention is separate from feed-forward layers.  
D) ✗ It uses recurrent connections to maintain sequence order — No recurrence in feed-forward layers.  

**Correct:** A


#### 12. How does the Transformer handle the problem of "loss of information" in long sequences?  
A) ✓ By using self-attention to directly connect all tokens regardless of distance — Attention links distant tokens directly.  
B) ✗ By stacking multiple recurrent layers to increase memory — Transformers do not use recurrence.  
C) ✓ By using positional encoding to preserve order information — Positional encoding helps maintain sequence structure.  
D) ✗ By limiting the input sequence length to a fixed size — Transformers can handle variable lengths.  

**Correct:** A, C


#### 13. Which of the following are true about the training process of Transformer-based summarizers?  
A) ✓ They optimize a weighted cross-entropy loss focusing on the summary portion — Loss weights focus on summary tokens.  
B) ✗ They generate summaries by predicting the entire summary at once — Summaries are generated word-by-word.  
C) ✓ They use tokenized input sequences with special end-of-sequence tokens — EOS tokens mark sequence boundaries.  
D) ✓ They rely on masked self-attention to prevent future token leakage during summary generation — Ensures proper autoregressive decoding.  

**Correct:** A, C, D


#### 14. What is a key difference between BERT and GPT models?  
A) ✓ BERT uses bidirectional attention, while GPT uses unidirectional attention — BERT attends both left and right, GPT only left.  
B) ✓ GPT is designed primarily for text generation, BERT for understanding tasks — GPT excels at generation, BERT at representation.  
C) ✗ BERT uses masked self-attention, GPT does not use attention mechanisms — Both use attention; BERT masks tokens during training.  
D) ✗ GPT uses encoder-decoder architecture, BERT uses only an encoder — GPT is decoder-only, BERT is encoder-only.  

**Correct:** A, B


#### 15. Which of the following statements about multi-head attention are correct?  
A) ✓ Each attention head learns to focus on different parts of the input — Heads capture diverse features.  
B) ✓ Multi-head attention concatenates the outputs of all heads before a linear layer — This is standard practice.  
C) ✗ Multi-head attention requires significantly more computation than single-head attention — Computational cost is similar.  
D) ✗ Multi-head attention eliminates the need for feed-forward layers — Feed-forward layers remain essential.  

**Correct:** A, B


#### 16. Why is the softmax function used in the attention mechanism?  
A) ✓ To convert raw attention scores into probabilities that sum to 1 — Softmax normalizes scores.  
B) ✗ To normalize the input embeddings before attention — Normalization is separate.  
C) ✗ To scale the dot products to prevent large values — Scaling is done by division, not softmax.  
D) ✗ To mask out future tokens in the decoder — Masking is done by adding large negative values before softmax.  

**Correct:** A


#### 17. Which of the following are true about the encoder-decoder attention in the Transformer?  
A) ✓ Queries come from the decoder, keys and values come from the encoder output — This connects decoder to encoder.  
B) ✓ It allows the decoder to attend to the entire input sequence context — Enables context-aware generation.  
C) ✗ It is only used in the encoder, not the decoder — It is used in the decoder.  
D) ✗ It replaces the need for positional encoding in the decoder — Positional encoding is still required.  

**Correct:** A, B


#### 18. What is the function of residual connections and layer normalization in the Transformer?  
A) ✓ To stabilize training and help gradients flow through deep networks — They improve training stability.  
B) ✗ To add positional information to embeddings — Positional encoding does this.  
C) ✗ To reduce the dimensionality of embeddings — Dimensionality is fixed or changed by linear layers.  
D) ✗ To prevent the model from attending to irrelevant tokens — Attention weights handle relevance.  

**Correct:** A


#### 19. Which of the following NLP tasks can Transformers be applied to?  
A) ✓ Named Entity Recognition (NER) — Transformers excel at sequence labeling.  
B) ✓ Text summarization — Transformers generate summaries effectively.  
C) ✓ Spell checking — Transformers can model character-level corrections.  
D) ✗ Image classification — Transformers are primarily for NLP, though vision transformers exist but not covered here.  

**Correct:** A, B, C


#### 20. During inference, how does a Transformer-based language model generate text?  
A) ✓ By predicting the next word one at a time using previously generated words — Autoregressive generation.  
B) ✗ By generating the entire output sequence simultaneously — Generation is sequential.  
C) ✓ By sampling from the probability distribution over the vocabulary at each step — Sampling introduces variability.  
D) ✗ By using recurrent connections to remember previous outputs — Transformers do not use recurrence.  

**Correct:** A, C