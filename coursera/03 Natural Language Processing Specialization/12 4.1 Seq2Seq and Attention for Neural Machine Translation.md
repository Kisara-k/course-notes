## 4.1 Seq2Seq and Attention for Neural Machine Translation

## Study Notes

### 1. üß† Introduction to Neural Machine Translation (NMT)

Neural Machine Translation (NMT) is a modern approach to automatically translating text from one language to another using neural networks. Unlike traditional rule-based or phrase-based translation systems, NMT models learn to translate entire sentences as sequences, capturing complex language patterns and context.

The core idea is to take a sentence in the source language (e.g., English) and produce a corresponding sentence in the target language (e.g., French). This is challenging because sentences vary in length, word order, and grammar between languages.


### 2. üîÑ Seq2Seq Model: The Foundation of NMT

The Sequence-to-Sequence (Seq2Seq) model, introduced by Google in 2014, is the backbone of many NMT systems. It consists of two main parts:

- **Encoder:** Reads and processes the input sentence (source language).
- **Decoder:** Generates the output sentence (target language).

#### How Seq2Seq Works

- The encoder takes a variable-length input sequence (words in the source sentence) and compresses it into a fixed-length vector called the **final hidden state**. This vector is supposed to capture the overall meaning of the input sentence.
- The decoder then uses this fixed-length vector as its initial hidden state to generate the output sequence word-by-word.

#### Key Components

- **Tokenization:** The input sentence is split into tokens (words or subwords).
- **Word Embeddings:** Each token is converted into a dense vector representation that captures semantic meaning.
- **Recurrent Neural Networks (RNNs):** Typically LSTMs (Long Short-Term Memory) or GRUs (Gated Recurrent Units) are used to process sequences, as they handle long-range dependencies and avoid issues like vanishing or exploding gradients.

#### Example

For the English sentence "tea time It's for," the encoder processes each word embedding through LSTM layers and produces a final hidden state. The decoder then starts with this hidden state and generates the French translation "C'est l'heure du th√©" step-by-step.


### 3. ‚ö†Ô∏è The Information Bottleneck Problem in Seq2Seq

A major limitation of the basic Seq2Seq model is the **information bottleneck**:

- The encoder compresses the entire input sentence into a single fixed-length vector.
- This fixed-size vector must contain all the information needed to generate the output sentence.
- As input sentences get longer or more complex, this fixed-length vector struggles to capture all necessary details.
- Consequently, the model‚Äôs performance decreases with longer sequences.

This bottleneck limits the model‚Äôs ability to translate accurately, especially for long or complex sentences.


### 4. üéØ Attention Mechanism: Solving the Bottleneck

The **Attention mechanism** was introduced to overcome the information bottleneck by allowing the decoder to look back at all the encoder‚Äôs hidden states, not just the final one.

#### How Attention Works

- Instead of relying on a single fixed vector, the decoder dynamically focuses on different parts of the input sentence at each step of output generation.
- At each decoding step, the model computes **attention weights** that indicate how much importance to give to each encoder hidden state.
- These weights are used to create a **context vector**, which is a weighted sum of all encoder hidden states.
- The context vector provides relevant information tailored to the current decoding step, improving translation quality.

#### Benefits

- The model can "attend" to the most relevant words in the input sentence when generating each output word.
- This is especially useful for languages with different word orders or grammar structures.
- Attention improves translation accuracy and allows handling longer sentences better.


### 5. üîç Attention Layer in Detail: Queries, Keys, and Values

The attention mechanism can be understood through the concepts of **Queries (Q)**, **Keys (K)**, and **Values (V)**:

- **Keys and Values:** These come from the encoder hidden states. Each hidden state acts as a key-value pair.
- **Query:** This is the decoder‚Äôs current hidden state, representing what the decoder is "looking for" in the input.

#### Computing Attention

1. **Similarity Score:** The model calculates a similarity score between the query and each key. This score measures how relevant each encoder hidden state is to the current decoding step.
2. **Softmax:** The similarity scores are normalized using a softmax function to produce attention weights (Œ±), which sum to 1.
3. **Context Vector:** The context vector is computed as the weighted sum of the values (encoder hidden states), using the attention weights.

#### Scaled Dot-Product Attention

- A popular method (from Vaswani et al., 2017) computes the dot product between queries and keys, then scales it by the square root of the key dimension to stabilize gradients.
- This operation is efficient and can be implemented with matrix multiplications and softmax.


### 6. üèóÔ∏è Putting It All Together: NMT Model with Attention

The full NMT model with attention includes:

- **Encoder:** Processes the input sentence and outputs a sequence of hidden states.
- **Pre-Attention Decoder:** Generates the next word based on the previous word and hidden state.
- **Attention Layer:** Uses the decoder‚Äôs hidden state as a query to compute attention weights over encoder hidden states (keys and values).
- **Context Vector:** Combined with the decoder‚Äôs hidden state to predict the next word.

During training, **teacher forcing** is used:

- The decoder receives the correct previous word as input (instead of its own prediction) to improve learning stability.
- This helps the model learn to generate accurate sequences faster.


### 7. üìä Evaluating Translations: BLEU and ROUGE Scores

To measure how well the model translates, we use evaluation metrics:

#### BLEU Score (BiLingual Evaluation Understudy)

- Compares the model‚Äôs candidate translation to one or more human reference translations.
- Measures how many words or phrases from the candidate appear in the references.
- Scores range from 0 to 1, with higher scores indicating better translations.
- Limitations: BLEU does not consider sentence meaning or grammar, so a high BLEU score doesn‚Äôt always mean a good translation.

#### ROUGE Score (Recall-Oriented Understudy for Gisting Evaluation)

- Focuses on recall: how many words from the reference appear in the candidate.
- ROUGE-N measures n-gram overlap.
- Often used in summarization but applicable to translation evaluation.
- Can be combined with BLEU and F1 scores for a more balanced evaluation.


### 8. üé≤ Sampling and Decoding Strategies

When generating translations, the model outputs a probability distribution over possible next words. Different decoding strategies affect the quality and diversity of translations:

#### Greedy Decoding

- Selects the most probable word at each step.
- Simple and fast but can lead to suboptimal overall sentences because it doesn‚Äôt consider future words.

#### Random Sampling

- Samples the next word randomly according to the predicted probabilities.
- Introduces diversity but can produce incoherent or inaccurate translations.

#### Temperature Sampling

- Adjusts randomness by scaling the probability distribution.
- Low temperature ‚Üí more confident, conservative choices.
- High temperature ‚Üí more random, diverse outputs.

#### Beam Search

- Keeps track of the top B (beam width) most probable sequences at each step.
- Explores multiple possible translations simultaneously.
- More computationally expensive but often produces better translations.
- Beam width of 1 is equivalent to greedy decoding.
- Needs length normalization to avoid bias toward shorter sequences.


### 9. üéØ Minimum Bayes Risk (MBR) Decoding

MBR is an advanced decoding strategy that:

- Generates multiple candidate translations.
- Computes similarity scores (e.g., ROUGE) between every pair of candidates.
- Selects the candidate with the highest average similarity to others.
- This approach aims to pick the most "consensus" translation, improving quality over random or greedy methods.


### Summary

Neural Machine Translation has evolved from basic Seq2Seq models to sophisticated architectures using attention mechanisms. Attention allows the model to dynamically focus on relevant parts of the input sentence, overcoming the fixed-length bottleneck and improving translation quality. Evaluation metrics like BLEU and ROUGE help measure performance, while decoding strategies such as beam search and MBR optimize the generation of accurate and fluent translations.

Understanding these components and how they fit together is essential for building and improving NMT systems.



<br>

## Key Points

#### 1. üîÑ Seq2Seq Model
- Seq2Seq maps variable-length input sequences to fixed-length memory vectors.
- Uses LSTMs or GRUs to avoid vanishing and exploding gradient problems.
- Inputs and outputs can have different lengths.
- The encoder produces a final hidden state that encodes the overall meaning of the input sentence.
- The decoder uses this fixed-length vector as the initial hidden state to generate the output sequence.

#### 2. ‚ö†Ô∏è Information Bottleneck in Seq2Seq
- The fixed-length vector from the encoder limits the amount of information passed to the decoder.
- As input sequence length increases, model performance decreases due to this bottleneck.
- Using only the final hidden state is suboptimal for long or complex sentences.

#### 3. üéØ Attention Mechanism
- Attention allows the decoder to focus on different encoder hidden states at each decoding step.
- The decoder computes attention weights over all encoder hidden states to create a context vector.
- The context vector is a weighted sum of encoder hidden states, dynamically changing per output word.
- Attention improves translation quality, especially for long sentences and languages with different grammar.

#### 4. üîç Queries, Keys, and Values in Attention
- Encoder hidden states serve as Keys and Values.
- Decoder hidden state at each step acts as the Query.
- Attention weights are computed by measuring similarity between Query and each Key.
- Softmax normalizes these similarity scores into attention weights.
- Scaled dot-product attention scales the dot product by the square root of the key dimension.

#### 5. üèóÔ∏è NMT Model with Attention Architecture
- Encoder outputs a sequence of hidden states.
- Pre-attention decoder generates hidden states used as Queries.
- Attention layer computes context vectors from encoder hidden states (Keys and Values).
- Context vectors combined with decoder hidden states predict the next word.
- Teacher forcing is used during training: decoder receives the correct previous word as input.

#### 6. üìä BLEU Score
- BLEU compares candidate translations to human references by counting overlapping words or phrases.
- Scores range from 0 to 1; higher is better.
- BLEU does not consider semantic meaning or sentence structure.

#### 7. üìà ROUGE Score
- ROUGE measures recall: how many words from the reference appear in the candidate.
- ROUGE-N measures n-gram overlap.
- Often used alongside BLEU for translation evaluation.

#### 8. üé≤ Decoding Strategies
- **Greedy decoding:** selects the most probable word at each step; fast but can be suboptimal.
- **Random sampling:** samples words based on probability distribution; can be too random.
- **Temperature sampling:** controls randomness; low temperature = conservative, high temperature = more random.
- **Beam search:** keeps top B sequences at each step; better translations but computationally expensive.
- Beam width 1 in beam search equals greedy decoding.
- Beam search requires length normalization to avoid bias toward short sequences.

#### 9. üéØ Minimum Bayes Risk (MBR) Decoding
- Generates multiple candidate translations.
- Computes similarity (e.g., ROUGE) between all candidate pairs.
- Selects the candidate with the highest average similarity to others.
- MBR improves translation quality over random sampling and greedy decoding.