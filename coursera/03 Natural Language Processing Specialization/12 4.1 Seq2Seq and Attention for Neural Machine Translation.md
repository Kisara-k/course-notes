## 4.1 Seq2Seq and Attention for Neural Machine Translation



### Key Points



#### 1. üîÑ Seq2Seq Model
- Seq2Seq maps variable-length input sequences to fixed-length memory vectors.
- Uses LSTMs or GRUs to avoid vanishing and exploding gradient problems.
- Inputs and outputs can have different lengths.
- The encoder produces a final hidden state that encodes the overall meaning of the input sentence.
- The decoder uses this fixed-length vector as the initial hidden state to generate the output sequence.

#### 2. ‚ö†Ô∏è Information Bottleneck in Seq2Seq
- The fixed-length vector from the encoder limits the amount of information passed to the decoder.
- As input sequence length increases, model performance decreases due to this bottleneck.
- Using only the final hidden state is suboptimal for long or complex sentences.

#### 3. üéØ Attention Mechanism
- Attention allows the decoder to focus on different encoder hidden states at each decoding step.
- The decoder computes attention weights over all encoder hidden states to create a context vector.
- The context vector is a weighted sum of encoder hidden states, dynamically changing per output word.
- Attention improves translation quality, especially for long sentences and languages with different grammar.

#### 4. üîç Queries, Keys, and Values in Attention
- Encoder hidden states serve as Keys and Values.
- Decoder hidden state at each step acts as the Query.
- Attention weights are computed by measuring similarity between Query and each Key.
- Softmax normalizes these similarity scores into attention weights.
- Scaled dot-product attention scales the dot product by the square root of the key dimension.

#### 5. üèóÔ∏è NMT Model with Attention Architecture
- Encoder outputs a sequence of hidden states.
- Pre-attention decoder generates hidden states used as Queries.
- Attention layer computes context vectors from encoder hidden states (Keys and Values).
- Context vectors combined with decoder hidden states predict the next word.
- Teacher forcing is used during training: decoder receives the correct previous word as input.

#### 6. üìä BLEU Score
- BLEU compares candidate translations to human references by counting overlapping words or phrases.
- Scores range from 0 to 1; higher is better.
- BLEU does not consider semantic meaning or sentence structure.

#### 7. üìà ROUGE Score
- ROUGE measures recall: how many words from the reference appear in the candidate.
- ROUGE-N measures n-gram overlap.
- Often used alongside BLEU for translation evaluation.

#### 8. üé≤ Decoding Strategies
- **Greedy decoding:** selects the most probable word at each step; fast but can be suboptimal.
- **Random sampling:** samples words based on probability distribution; can be too random.
- **Temperature sampling:** controls randomness; low temperature = conservative, high temperature = more random.
- **Beam search:** keeps top B sequences at each step; better translations but computationally expensive.
- Beam width 1 in beam search equals greedy decoding.
- Beam search requires length normalization to avoid bias toward short sequences.

#### 9. üéØ Minimum Bayes Risk (MBR) Decoding
- Generates multiple candidate translations.
- Computes similarity (e.g., ROUGE) between all candidate pairs.
- Selects the candidate with the highest average similarity to others.
- MBR improves translation quality over random sampling and greedy decoding.



<br>

## Study Notes





### 1. üß† Introduction to Neural Machine Translation (NMT)

Neural Machine Translation (NMT) is a modern approach to automatically translating text from one language to another using neural networks. Unlike traditional rule-based or phrase-based translation systems, NMT models learn to translate entire sentences as sequences, capturing complex language patterns and context.

The core idea is to take a sentence in the source language (e.g., English) and produce a corresponding sentence in the target language (e.g., French). This is challenging because sentences vary in length, word order, and grammar between languages.


### 2. üîÑ Seq2Seq Model: The Foundation of NMT

The Sequence-to-Sequence (Seq2Seq) model, introduced by Google in 2014, is the backbone of many NMT systems. It consists of two main parts:

- **Encoder:** Reads and processes the input sentence (source language).
- **Decoder:** Generates the output sentence (target language).

#### How Seq2Seq Works

- The encoder takes a variable-length input sequence (words in the source sentence) and compresses it into a fixed-length vector called the **final hidden state**. This vector is supposed to capture the overall meaning of the input sentence.
- The decoder then uses this fixed-length vector as its initial hidden state to generate the output sequence word-by-word.

#### Key Components

- **Tokenization:** The input sentence is split into tokens (words or subwords).
- **Word Embeddings:** Each token is converted into a dense vector representation that captures semantic meaning.
- **Recurrent Neural Networks (RNNs):** Typically LSTMs (Long Short-Term Memory) or GRUs (Gated Recurrent Units) are used to process sequences, as they handle long-range dependencies and avoid issues like vanishing or exploding gradients.

#### Example

For the English sentence "tea time It's for," the encoder processes each word embedding through LSTM layers and produces a final hidden state. The decoder then starts with this hidden state and generates the French translation "C'est l'heure du th√©" step-by-step.


### 3. ‚ö†Ô∏è The Information Bottleneck Problem in Seq2Seq

A major limitation of the basic Seq2Seq model is the **information bottleneck**:

- The encoder compresses the entire input sentence into a single fixed-length vector.
- This fixed-size vector must contain all the information needed to generate the output sentence.
- As input sentences get longer or more complex, this fixed-length vector struggles to capture all necessary details.
- Consequently, the model‚Äôs performance decreases with longer sequences.

This bottleneck limits the model‚Äôs ability to translate accurately, especially for long or complex sentences.


### 4. üéØ Attention Mechanism: Solving the Bottleneck

The **Attention mechanism** was introduced to overcome the information bottleneck by allowing the decoder to look back at all the encoder‚Äôs hidden states, not just the final one.

#### How Attention Works

- Instead of relying on a single fixed vector, the decoder dynamically focuses on different parts of the input sentence at each step of output generation.
- At each decoding step, the model computes **attention weights** that indicate how much importance to give to each encoder hidden state.
- These weights are used to create a **context vector**, which is a weighted sum of all encoder hidden states.
- The context vector provides relevant information tailored to the current decoding step, improving translation quality.

#### Benefits

- The model can "attend" to the most relevant words in the input sentence when generating each output word.
- This is especially useful for languages with different word orders or grammar structures.
- Attention improves translation accuracy and allows handling longer sentences better.


### 5. üîç Attention Layer in Detail: Queries, Keys, and Values

The attention mechanism can be understood through the concepts of **Queries (Q)**, **Keys (K)**, and **Values (V)**:

- **Keys and Values:** These come from the encoder hidden states. Each hidden state acts as a key-value pair.
- **Query:** This is the decoder‚Äôs current hidden state, representing what the decoder is "looking for" in the input.

#### Computing Attention

1. **Similarity Score:** The model calculates a similarity score between the query and each key. This score measures how relevant each encoder hidden state is to the current decoding step.
2. **Softmax:** The similarity scores are normalized using a softmax function to produce attention weights (Œ±), which sum to 1.
3. **Context Vector:** The context vector is computed as the weighted sum of the values (encoder hidden states), using the attention weights.

#### Scaled Dot-Product Attention

- A popular method (from Vaswani et al., 2017) computes the dot product between queries and keys, then scales it by the square root of the key dimension to stabilize gradients.
- This operation is efficient and can be implemented with matrix multiplications and softmax.


### 6. üèóÔ∏è Putting It All Together: NMT Model with Attention

The full NMT model with attention includes:

- **Encoder:** Processes the input sentence and outputs a sequence of hidden states.
- **Pre-Attention Decoder:** Generates the next word based on the previous word and hidden state.
- **Attention Layer:** Uses the decoder‚Äôs hidden state as a query to compute attention weights over encoder hidden states (keys and values).
- **Context Vector:** Combined with the decoder‚Äôs hidden state to predict the next word.

During training, **teacher forcing** is used:

- The decoder receives the correct previous word as input (instead of its own prediction) to improve learning stability.
- This helps the model learn to generate accurate sequences faster.


### 7. üìä Evaluating Translations: BLEU and ROUGE Scores

To measure how well the model translates, we use evaluation metrics:

#### BLEU Score (BiLingual Evaluation Understudy)

- Compares the model‚Äôs candidate translation to one or more human reference translations.
- Measures how many words or phrases from the candidate appear in the references.
- Scores range from 0 to 1, with higher scores indicating better translations.
- Limitations: BLEU does not consider sentence meaning or grammar, so a high BLEU score doesn‚Äôt always mean a good translation.

#### ROUGE Score (Recall-Oriented Understudy for Gisting Evaluation)

- Focuses on recall: how many words from the reference appear in the candidate.
- ROUGE-N measures n-gram overlap.
- Often used in summarization but applicable to translation evaluation.
- Can be combined with BLEU and F1 scores for a more balanced evaluation.


### 8. üé≤ Sampling and Decoding Strategies

When generating translations, the model outputs a probability distribution over possible next words. Different decoding strategies affect the quality and diversity of translations:

#### Greedy Decoding

- Selects the most probable word at each step.
- Simple and fast but can lead to suboptimal overall sentences because it doesn‚Äôt consider future words.

#### Random Sampling

- Samples the next word randomly according to the predicted probabilities.
- Introduces diversity but can produce incoherent or inaccurate translations.

#### Temperature Sampling

- Adjusts randomness by scaling the probability distribution.
- Low temperature ‚Üí more confident, conservative choices.
- High temperature ‚Üí more random, diverse outputs.

#### Beam Search

- Keeps track of the top B (beam width) most probable sequences at each step.
- Explores multiple possible translations simultaneously.
- More computationally expensive but often produces better translations.
- Beam width of 1 is equivalent to greedy decoding.
- Needs length normalization to avoid bias toward shorter sequences.


### 9. üéØ Minimum Bayes Risk (MBR) Decoding

MBR is an advanced decoding strategy that:

- Generates multiple candidate translations.
- Computes similarity scores (e.g., ROUGE) between every pair of candidates.
- Selects the candidate with the highest average similarity to others.
- This approach aims to pick the most "consensus" translation, improving quality over random or greedy methods.


### Summary

Neural Machine Translation has evolved from basic Seq2Seq models to sophisticated architectures using attention mechanisms. Attention allows the model to dynamically focus on relevant parts of the input sentence, overcoming the fixed-length bottleneck and improving translation quality. Evaluation metrics like BLEU and ROUGE help measure performance, while decoding strategies such as beam search and MBR optimize the generation of accurate and fluent translations.

Understanding these components and how they fit together is essential for building and improving NMT systems.



<br>

## Questions



#### 1. What is the primary limitation of the basic Seq2Seq model without attention in neural machine translation?  
A) It cannot handle variable-length input sequences  
B) It compresses the entire input sequence into a fixed-length vector, causing an information bottleneck  
C) It requires the input and output sequences to be the same length  
D) It cannot use LSTM or GRU units  


#### 2. In the Seq2Seq architecture, what role does the encoder‚Äôs final hidden state play?  
A) It initializes the decoder‚Äôs hidden state  
B) It represents the entire input sentence as a fixed-length vector  
C) It directly generates the output sequence  
D) It is discarded after encoding  


#### 3. Why are LSTMs or GRUs preferred over vanilla RNNs in Seq2Seq models?  
A) They are faster to train  
B) They avoid vanishing and exploding gradient problems  
C) They require fewer parameters  
D) They can handle variable-length sequences better  


#### 4. How does the attention mechanism improve the decoder‚Äôs ability to generate translations?  
A) By allowing the decoder to focus on all encoder hidden states dynamically  
B) By increasing the fixed-length vector size  
C) By using only the first encoder hidden state at every decoding step  
D) By ignoring the encoder hidden states and relying solely on the decoder‚Äôs previous output  


#### 5. In the attention mechanism, what are the ‚Äúqueries,‚Äù ‚Äúkeys,‚Äù and ‚Äúvalues‚Äù?  
A) Queries come from the encoder, keys and values come from the decoder  
B) Queries come from the decoder, keys and values come from the encoder  
C) Queries, keys, and values all come from the encoder  
D) Queries, keys, and values all come from the decoder  


#### 6. What is the purpose of scaling the dot product in scaled dot-product attention?  
A) To increase the magnitude of similarity scores  
B) To stabilize gradients and prevent extremely large values when the key dimension is large  
C) To normalize the attention weights to sum to one  
D) To reduce the computational complexity of the attention mechanism  


#### 7. Which of the following statements about teacher forcing during training is true?  
A) The decoder always uses its own previous predictions as input  
B) The decoder uses the correct previous word from the target sequence as input  
C) Teacher forcing can cause errors from early steps to propagate during training  
D) Teacher forcing slows down the training process significantly  


#### 8. Why might greedy decoding produce suboptimal translations in Seq2Seq models?  
A) It selects the most probable word at each step without considering future words  
B) It samples words randomly from the probability distribution  
C) It always produces the longest possible output sequence  
D) It requires beam search to function properly  


#### 9. How does beam search differ from greedy decoding?  
A) Beam search keeps track of multiple candidate sequences simultaneously  
B) Beam search always selects the single most probable word at each step  
C) Beam search is computationally cheaper than greedy decoding  
D) Beam search requires normalization to avoid bias toward shorter sequences  


#### 10. What is a potential drawback of beam search decoding?  
A) It always produces less accurate translations than greedy decoding  
B) It penalizes longer sequences unless length normalization is applied  
C) It cannot handle variable-length sequences  
D) It ignores the attention mechanism  


#### 11. Which of the following best describes the context vector in attention-based Seq2Seq models?  
A) A fixed vector representing the entire input sentence  
B) A weighted sum of encoder hidden states based on attention weights  
C) The decoder‚Äôs hidden state at the previous time step  
D) The embedding of the current output word  


#### 12. How are attention weights computed in the attention mechanism?  
A) By applying a softmax to similarity scores between the decoder‚Äôs query and encoder‚Äôs keys  
B) By averaging all encoder hidden states equally  
C) By multiplying the decoder‚Äôs previous output with the encoder‚Äôs final hidden state  
D) By randomly assigning weights to encoder hidden states  


#### 13. What is the main advantage of using attention in languages with very different grammatical structures?  
A) Attention allows the model to reorder words flexibly during translation  
B) Attention forces the output to follow the source language word order  
C) Attention eliminates the need for tokenization  
D) Attention reduces the vocabulary size needed for translation  


#### 14. Which of the following is NOT a limitation of the BLEU score?  
A) It does not consider semantic meaning of sentences  
B) It ignores sentence structure and grammar  
C) It always rewards longer sentences  
D) It can be artificially inflated by outputting common words  


#### 15. How does ROUGE-N differ from BLEU in evaluating translations?  
A) ROUGE-N focuses on recall, measuring how many reference words appear in the candidate  
B) ROUGE-N measures precision, like BLEU  
C) ROUGE-N is only used for image captioning, not translation  
D) ROUGE-N ignores n-gram overlaps  


#### 16. What is the effect of increasing the temperature parameter during sampling in decoding?  
A) The model becomes more confident and conservative in word selection  
B) The model produces more random and diverse outputs  
C) The model always selects the most probable word  
D) The model ignores the probability distribution over words  


#### 17. In Minimum Bayes Risk (MBR) decoding, how is the best candidate translation selected?  
A) By choosing the candidate with the highest individual probability  
B) By selecting the candidate with the highest average similarity to all other candidates  
C) By picking the shortest candidate sequence  
D) By randomly sampling from the candidate set  


#### 18. Why is teacher forcing important during training of Seq2Seq models?  
A) It prevents the model from learning to generate sequences  
B) It helps the model learn faster by providing the correct previous word as input  
C) It guarantees perfect translation during training  
D) It eliminates the need for attention mechanisms  


#### 19. Which of the following statements about the encoder in Seq2Seq models is true?  
A) It produces a sequence of hidden states, one for each input token  
B) It only outputs a single hidden state at the end of the input sequence  
C) It generates the output translation directly  
D) It cannot handle variable-length input sequences  


#### 20. How does the attention mechanism affect the fixed-length memory constraint in Seq2Seq models?  
A) It removes the fixed-length memory constraint by using all encoder hidden states dynamically  
B) It increases the fixed-length vector size to accommodate longer sentences  
C) It compresses the input sequence into a smaller fixed-length vector  
D) It replaces the decoder with a feedforward network  



<br>

## Answers



#### 1. What is the primary limitation of the basic Seq2Seq model without attention in neural machine translation?  
A) ‚úó It can handle variable-length input sequences.  
B) ‚úì The fixed-length vector causes an information bottleneck limiting performance on long sequences.  
C) ‚úó Input and output sequences can have different lengths in Seq2Seq.  
D) ‚úó LSTMs and GRUs are commonly used to avoid gradient issues, so this is not a limitation.  

**Correct:** B


#### 2. In the Seq2Seq architecture, what role does the encoder‚Äôs final hidden state play?  
A) ‚úì It initializes the decoder‚Äôs hidden state.  
B) ‚úì It represents the entire input sentence as a fixed-length vector.  
C) ‚úó It does not directly generate the output sequence; the decoder does.  
D) ‚úó It is not discarded; it is essential for decoding.  

**Correct:** A, B


#### 3. Why are LSTMs or GRUs preferred over vanilla RNNs in Seq2Seq models?  
A) ‚úó They are not necessarily faster to train.  
B) ‚úì They mitigate vanishing and exploding gradient problems.  
C) ‚úó They often have more parameters due to gating mechanisms.  
D) ‚úó Handling variable-length sequences is a property of RNNs in general, not specific to LSTMs/GRUs.  

**Correct:** B


#### 4. How does the attention mechanism improve the decoder‚Äôs ability to generate translations?  
A) ‚úì It allows dynamic focus on all encoder hidden states at each decoding step.  
B) ‚úó It does not increase the fixed-length vector size; it uses all hidden states instead.  
C) ‚úó It does not use only the first hidden state; it uses all with weights.  
D) ‚úó It does not ignore encoder states; it explicitly uses them.  

**Correct:** A


#### 5. In the attention mechanism, what are the ‚Äúqueries,‚Äù ‚Äúkeys,‚Äù and ‚Äúvalues‚Äù?  
A) ‚úó Queries come from the decoder, not the encoder.  
B) ‚úì Queries come from the decoder; keys and values come from the encoder.  
C) ‚úó Queries come from the decoder, not all from encoder.  
D) ‚úó Keys and values come from encoder, queries from decoder.  

**Correct:** B


#### 6. What is the purpose of scaling the dot product in scaled dot-product attention?  
A) ‚úó Scaling reduces, not increases, magnitude to stabilize training.  
B) ‚úì It stabilizes gradients by preventing large dot product values when key dimension is large.  
C) ‚úó Softmax normalizes weights, scaling is separate.  
D) ‚úó Scaling does not reduce computational complexity.  

**Correct:** B


#### 7. Which of the following statements about teacher forcing during training is true?  
A) ‚úó Teacher forcing uses the correct previous word, not the model‚Äôs own prediction.  
B) ‚úì The decoder uses the correct previous word from the target sequence as input.  
C) ‚úó Teacher forcing reduces error propagation during training.  
D) ‚úó Teacher forcing generally speeds up training convergence.  

**Correct:** B


#### 8. Why might greedy decoding produce suboptimal translations in Seq2Seq models?  
A) ‚úì It selects the most probable word at each step without considering future context.  
B) ‚úó Random sampling is different from greedy decoding.  
C) ‚úó Greedy decoding does not always produce the longest output.  
D) ‚úó Greedy decoding works independently of beam search.  

**Correct:** A


#### 9. How does beam search differ from greedy decoding?  
A) ‚úì Beam search keeps track of multiple candidate sequences simultaneously.  
B) ‚úó Greedy decoding selects a single most probable word; beam search explores multiple.  
C) ‚úó Beam search is more computationally expensive than greedy decoding.  
D) ‚úì Beam search requires length normalization to avoid bias toward short sequences.  

**Correct:** A, D


#### 10. What is a potential drawback of beam search decoding?  
A) ‚úó Beam search usually produces better, not worse, translations than greedy decoding.  
B) ‚úì It penalizes longer sequences unless length normalization is applied.  
C) ‚úó Beam search can handle variable-length sequences.  
D) ‚úó Beam search uses attention mechanisms as usual.  

**Correct:** B


#### 11. Which of the following best describes the context vector in attention-based Seq2Seq models?  
A) ‚úó It is not fixed; it changes at each decoding step.  
B) ‚úì It is a weighted sum of encoder hidden states based on attention weights.  
C) ‚úó The decoder‚Äôs hidden state is separate from the context vector.  
D) ‚úó The context vector is not the embedding of the current output word.  

**Correct:** B


#### 12. How are attention weights computed in the attention mechanism?  
A) ‚úì By applying softmax to similarity scores between decoder query and encoder keys.  
B) ‚úó Encoder hidden states are not averaged equally; weights vary.  
C) ‚úó Attention weights are not computed by multiplying decoder output with final encoder state.  
D) ‚úó Weights are learned, not random.  

**Correct:** A


#### 13. What is the main advantage of using attention in languages with very different grammatical structures?  
A) ‚úì Attention allows flexible reordering by focusing on relevant input words dynamically.  
B) ‚úó Attention does not force output to follow source word order.  
C) ‚úó Attention does not eliminate the need for tokenization.  
D) ‚úó Attention does not reduce vocabulary size.  

**Correct:** A


#### 14. Which of the following is NOT a limitation of the BLEU score?  
A) ‚úó BLEU does not consider semantic meaning.  
B) ‚úó BLEU ignores sentence structure and grammar.  
C) ‚úì BLEU does not always reward longer sentences; it can penalize them.  
D) ‚úó BLEU can be artificially inflated by outputting common words.  

**Correct:** C


#### 15. How does ROUGE-N differ from BLEU in evaluating translations?  
A) ‚úì ROUGE-N focuses on recall: how many reference words appear in the candidate.  
B) ‚úó ROUGE-N is recall-oriented, not precision like BLEU.  
C) ‚úó ROUGE-N is used in translation and summarization, not only image captioning.  
D) ‚úó ROUGE-N measures n-gram overlap, not ignoring it.  

**Correct:** A


#### 16. What is the effect of increasing the temperature parameter during sampling in decoding?  
A) ‚úó Lower temperature makes the model more confident, not higher.  
B) ‚úì Higher temperature increases randomness and diversity in outputs.  
C) ‚úó Model does not always select the most probable word at high temperature.  
D) ‚úó Temperature scales the probability distribution; it does not ignore it.  

**Correct:** B


#### 17. In Minimum Bayes Risk (MBR) decoding, how is the best candidate translation selected?  
A) ‚úó It is not based on individual probability alone.  
B) ‚úì It selects the candidate with the highest average similarity to all other candidates.  
C) ‚úó It does not pick the shortest sequence by default.  
D) ‚úó It is not random sampling.  

**Correct:** B


#### 18. Why is teacher forcing important during training of Seq2Seq models?  
A) ‚úó It helps the model learn to generate sequences, not prevent it.  
B) ‚úì It speeds up learning by providing the correct previous word as input.  
C) ‚úó It does not guarantee perfect translation during training.  
D) ‚úó It does not eliminate the need for attention.  

**Correct:** B


#### 19. Which of the following statements about the encoder in Seq2Seq models is true?  
A) ‚úì It produces a sequence of hidden states, one per input token.  
B) ‚úó It does not output only a single hidden state; all hidden states are available.  
C) ‚úó It does not generate the output translation directly.  
D) ‚úó It can handle variable-length input sequences.  

**Correct:** A, D


#### 20. How does the attention mechanism affect the fixed-length memory constraint in Seq2Seq models?  
A) ‚úì It removes the fixed-length memory constraint by dynamically using all encoder hidden states.  
B) ‚úó It does not increase the fixed-length vector size.  
C) ‚úó It does not compress the input into a smaller vector; it uses all hidden states.  
D) ‚úó It does not replace the decoder with a feedforward network.  

**Correct:** A

