## 4.3 Transfer Learning and Fine-Tuning

[Study Notes](#study-notes)

[Questions](#questions)



### Key Points



#### 1. üîÑ Transfer Learning  
- Transfer learning involves pre-training a model on a large dataset and fine-tuning it on a smaller, task-specific dataset.  
- Pre-training uses unlabeled data with self-supervised tasks like masked language modeling and next sentence prediction.  
- Fine-tuning adapts the pre-trained model to downstream tasks such as question answering, sentiment analysis, and translation.

#### 2. üß© NLP Models and Architectures  
- CBOW uses a fixed window and feed-forward neural networks to predict words based on surrounding context.  
- ELMo uses bi-directional LSTMs to capture full sentence context.  
- GPT uses a uni-directional Transformer decoder architecture (left-to-right context).  
- BERT uses a bi-directional Transformer encoder architecture with masked language modeling and next sentence prediction pre-training tasks.  
- T5 uses an encoder-decoder Transformer architecture and frames all NLP tasks as text-to-text problems.

#### 3. üß† BERT Pre-Training Objectives  
- Masked Language Modeling (MLM): 15% of tokens are masked; 80% replaced with [MASK], 10% with random tokens, 10% unchanged.  
- Next Sentence Prediction (NSP): Model predicts if sentence B follows sentence A.  
- BERT_base has 12 Transformer layers, 12 attention heads, and about 110 million parameters.

#### 4. ‚öôÔ∏è Fine-Tuning  
- Fine-tuning involves training the pre-trained model on labeled data for a specific downstream task.  
- Often includes adding a task-specific output layer (e.g., classification head).  
- Fine-tuning can be done on the entire model or selected layers.

#### 5. üî§ T5 Model and Training  
- T5 treats all NLP tasks as text-to-text tasks with task-specific prefixes (e.g., "translate:", "summarize:", "question:").  
- Uses an encoder-decoder Transformer with 12 layers each and about 220 million parameters.  
- Trained on multiple tasks simultaneously (multi-task learning).

#### 6. üõ†Ô∏è Hugging Face Ecosystem  
- Provides pre-trained model checkpoints, tokenizers, datasets, and training utilities.  
- Pipelines simplify running models for tasks like sentiment analysis and question answering.  
- Trainer API supports fine-tuning with evaluation metrics and training configurations.  
- Model Hub contains over 14,000 model checkpoints and 1,000 datasets.

#### 7. üìä Training Strategies  
- Data mixing strategies include equal mixing, proportional mixing, and temperature-scaled mixing.  
- Fine-tuning techniques include gradual unfreezing and adapter layers to optimize training efficiency.  
- Evaluation metrics vary by task: BLEU for translation, ROUGE for summarization, accuracy/F1 for classification.



<br>

## Study Notes





### 1. üîÑ What is Transfer Learning and Why Does It Matter?

**Introduction:**  
Transfer learning is a powerful technique in machine learning where a model trained on one task is reused or adapted for a different but related task. This approach is especially useful in natural language processing (NLP), where training large models from scratch requires massive amounts of data and computational resources. Transfer learning helps reduce training time, improve performance on smaller datasets, and leverage knowledge learned from large unlabeled datasets.

**Detailed Explanation:**  
Imagine you‚Äôve watched a movie and learned a lot about the story and characters. Later, when you watch a sequel or a related movie, you don‚Äôt start from zero‚Äîyou use what you already know to understand the new story better and faster. Similarly, in NLP, models are first **pre-trained** on large amounts of text data (often unlabeled) to learn general language understanding. Then, they are **fine-tuned** on specific "downstream" tasks like question answering, sentiment analysis, or translation.

- **Pre-training:** The model learns general language patterns, grammar, and context from huge datasets without explicit labels (self-supervised learning).
- **Fine-tuning:** The model is adjusted with labeled data for a specific task, improving its performance on that task.

This two-step process allows models to perform well even when labeled data is scarce.


### 2. üìö Pre-Training Tasks: Learning Language Without Labels

**Introduction:**  
Pre-training is the foundation of transfer learning in NLP. It involves training a model on large amounts of unlabeled text to learn language structure and context. This is done through self-supervised tasks, where the model creates its own "labels" from the input data.

**Detailed Explanation:**  
Two common pre-training tasks are:

- **Masked Language Modeling (MLM):** Randomly mask some words in a sentence and train the model to predict these missing words. For example, in the sentence "The legislators believed that they were on the ___ side of history," the model must predict the masked word "right." This forces the model to understand the context from both sides of the masked word, enabling **bi-directional context learning**.

- **Next Sentence Prediction (NSP):** The model is given two sentences and must predict whether the second sentence logically follows the first. This helps the model understand relationships between sentences, which is crucial for tasks like question answering and summarization.

These tasks do not require labeled data because the "labels" are generated from the text itself (e.g., masked words or sentence pairs).


### 3. üß© Key Models in Transfer Learning: CBOW, ELMo, GPT, BERT, and T5

**Introduction:**  
Over time, NLP models have evolved to better capture context and meaning in language. Understanding the differences between these models helps grasp how transfer learning has improved.

**Detailed Explanation:**

- **CBOW (Continuous Bag of Words):** An early model that predicts a word based on a fixed window of surrounding words. It uses a feed-forward neural network but has limited context because it only looks at a small window of words.

- **ELMo (Embeddings from Language Models):** Uses bi-directional LSTMs (a type of recurrent neural network) to capture full sentence context, meaning it looks at words before and after the target word. This was a big step forward in understanding context.

- **GPT (Generative Pre-trained Transformer):** Uses a Transformer decoder architecture and is **uni-directional**, meaning it predicts the next word based only on previous words (left-to-right). It‚Äôs great for generating text but less effective at understanding full context.

- **BERT (Bidirectional Encoder Representations from Transformers):** Uses a Transformer **encoder** architecture and is **bi-directional**, meaning it looks at the entire sentence context simultaneously. BERT is pre-trained with MLM and NSP tasks, making it excellent for understanding language and performing various NLP tasks after fine-tuning.

- **T5 (Text-to-Text Transfer Transformer):** A versatile model that treats every NLP problem as a text-to-text task. For example, translation, summarization, and question answering are all framed as converting input text into output text. T5 uses an encoder-decoder Transformer architecture and is trained on multiple tasks simultaneously (multi-task learning), improving its generalization.


### 4. ‚öôÔ∏è Fine-Tuning: Adapting Pre-Trained Models to Specific Tasks

**Introduction:**  
Fine-tuning is the process of taking a pre-trained model and training it further on a smaller, labeled dataset for a specific task. This step customizes the model‚Äôs general language understanding to the nuances of the target task.

**Detailed Explanation:**  
Fine-tuning usually involves:

- Adding a task-specific output layer (e.g., classification head for sentiment analysis).
- Training the entire model or just some layers on the labeled dataset.
- Adjusting model weights slightly to improve task-specific performance.

For example, BERT can be fine-tuned on the Stanford Question Answering Dataset (SQuAD) to answer questions based on a given passage. The model learns to predict the start and end positions of the answer span in the text.

Fine-tuning is efficient because the model already understands language broadly; it just needs to learn the specifics of the new task.


### 5. üèóÔ∏è BERT Architecture and Pre-Training Details

**Introduction:**  
BERT is a landmark model in NLP due to its bi-directional Transformer architecture and effective pre-training strategy. Understanding its structure and training objectives is key to grasping modern NLP.

**Detailed Explanation:**  
- **Architecture:** BERT_base has 12 Transformer layers (blocks), each with 12 attention heads, totaling about 110 million parameters. It uses positional embeddings to understand word order and segment embeddings to distinguish between sentence pairs.

- **Input Format:**  
  - Special tokens: `[CLS]` at the start (used for classification tasks), `[SEP]` to separate sentences.
  - Token embeddings represent words or subwords.
  - Segment embeddings indicate which sentence a token belongs to.
  - Position embeddings encode the position of each token in the sequence.

- **Pre-training Objectives:**  
  1. **Masked Language Modeling (MLM):** Randomly mask 15% of tokens. Of these, 80% are replaced with `[MASK]`, 10% with a random token, and 10% left unchanged. The model predicts the original tokens.
  2. **Next Sentence Prediction (NSP):** Given two sentences, predict if the second follows the first in the original text.

- **Loss Functions:**  
  - MLM uses cross-entropy loss to measure prediction accuracy of masked tokens.
  - NSP uses binary classification loss to predict sentence order.

This combination helps BERT learn deep, contextual language representations.


### 6. üîÑ T5: A Unified Text-to-Text Transformer

**Introduction:**  
T5 is a flexible model that frames all NLP tasks as converting input text to output text. This unified approach simplifies training and fine-tuning across diverse tasks.

**Detailed Explanation:**  
- **Architecture:** T5 uses an encoder-decoder Transformer with 12 layers each (for the base model) and about 220 million parameters.
- **Multi-task Training:** T5 is trained on many tasks simultaneously, such as translation, summarization, question answering, and classification. Each task is prefixed with a descriptive token like "translate English to German:" or "summarize:" to tell the model what to do.
- **Input/Output Format:**  
  - Input: A text prompt combining the task and the input data (e.g., "question: When is Pi day? context: Pi day is March 14").
  - Output: The model generates the answer or summary as text.
- **Advantages:**  
  - Simplifies the pipeline by using the same model and format for all tasks.
  - Benefits from multi-task learning, improving generalization.
  - Uses masked language modeling during pre-training.


### 7. üõ†Ô∏è Using Hugging Face for Transfer Learning and Fine-Tuning

**Introduction:**  
Hugging Face is a popular open-source ecosystem that makes it easy to use, fine-tune, and deploy state-of-the-art transformer models like BERT and T5.

**Detailed Explanation:**  
- **Transformers Library:** Provides pre-trained models and tools to load, fine-tune, and run inference with just a few lines of code.
- **Pipelines:** High-level APIs for common tasks like sentiment analysis, question answering, and text generation. They handle preprocessing, model inference, and postprocessing automatically.
- **Model Checkpoints:** Thousands of pre-trained models are available on the Hugging Face Model Hub, covering many languages and tasks.
- **Datasets:** Hugging Face offers easy access to thousands of datasets optimized for NLP tasks.
- **Tokenizers:** Convert raw text into tokens (numbers) that models understand, handling subtleties like subwords and special tokens.
- **Trainer API:** Simplifies training and fine-tuning with built-in support for evaluation metrics, learning rate schedules, and distributed training.
- **Example Workflow:**  
  1. Load a pre-trained model checkpoint (e.g., `bert-base-cased`).
  2. Load and preprocess your dataset.
  3. Fine-tune the model using the Trainer API.
  4. Use the fine-tuned model for inference.

This ecosystem dramatically lowers the barrier to applying transfer learning in NLP.


### 8. üìä Training Strategies and Evaluation

**Introduction:**  
Effective training and evaluation strategies are crucial to getting the best performance from transfer learning models.

**Detailed Explanation:**  
- **Data Mixing:** When training on multiple tasks, data can be mixed in different ways:
  - **Equal mixing:** Sample equally from each dataset.
  - **Proportional mixing:** Sample according to dataset size.
  - **Temperature-scaled mixing:** Adjust sampling probabilities to balance tasks.
- **Fine-tuning Techniques:**  
  - **Gradual unfreezing:** Start by training only the last layers, then progressively unfreeze earlier layers.
  - **Adapter layers:** Add small trainable layers to the model, keeping the original model mostly frozen to reduce training cost.
- **Evaluation Metrics:**  
  - Use task-specific metrics like accuracy, F1 score, BLEU (for translation), ROUGE (for summarization), etc.
  - Benchmarks like GLUE provide standardized datasets and leaderboards for general language understanding.


### Summary

Transfer learning in NLP leverages large-scale pre-training on unlabeled data to build powerful language models like BERT and T5. These models learn deep contextual representations through tasks like masked language modeling and next sentence prediction. Fine-tuning adapts these models to specific tasks with labeled data, enabling high performance even with limited data. Tools like Hugging Face make it easy to apply these techniques in practice, providing pre-trained models, datasets, and training utilities. Understanding the architectures, training objectives, and fine-tuning strategies is key to effectively using transfer learning in NLP.



<br>

## Questions



#### 1. What is the primary advantage of transfer learning in NLP?  
A) It eliminates the need for any labeled data in downstream tasks  
B) It reduces training time by leveraging pre-trained models  
C) It guarantees perfect accuracy on all downstream tasks  
D) It allows models to learn from both labeled and unlabeled data  

#### 2. Which of the following best describes the Masked Language Modeling (MLM) task used in BERT pre-training?  
A) Predicting the next word in a sequence given all previous words  
B) Predicting randomly masked tokens in a sentence using context from both directions  
C) Predicting whether two sentences are consecutive in a text  
D) Predicting the sentiment of a sentence based on masked words  

#### 3. Why does BERT use a bi-directional Transformer encoder instead of a uni-directional model like GPT?  
A) To allow the model to attend to both left and right context simultaneously  
B) To reduce the number of parameters in the model  
C) To enable the model to generate text more fluently  
D) To restrict the model to only past context for better prediction  

#### 4. In BERT pre-training, what percentage of tokens are typically selected for masking, and how are they treated?  
A) 15% of tokens are selected; 80% replaced with [MASK], 10% replaced with random tokens, 10% unchanged  
B) 50% of tokens are selected; all replaced with [MASK] tokens  
C) 15% of tokens are selected; all replaced with random tokens  
D) 10% of tokens are selected; 50% replaced with [MASK], 50% unchanged  

#### 5. Which of the following statements about Next Sentence Prediction (NSP) in BERT is true?  
A) NSP helps the model understand relationships between sentences  
B) NSP is used to predict masked words within a sentence  
C) NSP is a binary classification task predicting if sentence B follows sentence A  
D) NSP is only used during fine-tuning, not pre-training  

#### 6. How does T5 differ from BERT in terms of task formulation?  
A) T5 treats all NLP tasks as text-to-text problems  
B) BERT uses an encoder-decoder architecture, while T5 uses only an encoder  
C) T5 uses multi-task training, while BERT is trained on a single task  
D) BERT can only perform classification tasks, while T5 can only perform generation tasks  

#### 7. Which of the following are true about fine-tuning a pre-trained model?  
A) Fine-tuning always involves training the entire model from scratch  
B) Fine-tuning adapts the model to a specific downstream task using labeled data  
C) Fine-tuning can involve adding task-specific output layers  
D) Fine-tuning is unnecessary if the pre-trained model is large enough  

#### 8. What is the role of the special tokens [CLS] and [SEP] in BERT inputs?  
A) [CLS] is used to separate sentences, [SEP] marks the start of the input  
B) [CLS] is a classification token added at the start, [SEP] separates sentences  
C) Both tokens are used to mask words during pre-training  
D) They are only used during fine-tuning, not pre-training  

#### 9. Which of the following best describes the difference between feature-based transfer and fine-tuning?  
A) Feature-based transfer uses pre-trained model outputs as fixed features without updating model weights  
B) Fine-tuning updates the entire pre-trained model weights on the downstream task  
C) Feature-based transfer requires labeled data for pre-training  
D) Fine-tuning always freezes the pre-trained model layers  

#### 10. Why is self-supervised learning important in pre-training large language models?  
A) It allows models to learn from unlabeled data by creating their own prediction tasks  
B) It requires large amounts of labeled data to work effectively  
C) It is only used for training on downstream tasks  
D) It prevents the model from overfitting on small datasets  

#### 11. Which of the following statements about the Transformer architecture are correct?  
A) Transformers use attention mechanisms to weigh the importance of different words in a sequence  
B) Transformers rely on recurrent neural networks to process sequences sequentially  
C) Positional embeddings are used to encode the order of tokens in the input  
D) Transformers cannot process sequences longer than their fixed window size  

#### 12. What is a key difference between GPT and BERT in terms of context usage?  
A) GPT uses bi-directional context, BERT uses uni-directional context  
B) GPT uses uni-directional (left-to-right) context, BERT uses bi-directional context  
C) Both GPT and BERT use uni-directional context but differ in architecture  
D) GPT uses no context, BERT uses full context  

#### 13. In multi-task training with T5, how does the model know which task to perform?  
A) By using different model architectures for each task  
B) By prefixing the input text with a task-specific prompt (e.g., "translate:", "summarize:")  
C) By training separate models for each task and selecting the appropriate one at inference  
D) By using different tokenizers for each task  

#### 14. Which of the following are challenges or considerations when fine-tuning large pre-trained models?  
A) Risk of catastrophic forgetting if the model forgets pre-trained knowledge  
B) Need for large labeled datasets to fine-tune effectively  
C) Choosing which layers to freeze or unfreeze during training  
D) Fine-tuning always improves performance regardless of dataset size  

#### 15. How does Hugging Face simplify working with transformer models?  
A) By providing pre-trained model checkpoints and tokenizers ready to use  
B) By requiring users to implement all training code from scratch  
C) By offering pipelines that handle preprocessing, inference, and postprocessing  
D) By limiting models to only a few NLP tasks  

#### 16. Which of the following are true about the Stanford Question Answering Dataset (SQuAD) in the context of fine-tuning?  
A) It provides labeled question-answer pairs for training QA models  
B) It is used for pre-training language models like BERT  
C) Fine-tuning on SQuAD involves predicting answer spans within a passage  
D) SQuAD is an unlabeled dataset used for self-supervised learning  

#### 17. What is the purpose of adapter layers in fine-tuning large models?  
A) To add small trainable modules that reduce the number of parameters updated during fine-tuning  
B) To replace the entire pre-trained model with a smaller one  
C) To freeze all layers and only train the adapter layers for efficiency  
D) To increase the model size significantly for better performance  

#### 18. Which of the following statements about the GLUE benchmark are correct?  
A) GLUE is a collection of diverse NLP tasks used to evaluate general language understanding  
B) GLUE datasets are used primarily for pre-training large language models  
C) GLUE includes tasks like sentiment analysis, paraphrase detection, and entailment  
D) GLUE is model-specific and cannot be used to compare different architectures  

#### 19. How does temperature-scaled mixing affect multi-task training data sampling?  
A) It adjusts the probability of sampling from each dataset to balance training  
B) It always samples equally from all datasets regardless of size  
C) It increases the sampling probability of smaller datasets to prevent underfitting  
D) It decreases the sampling probability of larger datasets to speed up training  

#### 20. Which of the following best describe the input embeddings used in BERT?  
A) Token embeddings represent the meaning of each word or subword  
B) Segment embeddings distinguish between different sentences in the input  
C) Position embeddings encode the order of tokens in the sequence  
D) Embeddings are only used during fine-tuning, not pre-training  



<br>

## Answers



#### 1. What is the primary advantage of transfer learning in NLP?  
A) ‚úó It does not eliminate the need for labeled data in downstream tasks; fine-tuning still requires labeled data.  
B) ‚úì It reduces training time by leveraging knowledge from pre-trained models.  
C) ‚úó It does not guarantee perfect accuracy; performance depends on task and data.  
D) ‚úì It allows models to learn from both labeled (fine-tuning) and unlabeled (pre-training) data.  

**Correct:** B, D


#### 2. Which of the following best describes the Masked Language Modeling (MLM) task used in BERT pre-training?  
A) ‚úó This describes a uni-directional language model like GPT, not MLM.  
B) ‚úì MLM predicts masked tokens using context from both left and right (bi-directional).  
C) ‚úó This describes Next Sentence Prediction, not MLM.  
D) ‚úó MLM is about predicting masked words, not sentiment.  

**Correct:** B


#### 3. Why does BERT use a bi-directional Transformer encoder instead of a uni-directional model like GPT?  
A) ‚úì Bi-directional context allows BERT to use information from both sides of a token.  
B) ‚úó BERT has more parameters, not fewer.  
C) ‚úó GPT is better at text generation; BERT focuses on understanding.  
D) ‚úó BERT does not restrict context to past words; it uses full context.  

**Correct:** A


#### 4. In BERT pre-training, what percentage of tokens are typically selected for masking, and how are they treated?  
A) ‚úì Correct percentages and treatment as per BERT‚Äôs masking strategy.  
B) ‚úó 50% is too high; BERT uses 15%.  
C) ‚úó All replaced with random tokens is incorrect; only 10% replaced randomly.  
D) ‚úó Incorrect percentages and treatment.  

**Correct:** A


#### 5. Which of the following statements about Next Sentence Prediction (NSP) in BERT is true?  
A) ‚úì NSP helps model understand sentence relationships.  
B) ‚úó NSP is not about masked words; that‚Äôs MLM.  
C) ‚úì NSP is a binary classification task predicting if sentence B follows sentence A.  
D) ‚úó NSP is used during pre-training, not only fine-tuning.  

**Correct:** A, C


#### 6. How does T5 differ from BERT in terms of task formulation?  
A) ‚úì T5 frames all tasks as text-to-text problems.  
B) ‚úó BERT uses only an encoder; T5 uses encoder-decoder.  
C) ‚úì T5 is trained on multiple tasks simultaneously; BERT is pre-trained on MLM and NSP only.  
D) ‚úó BERT can perform generation tasks (e.g., masked token prediction), and T5 can do classification too.  

**Correct:** A, C


#### 7. Which of the following are true about fine-tuning a pre-trained model?  
A) ‚úó Fine-tuning does not train from scratch; it starts from pre-trained weights.  
B) ‚úì Fine-tuning adapts the model to a specific task using labeled data.  
C) ‚úì Adding task-specific output layers is common in fine-tuning.  
D) ‚úó Fine-tuning is necessary to adapt the model to new tasks.  

**Correct:** B, C


#### 8. What is the role of the special tokens [CLS] and [SEP] in BERT inputs?  
A) ‚úó Roles are reversed; [CLS] is not a separator.  
B) ‚úì [CLS] is a classification token at the start; [SEP] separates sentences.  
C) ‚úó They are not used for masking.  
D) ‚úó Used in both pre-training and fine-tuning.  

**Correct:** B


#### 9. Which of the following best describes the difference between feature-based transfer and fine-tuning?  
A) ‚úì Feature-based transfer uses fixed features from pre-trained models without updating weights.  
B) ‚úì Fine-tuning updates the entire model weights on the downstream task.  
C) ‚úó Feature-based transfer does not require labeled data for pre-training; pre-training is usually self-supervised.  
D) ‚úó Fine-tuning may freeze some layers but often updates many or all layers.  

**Correct:** A, B


#### 10. Why is self-supervised learning important in pre-training large language models?  
A) ‚úì It enables learning from unlabeled data by creating prediction tasks from the data itself.  
B) ‚úó It does not require large labeled datasets.  
C) ‚úó It is primarily used during pre-training, not downstream tasks.  
D) ‚úó While it helps generalization, preventing overfitting is not its main purpose.  

**Correct:** A


#### 11. Which of the following statements about the Transformer architecture are correct?  
A) ‚úì Attention mechanisms weigh the importance of different tokens.  
B) ‚úó Transformers do not use RNNs; they process sequences in parallel.  
C) ‚úì Positional embeddings encode token order since Transformers lack recurrence.  
D) ‚úó Transformers can process sequences longer than fixed windows using attention.  

**Correct:** A, C


#### 12. What is a key difference between GPT and BERT in terms of context usage?  
A) ‚úó GPT is uni-directional, not bi-directional.  
B) ‚úì GPT uses uni-directional (left-to-right) context; BERT uses bi-directional context.  
C) ‚úó BERT uses bi-directional context, not uni-directional.  
D) ‚úó GPT uses context; it‚Äôs not context-free.  

**Correct:** B


#### 13. In multi-task training with T5, how does the model know which task to perform?  
A) ‚úó T5 uses the same architecture for all tasks.  
B) ‚úì T5 uses task-specific prefixes in the input text to indicate the task.  
C) ‚úó T5 uses a single model for all tasks, not separate models.  
D) ‚úó Tokenizers are generally shared, not task-specific.  

**Correct:** B


#### 14. Which of the following are challenges or considerations when fine-tuning large pre-trained models?  
A) ‚úì Catastrophic forgetting can occur if fine-tuning overwrites pre-trained knowledge.  
B) ‚úó Fine-tuning can work with small datasets due to transfer learning.  
C) ‚úì Deciding which layers to freeze or unfreeze affects training efficiency and performance.  
D) ‚úó Fine-tuning does not always improve performance; it depends on data and task.  

**Correct:** A, C


#### 15. How does Hugging Face simplify working with transformer models?  
A) ‚úì Provides pre-trained models and tokenizers ready to use.  
B) ‚úó It provides high-level APIs, so users don‚Äôt need to implement training from scratch.  
C) ‚úì Pipelines automate preprocessing, inference, and postprocessing.  
D) ‚úó Supports a wide range of NLP tasks, not limited to a few.  

**Correct:** A, C


#### 16. Which of the following are true about the Stanford Question Answering Dataset (SQuAD) in the context of fine-tuning?  
A) ‚úì SQuAD provides labeled question-answer pairs for training QA models.  
B) ‚úó SQuAD is not used for pre-training; it is a downstream fine-tuning dataset.  
C) ‚úì Fine-tuning on SQuAD involves predicting answer spans within passages.  
D) ‚úó SQuAD is labeled, not unlabeled.  

**Correct:** A, C


#### 17. What is the purpose of adapter layers in fine-tuning large models?  
A) ‚úì Adapter layers are small trainable modules that reduce the number of parameters updated.  
B) ‚úó They do not replace the entire model.  
C) ‚úì Adapter layers allow freezing most of the model while training only adapters for efficiency.  
D) ‚úó Adapter layers do not increase model size significantly; they are lightweight.  

**Correct:** A, C


#### 18. Which of the following statements about the GLUE benchmark are correct?  
A) ‚úì GLUE is a collection of diverse NLP tasks for evaluating language understanding.  
B) ‚úó GLUE is used for evaluation, not pre-training.  
C) ‚úì GLUE includes tasks like sentiment analysis, paraphrase detection, and entailment.  
D) ‚úó GLUE is model-agnostic and used to compare different architectures.  

**Correct:** A, C, D


#### 19. How does temperature-scaled mixing affect multi-task training data sampling?  
A) ‚úì It adjusts sampling probabilities to balance training across datasets.  
B) ‚úó Equal sampling ignores dataset size differences.  
C) ‚úì It can increase sampling probability of smaller datasets to avoid underfitting.  
D) ‚úó It does not necessarily decrease sampling of larger datasets to speed training.  

**Correct:** A, C


#### 20. Which of the following best describe the input embeddings used in BERT?  
A) ‚úì Token embeddings represent word or subword meanings.  
B) ‚úì Segment embeddings distinguish between sentences in input pairs.  
C) ‚úì Position embeddings encode token order in the sequence.  
D) ‚úó Embeddings are used during both pre-training and fine-tuning.  

**Correct:** A, B, C

