## 2.2 Part of Speech Tagging and Hidden Markov Models

## Study Notes

### 1. üè∑Ô∏è What is Part of Speech (POS) Tagging?

Part of Speech tagging is a fundamental task in natural language processing (NLP) where each word in a sentence is assigned a label that indicates its grammatical role. These labels are called **part of speech tags** and include categories like noun, verb, adjective, adverb, determiner, punctuation, and more.

#### Why is POS tagging important?

- It helps computers understand the structure and meaning of sentences.
- It is a stepping stone for more complex NLP tasks such as named entity recognition, speech recognition, and co-reference resolution.
- For example, in the sentence "Why not learn something?", the word "learn" is tagged as a verb, and "something" as a noun.

#### Examples of POS tags:

- **Noun (NN):** something, nothing
- **Verb (VB):** learn, study
- **Determiner (DT):** the, a
- **Wh-adverb (WRB):** why, where
- **Punctuation:** sentence closers like periods or question marks

POS tagging is not just about labeling words but understanding their role in context, which can be ambiguous. For example, "learn" is usually a verb, but in some contexts, words can have multiple possible tags.


### 2. üîÑ Markov Chains and POS Tagging

To model the sequence of POS tags in a sentence, we use **Markov chains**. A Markov chain is a mathematical system that undergoes transitions from one state to another, where the probability of each state depends only on the previous state (this is called the Markov property).

#### How does this relate to POS tagging?

- Each POS tag is considered a **state** in the Markov chain.
- The sequence of tags in a sentence forms a path through these states.
- The probability of a tag depends on the tag that came before it (transition probability).

For example, after a determiner (like "the"), a noun is more likely than a verb. So, the transition probability from DT to NN might be high.

#### Transition matrix

This is a table that shows the probabilities of moving from one POS tag to another. For example:

| From \ To | NN (noun) | VB (verb) | O (other) |
|-----------|-----------|-----------|-----------|
| NN        | 0.2       | 0.2       | 0.6       |
| VB        | 0.4       | 0.3       | 0.3       |
| O         | 0.2       | 0.3       | 0.5       |

- Each row sums to 1 (or close to it).
- These probabilities are learned from a corpus (a large collection of text).

#### Initial probabilities

These represent the likelihood of a tag starting a sentence. For example:

- NN: 0.4
- VB: 0.1
- O: 0.5

This means sentences are more likely to start with a noun or other tags than a verb.


### 3. üîç Hidden Markov Models (HMMs)

A **Hidden Markov Model** is an extension of a Markov chain where the states are **hidden** (not directly observable), but we observe some output that depends probabilistically on the state.

#### How does this apply to POS tagging?

- The **hidden states** are the POS tags (e.g., noun, verb).
- The **observations** are the actual words in the sentence.
- We want to find the most likely sequence of hidden states (POS tags) given the observed words.

#### Components of an HMM:

1. **States:** POS tags (NN, VB, O, etc.)
2. **Observations:** Words in the sentence ("learn", "something", etc.)
3. **Transition probabilities:** Probability of moving from one POS tag to another.
4. **Emission probabilities:** Probability of a word being generated from a particular POS tag.

For example, the emission probability might tell us that the word "eat" is more likely to be emitted by a verb state than a noun state.


### 4. üìä Transition and Emission Probabilities

#### Transition probabilities

These are calculated by counting how often one tag follows another in a training corpus and then normalizing:

\[
P(\text{tag}_i | \text{tag}_{i-1}) = \frac{\text{Count}(\text{tag}_{i-1}, \text{tag}_i)}{\text{Count}(\text{tag}_{i-1})}
\]

For example, if "You" is tagged as a pronoun and "eat" as a verb, and "You eat" occurs 2 times out of 3 occurrences of "You", then:

\[
P(\text{eat} | \text{You}) = \frac{2}{3}
\]

#### Emission probabilities

These are the probabilities of a word being generated by a particular tag:

\[
P(\text{word} | \text{tag}) = \frac{\text{Count}(\text{tag}, \text{word})}{\text{Count}(\text{tag})}
\]

For example, the probability that the word "eat" is emitted by the verb tag might be 0.5.

#### Smoothing

Sometimes, certain tag transitions or emissions never appear in the training data, leading to zero probabilities. To avoid this, **smoothing** techniques add a small value (Œµ) to counts to ensure no zero probabilities, which helps the model generalize better.


### 5. üß© The Viterbi Algorithm: Finding the Best POS Tag Sequence

The Viterbi algorithm is a dynamic programming method used to find the most likely sequence of hidden states (POS tags) given a sequence of observed words.

#### Why do we need it?

- There are many possible tag sequences for a sentence.
- We want the sequence with the highest overall probability considering both transition and emission probabilities.

#### How does it work?

The algorithm proceeds in three main steps:

1. **Initialization:** Set up the initial probabilities for the first word with each possible tag.
2. **Forward pass:** For each subsequent word, calculate the highest probability of each tag by considering all possible previous tags and multiplying by transition and emission probabilities.
3. **Backward pass:** After processing all words, backtrack to find the sequence of tags that led to the highest probability.

#### Example:

For the sentence "Why not learn something?", the algorithm will:

- Start with initial probabilities for "Why".
- For "not", calculate probabilities for each tag based on "Why".
- Continue for "learn" and "something".
- Finally, backtrack to find the most probable tag sequence.

#### Implementation notes:

- Use **log probabilities** to avoid numerical underflow (very small numbers).
- Python indexing starts at 0, so be careful with array indices.


### 6. üìö Practical Example: Using Ezra Pound‚Äôs Poetry Corpus

To build the transition and emission matrices, a corpus is needed. The lecture uses Ezra Pound‚Äôs poem *In a Station of the Metro* as an example.

#### Steps:

- Add sentence start tokens `<s>` to mark beginnings.
- Tag each word with its POS.
- Count occurrences of tag pairs to build the transition matrix.
- Count occurrences of word-tag pairs to build the emission matrix.
- Apply smoothing to handle zero counts.

This process illustrates how real-world data is used to train an HMM for POS tagging.


### 7. üìù Summary and Key Takeaways

- **POS tagging** assigns grammatical categories to words, which is essential for understanding language.
- **Markov chains** model the sequence of POS tags using transition probabilities.
- **Hidden Markov Models** add the concept of hidden states (tags) and observable outputs (words), with emission probabilities linking them.
- **Transition and emission matrices** are learned from a tagged corpus.
- **Smoothing** prevents zero probabilities in these matrices.
- The **Viterbi algorithm** efficiently finds the most likely tag sequence for a given sentence.
- This framework is foundational for many NLP applications like named entity recognition and speech recognition.



<br>

## Key Points

#### 1. üè∑Ô∏è Part of Speech (POS) Tagging  
- POS tagging assigns grammatical categories (tags) like noun, verb, determiner, adverb, punctuation to each word in a sentence.  
- Common POS tags include NN (noun), VB (verb), DT (determiner), WRB (wh-adverb).  
- POS tagging is essential for NLP tasks such as named entity recognition, speech recognition, and co-reference resolution.

#### 2. üîÑ Markov Chains in POS Tagging  
- POS tags are modeled as states in a Markov chain.  
- Transition probabilities represent the likelihood of one POS tag following another.  
- Initial probabilities represent the likelihood of a POS tag starting a sentence.  
- Transition matrices are constructed from counts of tag pairs in a corpus and normalized to probabilities.

#### 3. üîç Hidden Markov Models (HMMs)  
- HMMs have hidden states (POS tags) and observable outputs (words).  
- Transition probabilities model the likelihood of moving between hidden states (tags).  
- Emission probabilities model the likelihood of a word being generated from a particular POS tag.  
- The goal is to find the most likely sequence of hidden states given the observed words.

#### 4. üìä Transition and Emission Probabilities  
- Transition probability formula: \( P(tag_i | tag_{i-1}) = \frac{Count(tag_{i-1}, tag_i)}{Count(tag_{i-1})} \)  
- Emission probability formula: \( P(word | tag) = \frac{Count(tag, word)}{Count(tag)} \)  
- Smoothing (adding a small Œµ) is used to avoid zero probabilities for unseen tag or word pairs.

#### 5. üß© Viterbi Algorithm  
- The Viterbi algorithm finds the most probable sequence of POS tags for a given sentence.  
- It consists of three steps: initialization, forward pass, and backward pass.  
- Uses dynamic programming to efficiently compute probabilities over all possible tag sequences.  
- Log probabilities are used in implementation to prevent numerical underflow.

#### 6. üìö Corpus Preparation and Matrix Population  
- Sentences are prepended with a start token `<s>` to mark sentence beginnings.  
- Transition and emission matrices are populated by counting occurrences in the tagged corpus.  
- Example corpus: Ezra Pound‚Äôs *In a Station of the Metro* used for demonstration.  
- Smoothing is applied to the counts before converting to probabilities.