## 2.2 Part of Speech Tagging and Hidden Markov Models

[Study Notes](#study-notes)

[Questions](#questions)



### Key Points



#### 1. üè∑Ô∏è Part of Speech (POS) Tagging  
- POS tagging assigns grammatical categories (tags) like noun, verb, determiner, adverb, punctuation to each word in a sentence.  
- Common POS tags include NN (noun), VB (verb), DT (determiner), WRB (wh-adverb).  
- POS tagging is essential for NLP tasks such as named entity recognition, speech recognition, and co-reference resolution.

#### 2. üîÑ Markov Chains in POS Tagging  
- POS tags are modeled as states in a Markov chain.  
- Transition probabilities represent the likelihood of one POS tag following another.  
- Initial probabilities represent the likelihood of a POS tag starting a sentence.  
- Transition matrices are constructed from counts of tag pairs in a corpus and normalized to probabilities.

#### 3. üîç Hidden Markov Models (HMMs)  
- HMMs have hidden states (POS tags) and observable outputs (words).  
- Transition probabilities model the likelihood of moving between hidden states (tags).  
- Emission probabilities model the likelihood of a word being generated from a particular POS tag.  
- The goal is to find the most likely sequence of hidden states given the observed words.

#### 4. üìä Transition and Emission Probabilities  
- Transition probability formula: \( P(tag_i | tag_{i-1}) = \frac{Count(tag_{i-1}, tag_i)}{Count(tag_{i-1})} \)  
- Emission probability formula: \( P(word | tag) = \frac{Count(tag, word)}{Count(tag)} \)  
- Smoothing (adding a small Œµ) is used to avoid zero probabilities for unseen tag or word pairs.

#### 5. üß© Viterbi Algorithm  
- The Viterbi algorithm finds the most probable sequence of POS tags for a given sentence.  
- It consists of three steps: initialization, forward pass, and backward pass.  
- Uses dynamic programming to efficiently compute probabilities over all possible tag sequences.  
- Log probabilities are used in implementation to prevent numerical underflow.

#### 6. üìö Corpus Preparation and Matrix Population  
- Sentences are prepended with a start token `<s>` to mark sentence beginnings.  
- Transition and emission matrices are populated by counting occurrences in the tagged corpus.  
- Example corpus: Ezra Pound‚Äôs *In a Station of the Metro* used for demonstration.  
- Smoothing is applied to the counts before converting to probabilities.



<br>

## Study Notes





### 1. üè∑Ô∏è What is Part of Speech (POS) Tagging?

Part of Speech tagging is a fundamental task in natural language processing (NLP) where each word in a sentence is assigned a label that indicates its grammatical role. These labels are called **part of speech tags** and include categories like noun, verb, adjective, adverb, determiner, punctuation, and more.

#### Why is POS tagging important?

- It helps computers understand the structure and meaning of sentences.
- It is a stepping stone for more complex NLP tasks such as named entity recognition, speech recognition, and co-reference resolution.
- For example, in the sentence "Why not learn something?", the word "learn" is tagged as a verb, and "something" as a noun.

#### Examples of POS tags:

- **Noun (NN):** something, nothing
- **Verb (VB):** learn, study
- **Determiner (DT):** the, a
- **Wh-adverb (WRB):** why, where
- **Punctuation:** sentence closers like periods or question marks

POS tagging is not just about labeling words but understanding their role in context, which can be ambiguous. For example, "learn" is usually a verb, but in some contexts, words can have multiple possible tags.


### 2. üîÑ Markov Chains and POS Tagging

To model the sequence of POS tags in a sentence, we use **Markov chains**. A Markov chain is a mathematical system that undergoes transitions from one state to another, where the probability of each state depends only on the previous state (this is called the Markov property).

#### How does this relate to POS tagging?

- Each POS tag is considered a **state** in the Markov chain.
- The sequence of tags in a sentence forms a path through these states.
- The probability of a tag depends on the tag that came before it (transition probability).

For example, after a determiner (like "the"), a noun is more likely than a verb. So, the transition probability from DT to NN might be high.

#### Transition matrix

This is a table that shows the probabilities of moving from one POS tag to another. For example:

| From \ To | NN (noun) | VB (verb) | O (other) |
|-----------|-----------|-----------|-----------|
| NN        | 0.2       | 0.2       | 0.6       |
| VB        | 0.4       | 0.3       | 0.3       |
| O         | 0.2       | 0.3       | 0.5       |

- Each row sums to 1 (or close to it).
- These probabilities are learned from a corpus (a large collection of text).

#### Initial probabilities

These represent the likelihood of a tag starting a sentence. For example:

- NN: 0.4
- VB: 0.1
- O: 0.5

This means sentences are more likely to start with a noun or other tags than a verb.


### 3. üîç Hidden Markov Models (HMMs)

A **Hidden Markov Model** is an extension of a Markov chain where the states are **hidden** (not directly observable), but we observe some output that depends probabilistically on the state.

#### How does this apply to POS tagging?

- The **hidden states** are the POS tags (e.g., noun, verb).
- The **observations** are the actual words in the sentence.
- We want to find the most likely sequence of hidden states (POS tags) given the observed words.

#### Components of an HMM:

1. **States:** POS tags (NN, VB, O, etc.)
2. **Observations:** Words in the sentence ("learn", "something", etc.)
3. **Transition probabilities:** Probability of moving from one POS tag to another.
4. **Emission probabilities:** Probability of a word being generated from a particular POS tag.

For example, the emission probability might tell us that the word "eat" is more likely to be emitted by a verb state than a noun state.


### 4. üìä Transition and Emission Probabilities

#### Transition probabilities

These are calculated by counting how often one tag follows another in a training corpus and then normalizing:

\[
P(\text{tag}_i | \text{tag}_{i-1}) = \frac{\text{Count}(\text{tag}_{i-1}, \text{tag}_i)}{\text{Count}(\text{tag}_{i-1})}
\]

For example, if "You" is tagged as a pronoun and "eat" as a verb, and "You eat" occurs 2 times out of 3 occurrences of "You", then:

\[
P(\text{eat} | \text{You}) = \frac{2}{3}
\]

#### Emission probabilities

These are the probabilities of a word being generated by a particular tag:

\[
P(\text{word} | \text{tag}) = \frac{\text{Count}(\text{tag}, \text{word})}{\text{Count}(\text{tag})}
\]

For example, the probability that the word "eat" is emitted by the verb tag might be 0.5.

#### Smoothing

Sometimes, certain tag transitions or emissions never appear in the training data, leading to zero probabilities. To avoid this, **smoothing** techniques add a small value (Œµ) to counts to ensure no zero probabilities, which helps the model generalize better.


### 5. üß© The Viterbi Algorithm: Finding the Best POS Tag Sequence

The Viterbi algorithm is a dynamic programming method used to find the most likely sequence of hidden states (POS tags) given a sequence of observed words.

#### Why do we need it?

- There are many possible tag sequences for a sentence.
- We want the sequence with the highest overall probability considering both transition and emission probabilities.

#### How does it work?

The algorithm proceeds in three main steps:

1. **Initialization:** Set up the initial probabilities for the first word with each possible tag.
2. **Forward pass:** For each subsequent word, calculate the highest probability of each tag by considering all possible previous tags and multiplying by transition and emission probabilities.
3. **Backward pass:** After processing all words, backtrack to find the sequence of tags that led to the highest probability.

#### Example:

For the sentence "Why not learn something?", the algorithm will:

- Start with initial probabilities for "Why".
- For "not", calculate probabilities for each tag based on "Why".
- Continue for "learn" and "something".
- Finally, backtrack to find the most probable tag sequence.

#### Implementation notes:

- Use **log probabilities** to avoid numerical underflow (very small numbers).
- Python indexing starts at 0, so be careful with array indices.


### 6. üìö Practical Example: Using Ezra Pound‚Äôs Poetry Corpus

To build the transition and emission matrices, a corpus is needed. The lecture uses Ezra Pound‚Äôs poem *In a Station of the Metro* as an example.

#### Steps:

- Add sentence start tokens `<s>` to mark beginnings.
- Tag each word with its POS.
- Count occurrences of tag pairs to build the transition matrix.
- Count occurrences of word-tag pairs to build the emission matrix.
- Apply smoothing to handle zero counts.

This process illustrates how real-world data is used to train an HMM for POS tagging.


### 7. üìù Summary and Key Takeaways

- **POS tagging** assigns grammatical categories to words, which is essential for understanding language.
- **Markov chains** model the sequence of POS tags using transition probabilities.
- **Hidden Markov Models** add the concept of hidden states (tags) and observable outputs (words), with emission probabilities linking them.
- **Transition and emission matrices** are learned from a tagged corpus.
- **Smoothing** prevents zero probabilities in these matrices.
- The **Viterbi algorithm** efficiently finds the most likely tag sequence for a given sentence.
- This framework is foundational for many NLP applications like named entity recognition and speech recognition.



<br>

## Questions



#### 1. What is the primary goal of Part of Speech (POS) tagging in natural language processing?  
A) To assign a grammatical category to each word in a sentence  
B) To translate sentences from one language to another  
C) To identify named entities like people and places  
D) To generate new sentences based on grammar rules  

#### 2. Which of the following are typical POS tags used in tagging?  
A) Noun  
B) Verb  
C) Determiner  
D) Sentiment  

#### 3. Why is it insufficient to tag words solely based on their dictionary definitions?  
A) Because words can have multiple possible tags depending on context  
B) Because dictionaries do not contain all words  
C) Because POS tagging requires semantic understanding, not just syntax  
D) Because tagging depends on the frequency of words in a corpus  

#### 4. In a Markov chain model for POS tagging, what does the "state" represent?  
A) A word in the sentence  
B) A POS tag  
C) A transition probability  
D) An emission probability  

#### 5. What does the Markov property imply in the context of POS tagging?  
A) The probability of a tag depends only on the previous tag  
B) The probability of a word depends on all previous words  
C) The probability of a tag depends on the entire sentence  
D) The probability of a word depends only on the current tag  

#### 6. Which of the following best describes the transition matrix in POS tagging?  
A) It contains probabilities of words given tags  
B) It contains probabilities of moving from one tag to another  
C) It contains probabilities of tags starting a sentence  
D) It contains probabilities of words following other words  

#### 7. What is the role of initial probabilities (œÄ) in a Markov model for POS tagging?  
A) To represent the likelihood of a word being a noun  
B) To represent the likelihood of a tag starting a sentence  
C) To represent the likelihood of a tag following another tag  
D) To represent the likelihood of a word being emitted by a tag  

#### 8. In a Hidden Markov Model (HMM), what is "hidden"?  
A) The observed words  
B) The POS tags  
C) The transition probabilities  
D) The emission probabilities  

#### 9. What are emission probabilities in an HMM for POS tagging?  
A) Probabilities of transitioning from one tag to another  
B) Probabilities of a tag generating a particular word  
C) Probabilities of a word starting a sentence  
D) Probabilities of a tag being the first tag in a sentence  

#### 10. Why is smoothing necessary when calculating transition and emission probabilities?  
A) To increase the probability of frequent events  
B) To avoid zero probabilities for unseen tag or word pairs  
C) To speed up the Viterbi algorithm  
D) To reduce the size of the transition matrix  

#### 11. Consider the sentence "Why not learn something?" Which of the following statements is true regarding POS tagging?  
A) "Why" is always tagged as a noun  
B) "Learn" can be tagged as a verb or noun depending on context  
C) "Something" is typically tagged as a noun  
D) "Not" is usually tagged as an adverb  

#### 12. How does the Viterbi algorithm find the most likely sequence of POS tags?  
A) By enumerating all possible tag sequences and selecting the best  
B) By using dynamic programming to efficiently compute the best path  
C) By randomly guessing tags and checking probabilities  
D) By only considering emission probabilities  

#### 13. Which of the following are steps in the Viterbi algorithm?  
A) Initialization  
B) Forward pass  
C) Backward pass  
D) Emission smoothing  

#### 14. Why are log probabilities used in implementations of the Viterbi algorithm?  
A) To simplify multiplication into addition and avoid numerical underflow  
B) To make probabilities easier to interpret  
C) To speed up matrix multiplication  
D) To avoid zero probabilities  

#### 15. If a transition probability from tag A to tag B is zero in the training data, what problem might arise without smoothing?  
A) The model will assign zero probability to any sequence containing that transition  
B) The model will ignore that transition and choose randomly  
C) The model will overestimate the probability of that transition  
D) The model will crash during decoding  

#### 16. Which of the following best describes the difference between Markov chains and Hidden Markov Models in POS tagging?  
A) Markov chains model observed words, HMMs model hidden tags  
B) Markov chains have observable states, HMMs have hidden states and observable outputs  
C) Markov chains use emission probabilities, HMMs do not  
D) HMMs assume independence between tags, Markov chains do not  

#### 17. When building the transition matrix from a corpus, what data is counted?  
A) The number of times a word appears in the corpus  
B) The number of times a tag follows another tag  
C) The number of times a word is emitted by a tag  
D) The number of sentences in the corpus  

#### 18. Which of the following statements about emission probabilities is correct?  
A) They depend on the frequency of a word given a tag in the training data  
B) They are always equal for all words under a given tag  
C) They represent the probability of a tag following another tag  
D) They are irrelevant in the Viterbi algorithm  

#### 19. In the context of POS tagging, what does the term "observable" refer to in an HMM?  
A) The POS tags  
B) The words in the sentence  
C) The transition probabilities  
D) The initial probabilities  

#### 20. Which of the following challenges can arise when using HMMs for POS tagging?  
A) Ambiguity in word tagging due to multiple possible tags  
B) Zero probabilities for unseen word-tag or tag-tag pairs  
C) Computational inefficiency in finding the best tag sequence  
D) The inability to model long-range dependencies beyond adjacent tags  



<br>

## Answers



#### 1. What is the primary goal of Part of Speech (POS) tagging in natural language processing?  
A) ‚úì Assign a grammatical category to each word in a sentence  
B) ‚úó Translation is a different task  
C) ‚úó Named entity recognition is separate from POS tagging  
D) ‚úó Sentence generation is unrelated  

**Correct:** A


#### 2. Which of the following are typical POS tags used in tagging?  
A) ‚úì Noun is a common POS tag  
B) ‚úì Verb is a common POS tag  
C) ‚úì Determiner is a common POS tag  
D) ‚úó Sentiment is not a POS tag, but a separate NLP task  

**Correct:** A,B,C


#### 3. Why is it insufficient to tag words solely based on their dictionary definitions?  
A) ‚úì Words can have multiple tags depending on context  
B) ‚úó Dictionary coverage is not the main issue here  
C) ‚úó POS tagging focuses on syntax, not deep semantics  
D) ‚úó Frequency alone does not solve ambiguity  

**Correct:** A


#### 4. In a Markov chain model for POS tagging, what does the "state" represent?  
A) ‚úó Words are observations, not states  
B) ‚úì States correspond to POS tags  
C) ‚úó Transition probabilities are parameters, not states  
D) ‚úó Emission probabilities are parameters, not states  

**Correct:** B


#### 5. What does the Markov property imply in the context of POS tagging?  
A) ‚úì The next tag depends only on the previous tag  
B) ‚úó Words do not depend on all previous words in Markov assumption  
C) ‚úó The entire sentence context is not considered in Markov models  
D) ‚úó Word depends on tag, not vice versa  

**Correct:** A


#### 6. Which of the following best describes the transition matrix in POS tagging?  
A) ‚úó This describes emission probabilities  
B) ‚úì Transition matrix contains probabilities of tag-to-tag transitions  
C) ‚úó Initial probabilities are separate from transition matrix  
D) ‚úó Word-to-word probabilities are not modeled here  

**Correct:** B


#### 7. What is the role of initial probabilities (œÄ) in a Markov model for POS tagging?  
A) ‚úó Initial probabilities relate to tags, not word categories  
B) ‚úì They represent likelihood of tags starting a sentence  
C) ‚úó Transition probabilities relate to tag-to-tag transitions, not initial  
D) ‚úó Emission probabilities relate words to tags, not initial tag likelihood  

**Correct:** B


#### 8. In a Hidden Markov Model (HMM), what is "hidden"?  
A) ‚úó Observed words are visible, not hidden  
B) ‚úì POS tags are hidden states  
C) ‚úó Transition probabilities are parameters, not hidden states  
D) ‚úó Emission probabilities are parameters, not hidden states  

**Correct:** B


#### 9. What are emission probabilities in an HMM for POS tagging?  
A) ‚úó Transition probabilities describe tag-to-tag moves  
B) ‚úì Emission probabilities describe likelihood of a word given a tag  
C) ‚úó Starting word probabilities are initial probabilities  
D) ‚úó Initial tag probabilities are separate from emission  

**Correct:** B


#### 10. Why is smoothing necessary when calculating transition and emission probabilities?  
A) ‚úó Smoothing does not increase frequent event probabilities  
B) ‚úì To avoid zero probabilities for unseen pairs  
C) ‚úó Smoothing does not affect algorithm speed directly  
D) ‚úó Smoothing does not reduce matrix size  

**Correct:** B


#### 11. Consider the sentence "Why not learn something?" Which of the following statements is true regarding POS tagging?  
A) ‚úó "Why" is usually tagged as a wh-adverb (WRB), not noun  
B) ‚úì "Learn" can be verb or noun depending on context  
C) ‚úì "Something" is typically tagged as a noun  
D) ‚úì "Not" is usually tagged as an adverb  

**Correct:** B,C,D


#### 12. How does the Viterbi algorithm find the most likely sequence of POS tags?  
A) ‚úó Enumerating all sequences is computationally infeasible  
B) ‚úì Uses dynamic programming to efficiently find best path  
C) ‚úó It does not guess randomly  
D) ‚úó It considers both transition and emission probabilities  

**Correct:** B


#### 13. Which of the following are steps in the Viterbi algorithm?  
A) ‚úì Initialization is the first step  
B) ‚úì Forward pass computes probabilities stepwise  
C) ‚úì Backward pass recovers best path  
D) ‚úó Emission smoothing is not a Viterbi step  

**Correct:** A,B,C


#### 14. Why are log probabilities used in implementations of the Viterbi algorithm?  
A) ‚úì To convert multiplication into addition and avoid underflow  
B) ‚úó Log probabilities are harder to interpret directly  
C) ‚úó Logarithms do not speed up matrix multiplication  
D) ‚úó Log probabilities do not prevent zero probabilities  

**Correct:** A


#### 15. If a transition probability from tag A to tag B is zero in the training data, what problem might arise without smoothing?  
A) ‚úì The model assigns zero probability to sequences with that transition  
B) ‚úó The model does not ignore transitions; zero probability is fatal  
C) ‚úó Zero counts do not cause overestimation  
D) ‚úó The model usually does not crash but produces zero likelihoods  

**Correct:** A


#### 16. Which of the following best describes the difference between Markov chains and Hidden Markov Models in POS tagging?  
A) ‚úó Markov chains model tags, not words; HMMs model hidden tags and observed words  
B) ‚úì Markov chains have observable states; HMMs have hidden states and observable outputs  
C) ‚úó Both use emission probabilities; Markov chains do not use emissions explicitly  
D) ‚úó HMMs assume Markov property; both model tag dependencies similarly  

**Correct:** B


#### 17. When building the transition matrix from a corpus, what data is counted?  
A) ‚úó Word frequency alone is not enough  
B) ‚úì Counts of tag pairs (tag following another tag)  
C) ‚úó Emission counts are separate  
D) ‚úó Number of sentences is unrelated to transition counts  

**Correct:** B


#### 18. Which of the following statements about emission probabilities is correct?  
A) ‚úì They depend on frequency of word given tag in training data  
B) ‚úó Emission probabilities vary by word, not equal for all words under a tag  
C) ‚úó Tag-to-tag probabilities are transition probabilities  
D) ‚úó Emission probabilities are essential in Viterbi algorithm  

**Correct:** A


#### 19. In the context of POS tagging, what does the term "observable" refer to in an HMM?  
A) ‚úó POS tags are hidden states  
B) ‚úì Words in the sentence are observable outputs  
C) ‚úó Transition probabilities are parameters, not observations  
D) ‚úó Initial probabilities are parameters, not observations  

**Correct:** B


#### 20. Which of the following challenges can arise when using HMMs for POS tagging?  
A) ‚úì Ambiguity due to multiple possible tags for words  
B) ‚úì Zero probabilities for unseen pairs without smoothing  
C) ‚úó Viterbi algorithm is efficient, not computationally prohibitive  
D) ‚úì HMMs model only adjacent tag dependencies, not long-range  

**Correct:** A,B,D

