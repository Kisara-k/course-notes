## 4.3 Transfer Learning and Fine-Tuning

## Questions

#### 1. What is the primary advantage of transfer learning in NLP?  
A) It eliminates the need for any labeled data in downstream tasks  
B) It reduces training time by leveraging pre-trained models  
C) It guarantees perfect accuracy on all downstream tasks  
D) It allows models to learn from both labeled and unlabeled data  

#### 2. Which of the following best describes the Masked Language Modeling (MLM) task used in BERT pre-training?  
A) Predicting the next word in a sequence given all previous words  
B) Predicting randomly masked tokens in a sentence using context from both directions  
C) Predicting whether two sentences are consecutive in a text  
D) Predicting the sentiment of a sentence based on masked words  

#### 3. Why does BERT use a bi-directional Transformer encoder instead of a uni-directional model like GPT?  
A) To allow the model to attend to both left and right context simultaneously  
B) To reduce the number of parameters in the model  
C) To enable the model to generate text more fluently  
D) To restrict the model to only past context for better prediction  

#### 4. In BERT pre-training, what percentage of tokens are typically selected for masking, and how are they treated?  
A) 15% of tokens are selected; 80% replaced with [MASK], 10% replaced with random tokens, 10% unchanged  
B) 50% of tokens are selected; all replaced with [MASK] tokens  
C) 15% of tokens are selected; all replaced with random tokens  
D) 10% of tokens are selected; 50% replaced with [MASK], 50% unchanged  

#### 5. Which of the following statements about Next Sentence Prediction (NSP) in BERT is true?  
A) NSP helps the model understand relationships between sentences  
B) NSP is used to predict masked words within a sentence  
C) NSP is a binary classification task predicting if sentence B follows sentence A  
D) NSP is only used during fine-tuning, not pre-training  

#### 6. How does T5 differ from BERT in terms of task formulation?  
A) T5 treats all NLP tasks as text-to-text problems  
B) BERT uses an encoder-decoder architecture, while T5 uses only an encoder  
C) T5 uses multi-task training, while BERT is trained on a single task  
D) BERT can only perform classification tasks, while T5 can only perform generation tasks  

#### 7. Which of the following are true about fine-tuning a pre-trained model?  
A) Fine-tuning always involves training the entire model from scratch  
B) Fine-tuning adapts the model to a specific downstream task using labeled data  
C) Fine-tuning can involve adding task-specific output layers  
D) Fine-tuning is unnecessary if the pre-trained model is large enough  

#### 8. What is the role of the special tokens [CLS] and [SEP] in BERT inputs?  
A) [CLS] is used to separate sentences, [SEP] marks the start of the input  
B) [CLS] is a classification token added at the start, [SEP] separates sentences  
C) Both tokens are used to mask words during pre-training  
D) They are only used during fine-tuning, not pre-training  

#### 9. Which of the following best describes the difference between feature-based transfer and fine-tuning?  
A) Feature-based transfer uses pre-trained model outputs as fixed features without updating model weights  
B) Fine-tuning updates the entire pre-trained model weights on the downstream task  
C) Feature-based transfer requires labeled data for pre-training  
D) Fine-tuning always freezes the pre-trained model layers  

#### 10. Why is self-supervised learning important in pre-training large language models?  
A) It allows models to learn from unlabeled data by creating their own prediction tasks  
B) It requires large amounts of labeled data to work effectively  
C) It is only used for training on downstream tasks  
D) It prevents the model from overfitting on small datasets  

#### 11. Which of the following statements about the Transformer architecture are correct?  
A) Transformers use attention mechanisms to weigh the importance of different words in a sequence  
B) Transformers rely on recurrent neural networks to process sequences sequentially  
C) Positional embeddings are used to encode the order of tokens in the input  
D) Transformers cannot process sequences longer than their fixed window size  

#### 12. What is a key difference between GPT and BERT in terms of context usage?  
A) GPT uses bi-directional context, BERT uses uni-directional context  
B) GPT uses uni-directional (left-to-right) context, BERT uses bi-directional context  
C) Both GPT and BERT use uni-directional context but differ in architecture  
D) GPT uses no context, BERT uses full context  

#### 13. In multi-task training with T5, how does the model know which task to perform?  
A) By using different model architectures for each task  
B) By prefixing the input text with a task-specific prompt (e.g., "translate:", "summarize:")  
C) By training separate models for each task and selecting the appropriate one at inference  
D) By using different tokenizers for each task  

#### 14. Which of the following are challenges or considerations when fine-tuning large pre-trained models?  
A) Risk of catastrophic forgetting if the model forgets pre-trained knowledge  
B) Need for large labeled datasets to fine-tune effectively  
C) Choosing which layers to freeze or unfreeze during training  
D) Fine-tuning always improves performance regardless of dataset size  

#### 15. How does Hugging Face simplify working with transformer models?  
A) By providing pre-trained model checkpoints and tokenizers ready to use  
B) By requiring users to implement all training code from scratch  
C) By offering pipelines that handle preprocessing, inference, and postprocessing  
D) By limiting models to only a few NLP tasks  

#### 16. Which of the following are true about the Stanford Question Answering Dataset (SQuAD) in the context of fine-tuning?  
A) It provides labeled question-answer pairs for training QA models  
B) It is used for pre-training language models like BERT  
C) Fine-tuning on SQuAD involves predicting answer spans within a passage  
D) SQuAD is an unlabeled dataset used for self-supervised learning  

#### 17. What is the purpose of adapter layers in fine-tuning large models?  
A) To add small trainable modules that reduce the number of parameters updated during fine-tuning  
B) To replace the entire pre-trained model with a smaller one  
C) To freeze all layers and only train the adapter layers for efficiency  
D) To increase the model size significantly for better performance  

#### 18. Which of the following statements about the GLUE benchmark are correct?  
A) GLUE is a collection of diverse NLP tasks used to evaluate general language understanding  
B) GLUE datasets are used primarily for pre-training large language models  
C) GLUE includes tasks like sentiment analysis, paraphrase detection, and entailment  
D) GLUE is model-specific and cannot be used to compare different architectures  

#### 19. How does temperature-scaled mixing affect multi-task training data sampling?  
A) It adjusts the probability of sampling from each dataset to balance training  
B) It always samples equally from all datasets regardless of size  
C) It increases the sampling probability of smaller datasets to prevent underfitting  
D) It decreases the sampling probability of larger datasets to speed up training  

#### 20. Which of the following best describe the input embeddings used in BERT?  
A) Token embeddings represent the meaning of each word or subword  
B) Segment embeddings distinguish between different sentences in the input  
C) Position embeddings encode the order of tokens in the sequence  
D) Embeddings are only used during fine-tuning, not pre-training



<br>

## Answers

#### 1. What is the primary advantage of transfer learning in NLP?  
A) ✗ It does not eliminate the need for labeled data in downstream tasks; fine-tuning still requires labeled data.  
B) ✓ It reduces training time by leveraging knowledge from pre-trained models.  
C) ✗ It does not guarantee perfect accuracy; performance depends on task and data.  
D) ✓ It allows models to learn from both labeled (fine-tuning) and unlabeled (pre-training) data.  

**Correct:** B, D


#### 2. Which of the following best describes the Masked Language Modeling (MLM) task used in BERT pre-training?  
A) ✗ This describes a uni-directional language model like GPT, not MLM.  
B) ✓ MLM predicts masked tokens using context from both left and right (bi-directional).  
C) ✗ This describes Next Sentence Prediction, not MLM.  
D) ✗ MLM is about predicting masked words, not sentiment.  

**Correct:** B


#### 3. Why does BERT use a bi-directional Transformer encoder instead of a uni-directional model like GPT?  
A) ✓ Bi-directional context allows BERT to use information from both sides of a token.  
B) ✗ BERT has more parameters, not fewer.  
C) ✗ GPT is better at text generation; BERT focuses on understanding.  
D) ✗ BERT does not restrict context to past words; it uses full context.  

**Correct:** A


#### 4. In BERT pre-training, what percentage of tokens are typically selected for masking, and how are they treated?  
A) ✓ Correct percentages and treatment as per BERT’s masking strategy.  
B) ✗ 50% is too high; BERT uses 15%.  
C) ✗ All replaced with random tokens is incorrect; only 10% replaced randomly.  
D) ✗ Incorrect percentages and treatment.  

**Correct:** A


#### 5. Which of the following statements about Next Sentence Prediction (NSP) in BERT is true?  
A) ✓ NSP helps model understand sentence relationships.  
B) ✗ NSP is not about masked words; that’s MLM.  
C) ✓ NSP is a binary classification task predicting if sentence B follows sentence A.  
D) ✗ NSP is used during pre-training, not only fine-tuning.  

**Correct:** A, C


#### 6. How does T5 differ from BERT in terms of task formulation?  
A) ✓ T5 frames all tasks as text-to-text problems.  
B) ✗ BERT uses only an encoder; T5 uses encoder-decoder.  
C) ✓ T5 is trained on multiple tasks simultaneously; BERT is pre-trained on MLM and NSP only.  
D) ✗ BERT can perform generation tasks (e.g., masked token prediction), and T5 can do classification too.  

**Correct:** A, C


#### 7. Which of the following are true about fine-tuning a pre-trained model?  
A) ✗ Fine-tuning does not train from scratch; it starts from pre-trained weights.  
B) ✓ Fine-tuning adapts the model to a specific task using labeled data.  
C) ✓ Adding task-specific output layers is common in fine-tuning.  
D) ✗ Fine-tuning is necessary to adapt the model to new tasks.  

**Correct:** B, C


#### 8. What is the role of the special tokens [CLS] and [SEP] in BERT inputs?  
A) ✗ Roles are reversed; [CLS] is not a separator.  
B) ✓ [CLS] is a classification token at the start; [SEP] separates sentences.  
C) ✗ They are not used for masking.  
D) ✗ Used in both pre-training and fine-tuning.  

**Correct:** B


#### 9. Which of the following best describes the difference between feature-based transfer and fine-tuning?  
A) ✓ Feature-based transfer uses fixed features from pre-trained models without updating weights.  
B) ✓ Fine-tuning updates the entire model weights on the downstream task.  
C) ✗ Feature-based transfer does not require labeled data for pre-training; pre-training is usually self-supervised.  
D) ✗ Fine-tuning may freeze some layers but often updates many or all layers.  

**Correct:** A, B


#### 10. Why is self-supervised learning important in pre-training large language models?  
A) ✓ It enables learning from unlabeled data by creating prediction tasks from the data itself.  
B) ✗ It does not require large labeled datasets.  
C) ✗ It is primarily used during pre-training, not downstream tasks.  
D) ✗ While it helps generalization, preventing overfitting is not its main purpose.  

**Correct:** A


#### 11. Which of the following statements about the Transformer architecture are correct?  
A) ✓ Attention mechanisms weigh the importance of different tokens.  
B) ✗ Transformers do not use RNNs; they process sequences in parallel.  
C) ✓ Positional embeddings encode token order since Transformers lack recurrence.  
D) ✗ Transformers can process sequences longer than fixed windows using attention.  

**Correct:** A, C


#### 12. What is a key difference between GPT and BERT in terms of context usage?  
A) ✗ GPT is uni-directional, not bi-directional.  
B) ✓ GPT uses uni-directional (left-to-right) context; BERT uses bi-directional context.  
C) ✗ BERT uses bi-directional context, not uni-directional.  
D) ✗ GPT uses context; it’s not context-free.  

**Correct:** B


#### 13. In multi-task training with T5, how does the model know which task to perform?  
A) ✗ T5 uses the same architecture for all tasks.  
B) ✓ T5 uses task-specific prefixes in the input text to indicate the task.  
C) ✗ T5 uses a single model for all tasks, not separate models.  
D) ✗ Tokenizers are generally shared, not task-specific.  

**Correct:** B


#### 14. Which of the following are challenges or considerations when fine-tuning large pre-trained models?  
A) ✓ Catastrophic forgetting can occur if fine-tuning overwrites pre-trained knowledge.  
B) ✗ Fine-tuning can work with small datasets due to transfer learning.  
C) ✓ Deciding which layers to freeze or unfreeze affects training efficiency and performance.  
D) ✗ Fine-tuning does not always improve performance; it depends on data and task.  

**Correct:** A, C


#### 15. How does Hugging Face simplify working with transformer models?  
A) ✓ Provides pre-trained models and tokenizers ready to use.  
B) ✗ It provides high-level APIs, so users don’t need to implement training from scratch.  
C) ✓ Pipelines automate preprocessing, inference, and postprocessing.  
D) ✗ Supports a wide range of NLP tasks, not limited to a few.  

**Correct:** A, C


#### 16. Which of the following are true about the Stanford Question Answering Dataset (SQuAD) in the context of fine-tuning?  
A) ✓ SQuAD provides labeled question-answer pairs for training QA models.  
B) ✗ SQuAD is not used for pre-training; it is a downstream fine-tuning dataset.  
C) ✓ Fine-tuning on SQuAD involves predicting answer spans within passages.  
D) ✗ SQuAD is labeled, not unlabeled.  

**Correct:** A, C


#### 17. What is the purpose of adapter layers in fine-tuning large models?  
A) ✓ Adapter layers are small trainable modules that reduce the number of parameters updated.  
B) ✗ They do not replace the entire model.  
C) ✓ Adapter layers allow freezing most of the model while training only adapters for efficiency.  
D) ✗ Adapter layers do not increase model size significantly; they are lightweight.  

**Correct:** A, C


#### 18. Which of the following statements about the GLUE benchmark are correct?  
A) ✓ GLUE is a collection of diverse NLP tasks for evaluating language understanding.  
B) ✗ GLUE is used for evaluation, not pre-training.  
C) ✓ GLUE includes tasks like sentiment analysis, paraphrase detection, and entailment.  
D) ✗ GLUE is model-agnostic and used to compare different architectures.  

**Correct:** A, C, D


#### 19. How does temperature-scaled mixing affect multi-task training data sampling?  
A) ✓ It adjusts sampling probabilities to balance training across datasets.  
B) ✗ Equal sampling ignores dataset size differences.  
C) ✓ It can increase sampling probability of smaller datasets to avoid underfitting.  
D) ✗ It does not necessarily decrease sampling of larger datasets to speed training.  

**Correct:** A, C


#### 20. Which of the following best describe the input embeddings used in BERT?  
A) ✓ Token embeddings represent word or subword meanings.  
B) ✓ Segment embeddings distinguish between sentences in input pairs.  
C) ✓ Position embeddings encode token order in the sequence.  
D) ✗ Embeddings are used during both pre-training and fine-tuning.  

**Correct:** A, B, C