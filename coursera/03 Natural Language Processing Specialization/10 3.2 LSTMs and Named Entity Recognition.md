## 3.2 LSTMs and Named Entity Recognition

[Study Notes](#study-notes)

[Questions](#questions)



### Key Points



#### 1. ü§ñ RNNs and Vanishing/Exploding Gradients  
- RNNs capture short-range dependencies but struggle with long-term dependencies.  
- Vanishing gradients occur when partial derivatives < 1 cause gradients to shrink exponentially during backpropagation through time.  
- Exploding gradients occur when partial derivatives > 1 cause gradients to grow exponentially during backpropagation.  
- Gradient clipping, identity RNNs with ReLU, and skip connections are common solutions to vanishing/exploding gradients.

#### 2. üß† LSTM Architecture and Gates  
- LSTMs have a cell state and a hidden state to carry information through time steps.  
- LSTMs use three gates: Forget gate (decides what to discard), Input gate (decides what new information to add), and Output gate (decides what to output).  
- Gates use sigmoid activation to output values between 0 (closed) and 1 (open).  
- The cell state is updated by combining the forget gate and input gate outputs with candidate cell state information.  
- LSTMs help avoid vanishing/exploding gradients by controlling information flow through gates.

#### 3. üè∑Ô∏è Named Entity Recognition (NER)  
- NER locates and extracts predefined entities such as places, organizations, people, times, and artifacts from text.  
- Entities are labeled using tags like B-per (person), B-geo (geographical), B-tim (time), and O (outside any entity).  
- Example: "Sharon flew to Miami last Friday." ‚Üí Sharon: B-per, Miami: B-geo, last Friday: B-tim.

#### 4. üßÆ Data Processing for NER Training  
- Words and entity classes are converted into numerical arrays (tokenization).  
- All sequences are padded to the same length using a <PAD> token for batch processing in LSTMs.  
- Training batches typically contain 64, 128, 256, or 512 sequences.  
- The model architecture includes an embedding layer, an LSTM layer, a dense layer, and a log softmax activation for classification.

#### 5. ‚úÖ Evaluating NER Models  
- Accuracy is computed by masking padded tokens to exclude them from evaluation.  
- The predicted class for each token is obtained by taking the arg max of the model‚Äôs output probabilities.  
- Masked accuracy is calculated as the ratio of correct predictions to total non-padded tokens.



<br>

## Study Notes



### 1. ü§ñ Understanding RNNs and the Vanishing Gradient Problem

#### Introduction to RNNs

Recurrent Neural Networks (RNNs) are a type of neural network designed to handle sequential data, such as sentences or time series. Unlike traditional neural networks, RNNs have loops that allow information to persist, meaning they can remember previous inputs while processing new ones. This makes them useful for tasks where context matters, like language modeling or speech recognition.

#### Advantages of RNNs

- **Capturing short-range dependencies:** RNNs can remember information from recent steps in a sequence, which helps in understanding context within a short window.
- **Memory efficiency:** Compared to some other models like n-gram models, RNNs use less RAM because they don‚Äôt need to store huge tables of word combinations.

#### Disadvantages of RNNs

- **Struggle with long-term dependencies:** RNNs find it difficult to remember information from far back in the sequence. For example, if a word at the start of a sentence influences the meaning of a word much later, a basic RNN might miss that connection.
- **Vanishing and exploding gradients:** When training RNNs using backpropagation through time (BPTT), the gradients (which guide learning) can become very small (vanish) or very large (explode). This makes training unstable or ineffective.

#### Backpropagation Through Time (BPTT)

BPTT is the method used to train RNNs. It involves unrolling the network through time steps and calculating gradients at each step. The gradient at a particular time depends on the product of many partial derivatives from previous steps. The longer the gap between the current step and the step where the gradient originates, the more these products multiply.

- If these partial derivatives are less than 1, multiplying many of them causes the gradient to shrink exponentially ‚Äî this is the **vanishing gradient** problem.
- If they are greater than 1, the gradient grows exponentially ‚Äî this is the **exploding gradient** problem.

#### Solutions to Vanishing and Exploding Gradients

- **Identity RNN with ReLU activation:** Using ReLU (Rectified Linear Unit) activation and identity weight matrices can help maintain gradient magnitude.
- **Gradient clipping:** This technique limits the size of gradients during training to prevent them from exploding.
- **Skip connections:** These allow gradients to flow more directly across time steps, reducing the chance of vanishing.


### 2. üß† Long Short-Term Memory (LSTM) Networks: A Better RNN

#### What is an LSTM?

LSTMs are a special kind of RNN designed to solve the vanishing gradient problem and better capture long-term dependencies. They do this by introducing a more complex internal structure that controls the flow of information.

#### Anatomy of an LSTM Unit

An LSTM unit contains:

- **Cell state:** This is like a conveyor belt running through the sequence, carrying information along with minimal changes.
- **Hidden state:** This is the output at each time step, which also carries information forward.
- **Gates:** These are special neural network layers that decide what information to keep, add, or output. They use sigmoid activations to produce values between 0 (completely block) and 1 (completely allow).

#### The Three Gates in LSTM

1. **Forget Gate:** Decides what information from the cell state should be discarded because it‚Äôs no longer relevant.
2. **Input Gate:** Decides what new information should be added to the cell state.
3. **Output Gate:** Decides what information from the cell state should be output as the hidden state for the current step.

#### How LSTM Works Step-by-Step

- The **forget gate** looks at the previous hidden state and current input and outputs a number between 0 and 1 for each piece of information in the cell state, deciding what to keep or forget.
- The **input gate** decides which new information to add to the cell state, using a combination of sigmoid and tanh activations.
- The **cell state** is updated by forgetting some old information and adding new candidate information.
- The **output gate** decides what part of the updated cell state to output as the hidden state, which will be used in the next step and possibly as output.

#### Why LSTMs Work Better

Because of these gates, LSTMs can selectively remember or forget information, allowing gradients to flow more effectively during training. This helps them capture long-term dependencies without suffering from vanishing or exploding gradients.

#### Applications of LSTMs

LSTMs are widely used in:

- Next-character or word prediction
- Chatbots and conversational AI
- Music composition
- Image captioning
- Speech recognition


### 3. üè∑Ô∏è Named Entity Recognition (NER): Extracting Meaningful Entities from Text

#### What is Named Entity Recognition?

NER is a natural language processing (NLP) task that involves locating and classifying predefined entities in text into categories such as:

- **Places** (e.g., Thailand)
- **Organizations** (e.g., Google)
- **People** (e.g., Barack Obama)
- **Time indicators** (e.g., December)
- **Artifacts** (e.g., Egyptian statue)

NER helps computers understand and extract important information from unstructured text.

#### Example of NER Labeling

Consider the sentence:  
"Sharon flew to Miami last Friday."

The labeled entities might look like this:  
- Sharon ‚Üí B-per (Beginning of a person‚Äôs name)  
- Miami ‚Üí B-geo (Beginning of a geographical location)  
- last Friday ‚Üí B-tim (Beginning of a time expression)

Other words like "flew" and "to" are labeled as O (Outside any entity).

#### Applications of NER

- Improving search engine results by understanding query context
- Enhancing recommendation engines by identifying key entities
- Customer service automation by extracting relevant information
- Automatic trading by recognizing company names, dates, and events in news


### 4. üßÆ Training Named Entity Recognition Models with LSTMs

#### Data Processing for NER

Before training, text data and entity labels must be converted into numerical form:

- **Assign numbers to words:** Each unique word in the vocabulary is mapped to a unique integer.
- **Assign numbers to entity classes:** Each entity type (e.g., B-per, B-geo, O) is also mapped to a unique integer.

For example, the sentence "Sharon flew to Miami last Friday." might be converted to:  
`[4282, 853, 187, 5388, 2894, 7]` for words, and  
`[O, O, B-geo, B-tim, B-per]` for entities.

#### Token Padding

LSTMs require input sequences to be the same length. To handle sentences of varying lengths:

- Choose a fixed sequence length.
- Shorter sequences are padded with a special `<PAD>` token to fill empty spaces.
- Padding ensures consistent input size for batch processing.

#### Training Process

- Convert each sentence and its entity labels into fixed-length numerical arrays.
- Group these arrays into batches (e.g., 64, 128, 256 samples per batch) for efficient training.
- Feed batches into an LSTM layer.
- Pass the LSTM output through a dense (fully connected) layer.
- Use a log softmax activation to predict the probability distribution over entity classes for each token.

#### Model Architecture Example (TensorFlow)

A typical NER model might look like this:

```python
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim),
    tf.keras.layers.LSTM(units=hidden_units, return_sequences=True),
    tf.keras.layers.Dense(num_classes),
    tf.keras.layers.Activation('log_softmax')
])
```


### 5. ‚úÖ Evaluating NER Models: Accuracy and Masking

#### Why Masking is Important

Since sequences are padded, the model outputs predictions for padding tokens as well. These padding tokens do not correspond to real words and should not be counted when calculating accuracy.

#### Computing Accuracy with Masking

- Pass the test set through the model to get predictions.
- For each token, find the predicted class with the highest probability (arg max).
- Create a mask that ignores padded tokens.
- Compare predicted classes to true labels only where the mask is active.
- Calculate accuracy as the ratio of correct predictions to total non-padded tokens.

#### Example Python Function for Masked Accuracy

```python
def masked_accuracy(y_true, y_pred):
    mask = tf.cast(tf.not_equal(y_true, pad_token_id), tf.float32)  # Mask for non-padding tokens
    y_pred_class = tf.math.argmax(y_pred, axis=-1)
    matches_true_pred = tf.cast(tf.equal(y_true, y_pred_class), tf.float32)
    matches_true_pred *= mask
    masked_acc = tf.reduce_sum(matches_true_pred) / tf.reduce_sum(mask)
    return masked_acc
```


### Summary

- **RNNs** are powerful for sequential data but suffer from vanishing/exploding gradients, limiting their ability to learn long-term dependencies.
- **LSTMs** solve this by using gates to control information flow, enabling them to remember or forget information as needed.
- **Named Entity Recognition (NER)** extracts meaningful entities from text, which is useful in many real-world applications.
- Training NER models involves converting text and labels into numerical arrays, padding sequences, and using LSTMs followed by dense layers.
- Evaluating NER models requires masking padded tokens to get an accurate measure of performance.



<br>

## Questions



#### 1. What are the main reasons why basic RNNs struggle with long-term dependencies?  
A) They have limited memory capacity in hidden states  
B) Vanishing gradients cause earlier information to be lost during training  
C) Exploding gradients cause the model to overfit on long sequences  
D) They require excessive RAM to store long sequences  

#### 2. In backpropagation through time (BPTT), why does the gradient tend to vanish or explode?  
A) Because the gradient is a product of many partial derivatives over time steps  
B) Because the gradient is computed only at the last time step  
C) Because partial derivatives can be less than or greater than 1, causing exponential decay or growth  
D) Because the model uses non-differentiable activation functions  

#### 3. Which of the following are effective methods to mitigate exploding gradients in RNNs?  
A) Gradient clipping  
B) Using tanh activation exclusively  
C) Skip connections  
D) Increasing the learning rate  

#### 4. How does the forget gate in an LSTM function?  
A) It decides which parts of the cell state to discard  
B) It adds new information to the cell state  
C) It controls the output of the hidden state  
D) It resets the hidden state to zero  

#### 5. What role does the input gate play in an LSTM?  
A) It decides what new information to add to the cell state  
B) It filters irrelevant information from the hidden state  
C) It controls the output of the LSTM at the current time step  
D) It forgets old information from the cell state  

#### 6. Why is the cell state in an LSTM often described as a "conveyor belt"?  
A) Because it carries information unchanged across time steps unless modified by gates  
B) Because it is reset at every time step  
C) Because it stores the output of the LSTM  
D) Because it is used only during backpropagation  

#### 7. Which activation functions are typically used inside LSTM gates and why?  
A) Sigmoid for gates to produce values between 0 and 1  
B) Tanh to shrink values between -1 and 1 for candidate cell states  
C) ReLU to avoid vanishing gradients in gates  
D) Softmax to select the most important information  

#### 8. What is the main advantage of LSTMs over vanilla RNNs?  
A) LSTMs can selectively remember or forget information, improving long-term dependency learning  
B) LSTMs require less computational power than RNNs  
C) LSTMs do not require backpropagation through time  
D) LSTMs eliminate the need for gradient clipping  

#### 9. In Named Entity Recognition (NER), what does the label "B-per" signify?  
A) Beginning of a person‚Äôs name entity  
B) Inside a person‚Äôs name entity  
C) Outside any entity  
D) Beginning of a place entity  

#### 10. Why is token padding necessary when training LSTM models for NER?  
A) To ensure all input sequences have the same length for batch processing  
B) To increase the vocabulary size  
C) To prevent vanishing gradients  
D) To mask irrelevant tokens during training  

#### 11. When computing accuracy for NER models, why must padding tokens be masked?  
A) Because padding tokens do not correspond to real words and should not affect accuracy  
B) Because padding tokens always have the correct label  
C) Because masking speeds up computation  
D) Because padding tokens contain entity information  

#### 12. Which of the following are true about the output gate in an LSTM?  
A) It decides what information from the cell state to output as the hidden state  
B) It controls what information is passed to the next time step  
C) It forgets irrelevant information from the cell state  
D) It uses a sigmoid activation to produce gating values  

#### 13. How does gradient clipping help during RNN training?  
A) It limits the maximum value of gradients to prevent them from exploding  
B) It normalizes gradients to have zero mean  
C) It increases the learning rate dynamically  
D) It prevents gradients from becoming too small  

#### 14. Which of the following statements about backpropagation through time (BPTT) are correct?  
A) The length of the gradient product depends on how far back in time the dependency is  
B) BPTT computes gradients only for the last time step  
C) Partial derivatives less than 1 cause gradients to vanish over long sequences  
D) Partial derivatives greater than 1 cause gradients to explode over long sequences  

#### 15. In the context of NER, what is the purpose of converting words and entity classes into numerical arrays?  
A) To enable the model to process text data as input  
B) To reduce the size of the dataset  
C) To allow batch processing of sequences  
D) To improve the interpretability of the model  

#### 16. Which of the following are common applications of LSTMs?  
A) Music composition  
B) Image captioning  
C) Sorting algorithms  
D) Speech recognition  

#### 17. What is the effect of using the tanh activation function on the candidate cell state in an LSTM?  
A) It scales values to be between -1 and 1, helping stabilize training  
B) It ensures all values are positive  
C) It produces binary outputs for gating  
D) It eliminates the need for the forget gate  

#### 18. Why might skip connections be used in RNN architectures?  
A) To allow gradients to flow more directly and reduce vanishing gradients  
B) To increase the depth of the network without increasing parameters  
C) To clip gradients automatically  
D) To reduce the number of time steps processed  

#### 19. During NER model training, what is the role of the dense layer after the LSTM?  
A) To map LSTM outputs to the number of entity classes  
B) To reduce the sequence length  
C) To embed words into vectors  
D) To apply the softmax or log softmax activation for classification  

#### 20. Which of the following statements about LSTM gates are true?  
A) Gates use sigmoid activations to produce values between 0 and 1  
B) The forget gate and input gate work together to update the cell state  
C) The output gate controls the next hidden state output  
D) Gates eliminate the need for backpropagation  



<br>

## Answers



#### 1. What are the main reasons why basic RNNs struggle with long-term dependencies?  
A) ‚úì Limited memory capacity in hidden states makes it hard to retain distant information  
B) ‚úì Vanishing gradients cause earlier information to be lost during training  
C) ‚úó Exploding gradients cause overfitting but are not the main reason for struggling with long-term dependencies  
D) ‚úó RNNs use less RAM, so this is not a disadvantage  

**Correct:** A, B


#### 2. In backpropagation through time (BPTT), why does the gradient tend to vanish or explode?  
A) ‚úì Gradient is a product of many partial derivatives over time steps, causing exponential effects  
B) ‚úó Gradient is computed at every time step, not only the last  
C) ‚úì Partial derivatives less than 1 cause vanishing, greater than 1 cause exploding gradients  
D) ‚úó Activation functions used (sigmoid, tanh) are differentiable  

**Correct:** A, C


#### 3. Which of the following are effective methods to mitigate exploding gradients in RNNs?  
A) ‚úì Gradient clipping limits gradient size to prevent explosion  
B) ‚úó Using tanh alone does not prevent exploding gradients  
C) ‚úì Skip connections help gradients flow better, reducing explosion risk  
D) ‚úó Increasing learning rate can worsen exploding gradients  

**Correct:** A, C


#### 4. How does the forget gate in an LSTM function?  
A) ‚úì It decides which parts of the cell state to discard  
B) ‚úó Adding new information is the input gate‚Äôs job  
C) ‚úó Output gate controls hidden state output, not forget gate  
D) ‚úó Forget gate does not reset hidden state to zero  

**Correct:** A


#### 5. What role does the input gate play in an LSTM?  
A) ‚úì Decides what new information to add to the cell state  
B) ‚úó It does not filter hidden state; forget gate handles discarding info  
C) ‚úó Output gate controls output, not input gate  
D) ‚úó Forget gate forgets old info, not input gate  

**Correct:** A


#### 6. Why is the cell state in an LSTM often described as a "conveyor belt"?  
A) ‚úì It carries information largely unchanged across time steps unless gates modify it  
B) ‚úó It is not reset at every time step  
C) ‚úó It stores more than just output; output is hidden state  
D) ‚úó It is used during forward pass, not only backpropagation  

**Correct:** A


#### 7. Which activation functions are typically used inside LSTM gates and why?  
A) ‚úì Sigmoid gates produce values between 0 and 1 to control flow  
B) ‚úì Tanh shrinks candidate cell state values between -1 and 1 for stability  
C) ‚úó ReLU is not typically used inside gates due to unbounded output  
D) ‚úó Softmax is not used in gates; it‚Äôs for classification  

**Correct:** A, B


#### 8. What is the main advantage of LSTMs over vanilla RNNs?  
A) ‚úì LSTMs selectively remember/forget, improving long-term dependency learning  
B) ‚úó LSTMs generally require more computation, not less  
C) ‚úó LSTMs still use backpropagation through time  
D) ‚úó Gradient clipping may still be used with LSTMs  

**Correct:** A


#### 9. In Named Entity Recognition (NER), what does the label "B-per" signify?  
A) ‚úì Beginning of a person‚Äôs name entity  
B) ‚úó "I-per" would indicate inside a person‚Äôs name, not "B-per"  
C) ‚úó "O" means outside any entity  
D) ‚úó "B-geo" or similar would indicate place, not person  

**Correct:** A


#### 10. Why is token padding necessary when training LSTM models for NER?  
A) ‚úì To ensure all input sequences have the same length for batch processing  
B) ‚úó Padding does not increase vocabulary size  
C) ‚úó Padding does not prevent vanishing gradients  
D) ‚úó Masking is separate from padding purpose  

**Correct:** A


#### 11. When computing accuracy for NER models, why must padding tokens be masked?  
A) ‚úì Padding tokens do not correspond to real words and should not affect accuracy  
B) ‚úó Padding tokens do not have correct labels; they are placeholders  
C) ‚úó Masking is for correctness, not speed  
D) ‚úó Padding tokens contain no entity information  

**Correct:** A


#### 12. Which of the following are true about the output gate in an LSTM?  
A) ‚úì Decides what information from the cell state to output as hidden state  
B) ‚úì Controls what information is passed to the next time step via hidden state  
C) ‚úó Forget gate discards irrelevant info, not output gate  
D) ‚úì Uses sigmoid activation to produce gating values between 0 and 1  

**Correct:** A, B, D


#### 13. How does gradient clipping help during RNN training?  
A) ‚úì Limits maximum gradient value to prevent exploding gradients  
B) ‚úó Does not normalize gradients to zero mean  
C) ‚úó Does not increase learning rate  
D) ‚úó Does not prevent gradients from becoming too small (vanishing)  

**Correct:** A


#### 14. Which of the following statements about backpropagation through time (BPTT) are correct?  
A) ‚úì Gradient product length depends on how far back in time the dependency is  
B) ‚úó BPTT computes gradients at all time steps, not only last  
C) ‚úì Partial derivatives less than 1 cause gradients to vanish over long sequences  
D) ‚úì Partial derivatives greater than 1 cause gradients to explode over long sequences  

**Correct:** A, C, D


#### 15. In the context of NER, what is the purpose of converting words and entity classes into numerical arrays?  
A) ‚úì Enables model to process text as numerical input  
B) ‚úó Does not reduce dataset size  
C) ‚úì Allows batch processing of sequences  
D) ‚úó Does not improve interpretability; it‚Äôs for computation  

**Correct:** A, C


#### 16. Which of the following are common applications of LSTMs?  
A) ‚úì Music composition  
B) ‚úì Image captioning  
C) ‚úó Sorting algorithms are not sequence modeling tasks  
D) ‚úì Speech recognition  

**Correct:** A, B, D


#### 17. What is the effect of using the tanh activation function on the candidate cell state in an LSTM?  
A) ‚úì Scales values between -1 and 1, stabilizing training  
B) ‚úó Does not ensure all positive values; tanh outputs negative and positive  
C) ‚úó Does not produce binary outputs; sigmoid does that  
D) ‚úó Does not eliminate need for forget gate  

**Correct:** A


#### 18. Why might skip connections be used in RNN architectures?  
A) ‚úì Allow gradients to flow more directly, reducing vanishing gradients  
B) ‚úó Skip connections do not increase depth without parameters  
C) ‚úó Skip connections do not clip gradients automatically  
D) ‚úó Skip connections do not reduce number of time steps processed  

**Correct:** A


#### 19. During NER model training, what is the role of the dense layer after the LSTM?  
A) ‚úì Maps LSTM outputs to number of entity classes for classification  
B) ‚úó Does not reduce sequence length; output length matches input length  
C) ‚úó Embedding layer, not dense, embeds words into vectors  
D) ‚úì Applies softmax or log softmax activation for classification probabilities  

**Correct:** A, D


#### 20. Which of the following statements about LSTM gates are true?  
A) ‚úì Gates use sigmoid activations to produce values between 0 and 1  
B) ‚úì Forget and input gates work together to update the cell state  
C) ‚úì Output gate controls the next hidden state output  
D) ‚úó Gates do not eliminate the need for backpropagation; they improve gradient flow  

**Correct:** A, B, C

