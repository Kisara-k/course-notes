## 3.2 LSTMs and Named Entity Recognition

## Study Notes

### 1. ü§ñ Understanding RNNs and the Vanishing Gradient Problem

#### Introduction to RNNs

Recurrent Neural Networks (RNNs) are a type of neural network designed to handle sequential data, such as sentences or time series. Unlike traditional neural networks, RNNs have loops that allow information to persist, meaning they can remember previous inputs while processing new ones. This makes them useful for tasks where context matters, like language modeling or speech recognition.

#### Advantages of RNNs

- **Capturing short-range dependencies:** RNNs can remember information from recent steps in a sequence, which helps in understanding context within a short window.
- **Memory efficiency:** Compared to some other models like n-gram models, RNNs use less RAM because they don‚Äôt need to store huge tables of word combinations.

#### Disadvantages of RNNs

- **Struggle with long-term dependencies:** RNNs find it difficult to remember information from far back in the sequence. For example, if a word at the start of a sentence influences the meaning of a word much later, a basic RNN might miss that connection.
- **Vanishing and exploding gradients:** When training RNNs using backpropagation through time (BPTT), the gradients (which guide learning) can become very small (vanish) or very large (explode). This makes training unstable or ineffective.

#### Backpropagation Through Time (BPTT)

BPTT is the method used to train RNNs. It involves unrolling the network through time steps and calculating gradients at each step. The gradient at a particular time depends on the product of many partial derivatives from previous steps. The longer the gap between the current step and the step where the gradient originates, the more these products multiply.

- If these partial derivatives are less than 1, multiplying many of them causes the gradient to shrink exponentially ‚Äî this is the **vanishing gradient** problem.
- If they are greater than 1, the gradient grows exponentially ‚Äî this is the **exploding gradient** problem.

#### Solutions to Vanishing and Exploding Gradients

- **Identity RNN with ReLU activation:** Using ReLU (Rectified Linear Unit) activation and identity weight matrices can help maintain gradient magnitude.
- **Gradient clipping:** This technique limits the size of gradients during training to prevent them from exploding.
- **Skip connections:** These allow gradients to flow more directly across time steps, reducing the chance of vanishing.


### 2. üß† Long Short-Term Memory (LSTM) Networks: A Better RNN

#### What is an LSTM?

LSTMs are a special kind of RNN designed to solve the vanishing gradient problem and better capture long-term dependencies. They do this by introducing a more complex internal structure that controls the flow of information.

#### Anatomy of an LSTM Unit

An LSTM unit contains:

- **Cell state:** This is like a conveyor belt running through the sequence, carrying information along with minimal changes.
- **Hidden state:** This is the output at each time step, which also carries information forward.
- **Gates:** These are special neural network layers that decide what information to keep, add, or output. They use sigmoid activations to produce values between 0 (completely block) and 1 (completely allow).

#### The Three Gates in LSTM

1. **Forget Gate:** Decides what information from the cell state should be discarded because it‚Äôs no longer relevant.
2. **Input Gate:** Decides what new information should be added to the cell state.
3. **Output Gate:** Decides what information from the cell state should be output as the hidden state for the current step.

#### How LSTM Works Step-by-Step

- The **forget gate** looks at the previous hidden state and current input and outputs a number between 0 and 1 for each piece of information in the cell state, deciding what to keep or forget.
- The **input gate** decides which new information to add to the cell state, using a combination of sigmoid and tanh activations.
- The **cell state** is updated by forgetting some old information and adding new candidate information.
- The **output gate** decides what part of the updated cell state to output as the hidden state, which will be used in the next step and possibly as output.

#### Why LSTMs Work Better

Because of these gates, LSTMs can selectively remember or forget information, allowing gradients to flow more effectively during training. This helps them capture long-term dependencies without suffering from vanishing or exploding gradients.

#### Applications of LSTMs

LSTMs are widely used in:

- Next-character or word prediction
- Chatbots and conversational AI
- Music composition
- Image captioning
- Speech recognition


### 3. üè∑Ô∏è Named Entity Recognition (NER): Extracting Meaningful Entities from Text

#### What is Named Entity Recognition?

NER is a natural language processing (NLP) task that involves locating and classifying predefined entities in text into categories such as:

- **Places** (e.g., Thailand)
- **Organizations** (e.g., Google)
- **People** (e.g., Barack Obama)
- **Time indicators** (e.g., December)
- **Artifacts** (e.g., Egyptian statue)

NER helps computers understand and extract important information from unstructured text.

#### Example of NER Labeling

Consider the sentence:  
"Sharon flew to Miami last Friday."

The labeled entities might look like this:  
- Sharon ‚Üí B-per (Beginning of a person‚Äôs name)  
- Miami ‚Üí B-geo (Beginning of a geographical location)  
- last Friday ‚Üí B-tim (Beginning of a time expression)

Other words like "flew" and "to" are labeled as O (Outside any entity).

#### Applications of NER

- Improving search engine results by understanding query context
- Enhancing recommendation engines by identifying key entities
- Customer service automation by extracting relevant information
- Automatic trading by recognizing company names, dates, and events in news


### 4. üßÆ Training Named Entity Recognition Models with LSTMs

#### Data Processing for NER

Before training, text data and entity labels must be converted into numerical form:

- **Assign numbers to words:** Each unique word in the vocabulary is mapped to a unique integer.
- **Assign numbers to entity classes:** Each entity type (e.g., B-per, B-geo, O) is also mapped to a unique integer.

For example, the sentence "Sharon flew to Miami last Friday." might be converted to:  
`[4282, 853, 187, 5388, 2894, 7]` for words, and  
`[O, O, B-geo, B-tim, B-per]` for entities.

#### Token Padding

LSTMs require input sequences to be the same length. To handle sentences of varying lengths:

- Choose a fixed sequence length.
- Shorter sequences are padded with a special `<PAD>` token to fill empty spaces.
- Padding ensures consistent input size for batch processing.

#### Training Process

- Convert each sentence and its entity labels into fixed-length numerical arrays.
- Group these arrays into batches (e.g., 64, 128, 256 samples per batch) for efficient training.
- Feed batches into an LSTM layer.
- Pass the LSTM output through a dense (fully connected) layer.
- Use a log softmax activation to predict the probability distribution over entity classes for each token.

#### Model Architecture Example (TensorFlow)

A typical NER model might look like this:

```python
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim),
    tf.keras.layers.LSTM(units=hidden_units, return_sequences=True),
    tf.keras.layers.Dense(num_classes),
    tf.keras.layers.Activation('log_softmax')
])
```


### 5. ‚úÖ Evaluating NER Models: Accuracy and Masking

#### Why Masking is Important

Since sequences are padded, the model outputs predictions for padding tokens as well. These padding tokens do not correspond to real words and should not be counted when calculating accuracy.

#### Computing Accuracy with Masking

- Pass the test set through the model to get predictions.
- For each token, find the predicted class with the highest probability (arg max).
- Create a mask that ignores padded tokens.
- Compare predicted classes to true labels only where the mask is active.
- Calculate accuracy as the ratio of correct predictions to total non-padded tokens.

#### Example Python Function for Masked Accuracy

```python
def masked_accuracy(y_true, y_pred):
    mask = tf.cast(tf.not_equal(y_true, pad_token_id), tf.float32)  # Mask for non-padding tokens
    y_pred_class = tf.math.argmax(y_pred, axis=-1)
    matches_true_pred = tf.cast(tf.equal(y_true, y_pred_class), tf.float32)
    matches_true_pred *= mask
    masked_acc = tf.reduce_sum(matches_true_pred) / tf.reduce_sum(mask)
    return masked_acc
```


### Summary

- **RNNs** are powerful for sequential data but suffer from vanishing/exploding gradients, limiting their ability to learn long-term dependencies.
- **LSTMs** solve this by using gates to control information flow, enabling them to remember or forget information as needed.
- **Named Entity Recognition (NER)** extracts meaningful entities from text, which is useful in many real-world applications.
- Training NER models involves converting text and labels into numerical arrays, padding sequences, and using LSTMs followed by dense layers.
- Evaluating NER models requires masking padded tokens to get an accurate measure of performance.



<br>

## Key Points

#### 1. ü§ñ RNNs and Vanishing/Exploding Gradients  
- RNNs capture short-range dependencies but struggle with long-term dependencies.  
- Vanishing gradients occur when partial derivatives < 1 cause gradients to shrink exponentially during backpropagation through time.  
- Exploding gradients occur when partial derivatives > 1 cause gradients to grow exponentially during backpropagation.  
- Gradient clipping, identity RNNs with ReLU, and skip connections are common solutions to vanishing/exploding gradients.

#### 2. üß† LSTM Architecture and Gates  
- LSTMs have a cell state and a hidden state to carry information through time steps.  
- LSTMs use three gates: Forget gate (decides what to discard), Input gate (decides what new information to add), and Output gate (decides what to output).  
- Gates use sigmoid activation to output values between 0 (closed) and 1 (open).  
- The cell state is updated by combining the forget gate and input gate outputs with candidate cell state information.  
- LSTMs help avoid vanishing/exploding gradients by controlling information flow through gates.

#### 3. üè∑Ô∏è Named Entity Recognition (NER)  
- NER locates and extracts predefined entities such as places, organizations, people, times, and artifacts from text.  
- Entities are labeled using tags like B-per (person), B-geo (geographical), B-tim (time), and O (outside any entity).  
- Example: "Sharon flew to Miami last Friday." ‚Üí Sharon: B-per, Miami: B-geo, last Friday: B-tim.

#### 4. üßÆ Data Processing for NER Training  
- Words and entity classes are converted into numerical arrays (tokenization).  
- All sequences are padded to the same length using a <PAD> token for batch processing in LSTMs.  
- Training batches typically contain 64, 128, 256, or 512 sequences.  
- The model architecture includes an embedding layer, an LSTM layer, a dense layer, and a log softmax activation for classification.

#### 5. ‚úÖ Evaluating NER Models  
- Accuracy is computed by masking padded tokens to exclude them from evaluation.  
- The predicted class for each token is obtained by taking the arg max of the model‚Äôs output probabilities.  
- Masked accuracy is calculated as the ratio of correct predictions to total non-padded tokens.