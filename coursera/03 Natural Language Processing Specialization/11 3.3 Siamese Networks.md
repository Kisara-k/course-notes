## 3.3 Siamese Networks

## Study Notes

### 1. ü§ù What Are Siamese Networks? An Introduction

Siamese Networks are a special type of neural network architecture designed not to classify inputs into fixed categories, but rather to **measure the similarity or difference between two inputs**. Unlike traditional classification models that say "this input belongs to class A," Siamese Networks answer questions like "Are these two inputs similar or not?" or "Do these two sentences mean the same thing?"

Imagine you have two questions:  
- "What is your age?"  
- "How old are you?"  

A classification model might struggle because these are different sentences, but a Siamese Network is designed to recognize that these two questions are essentially asking the same thing ‚Äî they are similar in meaning.

#### Why Use Siamese Networks?

- **Similarity detection:** They are great for tasks where you want to know if two things are alike, such as identifying duplicate questions, verifying signatures, or matching images.
- **One-shot learning:** They can learn to recognize new classes or examples with very few examples, by learning a similarity function rather than a fixed classification.
- **Flexibility:** They can be applied in many domains, including natural language processing (NLP), image recognition, and more.


### 2. üèóÔ∏è Architecture of Siamese Networks

At the core, a Siamese Network consists of **two identical subnetworks** that share the same architecture and weights. Each subnetwork processes one of the two inputs independently but identically, producing a vector representation (embedding) of each input.

#### Step-by-step architecture:

1. **Input:** Two inputs (e.g., two sentences or images) are fed into the network.
2. **Embedding:** Each input is converted into a numerical form (like word embeddings for text).
3. **Subnetwork (e.g., LSTM):** Each input embedding passes through the same neural network (often an LSTM for sequences) to produce a fixed-length vector representation.
4. **Similarity computation:** The two output vectors are compared using a similarity function, commonly **cosine similarity**, which measures how close the two vectors are in the vector space.
5. **Output:** A similarity score indicating how alike the two inputs are.

#### Why use LSTM?

For text inputs, Long Short-Term Memory (LSTM) networks are often used because they can capture the sequential nature of language, understanding context and word order.


### 3. üéØ Loss Functions and Training with Triplets

Training a Siamese Network requires a loss function that encourages the network to bring similar inputs closer together in the embedding space and push dissimilar inputs farther apart.

#### Triplet Loss

A popular approach is **Triplet Loss**, which uses three inputs:

- **Anchor (A):** A reference input (e.g., "What is your age?")
- **Positive (P):** An input similar to the anchor (e.g., "How old are you?")
- **Negative (N):** An input different from the anchor (e.g., "Where are you from?")

The goal is to ensure that the distance between the anchor and positive is smaller than the distance between the anchor and negative by at least a margin Œ± (alpha). This margin prevents trivial solutions where all embeddings collapse to the same point.

Mathematically, the loss tries to satisfy:  
`distance(A, P) + Œ± < distance(A, N)`

This encourages the network to learn embeddings where similar inputs cluster together, and dissimilar inputs are separated.

#### Types of triplets:

- **Easy triplets:** Negative is very different from anchor, so the network easily learns.
- **Hard triplets:** Negative is very similar to anchor, making training more challenging but more effective.
- **Random triplets:** Selected randomly, may not always be informative.


### 4. üî¢ Computing the Cost: How Training Works in Practice

To train the network, batches of triplets are prepared. For example, consider these pairs of questions:

- "What is your age?" and "How old are you?" (similar)
- "Where are you from?" and "Where are you going?" (different)

Each batch contains multiple such pairs, and the network computes embeddings for each input. Then, it calculates cosine similarities between pairs:

- Similar pairs should have high cosine similarity (close to 1).
- Dissimilar pairs should have low cosine similarity (close to -1 or 0).

The loss function uses these similarity scores to update the network weights, pushing embeddings of similar pairs closer and dissimilar pairs apart.

#### Hard Negative Mining

During training, it‚Äôs important to focus on **hard negatives** ‚Äî those negative examples that are close to the anchor in embedding space but should be far. This helps the network learn more effectively by focusing on challenging examples rather than easy ones.


### 5. üß† One-Shot Learning: Why Siamese Networks Are Powerful

Traditional classification models require many examples per class to learn to recognize them. In contrast, Siamese Networks excel at **one-shot learning**, where the model can recognize new classes or inputs after seeing only one or very few examples.

#### How?

Instead of learning to classify inputs into fixed categories, the network learns a **similarity function**. When a new example appears, the network compares it to known examples and decides if they are similar enough to be considered the same class.

This is especially useful in scenarios like:

- Face recognition with few images per person.
- Handwritten character recognition with limited samples.
- Duplicate question detection in NLP.


### 6. üß© Putting It All Together: Training and Testing Workflow

#### Training Phase:

- Prepare batches of question pairs labeled as duplicates or not.
- Convert questions into embeddings (numerical arrays).
- Pass each question through the Siamese subnetwork (e.g., LSTM) to get vector representations.
- Compute cosine similarity between pairs.
- Use triplet loss or contrastive loss to update the network weights.

#### Testing Phase:

- Given two new inputs, convert them into embeddings using the trained network.
- Calculate cosine similarity between the embeddings.
- Compare the similarity score to a threshold œÑ (tau).
- If similarity > œÑ, classify as similar (duplicate); otherwise, not similar.


### Summary

Siamese Networks are a powerful tool for learning **similarity** rather than classification. They use twin subnetworks to embed inputs into a vector space where similar inputs are close together. Training involves triplet loss to pull similar pairs closer and push dissimilar pairs apart. This architecture enables one-shot learning, making it ideal for tasks like duplicate question detection, signature verification, and more.

By understanding the architecture, loss functions, and training process, you can apply Siamese Networks to many real-world problems where measuring similarity is key.



<br>

## Key Points

#### 1. ü§ù Siamese Networks Purpose  
- Siamese Networks learn to identify similarity or difference between two inputs rather than classify them into fixed categories.  
- They are used to detect if two inputs (e.g., sentences) have the same meaning or are duplicates.

#### 2. üèóÔ∏è Siamese Network Architecture  
- Siamese Networks consist of two identical subnetworks sharing weights that process two inputs separately.  
- Each input is converted into an embedding vector, often using LSTM for sequential data like text.  
- The similarity between the two output vectors is computed using cosine similarity.

#### 3. üéØ Triplet Loss Function  
- Triplet loss uses three inputs: Anchor (A), Positive (P), and Negative (N).  
- The loss enforces that the distance between Anchor and Positive is smaller than the distance between Anchor and Negative by a margin Œ± (alpha).  
- Triplet loss encourages embeddings of similar inputs to be close and dissimilar inputs to be far apart.

#### 4. üî¢ Training Data and Batches  
- Training batches consist of pairs or triplets of inputs labeled as similar (positive) or dissimilar (negative).  
- Cosine similarity scores are computed for each pair in the batch to calculate the loss.  
- Hard negative mining focuses training on difficult negative examples that are close to the anchor in embedding space.

#### 5. üß† One-Shot Learning  
- Siamese Networks enable one-shot learning by learning a similarity function instead of classifying into fixed classes.  
- They can recognize new classes or inputs after seeing only one or few examples without retraining.

#### 6. üß© Testing Procedure  
- During testing, inputs are converted to embeddings using the trained Siamese subnetwork.  
- Cosine similarity between embeddings is calculated and compared to a threshold œÑ (tau).  
- If similarity > œÑ, inputs are classified as similar; otherwise, they are not.