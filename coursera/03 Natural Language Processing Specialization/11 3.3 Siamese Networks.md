## 3.3 Siamese Networks

[Study Notes](#study-notes)

[Questions](#questions)



### Key Points



#### 1. ü§ù Siamese Networks Purpose  
- Siamese Networks learn to identify similarity or difference between two inputs rather than classify them into fixed categories.  
- They are used to detect if two inputs (e.g., sentences) have the same meaning or are duplicates.

#### 2. üèóÔ∏è Siamese Network Architecture  
- Siamese Networks consist of two identical subnetworks sharing weights that process two inputs separately.  
- Each input is converted into an embedding vector, often using LSTM for sequential data like text.  
- The similarity between the two output vectors is computed using cosine similarity.

#### 3. üéØ Triplet Loss Function  
- Triplet loss uses three inputs: Anchor (A), Positive (P), and Negative (N).  
- The loss enforces that the distance between Anchor and Positive is smaller than the distance between Anchor and Negative by a margin Œ± (alpha).  
- Triplet loss encourages embeddings of similar inputs to be close and dissimilar inputs to be far apart.

#### 4. üî¢ Training Data and Batches  
- Training batches consist of pairs or triplets of inputs labeled as similar (positive) or dissimilar (negative).  
- Cosine similarity scores are computed for each pair in the batch to calculate the loss.  
- Hard negative mining focuses training on difficult negative examples that are close to the anchor in embedding space.

#### 5. üß† One-Shot Learning  
- Siamese Networks enable one-shot learning by learning a similarity function instead of classifying into fixed classes.  
- They can recognize new classes or inputs after seeing only one or few examples without retraining.

#### 6. üß© Testing Procedure  
- During testing, inputs are converted to embeddings using the trained Siamese subnetwork.  
- Cosine similarity between embeddings is calculated and compared to a threshold œÑ (tau).  
- If similarity > œÑ, inputs are classified as similar; otherwise, they are not.



<br>

## Study Notes



### 1. ü§ù What Are Siamese Networks? An Introduction

Siamese Networks are a special type of neural network architecture designed not to classify inputs into fixed categories, but rather to **measure the similarity or difference between two inputs**. Unlike traditional classification models that say "this input belongs to class A," Siamese Networks answer questions like "Are these two inputs similar or not?" or "Do these two sentences mean the same thing?"

Imagine you have two questions:  
- "What is your age?"  
- "How old are you?"  

A classification model might struggle because these are different sentences, but a Siamese Network is designed to recognize that these two questions are essentially asking the same thing ‚Äî they are similar in meaning.

#### Why Use Siamese Networks?

- **Similarity detection:** They are great for tasks where you want to know if two things are alike, such as identifying duplicate questions, verifying signatures, or matching images.
- **One-shot learning:** They can learn to recognize new classes or examples with very few examples, by learning a similarity function rather than a fixed classification.
- **Flexibility:** They can be applied in many domains, including natural language processing (NLP), image recognition, and more.


### 2. üèóÔ∏è Architecture of Siamese Networks

At the core, a Siamese Network consists of **two identical subnetworks** that share the same architecture and weights. Each subnetwork processes one of the two inputs independently but identically, producing a vector representation (embedding) of each input.

#### Step-by-step architecture:

1. **Input:** Two inputs (e.g., two sentences or images) are fed into the network.
2. **Embedding:** Each input is converted into a numerical form (like word embeddings for text).
3. **Subnetwork (e.g., LSTM):** Each input embedding passes through the same neural network (often an LSTM for sequences) to produce a fixed-length vector representation.
4. **Similarity computation:** The two output vectors are compared using a similarity function, commonly **cosine similarity**, which measures how close the two vectors are in the vector space.
5. **Output:** A similarity score indicating how alike the two inputs are.

#### Why use LSTM?

For text inputs, Long Short-Term Memory (LSTM) networks are often used because they can capture the sequential nature of language, understanding context and word order.


### 3. üéØ Loss Functions and Training with Triplets

Training a Siamese Network requires a loss function that encourages the network to bring similar inputs closer together in the embedding space and push dissimilar inputs farther apart.

#### Triplet Loss

A popular approach is **Triplet Loss**, which uses three inputs:

- **Anchor (A):** A reference input (e.g., "What is your age?")
- **Positive (P):** An input similar to the anchor (e.g., "How old are you?")
- **Negative (N):** An input different from the anchor (e.g., "Where are you from?")

The goal is to ensure that the distance between the anchor and positive is smaller than the distance between the anchor and negative by at least a margin Œ± (alpha). This margin prevents trivial solutions where all embeddings collapse to the same point.

Mathematically, the loss tries to satisfy:  
`distance(A, P) + Œ± < distance(A, N)`

This encourages the network to learn embeddings where similar inputs cluster together, and dissimilar inputs are separated.

#### Types of triplets:

- **Easy triplets:** Negative is very different from anchor, so the network easily learns.
- **Hard triplets:** Negative is very similar to anchor, making training more challenging but more effective.
- **Random triplets:** Selected randomly, may not always be informative.


### 4. üî¢ Computing the Cost: How Training Works in Practice

To train the network, batches of triplets are prepared. For example, consider these pairs of questions:

- "What is your age?" and "How old are you?" (similar)
- "Where are you from?" and "Where are you going?" (different)

Each batch contains multiple such pairs, and the network computes embeddings for each input. Then, it calculates cosine similarities between pairs:

- Similar pairs should have high cosine similarity (close to 1).
- Dissimilar pairs should have low cosine similarity (close to -1 or 0).

The loss function uses these similarity scores to update the network weights, pushing embeddings of similar pairs closer and dissimilar pairs apart.

#### Hard Negative Mining

During training, it‚Äôs important to focus on **hard negatives** ‚Äî those negative examples that are close to the anchor in embedding space but should be far. This helps the network learn more effectively by focusing on challenging examples rather than easy ones.


### 5. üß† One-Shot Learning: Why Siamese Networks Are Powerful

Traditional classification models require many examples per class to learn to recognize them. In contrast, Siamese Networks excel at **one-shot learning**, where the model can recognize new classes or inputs after seeing only one or very few examples.

#### How?

Instead of learning to classify inputs into fixed categories, the network learns a **similarity function**. When a new example appears, the network compares it to known examples and decides if they are similar enough to be considered the same class.

This is especially useful in scenarios like:

- Face recognition with few images per person.
- Handwritten character recognition with limited samples.
- Duplicate question detection in NLP.


### 6. üß© Putting It All Together: Training and Testing Workflow

#### Training Phase:

- Prepare batches of question pairs labeled as duplicates or not.
- Convert questions into embeddings (numerical arrays).
- Pass each question through the Siamese subnetwork (e.g., LSTM) to get vector representations.
- Compute cosine similarity between pairs.
- Use triplet loss or contrastive loss to update the network weights.

#### Testing Phase:

- Given two new inputs, convert them into embeddings using the trained network.
- Calculate cosine similarity between the embeddings.
- Compare the similarity score to a threshold œÑ (tau).
- If similarity > œÑ, classify as similar (duplicate); otherwise, not similar.


### Summary

Siamese Networks are a powerful tool for learning **similarity** rather than classification. They use twin subnetworks to embed inputs into a vector space where similar inputs are close together. Training involves triplet loss to pull similar pairs closer and push dissimilar pairs apart. This architecture enables one-shot learning, making it ideal for tasks like duplicate question detection, signature verification, and more.

By understanding the architecture, loss functions, and training process, you can apply Siamese Networks to many real-world problems where measuring similarity is key.



<br>

## Questions



#### 1. What is the primary purpose of a Siamese Network?  
A) To classify inputs into fixed categories  
B) To measure similarity or difference between two inputs  
C) To generate new data samples  
D) To cluster data without labels  

#### 2. Which of the following best describes the architecture of a Siamese Network?  
A) Two identical subnetworks sharing weights processing two inputs separately  
B) A single network processing concatenated inputs  
C) Two different networks with independent weights processing two inputs  
D) Multiple networks combined to classify multiple classes simultaneously  

#### 3. Why are LSTMs commonly used in Siamese Networks for NLP tasks?  
A) Because they can handle sequential data and capture context  
B) Because they reduce the dimensionality of input data  
C) Because they are faster than convolutional networks  
D) Because they ignore word order, focusing only on word frequency  

#### 4. What does the cosine similarity between two vectors represent in a Siamese Network?  
A) The Euclidean distance between two inputs  
B) The angle between two vector embeddings indicating similarity  
C) The probability that two inputs belong to the same class  
D) The sum of element-wise differences between vectors  

#### 5. In the context of triplet loss, what is the role of the margin Œ± (alpha)?  
A) To ensure the positive pair is exactly equal to the negative pair  
B) To enforce a minimum distance difference between positive and negative pairs  
C) To scale the embeddings before similarity calculation  
D) To normalize the input vectors  

#### 6. Which of the following statements about triplet loss is true?  
A) It requires three inputs: anchor, positive, and negative  
B) It minimizes the distance between anchor and negative examples  
C) It maximizes the distance between anchor and positive examples  
D) It encourages the anchor-positive distance to be smaller than anchor-negative distance by at least Œ±  

#### 7. What is a "hard negative" in the context of training Siamese Networks?  
A) A negative example that is very different from the anchor  
B) A negative example that is very similar to the anchor, making it difficult to distinguish  
C) A positive example mislabeled as negative  
D) An example that is easy for the network to classify correctly  

#### 8. Why is hard negative mining important during training?  
A) It speeds up training by ignoring difficult examples  
B) It helps the network learn better by focusing on challenging distinctions  
C) It reduces overfitting by removing easy examples  
D) It ensures the network only learns from positive pairs  

#### 9. How does one-shot learning differ from traditional classification?  
A) One-shot learning requires many examples per class  
B) One-shot learning learns a similarity function rather than fixed class boundaries  
C) One-shot learning cannot generalize to new classes  
D) One-shot learning requires retraining for every new class  

#### 10. Which of the following is NOT a typical application of Siamese Networks?  
A) Duplicate question detection  
B) Face verification  
C) Image generation  
D) Handwritten signature verification  

#### 11. When preparing batches for training a Siamese Network, which of the following is true?  
A) Batches contain only positive pairs  
B) Batches contain both positive and negative pairs  
C) Batches are always of size one  
D) Batches must contain triplets (anchor, positive, negative) for triplet loss  

#### 12. What happens if the margin Œ± in triplet loss is set too high?  
A) The network may fail to converge because the margin is too strict  
B) The network will easily satisfy the loss and stop learning  
C) The embeddings will collapse to a single point  
D) The network will ignore negative examples  

#### 13. Which similarity or distance metrics can be used in Siamese Networks?  
A) Only cosine similarity  
B) Only Euclidean distance  
C) Any similarity or distance metric, including cosine similarity and Euclidean distance  
D) Only Manhattan distance  

#### 14. In testing a Siamese Network, how is the decision made whether two inputs are similar?  
A) By checking if the cosine similarity exceeds a predefined threshold œÑ  
B) By classifying the inputs into one of K classes  
C) By computing the Euclidean distance and comparing it to zero  
D) By using the network‚Äôs softmax output  

#### 15. Which of the following statements about embedding vectors in Siamese Networks is correct?  
A) Embeddings are fixed-length vector representations of inputs  
B) Embeddings are raw input data converted to binary form  
C) Embeddings are always one-dimensional scalars  
D) Embeddings must be normalized before similarity calculation  

#### 16. What is the main challenge when training with random triplets?  
A) Random triplets always contain hard negatives  
B) Random triplets may include many easy negatives that provide little learning signal  
C) Random triplets require more computational resources than hard triplets  
D) Random triplets cause the network to overfit  

#### 17. Which of the following best explains why Siamese Networks are preferred over traditional classifiers for duplicate question detection?  
A) They require less labeled data for new questions  
B) They classify questions into fixed categories  
C) They learn a similarity function that generalizes to unseen questions  
D) They do not require any training  

#### 18. What is the significance of sharing weights between the two subnetworks in a Siamese Network?  
A) It reduces the number of parameters and ensures consistent feature extraction  
B) It allows each subnetwork to learn different features  
C) It prevents the network from learning similarity  
D) It increases the model‚Äôs capacity exponentially  

#### 19. Which of the following is true about the embedding vectors produced by the subnetworks?  
A) They are always orthogonal to each other  
B) They capture semantic meaning of the inputs in a continuous vector space  
C) They are discrete labels assigned to inputs  
D) They are used to compute similarity scores  

#### 20. In the context of Siamese Networks, what does the term "anchor" refer to?  
A) The input that serves as a reference point for comparison  
B) The negative example in a triplet  
C) The output similarity score  
D) The threshold used for classification  



<br>

## Answers



#### 1. What is the primary purpose of a Siamese Network?  
A) ‚úó Siamese Networks do not classify into fixed categories.  
B) ‚úì They measure similarity or difference between two inputs.  
C) ‚úó They do not generate new data samples.  
D) ‚úó Clustering without labels is not their main function.  

**Correct:** B


#### 2. Which of the following best describes the architecture of a Siamese Network?  
A) ‚úì Two identical subnetworks sharing weights processing two inputs separately.  
B) ‚úó Inputs are not concatenated; subnetworks process inputs independently.  
C) ‚úó Networks share weights; they are not independent.  
D) ‚úó Siamese Networks are not multiple networks for multi-class classification.  

**Correct:** A


#### 3. Why are LSTMs commonly used in Siamese Networks for NLP tasks?  
A) ‚úì LSTMs handle sequential data and capture context well.  
B) ‚úó Dimensionality reduction is not the main reason.  
C) ‚úó LSTMs are not necessarily faster than CNNs.  
D) ‚úó LSTMs do consider word order, which is important.  

**Correct:** A


#### 4. What does the cosine similarity between two vectors represent in a Siamese Network?  
A) ‚úó Cosine similarity measures angle, not Euclidean distance.  
B) ‚úì It measures the angle between vectors indicating similarity.  
C) ‚úó It is not a direct probability but a similarity score.  
D) ‚úó Cosine similarity is not the sum of element-wise differences.  

**Correct:** B


#### 5. In the context of triplet loss, what is the role of the margin Œ± (alpha)?  
A) ‚úó It does not make positive and negative pairs equal.  
B) ‚úì It enforces a minimum margin between positive and negative distances.  
C) ‚úó It does not scale embeddings directly.  
D) ‚úó It is not used for normalization.  

**Correct:** B


#### 6. Which of the following statements about triplet loss is true?  
A) ‚úì Triplet loss requires anchor, positive, and negative inputs.  
B) ‚úó It minimizes anchor-positive distance, not anchor-negative.  
C) ‚úó It minimizes anchor-positive distance, not maximizes.  
D) ‚úì It enforces anchor-positive distance to be smaller than anchor-negative by Œ±.  

**Correct:** A, D


#### 7. What is a "hard negative" in the context of training Siamese Networks?  
A) ‚úó Hard negatives are not very different; they are similar.  
B) ‚úì Hard negatives are very similar to the anchor, making training challenging.  
C) ‚úó Hard negatives are not mislabeled positives.  
D) ‚úó Hard negatives are difficult, not easy examples.  

**Correct:** B


#### 8. Why is hard negative mining important during training?  
A) ‚úó It does not ignore difficult examples; it focuses on them.  
B) ‚úì It helps the network learn better by focusing on challenging examples.  
C) ‚úó It does not reduce overfitting by removing easy examples.  
D) ‚úó It does not restrict learning to positive pairs only.  

**Correct:** B


#### 9. How does one-shot learning differ from traditional classification?  
A) ‚úó One-shot learning requires few examples, not many.  
B) ‚úì It learns a similarity function instead of fixed class boundaries.  
C) ‚úó It can generalize to new classes via similarity.  
D) ‚úó It does not require retraining for every new class.  

**Correct:** B


#### 10. Which of the following is NOT a typical application of Siamese Networks?  
A) ‚úó Duplicate question detection is a typical application.  
B) ‚úó Face verification is a typical application.  
C) ‚úì Image generation is not typical for Siamese Networks.  
D) ‚úó Handwritten signature verification is typical.  

**Correct:** C


#### 11. When preparing batches for training a Siamese Network, which of the following is true?  
A) ‚úó Batches contain both positive and negative pairs.  
B) ‚úì Batches contain both positive and negative pairs.  
C) ‚úó Batch size is usually more than one.  
D) ‚úì For triplet loss, batches must contain triplets (anchor, positive, negative).  

**Correct:** B, D


#### 12. What happens if the margin Œ± in triplet loss is set too high?  
A) ‚úì The network may fail to converge due to an overly strict margin.  
B) ‚úó The network will not easily satisfy the loss if margin is too high.  
C) ‚úó Embeddings collapsing is caused by other issues, not high margin.  
D) ‚úó The network does not ignore negatives due to margin size.  

**Correct:** A


#### 13. Which similarity or distance metrics can be used in Siamese Networks?  
A) ‚úó Not limited to cosine similarity only.  
B) ‚úó Not limited to Euclidean distance only.  
C) ‚úì Any similarity or distance metric can be used, including cosine and Euclidean.  
D) ‚úó Not limited to Manhattan distance only.  

**Correct:** C


#### 14. In testing a Siamese Network, how is the decision made whether two inputs are similar?  
A) ‚úì By checking if cosine similarity exceeds a threshold œÑ.  
B) ‚úó It does not classify into fixed classes during testing.  
C) ‚úó Euclidean distance is not always compared to zero.  
D) ‚úó Softmax output is not used in Siamese similarity testing.  

**Correct:** A


#### 15. Which of the following statements about embedding vectors in Siamese Networks is correct?  
A) ‚úì Embeddings are fixed-length vector representations of inputs.  
B) ‚úó Embeddings are not raw binary data.  
C) ‚úó Embeddings are multi-dimensional vectors, not scalars.  
D) ‚úì Embeddings are often normalized before similarity calculation.  

**Correct:** A, D


#### 16. What is the main challenge when training with random triplets?  
A) ‚úó Random triplets do not always contain hard negatives.  
B) ‚úì Random triplets often include many easy negatives that provide little learning signal.  
C) ‚úó Computational resources are not necessarily higher for random triplets.  
D) ‚úó Random triplets do not inherently cause overfitting.  

**Correct:** B


#### 17. Which of the following best explains why Siamese Networks are preferred over traditional classifiers for duplicate question detection?  
A) ‚úì They require less labeled data for new questions.  
B) ‚úó They do not classify into fixed categories.  
C) ‚úì They learn a similarity function that generalizes to unseen questions.  
D) ‚úó They do require training, just not retraining for new classes.  

**Correct:** A, C


#### 18. What is the significance of sharing weights between the two subnetworks in a Siamese Network?  
A) ‚úì It reduces parameters and ensures consistent feature extraction.  
B) ‚úó Sharing weights means subnetworks learn the same features, not different ones.  
C) ‚úó Sharing weights enables learning similarity, not prevents it.  
D) ‚úó Sharing weights does not exponentially increase capacity.  

**Correct:** A


#### 19. Which of the following is true about the embedding vectors produced by the subnetworks?  
A) ‚úó Embeddings are not always orthogonal.  
B) ‚úì They capture semantic meaning in a continuous vector space.  
C) ‚úó Embeddings are not discrete labels.  
D) ‚úì They are used to compute similarity scores.  

**Correct:** B, D


#### 20. In the context of Siamese Networks, what does the term "anchor" refer to?  
A) ‚úì The input serving as a reference point for comparison.  
B) ‚úó The negative example is distinct from the anchor.  
C) ‚úó Anchor is not the output similarity score.  
D) ‚úó Anchor is not a threshold.  

**Correct:** A

