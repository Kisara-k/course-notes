## 1.1 Sentiment Analysis with Logistic Regression

[Study Notes](#study-notes)

[Questions](#questions)



### Key Points



#### 1. üß† Sentiment Analysis and Logistic Regression  
- Sentiment analysis classifies text as positive (1) or negative (0).  
- Logistic regression is a supervised learning algorithm used for binary classification tasks like sentiment analysis.  
- The sigmoid function outputs a probability between 0 and 1 for classification.  

#### 2. üìö Vocabulary and Feature Extraction  
- Vocabulary is the set of unique words from the tweet corpus.  
- Tweets are represented as sparse vectors indicating word presence or frequency.  
- Sparse representations contain many zeros, causing large training and prediction times.  

#### 3. üî¢ Word Frequency Counts for Feature Extraction  
- The tweet corpus is divided into positive and negative classes.  
- Each word‚Äôs frequency is counted separately in positive and negative tweets (PosFreq and NegFreq).  
- Frequencies are stored in a dictionary mapping (word, class) pairs to counts.  

#### 4. üßπ Text Preprocessing  
- Preprocessing removes stopwords, punctuation, Twitter handles, and URLs.  
- Stemming reduces words to their root form (e.g., tuning ‚Üí tun).  
- Lowercasing normalizes text to avoid case sensitivity issues.  

#### 5. üõ†Ô∏è Feature Vector Construction  
- Features for each tweet include: bias term (1), sum of positive word frequencies, and sum of negative word frequencies.  
- Feature vectors are used as input to logistic regression.  

#### 6. ‚öôÔ∏è Logistic Regression Training  
- Parameters (weights and bias) are initialized before training.  
- Gradient descent updates parameters by minimizing the logistic cost function.  
- Training iterates until the cost is sufficiently low or a maximum number of iterations is reached.  

#### 7. üìâ Logistic Regression Cost Function  
- The cost function is binary cross-entropy, measuring prediction error.  
- Strong disagreement between prediction and true label results in high cost.  
- The goal is to minimize the cost function during training.  

#### 8. ‚úÖ Model Testing and Accuracy  
- Accuracy measures the percentage of correctly classified tweets on unseen data.  
- Model performance can be improved by adjusting learning rate, iterations, regularization, or features.  



<br>

## Study Notes



### 1. üìù Introduction to Sentiment Analysis with Logistic Regression

Sentiment analysis is a common task in natural language processing (NLP) where the goal is to determine the emotional tone behind a piece of text‚Äîtypically classifying it as positive or negative. For example, given a tweet like *"I am happy because I am learning NLP"*, we want to classify it as positive (label 1) or negative (label 0).

One straightforward and effective method to perform sentiment analysis is **logistic regression**, a supervised machine learning algorithm. Supervised learning means we train the model on labeled data (tweets with known sentiment) so it can learn to predict the sentiment of new, unseen tweets.

The process involves:

- Extracting **features** from the text (turning words into numbers the model can understand).
- Using these features to train a logistic regression model.
- Using the trained model to predict sentiment labels on new tweets.


### 2. üìö Vocabulary and Feature Extraction

Before training a model, we need to convert text into a numerical format. This step is called **feature extraction**.

#### Vocabulary

The **vocabulary** is the set of all unique words found in the tweets. For example, from the tweets:

- "I am happy because I am learning NLP"
- "I hated the movie"

The vocabulary would be:  
`[I, am, happy, because, learning, NLP, hated, the, movie]`

#### Feature Extraction and Sparse Representations

To represent a tweet numerically, we create a vector indicating the presence or frequency of each vocabulary word in the tweet. For example, the tweet "I am happy because I am learning NLP" might be represented as:

`[1, 1, 1, 1, 1, 1, 0, 0, 0]`

where each number corresponds to a word in the vocabulary (1 if the word is present, 0 if not).

This kind of vector is called a **sparse representation** because it contains many zeros (most words in the vocabulary do not appear in a single tweet). Sparse representations can cause problems:

- **Large training time:** Because the vectors are large and mostly zeros, computations become inefficient.
- **Large prediction time:** Similarly, making predictions takes longer.


### 3. üî¢ Using Word Frequencies for Feature Extraction

To improve feature extraction, we count how often each word appears in **positive** and **negative** tweets separately. This helps the model understand which words are more associated with positive or negative sentiment.

#### Frequency Counts

We divide the tweet corpus into two classes:

- **Positive tweets:** Tweets labeled as positive (1).
- **Negative tweets:** Tweets labeled as negative (0).

For each word in the vocabulary, we count:

- How many times it appears in positive tweets (PosFreq).
- How many times it appears in negative tweets (NegFreq).

For example, the word "happy" might appear 3 times in positive tweets and 0 times in negative tweets.

#### Frequency Dictionary

We store these counts in a dictionary mapping `(word, class)` pairs to their frequency:

```python
freqs = {
  ('happy', 1): 3,
  ('happy', 0): 0,
  ('sad', 1): 0,
  ('sad', 0): 2,
  ...
}
```

This dictionary is used to extract features for each tweet by summing the positive and negative frequencies of the words it contains.


### 4. üîç Preprocessing Text Data

Raw tweets contain a lot of noise that can confuse the model. Preprocessing cleans the text to improve feature extraction and model performance.

#### Common Preprocessing Steps

- **Removing stopwords:** Stopwords are common words like "and", "the", "is" that carry little sentiment information.
- **Removing punctuation:** Symbols like commas, periods, and exclamation marks are removed.
- **Removing handles and URLs:** Twitter handles (e.g., `@AndrewYNg`) and URLs are removed because they don't contribute to sentiment.
- **Stemming:** Reducing words to their root form (e.g., "tuning", "tuned", "tune" ‚Üí "tun") to treat different forms of the same word as one.
- **Lowercasing:** Converting all text to lowercase to avoid treating "Happy" and "happy" as different words.

Example:

Original tweet:  
`@YMourri and @AndrewYNg are tuning a GREAT AI. model at https://deeplearning.ai!!!`

After preprocessing:  
`[tun, great, ai, model]`

This cleaned version is easier for the model to work with.


### 5. üõ†Ô∏è Putting It All Together: Feature Extraction Pipeline

The entire process of preparing tweets for logistic regression involves:

1. **Preprocessing** each tweet to clean and normalize the text.
2. **Building the frequency dictionary** from the training set.
3. **Extracting features** for each tweet by:
   - Adding a bias term (usually 1).
   - Summing the positive frequencies of words in the tweet.
   - Summing the negative frequencies of words in the tweet.

This results in a feature vector for each tweet, for example:  
`[1, sum_pos_freq, sum_neg_freq]`

This vector is then used as input to the logistic regression model.


### 6. ‚öôÔ∏è Logistic Regression Overview

Logistic regression is a supervised learning algorithm used for binary classification tasks like sentiment analysis.

#### How Logistic Regression Works

- It takes the feature vector as input.
- Computes a weighted sum of the features plus a bias term.
- Applies the **sigmoid function** to squash the output into a probability between 0 and 1.
- If the output probability is greater than 0.5, the tweet is classified as positive; otherwise, negative.

#### Sigmoid Function

The sigmoid function is defined as:

\[
\sigma(z) = \frac{1}{1 + e^{-z}}
\]

where \( z \) is the weighted sum of features. The sigmoid outputs a value between 0 and 1, representing the probability of the tweet being positive.


### 7. üîÑ Training Logistic Regression with Gradient Descent

Training logistic regression means finding the best parameters (weights and bias) that minimize the difference between predicted and actual labels.

#### Steps in Training

1. **Initialize parameters** (weights and bias) randomly or to zero.
2. **Make predictions** using the current parameters.
3. **Calculate the cost** (loss) using the logistic cost function (binary cross-entropy).
4. **Compute gradients** of the cost with respect to parameters.
5. **Update parameters** by moving them in the direction that reduces the cost (gradient descent).
6. Repeat steps 2-5 until the cost is low enough or a maximum number of iterations is reached.


### 8. üìä Logistic Regression Cost Function

The cost function measures how well the model's predictions match the true labels. For logistic regression, the cost function is the **binary cross-entropy**:

\[
J(\theta) = -\frac{1}{m} \sum_{i=1}^m \left[ y^{(i)} \log(\hat{y}^{(i)}) + (1 - y^{(i)}) \log(1 - \hat{y}^{(i)}) \right]
\]

- \( y^{(i)} \) is the true label (0 or 1).
- \( \hat{y}^{(i)} \) is the predicted probability.
- \( m \) is the number of training examples.

#### Interpretation

- If the model strongly disagrees with the true label (e.g., predicts 0.99 for a negative tweet), the cost is very high.
- If the model strongly agrees (predicts close to 1 for a positive tweet), the cost is low.
- The goal is to minimize this cost during training.


### 9. ‚úÖ Testing and Evaluating the Model

After training, we test the model on a **validation set** (unseen tweets) to evaluate its performance.

#### Accuracy Metric

Accuracy is the percentage of tweets correctly classified by the model. For example, if the model correctly predicts 90 out of 100 tweets, accuracy is 90%.

#### Improving the Model

If accuracy is low, we can try:

- Adjusting the **learning rate** (step size in gradient descent).
- Increasing the **number of iterations**.
- Adding **regularization** to prevent overfitting.
- Engineering **new features** to better capture sentiment.


### Summary

This lecture covered the entire pipeline for building a sentiment analysis classifier using logistic regression:

- Understanding supervised learning and sentiment analysis.
- Extracting features from text using vocabulary and word frequencies.
- Preprocessing tweets to clean and normalize text.
- Implementing logistic regression with the sigmoid function.
- Training the model using gradient descent and minimizing the logistic cost function.
- Testing the model and evaluating accuracy.

By following these steps, you can build a simple yet effective tweet sentiment classifier!



<br>

## Questions



#### 1. What is the primary goal of sentiment analysis in the context of tweets?  
A) To identify the topic of the tweet  
B) To classify the tweet as positive or negative  
C) To extract named entities from the tweet  
D) To generate a summary of the tweet  

#### 2. Which of the following best describes a "sparse representation" of text data?  
A) A vector where most elements are zero  
B) A vector where all elements are one  
C) A vector with equal numbers of zeros and ones  
D) A vector that contains only unique words  

#### 3. Why can sparse representations cause problems in training logistic regression models?  
A) They increase the dimensionality, leading to longer training times  
B) They reduce the model‚Äôs ability to generalize  
C) They cause the model to overfit the training data  
D) They make prediction slower due to many zero values  

#### 4. When building a vocabulary from tweets, which of the following is true?  
A) The vocabulary contains all words from all tweets, including duplicates  
B) The vocabulary contains only unique words from the entire corpus  
C) The vocabulary excludes stopwords and punctuation by default  
D) The vocabulary is updated dynamically during prediction  

#### 5. What is the purpose of counting positive and negative frequencies of words in the corpus?  
A) To determine the sentiment polarity of each word  
B) To remove words that appear equally in both classes  
C) To create features that reflect how often words appear in each class  
D) To normalize the length of tweets  

#### 6. Which of the following is NOT a typical preprocessing step before feature extraction?  
A) Removing punctuation  
B) Lowercasing all words  
C) Adding stopwords to the vocabulary  
D) Stemming words to their root form  

#### 7. How does stemming help improve the feature extraction process?  
A) By increasing the size of the vocabulary  
B) By grouping different forms of a word into one feature  
C) By removing all stopwords from the text  
D) By converting words into their synonyms  

#### 8. What does the bias term in the feature vector represent?  
A) The frequency of the most common word in the tweet  
B) A constant value to allow the model to fit data better  
C) The difference between positive and negative word counts  
D) The length of the tweet  

#### 9. In logistic regression, what does the sigmoid function output represent?  
A) The exact class label (0 or 1)  
B) The probability that the input belongs to the positive class  
C) The weighted sum of input features  
D) The error between predicted and actual labels  

#### 10. Which of the following statements about the logistic regression cost function is true?  
A) It penalizes predictions that strongly disagree with the true label more heavily  
B) It is minimized when the model predicts probabilities close to 0.5 for all examples  
C) It uses binary cross-entropy to measure prediction error  
D) It is always positive and decreases as the model improves  

#### 11. During gradient descent training, what happens if the learning rate is set too high?  
A) The model converges faster without any issues  
B) The cost function may oscillate or diverge  
C) The model will underfit the training data  
D) The parameters update too slowly  

#### 12. Why is it important to test the logistic regression model on a validation set?  
A) To check if the model memorized the training data  
B) To evaluate the model‚Äôs performance on unseen data  
C) To tune hyperparameters like learning rate and iterations  
D) To increase the size of the training set  

#### 13. Which of the following can improve the accuracy of a logistic regression sentiment classifier?  
A) Increasing the number of training iterations  
B) Adding regularization to prevent overfitting  
C) Using raw tweets without preprocessing  
D) Engineering new features that capture sentiment better  

#### 14. What is the role of the frequency dictionary (freqs) in feature extraction?  
A) It maps each word to its sentiment polarity score  
B) It stores the count of each word in positive and negative classes  
C) It removes stopwords from the tweets  
D) It converts tweets into sparse vectors  

#### 15. Consider the tweet: "I am not happy." Which of the following feature extraction outcomes is most likely?  
A) High positive frequency sum, low negative frequency sum  
B) High negative frequency sum, low positive frequency sum  
C) Equal positive and negative frequency sums  
D) Zero frequency sums because of stopwords  

#### 16. Which of the following is a potential drawback of removing all stopwords during preprocessing?  
A) It may remove words that carry sentiment in some contexts  
B) It always improves model accuracy  
C) It increases the size of the vocabulary unnecessarily  
D) It causes the model to ignore punctuation  

#### 17. How does logistic regression differ from linear regression in the context of sentiment analysis?  
A) Logistic regression outputs probabilities, linear regression outputs continuous values  
B) Logistic regression can only handle binary classification, linear regression cannot  
C) Logistic regression uses the sigmoid function, linear regression does not  
D) Logistic regression requires feature extraction, linear regression does not  

#### 18. What does a cost value close to zero indicate during logistic regression training?  
A) The model is making poor predictions  
B) The model‚Äôs predictions closely match the true labels  
C) The model is overfitting the training data  
D) The model has not yet started learning  

#### 19. Why might the presence of URLs and Twitter handles in tweets negatively impact sentiment analysis?  
A) They often contain sentiment words  
B) They add noise and irrelevant information to the text  
C) They increase the vocabulary size unnecessarily  
D) They always indicate negative sentiment  

#### 20. When extracting features from a tweet, why is it useful to include both the sum of positive and negative word frequencies?  
A) To capture the overall sentiment balance in the tweet  
B) To ensure the feature vector is sparse  
C) To allow the model to distinguish between positive and negative cues  
D) To reduce the dimensionality of the input data  



<br>

## Answers



#### 1. What is the primary goal of sentiment analysis in the context of tweets?  
A) ‚úó Identifying the topic is not the main goal here.  
B) ‚úì Correct: Sentiment analysis classifies tweets as positive or negative.  
C) ‚úó Named entity recognition is a different NLP task.  
D) ‚úó Summarization is unrelated to sentiment classification.  

**Correct:** B


#### 2. Which of the following best describes a "sparse representation" of text data?  
A) ‚úì Correct: Sparse vectors mostly contain zeros.  
B) ‚úó All ones is dense, not sparse.  
C) ‚úó Equal zeros and ones is not necessarily sparse.  
D) ‚úó Vocabulary is a set of unique words, not a vector.  

**Correct:** A


#### 3. Why can sparse representations cause problems in training logistic regression models?  
A) ‚úì Correct: High dimensionality with many zeros slows training.  
B) ‚úó Sparse vectors don‚Äôt inherently reduce generalization.  
C) ‚úó Overfitting is not directly caused by sparsity.  
D) ‚úì Correct: Prediction time increases due to large sparse vectors.  

**Correct:** A, D


#### 4. When building a vocabulary from tweets, which of the following is true?  
A) ‚úó Vocabulary contains unique words, not duplicates.  
B) ‚úì Correct: Vocabulary is the set of unique words in corpus.  
C) ‚úó Stopwords and punctuation removal is a separate preprocessing step.  
D) ‚úó Vocabulary is fixed after training, not updated during prediction.  

**Correct:** B


#### 5. What is the purpose of counting positive and negative frequencies of words in the corpus?  
A) ‚úì Correct: Helps identify word sentiment association.  
B) ‚úó Words appearing equally are not necessarily removed.  
C) ‚úì Correct: Frequencies create features reflecting class association.  
D) ‚úó Frequency counts do not normalize tweet length.  

**Correct:** A, C


#### 6. Which of the following is NOT a typical preprocessing step before feature extraction?  
A) ‚úó Removing punctuation is typical.  
B) ‚úó Lowercasing is typical.  
C) ‚úì Correct: Adding stopwords is not done; usually removed.  
D) ‚úó Stemming is typical.  

**Correct:** C


#### 7. How does stemming help improve the feature extraction process?  
A) ‚úó Stemming reduces vocabulary size, not increases it.  
B) ‚úì Correct: Groups word variants into one root form.  
C) ‚úó Stemming does not remove stopwords.  
D) ‚úó Stemming does not convert words into synonyms.  

**Correct:** B


#### 8. What does the bias term in the feature vector represent?  
A) ‚úó Bias is not related to word frequency.  
B) ‚úì Correct: Bias is a constant to help model fit better.  
C) ‚úó Bias is not the difference between positive and negative counts.  
D) ‚úó Bias is not tweet length.  

**Correct:** B


#### 9. In logistic regression, what does the sigmoid function output represent?  
A) ‚úó Sigmoid outputs probability, not exact class.  
B) ‚úì Correct: Output is probability of positive class.  
C) ‚úó Weighted sum is input to sigmoid, not output.  
D) ‚úó Sigmoid output is not error.  

**Correct:** B


#### 10. Which of the following statements about the logistic regression cost function is true?  
A) ‚úì Correct: Strong disagreement leads to high cost.  
B) ‚úó Cost is minimized when predictions are close to true labels, not 0.5.  
C) ‚úì Correct: Binary cross-entropy is used as cost.  
D) ‚úì Correct: Cost is positive and decreases as model improves.  

**Correct:** A, C, D


#### 11. During gradient descent training, what happens if the learning rate is set too high?  
A) ‚úó Too high learning rate can cause divergence, not faster convergence.  
B) ‚úì Correct: Cost may oscillate or diverge.  
C) ‚úó Underfitting is usually due to too low capacity or iterations.  
D) ‚úó Parameters update too fast, not too slow.  

**Correct:** B


#### 12. Why is it important to test the logistic regression model on a validation set?  
A) ‚úì Correct: To check if model memorized training data (overfitting).  
B) ‚úì Correct: To evaluate performance on unseen data.  
C) ‚úì Correct: To tune hyperparameters effectively.  
D) ‚úó Validation set is not used to increase training size.  

**Correct:** A, B, C


#### 13. Which of the following can improve the accuracy of a logistic regression sentiment classifier?  
A) ‚úì Correct: More iterations can improve training.  
B) ‚úì Correct: Regularization helps prevent overfitting.  
C) ‚úó Using raw tweets without preprocessing usually hurts accuracy.  
D) ‚úì Correct: New features can capture sentiment better.  

**Correct:** A, B, D


#### 14. What is the role of the frequency dictionary (freqs) in feature extraction?  
A) ‚úó It does not assign sentiment polarity scores directly.  
B) ‚úì Correct: Stores counts of words in positive and negative classes.  
C) ‚úó It does not remove stopwords.  
D) ‚úó It does not convert tweets into sparse vectors directly.  

**Correct:** B


#### 15. Consider the tweet: "I am not happy." Which of the following feature extraction outcomes is most likely?  
A) ‚úó "happy" is positive, but "not" negates it, so positive sum likely low.  
B) ‚úì Correct: Negative frequency sum likely higher due to "not".  
C) ‚úó Positive and negative sums unlikely equal due to negation.  
D) ‚úó Stopwords like "not" are important here; sums won‚Äôt be zero.  

**Correct:** B


#### 16. Which of the following is a potential drawback of removing all stopwords during preprocessing?  
A) ‚úì Correct: Some stopwords can carry sentiment in context.  
B) ‚úó Removing stopwords does not always improve accuracy.  
C) ‚úó Removing stopwords reduces vocabulary size, not increases it.  
D) ‚úó Stopwords removal does not affect punctuation handling.  

**Correct:** A


#### 17. How does logistic regression differ from linear regression in the context of sentiment analysis?  
A) ‚úì Correct: Logistic outputs probabilities; linear outputs continuous values.  
B) ‚úó Linear regression can be used for binary classification but is less suitable.  
C) ‚úì Correct: Logistic uses sigmoid; linear does not.  
D) ‚úó Both require feature extraction; this is not a difference.  

**Correct:** A, C


#### 18. What does a cost value close to zero indicate during logistic regression training?  
A) ‚úó Low cost means good predictions, not poor.  
B) ‚úì Correct: Predictions closely match true labels.  
C) ‚úó Overfitting is not indicated by low cost alone.  
D) ‚úó Low cost means model has learned, not that it hasn‚Äôt started.  

**Correct:** B


#### 19. Why might the presence of URLs and Twitter handles in tweets negatively impact sentiment analysis?  
A) ‚úó URLs and handles rarely contain sentiment words.  
B) ‚úì Correct: They add noise and irrelevant info.  
C) ‚úì Correct: They increase vocabulary size unnecessarily.  
D) ‚úó They do not always indicate negative sentiment.  

**Correct:** B, C


#### 20. When extracting features from a tweet, why is it useful to include both the sum of positive and negative word frequencies?  
A) ‚úì Correct: Captures overall sentiment balance.  
B) ‚úó Including both does not ensure sparsity.  
C) ‚úì Correct: Helps model distinguish positive vs negative cues.  
D) ‚úó It does not reduce dimensionality; it summarizes frequencies.  

**Correct:** A, C

