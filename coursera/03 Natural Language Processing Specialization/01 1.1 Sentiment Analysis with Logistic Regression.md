## 1.1 Sentiment Analysis with Logistic Regression

## Study Notes

### 1. üìù Introduction to Sentiment Analysis with Logistic Regression

Sentiment analysis is a common task in natural language processing (NLP) where the goal is to determine the emotional tone behind a piece of text‚Äîtypically classifying it as positive or negative. For example, given a tweet like *"I am happy because I am learning NLP"*, we want to classify it as positive (label 1) or negative (label 0).

One straightforward and effective method to perform sentiment analysis is **logistic regression**, a supervised machine learning algorithm. Supervised learning means we train the model on labeled data (tweets with known sentiment) so it can learn to predict the sentiment of new, unseen tweets.

The process involves:

- Extracting **features** from the text (turning words into numbers the model can understand).
- Using these features to train a logistic regression model.
- Using the trained model to predict sentiment labels on new tweets.


### 2. üìö Vocabulary and Feature Extraction

Before training a model, we need to convert text into a numerical format. This step is called **feature extraction**.

#### Vocabulary

The **vocabulary** is the set of all unique words found in the tweets. For example, from the tweets:

- "I am happy because I am learning NLP"
- "I hated the movie"

The vocabulary would be:  
`[I, am, happy, because, learning, NLP, hated, the, movie]`

#### Feature Extraction and Sparse Representations

To represent a tweet numerically, we create a vector indicating the presence or frequency of each vocabulary word in the tweet. For example, the tweet "I am happy because I am learning NLP" might be represented as:

`[1, 1, 1, 1, 1, 1, 0, 0, 0]`

where each number corresponds to a word in the vocabulary (1 if the word is present, 0 if not).

This kind of vector is called a **sparse representation** because it contains many zeros (most words in the vocabulary do not appear in a single tweet). Sparse representations can cause problems:

- **Large training time:** Because the vectors are large and mostly zeros, computations become inefficient.
- **Large prediction time:** Similarly, making predictions takes longer.


### 3. üî¢ Using Word Frequencies for Feature Extraction

To improve feature extraction, we count how often each word appears in **positive** and **negative** tweets separately. This helps the model understand which words are more associated with positive or negative sentiment.

#### Frequency Counts

We divide the tweet corpus into two classes:

- **Positive tweets:** Tweets labeled as positive (1).
- **Negative tweets:** Tweets labeled as negative (0).

For each word in the vocabulary, we count:

- How many times it appears in positive tweets (PosFreq).
- How many times it appears in negative tweets (NegFreq).

For example, the word "happy" might appear 3 times in positive tweets and 0 times in negative tweets.

#### Frequency Dictionary

We store these counts in a dictionary mapping `(word, class)` pairs to their frequency:

```python
freqs = {
  ('happy', 1): 3,
  ('happy', 0): 0,
  ('sad', 1): 0,
  ('sad', 0): 2,
  ...
}
```

This dictionary is used to extract features for each tweet by summing the positive and negative frequencies of the words it contains.


### 4. üîç Preprocessing Text Data

Raw tweets contain a lot of noise that can confuse the model. Preprocessing cleans the text to improve feature extraction and model performance.

#### Common Preprocessing Steps

- **Removing stopwords:** Stopwords are common words like "and", "the", "is" that carry little sentiment information.
- **Removing punctuation:** Symbols like commas, periods, and exclamation marks are removed.
- **Removing handles and URLs:** Twitter handles (e.g., `@AndrewYNg`) and URLs are removed because they don't contribute to sentiment.
- **Stemming:** Reducing words to their root form (e.g., "tuning", "tuned", "tune" ‚Üí "tun") to treat different forms of the same word as one.
- **Lowercasing:** Converting all text to lowercase to avoid treating "Happy" and "happy" as different words.

Example:

Original tweet:  
`@YMourri and @AndrewYNg are tuning a GREAT AI. model at https://deeplearning.ai!!!`

After preprocessing:  
`[tun, great, ai, model]`

This cleaned version is easier for the model to work with.


### 5. üõ†Ô∏è Putting It All Together: Feature Extraction Pipeline

The entire process of preparing tweets for logistic regression involves:

1. **Preprocessing** each tweet to clean and normalize the text.
2. **Building the frequency dictionary** from the training set.
3. **Extracting features** for each tweet by:
   - Adding a bias term (usually 1).
   - Summing the positive frequencies of words in the tweet.
   - Summing the negative frequencies of words in the tweet.

This results in a feature vector for each tweet, for example:  
`[1, sum_pos_freq, sum_neg_freq]`

This vector is then used as input to the logistic regression model.


### 6. ‚öôÔ∏è Logistic Regression Overview

Logistic regression is a supervised learning algorithm used for binary classification tasks like sentiment analysis.

#### How Logistic Regression Works

- It takes the feature vector as input.
- Computes a weighted sum of the features plus a bias term.
- Applies the **sigmoid function** to squash the output into a probability between 0 and 1.
- If the output probability is greater than 0.5, the tweet is classified as positive; otherwise, negative.

#### Sigmoid Function

The sigmoid function is defined as:

\[
\sigma(z) = \frac{1}{1 + e^{-z}}
\]

where \( z \) is the weighted sum of features. The sigmoid outputs a value between 0 and 1, representing the probability of the tweet being positive.


### 7. üîÑ Training Logistic Regression with Gradient Descent

Training logistic regression means finding the best parameters (weights and bias) that minimize the difference between predicted and actual labels.

#### Steps in Training

1. **Initialize parameters** (weights and bias) randomly or to zero.
2. **Make predictions** using the current parameters.
3. **Calculate the cost** (loss) using the logistic cost function (binary cross-entropy).
4. **Compute gradients** of the cost with respect to parameters.
5. **Update parameters** by moving them in the direction that reduces the cost (gradient descent).
6. Repeat steps 2-5 until the cost is low enough or a maximum number of iterations is reached.


### 8. üìä Logistic Regression Cost Function

The cost function measures how well the model's predictions match the true labels. For logistic regression, the cost function is the **binary cross-entropy**:

\[
J(\theta) = -\frac{1}{m} \sum_{i=1}^m \left[ y^{(i)} \log(\hat{y}^{(i)}) + (1 - y^{(i)}) \log(1 - \hat{y}^{(i)}) \right]
\]

- \( y^{(i)} \) is the true label (0 or 1).
- \( \hat{y}^{(i)} \) is the predicted probability.
- \( m \) is the number of training examples.

#### Interpretation

- If the model strongly disagrees with the true label (e.g., predicts 0.99 for a negative tweet), the cost is very high.
- If the model strongly agrees (predicts close to 1 for a positive tweet), the cost is low.
- The goal is to minimize this cost during training.


### 9. ‚úÖ Testing and Evaluating the Model

After training, we test the model on a **validation set** (unseen tweets) to evaluate its performance.

#### Accuracy Metric

Accuracy is the percentage of tweets correctly classified by the model. For example, if the model correctly predicts 90 out of 100 tweets, accuracy is 90%.

#### Improving the Model

If accuracy is low, we can try:

- Adjusting the **learning rate** (step size in gradient descent).
- Increasing the **number of iterations**.
- Adding **regularization** to prevent overfitting.
- Engineering **new features** to better capture sentiment.


### Summary

This lecture covered the entire pipeline for building a sentiment analysis classifier using logistic regression:

- Understanding supervised learning and sentiment analysis.
- Extracting features from text using vocabulary and word frequencies.
- Preprocessing tweets to clean and normalize text.
- Implementing logistic regression with the sigmoid function.
- Training the model using gradient descent and minimizing the logistic cost function.
- Testing the model and evaluating accuracy.

By following these steps, you can build a simple yet effective tweet sentiment classifier!



<br>

## Key Points

#### 1. üß† Sentiment Analysis and Logistic Regression  
- Sentiment analysis classifies text as positive (1) or negative (0).  
- Logistic regression is a supervised learning algorithm used for binary classification tasks like sentiment analysis.  
- The sigmoid function outputs a probability between 0 and 1 for classification.  

#### 2. üìö Vocabulary and Feature Extraction  
- Vocabulary is the set of unique words from the tweet corpus.  
- Tweets are represented as sparse vectors indicating word presence or frequency.  
- Sparse representations contain many zeros, causing large training and prediction times.  

#### 3. üî¢ Word Frequency Counts for Feature Extraction  
- The tweet corpus is divided into positive and negative classes.  
- Each word‚Äôs frequency is counted separately in positive and negative tweets (PosFreq and NegFreq).  
- Frequencies are stored in a dictionary mapping (word, class) pairs to counts.  

#### 4. üßπ Text Preprocessing  
- Preprocessing removes stopwords, punctuation, Twitter handles, and URLs.  
- Stemming reduces words to their root form (e.g., tuning ‚Üí tun).  
- Lowercasing normalizes text to avoid case sensitivity issues.  

#### 5. üõ†Ô∏è Feature Vector Construction  
- Features for each tweet include: bias term (1), sum of positive word frequencies, and sum of negative word frequencies.  
- Feature vectors are used as input to logistic regression.  

#### 6. ‚öôÔ∏è Logistic Regression Training  
- Parameters (weights and bias) are initialized before training.  
- Gradient descent updates parameters by minimizing the logistic cost function.  
- Training iterates until the cost is sufficiently low or a maximum number of iterations is reached.  

#### 7. üìâ Logistic Regression Cost Function  
- The cost function is binary cross-entropy, measuring prediction error.  
- Strong disagreement between prediction and true label results in high cost.  
- The goal is to minimize the cost function during training.  

#### 8. ‚úÖ Model Testing and Accuracy  
- Accuracy measures the percentage of correctly classified tweets on unseen data.  
- Model performance can be improved by adjusting learning rate, iterations, regularization, or features.