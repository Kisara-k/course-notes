## 2.3 Autocomplete and Language Models

## Questions

#### 1. What does an N-gram language model primarily estimate?  
A) The grammatical correctness of a sentence  
B) The probability of a word given the previous N-1 words  
C) The semantic meaning of a sentence  
D) The probability of a sequence of words  

#### 2. Which of the following are true about the Markov assumption in N-gram models?  
A) It assumes the next word depends only on the previous N-1 words  
B) It allows modeling entire sentences without approximation  
C) It simplifies sequence probability calculation  
D) It assumes all words in a sentence are independent  

#### 3. Why are start `<s>` and end `</s>` tokens added to sentences in N-gram models?  
A) To mark sentence boundaries explicitly  
B) To increase the vocabulary size  
C) To help the model learn when sentences begin and end  
D) To improve smoothing performance  

#### 4. Given a corpus with the sentence “I am happy”, which of the following are valid bigrams?  
A) (I, am)  
B) (am, happy)  
C) (happy, I)  
D) (I, happy)  

#### 5. When calculating the probability of a sentence using a bigram model, which of the following is true?  
A) Multiply the probabilities of each word given the previous word  
B) Use the probability of each word independently  
C) Use the probability of each word given the entire previous sentence  
D) Add the probabilities of each bigram  

#### 6. What is the main reason for using log probabilities in language models?  
A) To make probabilities larger  
B) To avoid numerical underflow when multiplying many small probabilities  
C) To simplify the calculation of perplexity  
D) To convert probabilities into percentages  

#### 7. Which of the following statements about perplexity are correct?  
A) Lower perplexity indicates a better language model  
B) Perplexity measures how well a model predicts unseen data  
C) Perplexity is always greater than 1  
D) Perplexity can be directly interpreted as accuracy  

#### 8. How does the `<UNK>` token help in language modeling?  
A) It replaces all rare or unseen words in the corpus  
B) It increases the size of the vocabulary indefinitely  
C) It allows the model to assign non-zero probability to unknown words  
D) It is only used during testing, not training  

#### 9. Which of the following are challenges when estimating N-gram probabilities from a corpus?  
A) Some valid N-grams may never appear in the training data  
B) The corpus may contain out-of-vocabulary words  
C) The corpus always contains every possible N-gram  
D) Multiplying many probabilities can cause underflow  

#### 10. What is the effect of add-one (Laplacian) smoothing on N-gram probabilities?  
A) It assigns zero probability to unseen N-grams  
B) It adds one to all N-gram counts to avoid zero probabilities  
C) It can overly smooth probabilities, reducing model accuracy  
D) It only affects unigrams, not higher-order N-grams  

#### 11. In backoff smoothing, if a trigram is missing from the corpus, what happens?  
A) The model uses the corresponding bigram probability instead  
B) The model assigns zero probability to the trigram  
C) The model interpolates trigram and bigram probabilities equally  
D) The model ignores the missing trigram and moves on  

#### 12. When splitting a corpus into training, validation, and test sets, which of the following are best practices?  
A) Use 80% training, 10% validation, 10% test for small corpora  
B) Use 98% training, 1% validation, 1% test for large corpora  
C) Use the same data for training and testing to maximize data usage  
D) Ensure test data is unseen during training  

#### 13. Which of the following statements about N-gram count matrices are true?  
A) Rows correspond to (N-1)-grams and columns correspond to possible next words  
B) Each cell contains the probability of the N-gram  
C) Counts are converted to probabilities by dividing by the row sum  
D) The count matrix is always square  

#### 14. Why might a trigram model have lower perplexity than a bigram model?  
A) It considers more context, leading to better predictions  
B) It always has more data to train on  
C) It ignores start and end tokens  
D) It uses fewer parameters than a bigram model  

#### 15. Which of the following are true about out-of-vocabulary (OOV) words?  
A) They are words not seen in the training corpus  
B) They are replaced by `<UNK>` during preprocessing  
C) They always cause the model to assign zero probability to sentences containing them  
D) They can be handled by smoothing techniques  

#### 16. What is the primary purpose of interpolation in smoothing?  
A) To combine probabilities from different N-gram orders  
B) To discard lower-order N-gram probabilities  
C) To assign zero probability to unseen N-grams  
D) To increase the vocabulary size  

#### 17. Consider the sentence probability approximation using bigrams: P(Mary likes cats) = P(Mary|<s>) * P(likes|Mary) * P(cats|likes) * P(</s>|cats). Which of the following is true?  
A) The start token `<s>` is necessary to model sentence beginning  
B) The end token `</s>` is necessary to model sentence ending  
C) The probability of the first word is unconditional  
D) The model assumes independence between words  

#### 18. Which of the following statements about smoothing methods are correct?  
A) Add-k smoothing generalizes add-one smoothing by adding a constant k  
B) Kneser-Ney smoothing is an advanced method that improves probability estimates for rare N-grams  
C) Good-Turing smoothing redistributes probability mass from seen to unseen N-grams  
D) Smoothing always increases the probability of frequent N-grams  

#### 19. When evaluating a language model, why is it important to use the same vocabulary across models?  
A) To ensure perplexity comparisons are fair and meaningful  
B) Because vocabulary size does not affect model performance  
C) To avoid introducing `<UNK>` inconsistencies  
D) Because different vocabularies always produce the same probabilities  

#### 20. Which of the following best describe the limitations of N-gram language models?  
A) They cannot capture long-range dependencies beyond N-1 words  
B) They require very large corpora to estimate probabilities accurately  
C) They always produce grammatically correct sentences  
D) They assign zero probability to unseen N-grams without smoothing



<br>

## Answers

#### 1. What does an N-gram language model primarily estimate?  
A) ✗ It does not directly assess grammatical correctness.  
B) ✓ It estimates the probability of a word given the previous N-1 words.  
C) ✗ Semantic meaning is beyond simple N-gram models.  
D) ✓ It estimates the probability of a sequence of words via chain rule approximation.  

**Correct:** B, D


#### 2. Which of the following are true about the Markov assumption in N-gram models?  
A) ✓ Correct, it limits dependency to previous N-1 words.  
B) ✗ It is an approximation, not exact modeling of entire sentences.  
C) ✓ Simplifies probability calculations by reducing context.  
D) ✗ Words are not independent; they depend on previous words.  

**Correct:** A, C


#### 3. Why are start `<s>` and end `</s>` tokens added to sentences in N-gram models?  
A) ✓ They explicitly mark sentence boundaries.  
B) ✗ They do not increase vocabulary size significantly.  
C) ✓ Help model learn sentence start and end positions.  
D) ✗ Smoothing is unrelated to start/end tokens.  

**Correct:** A, C


#### 4. Given a corpus with the sentence “I am happy”, which of the following are valid bigrams?  
A) ✓ (I, am) appears consecutively.  
B) ✓ (am, happy) appears consecutively.  
C) ✗ (happy, I) does not appear in order.  
D) ✗ (I, happy) skips a word, not a bigram.  

**Correct:** A, B


#### 5. When calculating the probability of a sentence using a bigram model, which of the following is true?  
A) ✓ Multiply conditional probabilities of each word given previous word.  
B) ✗ Words are not independent; context matters.  
C) ✗ Bigram model only considers previous one word, not entire sentence.  
D) ✗ Probabilities multiply, not add.  

**Correct:** A


#### 6. What is the main reason for using log probabilities in language models?  
A) ✗ Logarithms do not make probabilities larger.  
B) ✓ Avoid numerical underflow from multiplying many small probabilities.  
C) ✗ Log probabilities simplify multiplication, not perplexity calculation directly.  
D) ✗ Log probabilities are not percentages.  

**Correct:** B


#### 7. Which of the following statements about perplexity are correct?  
A) ✓ Lower perplexity means better predictive performance.  
B) ✓ Measures how well model predicts unseen data.  
C) ✗ Perplexity can be less than 1 in rare cases (e.g., degenerate distributions).  
D) ✗ Perplexity is not accuracy; it measures uncertainty.  

**Correct:** A, B


#### 8. How does the `<UNK>` token help in language modeling?  
A) ✓ Replaces rare or unseen words to handle OOV.  
B) ✗ It limits vocabulary size by grouping unknowns.  
C) ✓ Allows assigning non-zero probability to unknown words.  
D) ✗ Used during both training and testing preprocessing.  

**Correct:** A, C


#### 9. Which of the following are challenges when estimating N-gram probabilities from a corpus?  
A) ✓ Some valid N-grams may be missing in training data.  
B) ✓ OOV words appear in input but not in training corpus.  
C) ✗ Corpus rarely contains every possible N-gram.  
D) ✓ Multiplying many probabilities can cause underflow.  

**Correct:** A, B, D


#### 10. What is the effect of add-one (Laplacian) smoothing on N-gram probabilities?  
A) ✗ It prevents zero probabilities for unseen N-grams.  
B) ✓ Adds one to all counts to avoid zeros.  
C) ✓ Can overly smooth and reduce accuracy by inflating rare counts.  
D) ✗ Affects all N-grams, not just unigrams.  

**Correct:** B, C


#### 11. In backoff smoothing, if a trigram is missing from the corpus, what happens?  
A) ✓ The model backs off to bigram probability.  
B) ✗ It does not assign zero probability directly.  
C) ✗ Katz backoff discounts but does not interpolate equally.  
D) ✗ The model does not ignore missing N-grams.  

**Correct:** A


#### 12. When splitting a corpus into training, validation, and test sets, which of the following are best practices?  
A) ✓ 80/10/10 split for small corpora is standard.  
B) ✓ 98/1/1 split for large corpora is common.  
C) ✗ Using same data for training and testing causes overfitting.  
D) ✓ Test data must be unseen during training for valid evaluation.  

**Correct:** A, B, D


#### 13. Which of the following statements about N-gram count matrices are true?  
A) ✓ Rows correspond to (N-1)-grams, columns to next words.  
B) ✗ Count matrix contains raw counts, not probabilities.  
C) ✓ Probabilities are computed by dividing counts by row sums.  
D) ✗ Count matrix is not necessarily square; depends on vocabulary.  

**Correct:** A, C


#### 14. Why might a trigram model have lower perplexity than a bigram model?  
A) ✓ More context leads to better predictions.  
B) ✗ Trigram models often have less data per N-gram, not more.  
C) ✗ Start/end tokens are used in both models.  
D) ✗ Trigram models have more parameters, not fewer.  

**Correct:** A


#### 15. Which of the following are true about out-of-vocabulary (OOV) words?  
A) ✓ Words not seen in training corpus.  
B) ✓ Replaced by `<UNK>` during preprocessing.  
C) ✗ With `<UNK>`, model does not assign zero probability.  
D) ✗ Smoothing does not directly handle OOV; `<UNK>` does.  

**Correct:** A, B


#### 16. What is the primary purpose of interpolation in smoothing?  
A) ✓ Combine probabilities from different N-gram orders.  
B) ✗ It does not discard lower-order probabilities.  
C) ✗ It prevents zero probabilities rather than assigning them.  
D) ✗ It does not increase vocabulary size.  

**Correct:** A


#### 17. Consider the sentence probability approximation using bigrams: P(Mary likes cats) = P(Mary|<s>) * P(likes|Mary) * P(cats|likes) * P(</s>|cats). Which of the following is true?  
A) ✓ `<s>` models sentence start explicitly.  
B) ✓ `</s>` models sentence end explicitly.  
C) ✗ The first word’s probability is conditional on `<s>`, not unconditional.  
D) ✗ Model assumes dependence on previous word, not independence.  

**Correct:** A, B


#### 18. Which of the following statements about smoothing methods are correct?  
A) ✓ Add-k smoothing generalizes add-one by adding any constant k.  
B) ✓ Kneser-Ney is an advanced smoothing method improving rare N-gram estimates.  
C) ✓ Good-Turing redistributes probability mass to unseen N-grams.  
D) ✗ Smoothing reduces probabilities of frequent N-grams to allocate mass to unseen ones.  

**Correct:** A, B, C


#### 19. When evaluating a language model, why is it important to use the same vocabulary across models?  
A) ✓ Ensures fair perplexity comparison.  
B) ✗ Vocabulary size affects model performance.  
C) ✓ Avoids inconsistencies in `<UNK>` handling.  
D) ✗ Different vocabularies produce different probabilities, so not comparable.  

**Correct:** A, C


#### 20. Which of the following best describe the limitations of N-gram language models?  
A) ✓ Cannot capture dependencies beyond N-1 words.  
B) ✓ Require large corpora for accurate probability estimation.  
C) ✗ Do not guarantee grammatical correctness.  
D) ✓ Assign zero probability to unseen N-grams without smoothing.  

**Correct:** A, B, D