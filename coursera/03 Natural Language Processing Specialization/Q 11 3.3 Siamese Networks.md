## 3.3 Siamese Networks

## Questions

#### 1. What is the primary purpose of a Siamese Network?  
A) To classify inputs into fixed categories  
B) To measure similarity or difference between two inputs  
C) To generate new data samples  
D) To cluster data without labels  

#### 2. Which of the following best describes the architecture of a Siamese Network?  
A) Two identical subnetworks sharing weights processing two inputs separately  
B) A single network processing concatenated inputs  
C) Two different networks with independent weights processing two inputs  
D) Multiple networks combined to classify multiple classes simultaneously  

#### 3. Why are LSTMs commonly used in Siamese Networks for NLP tasks?  
A) Because they can handle sequential data and capture context  
B) Because they reduce the dimensionality of input data  
C) Because they are faster than convolutional networks  
D) Because they ignore word order, focusing only on word frequency  

#### 4. What does the cosine similarity between two vectors represent in a Siamese Network?  
A) The Euclidean distance between two inputs  
B) The angle between two vector embeddings indicating similarity  
C) The probability that two inputs belong to the same class  
D) The sum of element-wise differences between vectors  

#### 5. In the context of triplet loss, what is the role of the margin α (alpha)?  
A) To ensure the positive pair is exactly equal to the negative pair  
B) To enforce a minimum distance difference between positive and negative pairs  
C) To scale the embeddings before similarity calculation  
D) To normalize the input vectors  

#### 6. Which of the following statements about triplet loss is true?  
A) It requires three inputs: anchor, positive, and negative  
B) It minimizes the distance between anchor and negative examples  
C) It maximizes the distance between anchor and positive examples  
D) It encourages the anchor-positive distance to be smaller than anchor-negative distance by at least α  

#### 7. What is a "hard negative" in the context of training Siamese Networks?  
A) A negative example that is very different from the anchor  
B) A negative example that is very similar to the anchor, making it difficult to distinguish  
C) A positive example mislabeled as negative  
D) An example that is easy for the network to classify correctly  

#### 8. Why is hard negative mining important during training?  
A) It speeds up training by ignoring difficult examples  
B) It helps the network learn better by focusing on challenging distinctions  
C) It reduces overfitting by removing easy examples  
D) It ensures the network only learns from positive pairs  

#### 9. How does one-shot learning differ from traditional classification?  
A) One-shot learning requires many examples per class  
B) One-shot learning learns a similarity function rather than fixed class boundaries  
C) One-shot learning cannot generalize to new classes  
D) One-shot learning requires retraining for every new class  

#### 10. Which of the following is NOT a typical application of Siamese Networks?  
A) Duplicate question detection  
B) Face verification  
C) Image generation  
D) Handwritten signature verification  

#### 11. When preparing batches for training a Siamese Network, which of the following is true?  
A) Batches contain only positive pairs  
B) Batches contain both positive and negative pairs  
C) Batches are always of size one  
D) Batches must contain triplets (anchor, positive, negative) for triplet loss  

#### 12. What happens if the margin α in triplet loss is set too high?  
A) The network may fail to converge because the margin is too strict  
B) The network will easily satisfy the loss and stop learning  
C) The embeddings will collapse to a single point  
D) The network will ignore negative examples  

#### 13. Which similarity or distance metrics can be used in Siamese Networks?  
A) Only cosine similarity  
B) Only Euclidean distance  
C) Any similarity or distance metric, including cosine similarity and Euclidean distance  
D) Only Manhattan distance  

#### 14. In testing a Siamese Network, how is the decision made whether two inputs are similar?  
A) By checking if the cosine similarity exceeds a predefined threshold τ  
B) By classifying the inputs into one of K classes  
C) By computing the Euclidean distance and comparing it to zero  
D) By using the network’s softmax output  

#### 15. Which of the following statements about embedding vectors in Siamese Networks is correct?  
A) Embeddings are fixed-length vector representations of inputs  
B) Embeddings are raw input data converted to binary form  
C) Embeddings are always one-dimensional scalars  
D) Embeddings must be normalized before similarity calculation  

#### 16. What is the main challenge when training with random triplets?  
A) Random triplets always contain hard negatives  
B) Random triplets may include many easy negatives that provide little learning signal  
C) Random triplets require more computational resources than hard triplets  
D) Random triplets cause the network to overfit  

#### 17. Which of the following best explains why Siamese Networks are preferred over traditional classifiers for duplicate question detection?  
A) They require less labeled data for new questions  
B) They classify questions into fixed categories  
C) They learn a similarity function that generalizes to unseen questions  
D) They do not require any training  

#### 18. What is the significance of sharing weights between the two subnetworks in a Siamese Network?  
A) It reduces the number of parameters and ensures consistent feature extraction  
B) It allows each subnetwork to learn different features  
C) It prevents the network from learning similarity  
D) It increases the model’s capacity exponentially  

#### 19. Which of the following is true about the embedding vectors produced by the subnetworks?  
A) They are always orthogonal to each other  
B) They capture semantic meaning of the inputs in a continuous vector space  
C) They are discrete labels assigned to inputs  
D) They are used to compute similarity scores  

#### 20. In the context of Siamese Networks, what does the term "anchor" refer to?  
A) The input that serves as a reference point for comparison  
B) The negative example in a triplet  
C) The output similarity score  
D) The threshold used for classification



<br>

## Answers

#### 1. What is the primary purpose of a Siamese Network?  
A) ✗ Siamese Networks do not classify into fixed categories.  
B) ✓ They measure similarity or difference between two inputs.  
C) ✗ They do not generate new data samples.  
D) ✗ Clustering without labels is not their main function.  

**Correct:** B


#### 2. Which of the following best describes the architecture of a Siamese Network?  
A) ✓ Two identical subnetworks sharing weights processing two inputs separately.  
B) ✗ Inputs are not concatenated; subnetworks process inputs independently.  
C) ✗ Networks share weights; they are not independent.  
D) ✗ Siamese Networks are not multiple networks for multi-class classification.  

**Correct:** A


#### 3. Why are LSTMs commonly used in Siamese Networks for NLP tasks?  
A) ✓ LSTMs handle sequential data and capture context well.  
B) ✗ Dimensionality reduction is not the main reason.  
C) ✗ LSTMs are not necessarily faster than CNNs.  
D) ✗ LSTMs do consider word order, which is important.  

**Correct:** A


#### 4. What does the cosine similarity between two vectors represent in a Siamese Network?  
A) ✗ Cosine similarity measures angle, not Euclidean distance.  
B) ✓ It measures the angle between vectors indicating similarity.  
C) ✗ It is not a direct probability but a similarity score.  
D) ✗ Cosine similarity is not the sum of element-wise differences.  

**Correct:** B


#### 5. In the context of triplet loss, what is the role of the margin α (alpha)?  
A) ✗ It does not make positive and negative pairs equal.  
B) ✓ It enforces a minimum margin between positive and negative distances.  
C) ✗ It does not scale embeddings directly.  
D) ✗ It is not used for normalization.  

**Correct:** B


#### 6. Which of the following statements about triplet loss is true?  
A) ✓ Triplet loss requires anchor, positive, and negative inputs.  
B) ✗ It minimizes anchor-positive distance, not anchor-negative.  
C) ✗ It minimizes anchor-positive distance, not maximizes.  
D) ✓ It enforces anchor-positive distance to be smaller than anchor-negative by α.  

**Correct:** A, D


#### 7. What is a "hard negative" in the context of training Siamese Networks?  
A) ✗ Hard negatives are not very different; they are similar.  
B) ✓ Hard negatives are very similar to the anchor, making training challenging.  
C) ✗ Hard negatives are not mislabeled positives.  
D) ✗ Hard negatives are difficult, not easy examples.  

**Correct:** B


#### 8. Why is hard negative mining important during training?  
A) ✗ It does not ignore difficult examples; it focuses on them.  
B) ✓ It helps the network learn better by focusing on challenging examples.  
C) ✗ It does not reduce overfitting by removing easy examples.  
D) ✗ It does not restrict learning to positive pairs only.  

**Correct:** B


#### 9. How does one-shot learning differ from traditional classification?  
A) ✗ One-shot learning requires few examples, not many.  
B) ✓ It learns a similarity function instead of fixed class boundaries.  
C) ✗ It can generalize to new classes via similarity.  
D) ✗ It does not require retraining for every new class.  

**Correct:** B


#### 10. Which of the following is NOT a typical application of Siamese Networks?  
A) ✗ Duplicate question detection is a typical application.  
B) ✗ Face verification is a typical application.  
C) ✓ Image generation is not typical for Siamese Networks.  
D) ✗ Handwritten signature verification is typical.  

**Correct:** C


#### 11. When preparing batches for training a Siamese Network, which of the following is true?  
A) ✗ Batches contain both positive and negative pairs.  
B) ✓ Batches contain both positive and negative pairs.  
C) ✗ Batch size is usually more than one.  
D) ✓ For triplet loss, batches must contain triplets (anchor, positive, negative).  

**Correct:** B, D


#### 12. What happens if the margin α in triplet loss is set too high?  
A) ✓ The network may fail to converge due to an overly strict margin.  
B) ✗ The network will not easily satisfy the loss if margin is too high.  
C) ✗ Embeddings collapsing is caused by other issues, not high margin.  
D) ✗ The network does not ignore negatives due to margin size.  

**Correct:** A


#### 13. Which similarity or distance metrics can be used in Siamese Networks?  
A) ✗ Not limited to cosine similarity only.  
B) ✗ Not limited to Euclidean distance only.  
C) ✓ Any similarity or distance metric can be used, including cosine and Euclidean.  
D) ✗ Not limited to Manhattan distance only.  

**Correct:** C


#### 14. In testing a Siamese Network, how is the decision made whether two inputs are similar?  
A) ✓ By checking if cosine similarity exceeds a threshold τ.  
B) ✗ It does not classify into fixed classes during testing.  
C) ✗ Euclidean distance is not always compared to zero.  
D) ✗ Softmax output is not used in Siamese similarity testing.  

**Correct:** A


#### 15. Which of the following statements about embedding vectors in Siamese Networks is correct?  
A) ✓ Embeddings are fixed-length vector representations of inputs.  
B) ✗ Embeddings are not raw binary data.  
C) ✗ Embeddings are multi-dimensional vectors, not scalars.  
D) ✓ Embeddings are often normalized before similarity calculation.  

**Correct:** A, D


#### 16. What is the main challenge when training with random triplets?  
A) ✗ Random triplets do not always contain hard negatives.  
B) ✓ Random triplets often include many easy negatives that provide little learning signal.  
C) ✗ Computational resources are not necessarily higher for random triplets.  
D) ✗ Random triplets do not inherently cause overfitting.  

**Correct:** B


#### 17. Which of the following best explains why Siamese Networks are preferred over traditional classifiers for duplicate question detection?  
A) ✓ They require less labeled data for new questions.  
B) ✗ They do not classify into fixed categories.  
C) ✓ They learn a similarity function that generalizes to unseen questions.  
D) ✗ They do require training, just not retraining for new classes.  

**Correct:** A, C


#### 18. What is the significance of sharing weights between the two subnetworks in a Siamese Network?  
A) ✓ It reduces parameters and ensures consistent feature extraction.  
B) ✗ Sharing weights means subnetworks learn the same features, not different ones.  
C) ✗ Sharing weights enables learning similarity, not prevents it.  
D) ✗ Sharing weights does not exponentially increase capacity.  

**Correct:** A


#### 19. Which of the following is true about the embedding vectors produced by the subnetworks?  
A) ✗ Embeddings are not always orthogonal.  
B) ✓ They capture semantic meaning in a continuous vector space.  
C) ✗ Embeddings are not discrete labels.  
D) ✓ They are used to compute similarity scores.  

**Correct:** B, D


#### 20. In the context of Siamese Networks, what does the term "anchor" refer to?  
A) ✓ The input serving as a reference point for comparison.  
B) ✗ The negative example is distinct from the anchor.  
C) ✗ Anchor is not the output similarity score.  
D) ✗ Anchor is not a threshold.  

**Correct:** A