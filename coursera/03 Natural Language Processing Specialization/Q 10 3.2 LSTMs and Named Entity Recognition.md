## 3.2 LSTMs and Named Entity Recognition

## Questions

#### 1. What are the main reasons why basic RNNs struggle with long-term dependencies?  
A) They have limited memory capacity in hidden states  
B) Vanishing gradients cause earlier information to be lost during training  
C) Exploding gradients cause the model to overfit on long sequences  
D) They require excessive RAM to store long sequences  

#### 2. In backpropagation through time (BPTT), why does the gradient tend to vanish or explode?  
A) Because the gradient is a product of many partial derivatives over time steps  
B) Because the gradient is computed only at the last time step  
C) Because partial derivatives can be less than or greater than 1, causing exponential decay or growth  
D) Because the model uses non-differentiable activation functions  

#### 3. Which of the following are effective methods to mitigate exploding gradients in RNNs?  
A) Gradient clipping  
B) Using tanh activation exclusively  
C) Skip connections  
D) Increasing the learning rate  

#### 4. How does the forget gate in an LSTM function?  
A) It decides which parts of the cell state to discard  
B) It adds new information to the cell state  
C) It controls the output of the hidden state  
D) It resets the hidden state to zero  

#### 5. What role does the input gate play in an LSTM?  
A) It decides what new information to add to the cell state  
B) It filters irrelevant information from the hidden state  
C) It controls the output of the LSTM at the current time step  
D) It forgets old information from the cell state  

#### 6. Why is the cell state in an LSTM often described as a "conveyor belt"?  
A) Because it carries information unchanged across time steps unless modified by gates  
B) Because it is reset at every time step  
C) Because it stores the output of the LSTM  
D) Because it is used only during backpropagation  

#### 7. Which activation functions are typically used inside LSTM gates and why?  
A) Sigmoid for gates to produce values between 0 and 1  
B) Tanh to shrink values between -1 and 1 for candidate cell states  
C) ReLU to avoid vanishing gradients in gates  
D) Softmax to select the most important information  

#### 8. What is the main advantage of LSTMs over vanilla RNNs?  
A) LSTMs can selectively remember or forget information, improving long-term dependency learning  
B) LSTMs require less computational power than RNNs  
C) LSTMs do not require backpropagation through time  
D) LSTMs eliminate the need for gradient clipping  

#### 9. In Named Entity Recognition (NER), what does the label "B-per" signify?  
A) Beginning of a person’s name entity  
B) Inside a person’s name entity  
C) Outside any entity  
D) Beginning of a place entity  

#### 10. Why is token padding necessary when training LSTM models for NER?  
A) To ensure all input sequences have the same length for batch processing  
B) To increase the vocabulary size  
C) To prevent vanishing gradients  
D) To mask irrelevant tokens during training  

#### 11. When computing accuracy for NER models, why must padding tokens be masked?  
A) Because padding tokens do not correspond to real words and should not affect accuracy  
B) Because padding tokens always have the correct label  
C) Because masking speeds up computation  
D) Because padding tokens contain entity information  

#### 12. Which of the following are true about the output gate in an LSTM?  
A) It decides what information from the cell state to output as the hidden state  
B) It controls what information is passed to the next time step  
C) It forgets irrelevant information from the cell state  
D) It uses a sigmoid activation to produce gating values  

#### 13. How does gradient clipping help during RNN training?  
A) It limits the maximum value of gradients to prevent them from exploding  
B) It normalizes gradients to have zero mean  
C) It increases the learning rate dynamically  
D) It prevents gradients from becoming too small  

#### 14. Which of the following statements about backpropagation through time (BPTT) are correct?  
A) The length of the gradient product depends on how far back in time the dependency is  
B) BPTT computes gradients only for the last time step  
C) Partial derivatives less than 1 cause gradients to vanish over long sequences  
D) Partial derivatives greater than 1 cause gradients to explode over long sequences  

#### 15. In the context of NER, what is the purpose of converting words and entity classes into numerical arrays?  
A) To enable the model to process text data as input  
B) To reduce the size of the dataset  
C) To allow batch processing of sequences  
D) To improve the interpretability of the model  

#### 16. Which of the following are common applications of LSTMs?  
A) Music composition  
B) Image captioning  
C) Sorting algorithms  
D) Speech recognition  

#### 17. What is the effect of using the tanh activation function on the candidate cell state in an LSTM?  
A) It scales values to be between -1 and 1, helping stabilize training  
B) It ensures all values are positive  
C) It produces binary outputs for gating  
D) It eliminates the need for the forget gate  

#### 18. Why might skip connections be used in RNN architectures?  
A) To allow gradients to flow more directly and reduce vanishing gradients  
B) To increase the depth of the network without increasing parameters  
C) To clip gradients automatically  
D) To reduce the number of time steps processed  

#### 19. During NER model training, what is the role of the dense layer after the LSTM?  
A) To map LSTM outputs to the number of entity classes  
B) To reduce the sequence length  
C) To embed words into vectors  
D) To apply the softmax or log softmax activation for classification  

#### 20. Which of the following statements about LSTM gates are true?  
A) Gates use sigmoid activations to produce values between 0 and 1  
B) The forget gate and input gate work together to update the cell state  
C) The output gate controls the next hidden state output  
D) Gates eliminate the need for backpropagation



<br>

## Answers

#### 1. What are the main reasons why basic RNNs struggle with long-term dependencies?  
A) ✓ Limited memory capacity in hidden states makes it hard to retain distant information  
B) ✓ Vanishing gradients cause earlier information to be lost during training  
C) ✗ Exploding gradients cause overfitting but are not the main reason for struggling with long-term dependencies  
D) ✗ RNNs use less RAM, so this is not a disadvantage  

**Correct:** A, B


#### 2. In backpropagation through time (BPTT), why does the gradient tend to vanish or explode?  
A) ✓ Gradient is a product of many partial derivatives over time steps, causing exponential effects  
B) ✗ Gradient is computed at every time step, not only the last  
C) ✓ Partial derivatives less than 1 cause vanishing, greater than 1 cause exploding gradients  
D) ✗ Activation functions used (sigmoid, tanh) are differentiable  

**Correct:** A, C


#### 3. Which of the following are effective methods to mitigate exploding gradients in RNNs?  
A) ✓ Gradient clipping limits gradient size to prevent explosion  
B) ✗ Using tanh alone does not prevent exploding gradients  
C) ✓ Skip connections help gradients flow better, reducing explosion risk  
D) ✗ Increasing learning rate can worsen exploding gradients  

**Correct:** A, C


#### 4. How does the forget gate in an LSTM function?  
A) ✓ It decides which parts of the cell state to discard  
B) ✗ Adding new information is the input gate’s job  
C) ✗ Output gate controls hidden state output, not forget gate  
D) ✗ Forget gate does not reset hidden state to zero  

**Correct:** A


#### 5. What role does the input gate play in an LSTM?  
A) ✓ Decides what new information to add to the cell state  
B) ✗ It does not filter hidden state; forget gate handles discarding info  
C) ✗ Output gate controls output, not input gate  
D) ✗ Forget gate forgets old info, not input gate  

**Correct:** A


#### 6. Why is the cell state in an LSTM often described as a "conveyor belt"?  
A) ✓ It carries information largely unchanged across time steps unless gates modify it  
B) ✗ It is not reset at every time step  
C) ✗ It stores more than just output; output is hidden state  
D) ✗ It is used during forward pass, not only backpropagation  

**Correct:** A


#### 7. Which activation functions are typically used inside LSTM gates and why?  
A) ✓ Sigmoid gates produce values between 0 and 1 to control flow  
B) ✓ Tanh shrinks candidate cell state values between -1 and 1 for stability  
C) ✗ ReLU is not typically used inside gates due to unbounded output  
D) ✗ Softmax is not used in gates; it’s for classification  

**Correct:** A, B


#### 8. What is the main advantage of LSTMs over vanilla RNNs?  
A) ✓ LSTMs selectively remember/forget, improving long-term dependency learning  
B) ✗ LSTMs generally require more computation, not less  
C) ✗ LSTMs still use backpropagation through time  
D) ✗ Gradient clipping may still be used with LSTMs  

**Correct:** A


#### 9. In Named Entity Recognition (NER), what does the label "B-per" signify?  
A) ✓ Beginning of a person’s name entity  
B) ✗ "I-per" would indicate inside a person’s name, not "B-per"  
C) ✗ "O" means outside any entity  
D) ✗ "B-geo" or similar would indicate place, not person  

**Correct:** A


#### 10. Why is token padding necessary when training LSTM models for NER?  
A) ✓ To ensure all input sequences have the same length for batch processing  
B) ✗ Padding does not increase vocabulary size  
C) ✗ Padding does not prevent vanishing gradients  
D) ✗ Masking is separate from padding purpose  

**Correct:** A


#### 11. When computing accuracy for NER models, why must padding tokens be masked?  
A) ✓ Padding tokens do not correspond to real words and should not affect accuracy  
B) ✗ Padding tokens do not have correct labels; they are placeholders  
C) ✗ Masking is for correctness, not speed  
D) ✗ Padding tokens contain no entity information  

**Correct:** A


#### 12. Which of the following are true about the output gate in an LSTM?  
A) ✓ Decides what information from the cell state to output as hidden state  
B) ✓ Controls what information is passed to the next time step via hidden state  
C) ✗ Forget gate discards irrelevant info, not output gate  
D) ✓ Uses sigmoid activation to produce gating values between 0 and 1  

**Correct:** A, B, D


#### 13. How does gradient clipping help during RNN training?  
A) ✓ Limits maximum gradient value to prevent exploding gradients  
B) ✗ Does not normalize gradients to zero mean  
C) ✗ Does not increase learning rate  
D) ✗ Does not prevent gradients from becoming too small (vanishing)  

**Correct:** A


#### 14. Which of the following statements about backpropagation through time (BPTT) are correct?  
A) ✓ Gradient product length depends on how far back in time the dependency is  
B) ✗ BPTT computes gradients at all time steps, not only last  
C) ✓ Partial derivatives less than 1 cause gradients to vanish over long sequences  
D) ✓ Partial derivatives greater than 1 cause gradients to explode over long sequences  

**Correct:** A, C, D


#### 15. In the context of NER, what is the purpose of converting words and entity classes into numerical arrays?  
A) ✓ Enables model to process text as numerical input  
B) ✗ Does not reduce dataset size  
C) ✓ Allows batch processing of sequences  
D) ✗ Does not improve interpretability; it’s for computation  

**Correct:** A, C


#### 16. Which of the following are common applications of LSTMs?  
A) ✓ Music composition  
B) ✓ Image captioning  
C) ✗ Sorting algorithms are not sequence modeling tasks  
D) ✓ Speech recognition  

**Correct:** A, B, D


#### 17. What is the effect of using the tanh activation function on the candidate cell state in an LSTM?  
A) ✓ Scales values between -1 and 1, stabilizing training  
B) ✗ Does not ensure all positive values; tanh outputs negative and positive  
C) ✗ Does not produce binary outputs; sigmoid does that  
D) ✗ Does not eliminate need for forget gate  

**Correct:** A


#### 18. Why might skip connections be used in RNN architectures?  
A) ✓ Allow gradients to flow more directly, reducing vanishing gradients  
B) ✗ Skip connections do not increase depth without parameters  
C) ✗ Skip connections do not clip gradients automatically  
D) ✗ Skip connections do not reduce number of time steps processed  

**Correct:** A


#### 19. During NER model training, what is the role of the dense layer after the LSTM?  
A) ✓ Maps LSTM outputs to number of entity classes for classification  
B) ✗ Does not reduce sequence length; output length matches input length  
C) ✗ Embedding layer, not dense, embeds words into vectors  
D) ✓ Applies softmax or log softmax activation for classification probabilities  

**Correct:** A, D


#### 20. Which of the following statements about LSTM gates are true?  
A) ✓ Gates use sigmoid activations to produce values between 0 and 1  
B) ✓ Forget and input gates work together to update the cell state  
C) ✓ Output gate controls the next hidden state output  
D) ✗ Gates do not eliminate the need for backpropagation; they improve gradient flow  

**Correct:** A, B, C