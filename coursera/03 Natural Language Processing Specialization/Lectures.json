[
  {
    "index": 1,
    "title": "1.1 Sentiment Analysis with Logistic Regression",
    "content": " Supervised ML and Sentiment Analysis Outline Review Supervised ML Build your own tweet classifier! Parameters Supervised ML (training) Prediction Function Output Labels Features Cost Output Label Sentiment analysis Tweet I am happy because I am learning NLP Positive: 1 Negative: 0 Logistic regression Sentiment analysis I am happy because I am learning NLP Train Classify Positive: 1 Summary Features, Labels Train Predict Extract features Train LR Predict sentiment Vocabulary and Feature Extraction Outline Vocabulary Feature extraction Sparse representations and some of their issues Vocabulary I am happy because I am learning NLP I hated the movie [ I, am, happy, because, learning, NLP, hated, the, movie ] .......... Tweets: [tweet_1, tweet_2, ..., tweet_m] Feature extraction I am happy because I am learning NLP [ I , am, happy, because, learning, NLP, hated, the, movie ] ... [ 1, 1, 0 ] ... A lot of zeros! That's a sparse representation. Problems with sparse representations I am happy because I am learning NLP [ 1 , 1 , 1 , 1 , 1 , 1 , ... , 0 , ... , 0 , 0 , 0 ] All zeros! 1. Large training time 2. Large prediction time Summary Vocabulary: set of unique words Vocabulary, Text [1 ..... 0 ..... 1 .. 0 .. 1 .. 0] Sparse representations are problematic for training and prediction times Negative and Positive Frequencies Outline Populate your vocabulary with a frequency count for each class Positive and negative counts Vocabulary happy because learning NLP sad not I am happy because I am learning NLP I am happy I am sad, I am not learning NLP I am sad Corpus Positive and negative counts I am happy because I am learning NLP I am happy Positive tweets I am sad, I am not learning NLP I am sad Negative tweets Positive and negative counts I am happy because I am learning NLP I am happy Positive tweets Vocabulary happy because learning NLP sad not PosFreq (1) Positive and negative counts I am sad, I am not learning NLP I am sad Negative tweets Vocabulary happy because learning NLP sad not NegFreq (0) Word frequency in classes Vocabulary happy because learning NLP sad not PosFreq (1) NegFreq (0) freqs: dictionary mapping from (word, class) to frequency Summary Divide tweet corpus into two classes: positive and negative Count each time each word appears in either class ➔Feature extraction for training and prediction! Feature extraction with frequencies Outline Extract features from your frequencies dictionary to create a features vector Word frequency in classes Vocabulary happy because learning NLP sad not PosFreq (1) NegFreq (0) freqs: dictionary mapping from (word, class) to frequency Feature extraction freqs: dictionary mapping from (word, class) to frequency Features of tweet m Bias Sum Pos. Frequencies Sum Neg. Frequencies Feature extraction Vocabulary happy because learning NLP sad not PosFreq (1) I am sad, I am not learning NLP Feature extraction Vocabulary happy because learning NLP sad not I am sad, I am not learning NLP NegFreq (0) Feature extraction I am sad, I am not learning NLP Summary Dictionary mapping (word,class) to frequencies ➔Cleaning unimportant information from your tweets Preprocessing Outline Removing stopwords, punctuation, handles and URLs Stemming Lowercasing Preprocessing: stop words and punctuation @YMourri and @AndrewYNg are tuning a GREAT AI. model at https://!!! Punctuation Stop words and are has for Preprocessing: stop words and punctuation @YMourri and @AndrewYNg are tuning a GREAT AI. model at https://!!! Stop words and are has for Punctuation @YMourri @AndrewYNg tuning GREAT AI. model https://!!! Preprocessing: stop words and punctuation Stop words and has for Punctuation @YMourri @AndrewYNg tuning GREAT AI. model https://!!! @YMourri @AndrewYNg tuning GREAT AI. model https:// Preprocessing: Handles and URLs @YMourri @AndrewYNg tuning GREAT AI. model https:// tuning GREAT AI. model Preprocessing: Stemming and lowercasing tuning GREAT AI. model tun tune tuned tuning GREAT. Great great great Preprocessed tweet: [tun, great, ai, model] Summary Stop words, punctuation, handles and URLs Stemming Lowercasing Less unnecessary info Better times Putting it all together Outline Generalize the process How to code it! [1, 4, Sum positive frequencies General overview I am Happy Because i am learning NLP @deeplearning [happy, learn, nlp] Bias Sum negative frequencies Preprocessing Feature Extraction [[1, 40, 20], General overview I am Happy Because i am learning NLP @deeplearning [happy, learn, nlp] I am sad not learning NLP I am sad :( ... [sad, not, learn, nlp] [sad] ... [1, 20, 50], [1, 5, 35]] ... General Implementation X = np.zeros((m,3)) #Initialize matrix X X[i,:] = extract_features(p_tweet,freqs) #Extract Features for i in range(m): #For every tweet p_tweet = process_tweet(tweets[i]) #Process tweet freqs = build_freqs(tweets,labels) #Build frequencies dictionary Summary Implement the feature extraction algorithm for your entire set of tweets Almost ready to train! Logistic Regression Overview Outline Supervised learning and logistic regression Sigmoid function Parameters Output Labels Features Overview of logistic regression Cost Output Label Sigmoid Overview of logistic regression Overview of logistic regression @YMourri and @AndrewYNg are tuning a GREAT AI. model [tun, ai, great, model] 4.92 , negative Summary Sigmoid function , positive Logistic Regression: Training Outline Review the steps in the training process Overview of gradient descent Training LR Iteration Cost Training LR Until good enough Initialize parameters Classify/predict Get gradient Update Get Loss Summary Visualize how gradient descent works Use gradient descent to train your logistic regression classifier ➔Compute the accuracy of your model Logistic Regression: Testing Outline Using your validation set to compute model accuracy What the accuracy metric means Testing logistic regression Testing logistic regression Testing logistic regression Testing logistic regression Summary Performance on unseen data Accuracy To improve model: step size, number of iterations, regularization, new features, etc. Logistic Regression: Cost Function Outline ●Overview of the logistic cost function, AKA the binary cross-entropy function Cost function for logistic regression Cost function for logistic regression Cost function for logistic regression any 0.99 -inf Cost function for logistic regression any 0.01 -inf Cost function for logistic regression Cost function for logistic regression Summary ●Strong disagreement = high cost ●Strong agreement = low cost ●Aim for the lowest cost!"
  },
  {
    "index": 2,
    "title": "1.2 Sentiment Analysis with Naive Bayes",
    "content": " Probability and Bayes' Rule Outline Probabilities Bayes' rule (Applied in different fields, including NLP) Build your own Naive-Bayes tweet classifier! Introduction Corpus of tweets Positive Negative Positive Negative “happy” Tweets containing the word “happy” Probabilities Corpus of tweets Positive Negative “happy” A → Positive tweet P(A) = P(Positive) = Npos / N Probabilities Corpus of tweets Positive Negative “happy” A → Positive tweet P(A) = Npos / N = 13 / 20 = 0.65 P(Negative) = 1 - P(Positive) = 0.35 Probabilities “happy” Tweets containing the word “happy” B → tweet contains “happy”. P(B) = P(happy) = Nhappy / N P(B) = 4 / 20 = 0.2 Probability of the intersection Corpus “happy” Positive Positive “happy” “happy” Positive Bayes' Rule Conditional Probabilities Positive “happy” P(A | B) = P(Positive | “happy”) P(A | B) = 3 / 4 = 0.75 Corpus “happy” “happy” Positive Conditional Probabilities Positive “happy” P(B | A) = P(“happy”| Positive) P(B | A) = 3 / 13 = 0.231 Corpus Positive “happy” Positive Conditional probabilities Probability of B, given A happened Looking at the elements of set A, the chance that one also belongs to set B Conditional probabilities Corpus “happy” Positive “happy” Positive Bayes' rule Quiz Objective: Derive Bayes' rule from the equations given on the last slide. Question: From the equations presented below, express the probability of a tweet being positive given that it contains the word happy in terms of the probability of a tweet containing the word happy given that it is positive Type: Multiple Choice, single answer Options and solution: That's right. You just derived Bayes' rule. The ratio is upside-down in this equation. Your result should not include any intersection probabilities. Your result should not include any intersection probabilities. Bayes' rule Quiz: Bayes' Rule Applied Objective: Compute conditional probability using Bayes Rule Question: Here, again, is Bayes' rule: Suppose that in your dataset, 25% of the positive tweets contain the word 'happy'. You also know that a total of 13% of the tweets in your dataset contain the word 'happy', and that 40% of the total number of tweets are positive. You observe the tweet: ''happy to learn NLP'. What is the probability that this tweet is positive? Type: Multiple Choice, single answer Options and solution: A: P(Positive | “happy” ) = 0.77 That's right. You just applied Bayes' rule. B: P(Positive | “happy” ) = 0.08 Oops, looks like you might have the ratio of P(X) and P(Y) upside-down. C: P(Positive | “happy” ) = 0.10 Remember to calculate the ratio in the formula for Bayes' rule. D: P(Positive | “happy” ) = 1.92 Did you use the probability of a tweet being positive? Remember that a fractional probability must be between 0 and 1. Summary Conditional probabilities Bayes' Rule Naïve Bayes Introduction Naïve Bayes for Sentiment Analysis I am happy because I am learning NLP I am happy, not sad. Positive tweets I am sad, I am not learning NLP I am sad, not happy Negative tweets word happy because learning NLP sad not Pos Neg Nclass Neg 0.25 0.25 0.08 0.01 0.08 0.08 0.08 0.17 P(wi | class) word happy because learning NLP sad not Pos Neg Nclass word happy because learning NLP sad not Pos 0.24 0.24 0.15 0.08 0.08 0.08 0.08 0.08 Sum P(wi | class) word happy because learning NLP sad not Pos 0.24 0.24 0.15 0.08 0.08 0.08 0.08 0.08 Neg 0.25 0.25 0.08 0.08 0.08 0.08 0.17 Neg 0.20 0.20 0.10 0.05 0.10 0.10 0.10 0.15 Tweet: I am happy today; I am learning. Naïve Bayes > 1 Pos 0.20 0.20 0.14 0.10 0.10 0.10 0.10 0.10 word happy because learning NLP sad not Summary Naive Bayes inference condition rule for binary classification Table of probabilities Laplacian Smoothing Laplacian Smoothing class ∈{Positive, Negative} Nclass = frequency of all words in class Vclass = number of unique words in class Neg 0.20 0.20 0.10 0.05 0.10 0.10 0.10 0.15 Pos 0.19 0.19 0.14 0.10 0.10 0.10 0.10 0.10 Introducing P(wi | class) with smoothing word happy because learning NLP sad not Pos Neg Nclass word happy because learning NLP sad not Sum V = 8 Laplacian Smoothing Summary Laplacian smoothing to avoid P(wi|class) = 0 Naïve Bayes formula Log Likelihood, Part 1 Ratio of probabilities ratio 1.4 0.6 0.6 ratio(wi) = P(wi | Pos) P(wi | Neg) Neutral Negative Positive Pos 0.19 0.19 0.14 0.10 0.10 0.10 0.10 0.10 Neg 0.20 0.20 0.10 0.05 0.10 0.10 0.15 0.15 word happy because learning NLP sad not freq(wi , 1) + 1 freq(wi , 0) + 1 Naïve Bayes' inference class ∈{pos, neg} w -> Set of m words in a tweet ●A simple, fast, and powerful baseline ●A probabilistic model used for classification Log Likelihood ●Products bring risk of underflow log prior + log likelihood tweet: I am happy because I am learning. Calculating Lambda word happy because learning NLP sad not Pos 0.05 0.04 0.09 0.01 0.03 0.02 0.01 0.02 Neg 0.05 0.04 0.01 0.01 0.01 0.02 0.09 0.03 2.2 1.1 -2.2 -0.4 doc: I am happy because I am learning. Summing the Lambdas word happy because learning NLP sad not Pos 0.05 0.04 0.09 0.01 0.03 0.02 0.01 0.02 Neg 0.05 0.04 0.01 0.01 0.01 0.02 0.09 0.03 2.2 1.1 -2.2 -0.4 Summary Word sentiment Log Likelihood, Part 2 doc: I am happy because I am learning. Log Likelihood word happy because learning NLP sad not Pos 0.05 0.04 0.09 0.01 0.03 0.02 0.01 0.02 Neg 0.05 0.04 0.01 0.01 0.01 0.02 0.09 0.03 2.2 1.1 -2.2 -0.4 log likelihood = 0 + 2.2 1.1 3.3 Log Likelihood Neutral > 1 Negative Positive > 0 Neutral Negative Positive 3.3 > 0 Summary > 0 Tweet sentiment: Neutral Negative Positive Training Naïve Bayes Outline Five steps for training a Naïve Bayes model Training Naïve Bayes I am happy because I am learning NLP I am happy, not sad. @NLP Positive tweets I am sad, I am not learning NLP I am sad, not happy!! Negative tweets Step 1: Preprocess Step 0: Collect and annotate corpus [happi, because, learn, NLP] [happi, not, sad] Positive tweets [sad, not, learn, NLP] [sad, not, happi] Negative tweets Lowercase Remove punctuation, urls, names Remove stop words Stemming Tokenize sentences Training Naïve Bayes word happi because learn NLP sad not Pos Neg Step 2: Word count Nclass [happi, because, learn, NLP] [happi, not, sad] Positive tweets [sad, not, learn, NLP] [sad, not, happi] Negative tweets freq(w, class) Training Naïve Bayes word happy because learning NLP sad not Pos 0.23 0.15 0.08 0.08 0.08 0.08 Neg 0.15 0.07 0.08 0.08 0.17 0.17 0.43 0.6 -0.75 -0.75 Step 4: Get lambda word happi because learn NLP sad not Pos Neg Nclass freq(w, class) Step 3: Training Naïve Bayes Step 5: Get the log prior Dpos = Number of positive tweets Dneg = Number of negative tweets If dataset is balanced, Dpos = Dneg and logprior = 0. Summary 1. Get or annotate a dataset with positive and negative tweets 2. Preprocess the tweets: process_tweet(tweet) ➞[w1, w2, w3, ...] 3. Compute freq(w, class) Get P(w | pos), P(w | neg) 5. Get λ(w) 6. Compute logprior = log(P(pos) / P(neg)) Testing Naïve Bayes Outline ●Using your validation set to compute model accuracy ●Predict using a Näive Bayes Model log-likelihood dictionary Predict using Naïve Bayes word the happi because pass NLP sad not -0.01 -0.01 0.63 0.01 0.5 -0.75 -0.75 > 0 Tweet: I passed the NLP interview. Tweet: [I, pass, the, NLP, interview] Testing Naïve Bayes Testing Naïve Bayes Summary Performance on unseen data Accuracy What about words that do not appear in λ(w)? Predict using and for each new tweet Applications of Naïve Bayes Applications of Naïve Bayes Applications of Naïve Bayes Author identification: Spam filtering: Applications of Naïve Bayes \"Icon made by Vector Market from www.flaticon.com\" Information retrieval: Retrieve document if Applications of Naïve Bayes \"Pictures with CC\" Bank: Word disambiguation: Naïve Bayes Applications Sentiment analysis Word disambiguation Author identification Information retrieval Simple, fast and robust! Naïve Bayes Assumptions Outline ●Relative frequency in corpus ●Independence Naïve Bayes Assumptions ●Independence “It is sunny and hot in the Sahara desert.” Naïve Bayes Assumptions “It's always cold and snowy in ___ .” spring?? summer? fall?? winter?? Naïve Bayes Assumptions ●Relative frequencies in corpus Summary ●Independence: Not true in NLP ●Relative frequency of classes affect the model Error Analysis Outline ●Word order ●Removing punctuation and stop words ●Adversarial attacks Processing as a Source of Errors: Punctuation Tweet: My beloved grandmother :(X processed_tweet: [belov, grandmoth] Processing as a Source of Errors: Removing Words Tweet: This is not good, because your attitude is not even close to being nice. processed_tweet: [good, attitude, close, nice] Processing as a Source of Errors: Word Order Tweet: I am happy because I do not go. Tweet: I am not happy because I did go. Adversarial attacks Tweet: This is a ridiculously powerful movie. The plot was gripping and I cried right through until the ending! processed_tweet: [ridicul, power, movi, plot, grip, cry, end] Sarcasm, Irony and Euphemisms Summary ●Removing punctuation ●Word order ●Removing words ●Adversarial attacks"
  },
  {
    "index": 3,
    "title": "1.3 Vector Space Models",
    "content": " Vector Space Models. Outline ●Vector space models ●Advantages ●Applications Why learn vector space models? What is your age? How old are you? Where are you heading? Where are you from? Different meaning Same Meaning Vector space models applications Machine Translation You eat cereal from a bowl You buy something and someone else sells it Information Extraction Chatbots Fundamental concept “You shall know a word by the company it keeps” Firth, 1957 (Firth, J. R. 1957:11) Representation that captures relative meaning Represent words and documents as vectors Summary Word by Word and Word by Doc. Outline • Co-occurrence Vector representation • Relationships between words/documents Word by Word Design Number of times they occur together within a certain distance k I like simple data I prefer simple raw data data simple k=2 raw like Word by Document Design Number of times a word occurs within a certain category Entertainment data Entertainment Economy Machine Learning Economy Machine Learning Corpus film Vector Space Measures of “similarity:” Angle Distance Economy Entertainment data film data Entertainment Economy film Summary W/W and W/D, counts of occurrence Vector Spaces Similarity between words/documents Euclidean Distance Outline Euclidean distance N-dimension vector representations comparison Euclidean distance Corpus B: (9320,1000) Corpus A: (500,7000) Entertainment data film Euclidean distance Corpus B: (9320,1000) Corpus A: (500,7000) data film Entertainment Euclidean distance for n-dimensional vectors data boba ice-cream drinks food Norm of ( - Euclidean distance in Python # Create numpy vectors v and w v = np.array([1, 6, 8]) w = np.array([0, 4, 6]) # Calculate the Euclidean distance d d = np.linalg.norm(v-w) # Print the result print(\"The Euclidean distance between v and w is: \", d) The Euclidean distance between v and w is: 3 Straight line between points Summary Norm of the difference between vectors Cosine Similarity: Intuition Outline Problems with Euclidean Distance ●Cosine similarity Euclidean distance vs Cosine similarity Agriculture corpus (20,40) The cosine of the angle between the vectors Euclidean distance: d2 < d1 Angles comparison: β > Food corpus (5,15) History corpus (30,20) eggs disease Summary Cosine similarity when corpora are different sizes Cosine Similarity Outline How to get the cosine of the angle between two vectors Relation of this metric to similarity Previous definitions Vector norm Dot product Cosine Similarity Agriculture corpus (20,40) History corpus (30,20) eggs disease Cosine Similarity eggs disease eggs disease Similar Dissimilar Summary Cosine Similarity Cosine Similarity gives values between 0 and 1 Manipulating Words in Vector Spaces Outline ●How to use vector representations Manipulating word vectors USA Washington Russia Manipulating word vectors [Mikolov et al, 2013, Distributed Representations of Words and Phrases and their Compositionality] Washington (10,5) Moscow (9,3) Tokyo(8.5, Ankara(8.5, 0.9) Turkey (3,1) USA (5,6) Russia (5,5) Japan (4,3) (10,4) Washington - USA = Russia + = Moscow Summary • Use known relationships to make predictions Visualization and PCA Outline ●Some motivation for visualization ●Principal Component Analysis Visualization of word vectors How can you visualize if your representation captures these relationships? oil & gas town & city oil 0.20 0.10 gas 2.10 3.40 city 9.30 52.1 town 6.20 34.3 d > 2 Visualization of word vectors oil 0.20 0.10 gas 2.10 3.40 city 9.30 52.1 town 6.20 34.3 oil 2.30 21.2 gas 1.56 19.3 city 13.4 34.1 town 15.6 29.8 d = 2 d > 2 PCA village town city country gas oil petroleum happy joyful sad Visualization of word vectors Principal Component Analysis Uncorrelated Features Dimensionality Reduction Summary • Original Space Uncorrelated features Dimension reduction • Visualization to see words relationships in the vector space PCA Algorithm Outline ●How to get uncorrelated features ●How to reduce dimensions while retaining as much information as possible Principal Component Analysis Uncorrelated Features Dimensionality Reduction PCA algorithm Eigenvector: Uncorrelated features for your data Eigenvalue: the amount of information retained by each feature PCA algorithm Eigenvectors Eigenvalues Mean Normalize Data Get Covariance Matrix Perform SVD PCA. algorithm Eigenvectors Eigenvalues Dot Product to Project Data Percentage of Retained Variance Summary • Eigenvectors give the direction of uncorrelated features • Eigenvalues are the variance of the new features • Dot product gives the projection on uncorrelated features"
  },
  {
    "index": 4,
    "title": "1.4 Machine Translation and Document Search",
    "content": " Overview What you'll be able to do! machine translation “hello!” “bonjour!” document search ”Can I get a refund?” “What's your return policy?” “May I get my money back?” Learning Objectives Transform vector “K nearest neighbors” Hash tables Divide vector space into regions Locality sensitive hashing Approximated nearest neighbors Machine translation Document search Transforming word vectors Outline Translation = Transformation How to get a good transformation Overview of Translation “cat” “chat” [ 1, 0, 1] [ 2, 3, 2] [ 2, 3, 3] [ 99, 88, 55] [ 1, 0, 1] [ -9, -1, -2] Eng Transform with a Matrix! Transforming vectors Transforming vectors Try it yourself! R = np.array([[2,0], [0,-2]]) x = np.array([[1,1]]) np.dot(x,R) array([[2,-2]]) Align word vectors [ “cat” vector ] [ “zebra” vector ] [ ... vector [ “chat” vecteur ] [ “zébresse” vecteur ] [ ... vecteur subsets of the full vocabulary initialize R gradient update in a loop: Solving for R Frobenius norm Try it yourself! Frobenius norm A = np.array([[2,2], [2,2]]) A_squared = np.square(A) A_squared array([[4,4], [4,4]]) A_Frobenious = np.sqrt(np.sum(A_squared)) A_Frobenious 4.0 Frobenius norm squared Gradient Implement in the assignment! Summary minimize K-nearest neighbors Outline K closest matches K-nearest neighbors Finding the translation “hello” “bonjour” R matrix “salut” [transformed] Find similar words Nearest neighbours Friend You Location San Francisco Shanghai Bangalore Los Angeles Nearest Hash tables! Nearest neighbors Summary K-nearest neighbors, for closest matches Hash tables Hash tables and hash functions Outline Hash values Hash functions Hash tables hash = 0 hash = 1 hash = 2 Hash tables hash = 0 hash = 1 hash = 2 Hash function Hash value = vector % number of buckets Hash value Hash function (vector) Create a basic hash table def basic_hash_table(value_l,n_buckets): for value in value_l: hash_value = hash_function(value,n_buckets) hash_table[hash_value].append(value) return hash_table hash_table = {i:[] for i in range(n_buckets)} def hash_function(value_l,n_buckets): return int(value_l) % n_buckets Hash function Hash function by location? Locality sensitive hashing, next! Summary Hash value Hash function (vector) Locality sensitive hashing Outline Locality sensitive hashing with planes in vector spaces Locality Sensitive Hashing “Planes” Planes normal vector Planes Which side of the plane? Which side of the plane? Which side of the plane? Which side of the plane? Which side of the plane? Notice the signs? Visualizing a dot product Projection Visualizing a dot product Visualizing a dot product Sign indicates direction Which side of the plane? Try it! def side_of_plane(P,v): return sign_of_dot_product_scalar dotproduct = np.dot(P,v.T) sign_of_dot_product = np.sign(dotproduct) sign_of_dot_product_scalar= np.asscalar(sign_of_dot_product) Summary Sign of dot product Hash values Multiple Planes Outline Multiple planes Dot products Hash values Multiple planes single hash value? Multiple planes, single hash value? Multiple planes, single hash value! hash = Multiple planes, single hash value!! def hash_multiple_plane(P_l,v): for i, P in enumerate(P_l): sign = side_of_plane(P,v) hash_i = 1 if sign >=0 else 0 hash_value += 2**i * hash_i return hash_value hash_value = 0 Try it! Summary Planes Sign of dot product Hash values Approximate nearest neighbors Outline Multiple sets of planes for approximate K-nearest neighbors Random planes Cultural reference: Spider-Man: Into the Spider-Verse Multiple sets of random planes Multiple sets of random planes Multiple sets of random planes Multiple sets of random planes Approximate nearest (friendly) neighbors Make one set of random planes def side_of_plane_matrix(P,v): dotproduct = np.dot(P,v.T) sign_of_dot_product = np.sign(dotproduct) return sign_of_dot_product num_dimensions = 2 #300 in assignment num_planes = 3 #10 in assignment random_planes_matrix = np.random.normal( size=(num_planes, num_dimensions)) v = np.array([[2,2]]) array([[ 1.76405235 0.40015721] [ 0.97873798 2.2408932 ] [ 1.86755799 -0.97727788]]) array([[1.] [1.] [1.]) num_planes_matrix = side_of_plane_matrix( random_planes_matrix,v) See notebook for calculating the hash value! Summary Multiple universes Locality sensitive A. K-NN hashing Searching documents Outline Representation for documents Document search with K-nearest neighbors Document representation I love learning! [-1, 0, 1] [1, 0, 1] [1, 0, 1] [?, ?, ?] love learning I love learning! [1, 0, 3] Document Search K-NN! Document vectors Try it! for word in words_in_document: document_embedding += word_embedding.get(word,0) word_embedding = {“I”: np.array([1,0,1]), “love”: np.array([-1,0,1]), “learning”: np.array([1,0,1])} words_in_document = ['I', 'love', 'learning'] document_embedding = np.array([0,0,0]) array([1 0 3]) print(document_embedding) Revisit Learning Objectives Practice your skills in the assignment! Transform vector “K nearest neighbors” Hash tables Divide vector space into regions Locality sensitive hashing Approximated nearest neighbors Machine translation Document search"
  },
  {
    "index": 5,
    "title": "2.1 Autocorrect",
    "content": " Overview Learning objectives Learning objectives What is autocorrect.? Learning objectives What is autocorrect? Building the model Learning objectives What is autocorrect? Building the model Minimum edit distance Learning objectives What is autocorrect? Building the model Minimum edit distance Minimum edit distance algorithm Autocorrect What is autocorrect? What is autocorrect? Phones What is autocorrect? Phones Tablets What is autocorrect? Phones Tablets Computers What is autocorrect? Phones Tablets Computers What is autocorrect? Example: Happy birthday deah friend! 🎂 What is autocorrect ? Example: Happy birthday dear friend! 🎂 What is autocorrect? Example: Happy birthday deer friend! 🦌?? How it works 1. Identify a misspelled word 2. Find strings n edit distance away 3. Filter candidates 4. Calculate word probabilities How it works 1. Identify a misspelled word 2. Find strings n edit distance away 3. Filter candidates 4. Calculate word probabilities deah How it works 1. Identify a misspelled word 2. Find strings n edit distance away 3. Filter candidates 4. Calculate word probabilities deah _eah d_ar de_r ... etc How it works 1. Identify a misspelled word 2. Find strings n edit distance away 3. Filter candidates 4. Calculate word probabilities deah yeah dear dean ... etc How it works 1. Identify a misspelled word 2. Find strings n edit distance away 3. Filter candidates 4. Calculate word probabilities deah yeah dear dean ... etc How it works 1. Identify a misspelled word 2. Find strings n edit distance away 3. Filter candidates 4. Calculate word probabilities deah → dear ✅ yeah dear dean ... etc Building the model Building the model 1. Identify a misspelled word 2. Find strings n edit distance away 3. Filter candidates 4. Calculate word probabilities Building the model 1. Identify a misspelled word 2. Find strings n edit distance away 3. Filter candidates 4. Calculate word probabilities Building the model 1. Identify a misspelled word deah ?? 🤔 Building the model 1. Identify a misspelled word deah Building the model 1. Identify a misspelled word deah Happy birthday deer Building the model 1. Identify a misspelled word deah Happy birthday deer ! 🦌✅ Building the model 1. Identify a misspelled word 2. Find strings n edit distance away 3. Filter candidates 4. Calculate word probabilities Building the model Find strings n edit distance away Building the model Find strings n edit distance away Edit: an operation performed on a string to change it Building the model Find strings n edit distance away Edit: an operation performed on a string to change it Insert (add a letter) Building the model Find strings n edit distance away Edit: an operation performed on a string to change it Insert (add a letter) 'to': 'top', 'two' ... Building the model Find strings n edit distance away Edit: an operation performed on a string to change it Insert (add a letter) 'to': 'top', 'two' ... Delete (remove a letter) Building the model Find strings n edit distance away Edit: an operation performed on a string to change it Insert (add a letter) 'to': 'top', 'two' ... Delete (remove a letter) 'hat': 'ha', 'at', 'ht' Building the model Find strings n edit distance away Edit: an operation performed on a string to change it Insert (add a letter) 'to': 'top', 'two' ... Delete (remove a letter) 'hat': 'ha', 'at', 'ht' Switch (swap 2 adjacent letters) Building the model Find strings n edit distance away Edit: an operation performed on a string to change it Insert (add a letter) 'to': 'top', 'two' ... Delete (remove a letter) 'hat': 'ha', 'at', 'ht' Switch (swap 2 adjacent letters) 'eta': 'eat', 'tea' Building the model Find strings n edit distance away Edit: an operation performed on a string to change it Insert (add a letter) 'to': 'top', 'two' ... Delete (remove a letter) 'hat': 'ha', 'at', 'ht' Switch (swap 2 adjacent letters) 'eta': 'eat', 'tea' 'ate' Building the model Find strings n edit distance away Edit: an operation performed on a string to change it Insert (add a letter) 'to': 'top', 'two' ... Delete (remove a letter) 'hat': 'ha', 'at', 'ht' Switch (swap 2 adjacent letters) 'eta': 'eat', 'tea' Building the model Find strings n edit distance away Edit: an operation performed on a string to change it Insert (add a letter) 'to': 'top', 'two' ... Delete (remove a letter) 'hat': 'ha', 'at', 'ht' Switch (swap 2 adjacent letters) 'eta': 'eat', 'tea' Building the model Find strings n edit distance away Given a string find all possible strings that are n edit distance away using Input Delete Switch Replace deah _eah d_ar de_r ... etc Building the model 1. Identify a misspelled word 2. Find strings n edit distance away 3. Filter candidates 4. Calculate word probabilities Building the model Filter candidates deah _eah d_ar de_r ... etc Building the model Filter candidates deah deah _eah yeah d_ar → dear de_r dean ... etc ... etc Building the model II Building the model 1. Identify a misspelled word 2. Find strings n edit distance away 3. Filter candidates 4. Calculate word probabilities Building the model Calculate word probabilities Building the model Calculate word probabilities Example: “I am happy because I am learning” Word Count happy because learning Total : 7 Building the model Calculate word probabilities Example: “I am happy because I am learning” Word Count happy because learning Total : 7 Building the model Calculate word probabilities Example: “I am happy because I am learning” Word Count happy because learning Total : 7 Building the model Calculate word probabilities Example: “I am happy because I am learning” Word Count happy because learning Total : 7 Building the model Calculate word probabilities Example: “I am happy because I am learning” Word Count happy because learning Total : 7 Probability of a word Number of times the word appears Total size of the corpus Building the model Calculate word probabilities Example: “I am happy because I am learning” Word Count happy because learning Total : 7 Probability of a word Number of times the word appears Total size of the corpus Building the model Calculate word probabilities deah yeah dear dean ... etc Building the model Calculate word probabilities deah → dear ✅ yeah dear dean ... etc Summary 1. Identify a misspelled word 2. Find strings n edit distance away 1. Filter candidates 2. Calculate word probabilities deah → dear ☑️ yeah dear dean ... etc Insert Delete Switch Replace Summary 1. Identify a misspelled word 2. Find strings n edit distance away 1. Filter candidates 2. Calculate word probabilities deah → dear ☑️ yeah dear dean ... etc Insert Delete Switch Replace Summary 1. Identify a misspelled word 2. Find strings n edit distance away 1. Filter candidates 2. Calculate word probabilities deah → dear ☑️ yeah dear dean ... etc Insert Delete Switch Replace Summary 1. Identify a misspelled word 2. Find strings n edit distance away 1. Filter candidates 2. Calculate word probabilities deah → dear ☑️ _eah d_ar de_r ... etc Insert Delete Switch Replace Summary 1. Identify a misspelled word 2. Find strings n edit distance away 1. Filter candidates 2. Calculate word probabilities deah → dear ☑️ yeah dear dean ... etc Insert Delete Switch Replace Summary 1. Identify a misspelled word 2. Find strings n edit distance away 1. Filter candidates 2. Calculate word probabilities deah → dear ☑️ yeah dear dean ... etc Insert Delete Switch Replace Summary 1. Identify a misspelled word 2. Find strings n edit distance away 1. Filter candidates 2. Calculate word probabilities deah → dear ✅ yeah dear dean ... etc Insert Delete Switch Replace Summary 1. Identify a misspelled word 2. Find strings n edit distance away 1. Filter candidates 2. Calculate word probabilities deah → dear ✅ yeah dear dean ... etc Insert Delete Switch Replace Minimum edit distance How to evaluate similarity between 2 strings? Minimum number of edits needed to transform 1 string into the other Spelling correction, document similarity, machine translation, DNA sequencing, and more Minimum edit distance Minimum edit distance Edits: Insert (add a letter) 'to': 'top', 'two' ... Delete (remove a letter) 'hat': 'ha', 'at', 'ht' Replace (change 1 letter to another) 'jaw': 'jar', 'paw', Minimum edit distance Example: Source: Minimum edit distance Example: Source: Target: Minimum edit distance Example: Source: Target: What is the minimum number of edits to make this happen ? Minimum edit distance Example: Source: Target: p → s : replace Minimum edit distance Example: Source: Target: p → s : replace l → t : replace Minimum edit distance Example: Source: Target: p → s : replace l → t : replace Minimum edit distance Example: Source: Target: p → s : replace l → t : replace Minimum edit distance Example: Source: Target: edits = 2 p → s : replace l → t : replace Minimum edit distance Example: Source: Target: edits = 2 Edit cost: Insert Delete Replace 2 p → s : replace l → t : replace Minimum edit distance Example: Source: Target: edits = 2 p → s : replace l → t : replace Edit cost: Insert Delete Replace 2 edit distance = 2 * 2 = 4 Minimum edit distance Example: Minimum edit distance Example: CCAAGGGGTGACTCTAGTTTAATATAACTGAGATCAAATTATATGGGTGAT 🧬!!. Minimum edit distance algorithm Minimum edit distance Source: play → Target: stay Minimum edit distance Source: play → Target: stay Minimum edit distance Source: play → Target: stay Minimum edit distance Source: play → Target: stay Minimum edit distance Source: play → Target: stay Minimum edit distance Source: play → Target: stay D[ ] Minimum edit distance Source: play → Target: stay D[ ] D[2,3] = pl → sta Minimum edit distance Source: play → Target: stay D[ ] D[2,3] = pl → sta D[2,3] = source[:2] → target[:3] Minimum edit distance Source: play → Target: stay D[ ] D[2,3] = pl → sta D[2,3] = source[:2] → target[:3] D[ i, j ] = source[ : i ] → target[ : j ] Minimum edit distance Source: play → Target: stay D[ ] D[ i, j ] = source[ : i ] → target[ : j ] Minimum edit distance Source: play → Target: stay D[ ] D[ i, j ] = source[ : i ] → target[ : j ] D[ m, n ] = source → target Minimum edit distance Source: play → Target: stay D[ ] D[ i, j ] = source[ : i ] → target[ : j ] D[ m, n ] = source → target Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 # → # Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 # → # Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 p → # Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 p → # delete Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 # → s Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 # → s insert Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 p → s Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 p → s insert + delete: p → ps → s Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 p → s insert + delete: p → ps → s Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 p → s insert + delete: p → ps → s: Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 p → s insert + delete: p → ps → s: delete + insert: p → # → s Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 p → s insert + delete: p → ps → s: delete + insert: p → # → s Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 p → s insert + delete: p → ps → s: delete + insert: p → # → s: 2 Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 p → s insert + delete: p → ps → s: delete + insert: p → # → s: 2 replace: p → s Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 p → s insert + delete: p → ps → s: delete + insert: p → # → s: 2 replace: p → s Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 p → s insert + delete: p → ps → s: delete + insert: p → # → s: 2 replace: p → s: Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 p → s insert + delete: p → ps → s: delete + insert: p → # → s: 2 replace: p → s: Minimum edit distance algorithm II Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 play → # Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 play → # D[ i, j ] = D[ i-1, j ] + del_cost Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 play → # D[ i, j ] = D[ i-1, j ] + del_cost Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 play → # D[ i, j ] = D[ i-1, j ] + del_cost D[4,0] = play → # = source[ :4] → target[0] Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 # → play Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 # → play D[ i, j ] = D[ i, j-1 ] + ins_cost Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 # → play D[ i, j ] = D[ i, j-1 ] + ins_cost Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 p → s D[i, j] = D[i - 1, j] + del_cost min D[i, j - 1] + ins_cost D[i - 1, j - 1] + rep_cost; if src[i] ≠ tar[j] if src[i] = tar[j] Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 p → s D[i, j] = D[i - 1, j] + del_cost min D[i, j - 1] + ins_cost D[i - 1, j - 1] + rep_cost; if src[i] ≠ tar[j] if src[i] = tar[j] Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 p → s D[i, j] = D[i - 1, j] + del_cost min D[i, j - 1] + ins_cost D[i - 1, j - 1] + rep_cost; if src[i] ≠ tar[j] if src[i] = tar[j] Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 p → s D[i, j] = D[i - 1, j] + del_cost min D[i, j - 1] + ins_cost D[i - 1, j - 1] + rep_cost; if src[i] ≠ tar[j] if src[i] = tar[j] Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 p → s D[i, j] = D[i - 1, j] + del_cost min D[i, j - 1] + ins_cost D[i - 1, j - 1] + rep_cost; if src[i] ≠ tar[j] if src[i] = tar[j] # dev purposes only # image of how previous slide should be # appearing for everyone ! Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 FORMULAS BUILDING ONLY EQUATION USED IN NEXT SLIDES D[. i - 1, j] + del_cost D[i, j]= min D[i, j - 1] + ins_cost D[i - 1, j - 1] + rep_cost; if src[i] ≠ tar[j] if src[i] = tar[j] Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 p → s D[ i-1, j ] + 1 = 2 D[ i, j-1 ] + 1 = 2 D[ i-1, j-1 ] + 2 = 2 Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 p → s D[ i-1, j ] + 1 = 2 D[ i, j-1 ] + 1 = 2 D[ i-1, j-1 ] + 2 = 2 Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 p → s D[ i-1, j ] + 1 = 2 D[ i, j-1 ] + 1 = 2 D[ i-1, j-1 ] + 2 = 2 Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 p → s D[ i-1, j ] + 1 = 2 D[ i, j-1 ] + 1 = 2 D[ i-1, j-1 ] + 2 = 2 Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 p → s D[ i-1, j ] + 1 = 2 D[ i, j-1 ] + 1 = 2 min = 2 D[ i-1, j-1 ] + 2 = 2 Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 Minimum edit distance Source: to → Target: go Cost: insert: 1, delete: 1, replace: 2 FOR QUIZ SETUP ONLY. ... USED FOR IMAGES ON QUIZ IN NEXT SLIDE. Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 play → stay D[ m, n ] = 4 Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 play → stay D[ m, n ] = 4 Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 play → stay D[ m, n ] = 4 Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 Minimum edit distance algorithm III Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 Levenshtein distance Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 Levenshtein distance Backtrace Minimum edit distance Source: play → Target: stay Cost: insert: 1, delete: 1, replace: 2 Levenshtein distance Backtrace Dynamic programming Summary Summary - learning objectives What is autocorrect ? Building the model Minimum edit distance Minimum edit distance algorithm"
  },
  {
    "index": 6,
    "title": "2.2 Part of Speech Tagging and Hidden Markov Models",
    "content": " Part of Speech Tagging Outline What is part of speech tagging? Markov chains Hidden Markov models Viterbi algorithm Example Coding assignment! What is part of speech? Why not learn something ? adverb adverb verb noun punctuation mark, sentence closer Part of speech (POS) tagging Part of speech tags: Why not learn something ? lexical term tag example noun something, nothing verb learn, study determiner the, a w-adverb WRB why, where ..... WRB Applications of POS tagging Named entities Speech recognition Co-reference resolution 324m Markov Chains verb? noun? ... ? Example Why not learn ... verb verb? noun? ... ? Part of Speech Dependencies Why not learn ... verb The Most Likely Next Word Why not learn verb swimming? noun Less Likely Words Why not learn verb swim? verb Visual Representation verb noun 0.6 0.2 What are Markov chains? States Markov Chains and POS Tags POS tags as States Transition probabilities 0.2 0.6 0.2 0.4 0.2 0.5 0.3 0.3 0.3 Transition probabilities 0.2 0.6 0.2 0.4 0.2 0.5 0.3 0.3 Why not learn something ? 0.3 Transition probabilities Why not learn something ? 0.2 0.6 0.2 0.4 0.2 0.5 0.3 0.3 0.3 Transition probabilities Why not learn something ? 0.2 0.6 0.2 0.4 0.2 0.5 0.3 0.3 0.3 The transition matrix NN (noun) 0.2 0.2 0.6 VB (verb) 0.4 0.3 0.3 O (other) 0.2 0.3 0.5 0.2 0.6 0.2 0.4 0.2 0.5 0.3 0.3 0.3 The transition matrix NN (noun) 0.2 0.2 0.6 VB (verb) 0.4 0.3 0.3 O (other) 0.2 0.3 0.5 0.2 0.6 0.2 0.4 0.2 0.5 0.3 0.3 0.3 The first word 0.2 0.6 0.2 0.4 0.2 0.5 0.3 0.3 0.3 Why not learn something ? NN? VB? Initial probabilities 𝜋(initial) 0.4 0.1 0.5 NN (noun) 0.2 0.2 0.6 VB (verb) 0.4 0.3 0.3 O (other) 0.2 0.3 0.5 0.4 0.1 0.5 0.2 0.6 0.2 0.4 0.2 0.5 0.3 0.3 0.3 Transition table and matrix 𝜋(initial) 0.4 0.1 0.5 NN (noun) 0.2 0.2 0.6 VB (verb) 0.4 0.3 0.3 O (other) 0.2 0.3 0.5 Summary States Transition matrix Hidden Markov Models Hidden Markov Model hidden states jump = verb you jump = ? machine jump = verb run = verb fly = verb you jump run fly machine *observable Transition probabilities 𝜋(initial) 0.4 0.1 0.5 NN (noun) 0.2 0.2 0.6 VB (verb) 0.4 0.3 0.3 O (other) 0.2 0.3 0.5 0.4 0.1 0.5 0.2 0.6 0.2 0.4 0.2 0.5 0.3 0.3 0.3 Emission probabilities going eat ... 0.3 0.1 0.5 hidden states observables Emission probabilities going eat ... 0.3 0.1 0.5 going eat ... NN (noun) 0.5 0.1 0.02 VB (verb) 0.3 0.1 0.5 O (other) 0.3 0.5 0.68 Emission probabilities going eat ... 0.3 0.1 0.5 going eat ... NN (noun) 0.5 0.1 0.02 VB (verb) 0.3 0.1 0.5 O (other) 0.3 0.5 0.68 The emission matrix He lay on his back. I'll be back. going eat ... NN (noun) 0.5 0.1 0.02 VB (verb) 0.3 0.1 0.5 O (other) 0.3 0.5 0.68 Summary States Transition matrix Emission matrix Calculating Probabilities Transition probabilities You eat The oatmeal You eat corpus Transition probabilities You eat You eat corpus The oatmeal You + eat Count: 2 You + e?t Count: 3 Transition probabilities You eat You eat corpus The oatmeal transition probability: You + eat = ⅔ Transition probabilities Count occurrences of tag pairs Transition probabilities Count occurrences of tag pairs Calculate probabilities using the counts The corpus Ezra Pound - In a Station of the Metro The apparition of these faces in the crowd : Petals on a wet , black bough . Preparation of the corpus <s> In a Station of the Metro <s> The apparition of these faces in the crowd <s> Petals on a wet , black bough . Ezra Pound - Preparation of the corpus <s> in a station of the metro <s> the apparition of these faces in the crowd <s> petals on a wet , black bough . Ezra Pound - Populating the Transition Matrix Populating the transition matrix <s> in a station of the metro <s> the apparition of these faces in the crowd <s> petals on a wet , black bough . Ezra Pound - NN (noun) VB (verb) O (other) Populating the transition matrix <s> in a station of the metro <s> the apparition of these faces in the crowd <s> petals on a wet , black bough . Ezra Pound - NN (noun) VB (verb) O (other) Populating the transition matrix NN (noun) VB (verb) O (other) <s> in a station of the metro <s> the apparition of these faces in the crowd <s> petals on a wet , black bough . Ezra Pound - Populating the transition matrix NN (noun) VB (verb) O (other) <s> in a station of the metro <s> the apparition of these faces in the crowd <s> petals on a wet , black bough . Ezra Pound - Populating the transition matrix NN (noun) VB (verb) O (other) <s> in a station of the metro <s> the apparition of these faces in the crowd <s> petals on a wet , black bough . Ezra Pound - Populating the transition matrix NN (noun) VB (verb) O (other) <s> in a station of the metro <s> the apparition of these faces in the crowd <s> petals on a wet , black bough . Ezra Pound - Populating the transition matrix <s> in a station of the metro <s> the apparition of these faces in the crowd <s> petals on a wet , black bough . Ezra Pound - NN (noun) VB (verb) O (other) Populating the transition matrix NN (noun) VB (verb) O (other) <s> in a station of the metro <s> the apparition of these faces in the crowd <s> petals on a wet , black bough . Ezra Pound - Populating the transition matrix NN (noun) VB (verb) O (other) <s> in a station of the metro <s> the apparition of these faces in the crowd <s> petals on a wet , black bough . Ezra Pound - Populating the transition matrix NN (noun) VB (verb) O (other) <s> in a station of the metro <s> the apparition of these faces in the crowd <s> petals on a wet , black bough . Ezra Pound - Populating the transition matrix NN (noun) VB (verb) O (other) <s> in a station of the metro <s> the apparition of these faces in the crowd <s> petals on a wet , black bough . Ezra Pound - Populating the transition matrix NN (noun) VB (verb) O (other) <s> in a station of the metro <s> the apparition of these faces in the crowd <s> petals on a wet , black bough . Ezra Pound - Populating the transition matrix Populating the transition matrix Populating the transition matrix Populating the transition matrix Smoothing 1+ε 0+ε 2+ε 3+3*ε 0+ε 0+ε 6+ε 6+3*ε 0+ε 0+ε 0+ε 0+3*ε 6+ε 0+ε 8+ε 14+3*ε Smoothing 0.3333 0.0003 0.6663 0.0001 0.0001 0.9996 0.3333 0.3333 0.3333 0.4285 0.0000 0.5713 Populating the Emission Matrix Emission probabilities You eat The oatmeal You eat corpus Transition probabilities You eat You eat corpus The oatmeal You Count: 2 You Count: 3 Transition probabilities You eat You eat corpus The oatmeal emission probability: You = ⅔ The emission matrix <s> in a station of the metro <s> the apparition of these faces in the crowd <s> petals on a wet , black bough . Ezra Pound - ... NN (noun) VB (verb) O (other) The emission matrix Ezra Pound - ... NN (noun) VB (verb) O (other) <s> in a station of the metro <s> the apparition of these faces in the crowd <s> petals on a wet , black bough . The emission matrix Ezra Pound - ... NN (noun) VB (verb) O (other) <s> in a station of the metro <s> the apparition of these faces in the crowd <s> petals on a wet , black bough . The emission matrix Ezra Pound - ... NN (noun) VB (verb) O (other) <s> in a station of the metro <s> the apparition of these faces in the crowd <s> petals on a wet , black bough . The emission matrix Ezra Pound - ... NN (noun) VB (verb) O (other) <s> in a station of the metro <s> the apparition of these faces in the crowd <s> petals on a wet , black bough . The emission matrix ... NN (noun) ..... VB (verb) ..... O (other) ..... Summary 1. Calculate transition and emission matrix 1. How to apply smoothing The Viterbi Algorithm Why not learn something ? Viterbi algorithm - a graph algorithm <s> love learn love pony sweets you love eat learn Viterbi algorithm - a graph algorithm love pony sweets you <s> love learn 0.15 love eat learn 0.3 0.5 Viterbi algorithm - a graph algorithm love pony sweets you <s> love learn 0.15 love eat learn 0.5 0.5 0.1 0.5 Viterbi algorithm - a graph algorithm love pony sweets you <s> love learn 0.15 0.25 love eat learn 0.5 0.5 Viterbi algorithm - a graph algorithm love pony sweets you <s> love learn 0.15 0.25 0.08 love eat learn 0.2 0.4 Viterbi algorithm - a graph algorithm love pony sweets you <s> love learn 0.15 0.25 0.08 0.1 love eat learn 0.5 0.2 Viterbi algorithm - a graph algorithm love pony sweets you <s> love learn 0.15 * 0.25 * 0.08 * 0.1 love eat learn 0.5 0.2 Probability for this sequence of hidden states: 0.0003 Viterbi algorithm - Steps 1. Initialization step 2. Forward pass 3. Backward pass ......... Viterbi: Initialization Viterbi algorithm - Steps 1. Initialization step Initialization step ... c1,1 ... cN,1 Initialization step ... d1,1 ... dN,1 Viterbi: Forward Pass Viterbi algorithm - Steps 2. Forward pass Forward pass ... c1,1 c1,2 c1,K ... cN,1 cN,2 cN,K Forward pass ... c1,1 c1,2 c1,K ... cN,1 cN,2 cN,K Forward pass ... c1,1 c1,2 c1,K ... cN,1 cN,2 cN,K Forward pass ... c1,1 c1,2 c1,K ... cN,1 cN,2 cN,K Forward pass ... c1,1 c1,2 c1,K ... cN,1 cN,2 cN,K Forward pass ... d1,1 d1,2 d1,K ... dN,1 dN,2 dN,K Viterbi: Backward Pass Viterbi algorithm - Steps 3. Backward pass Backward pass ... c1,1 c1,2 c1,K ... cN,1 cN,2 cN,K ... d1,1 d1,2 d1,K ... dN,1 dN,2 dN,K Backward pass <s> Backward pass 0.25 0.125 0.025 0.0125 0.01 0.1 0.025 0.05 0.01 0.003 0.3 0.05 0.025 0.02 0.0000 0.2 0.1 0.000 0.0025 0.0003 Backward pass <s> Backward pass <s> Backward pass <s> Backward pass <s> Backward pass <s> Backward pass <s> Backward pass <s> Backward pass <s> Implementation notes 1. In Python index starts with 0! 2. Use log probabilities Summary 1. From word sequence to POS tag sequence 2. Viterbi algorithm 3. Log probabilities"
  },
  {
    "index": 7,
    "title": "2.3 Autocomplete and Language Models",
    "content": " N-Grams: Overview Estimate probability of word sequences Estimate probability of a word following a sequence of words What you'll be able to do! Create language model (LM) from text corpus to “Lyn is eating ... “ “chocolate“ “eggs“ “toast“ Language model Text corpus Apply this concept to autocomplete a sentence with most likely suggestions Other Applications Spelling correction “He entered the ship to buy some groceries” - “ship” a dictionary word • P(entered the shop to buy) > P(entered the ship to buy) Speech recognition P(I saw a van) > P(eyes awe of an) Augmentative communication Predict most likely word from menu for people unable to physically talk or sign. (Newell et al., 1998) Learning objectives Process text corpus to N-gram language model Out of vocabulary words Smoothing for previously unseen N- grams Language model evaluation Sentence auto-complete N-grams and Probabilities Outline N-grams and conditional probability from corpus What are N-grams? N-gram Corpus: I am happy because I am learning An N-gram is a sequence of N words Bigrams: { I am , am happy , happy because ... } Unigrams: { I , am , happy , because , learning } Trigrams: { I am happy , am happy because, ... } I happy Sequence notation Corpus: This is great ... teacher drinks tea. Unigram probability Corpus: I am happy because I am learning Size of corpus m = 7 Probability of unigram: Bigram probability Corpus: I am happy because I am learning I happy Probability of a bigram: Trigram Probability Corpus: I am happy because I am learning Probability of a trigram: N-gram probability Probability of N-gram: Quiz Objective: Apply n-gram probability calculation on sample corpus and 3-gram. Question: Corpus: “In every place of great resort the monster was the fashion. They sang of it in the cafes, ridiculed it in the papers, and represented it on the stage. ” (Jules Verne, Twenty Thousand Leagues under the Sea) In the context of our corpus, what is the probability of word “papers” following the phrase “it in the”. Type: Multiple Choice, single answer Options and solution: P(papers|it in the) =1 P(papers|it in the) = 2/3 P(papers|it in the) = 1/2 P(papers|it in the) = 0 = C(it in the papers)/C(it in the) Sequence Probabilities Outline Sequence probability shortcomings Sequence probability Approximation by N-gram probabilities Probability of a sequence Conditional probability and chain rule reminder Given a sentence, what is its probability? Probability of a sequence Sentence not in corpus Problem: Corpus almost never contains the exact sentence we're interested in or even its longer subsequences! Input: the teacher drinks tea Both likely the teacher drinks tea Approximation of sequence probability Approximation of sequence probability Markov assumption: only last N words matter ●Bigram ●N-gram ●Entire sentence modeled with bigram Quiz Objective: Apply sequence probability approximation with bigrams. Question: Given these conditional probabilities P(Mary)=0.1; P(likes)=0.2; P(cats)=0.3 P(Mary|likes) =0.2; P(likes|Mary) =0.3; P(cats|likes)=0.1; P(likes|cats)=0.4 Approximate the probability of the following sentence with bigrams: “Mary likes cats” Type: Multiple Choice, single answer Options and solution: P(Mary likes cats) =1 P(Mary likes cats) = 0.003 P(Mary likes cats) = 0.008 P(Mary likes cats) = 0 Starting and Ending Sentences Outline End of sentence symbol </s> Start of sentence symbols <s> Start of sentence token <s> the teacher drinks tea <s> the teacher drinks tea N-gram model: add N-1 start tokens <s> Start of sentence token <s> for N-grams Trigram: the teacher drinks tea => <s> <s> the teacher drinks tea End of sentence token </s> - motivation Corpus: <s> Lyn drinks chocolate <s> John drinks End of sentence token </s> - motivation Corpus <s> yes no <s> yes yes <s> no no Sentences of length 2: <s> yes yes <s> yes no <s> no no <s> no yes End of sentence token </s> - motivation Corpus <s> yes no <s> yes yes <s> no no Sentences of length 2: <s> yes yes <s> yes no <s> no no <s> no yes End of sentence token </s> - motivation Corpus <s> yes no <s> yes yes <s> no no Sentences of length 3: <s> yes yes yes <s> yes yes no <s> no no no End of sentence token </s> - motivation Corpus <s> yes no <s> yes yes <s> no no <s> the teacher drinks tea => <s> the teacher drinks tea </s> End of sentence token </s> - solution Bigram Corpus: <s> Lyn drinks chocolate </s> <s> John drinks </s> End of sentence token </s> for N-grams N-gram => just one </s> E.g. Trigram: the teacher drinks tea => <s> <s> the teacher drinks tea </s> Corpus <s> Lyn drinks chocolate </s> <s> John drinks tea </s> <s> Lyn eats chocolate </s> Example - bigram Quiz Objective: Apply sequence probability approximation with bigrams after adding start and end word. Question: Given these conditional probabilities P(Mary)=0.1; P(likes)=0.2; P(cats)=0.3 P(Mary|<s>)=0.2; P(</s>|cats)=0.6 P(likes|Mary) =0.3; P(cats|likes)=0.1 Approximate the probability of the following sentence with bigrams: “<s> Mary likes cats </s>” Type: Multiple Choice, single answer Options and solution: P(<s> Mary likes cats </s>) =0.0036 P(<s> Mary likes cats </s>) = 0.003 P(<s> Mary likes cats </s>) = 1 P(<s> Mary likes cats </s>) = 0 The N-gram Language Model Outline Count matrix Probability matrix Language model Log probability to avoid underflow Generative language model Count matrix Corpus: <s>I study I learn</s> Rows: unique corpus (N-1)-grams Columns: unique corpus words Bigram count matrix “study I” bigram <s> </s> study learn <s> </s> study learn Probability matrix • Divide each cell by its row sum Corpus: <s>I study I learn</s> Count matrix (bigram) Probability matrix <s> </s> study learn sum <s> </s> study learn <s> </s> study learn <s> </s> 0.5 0.5 study learn Language model ●probability matrix => language model Sentence probability Next word prediction Sentence probability: <s> I learn </s> <s> </s> study learn <s> </s> 0.5 0.5 study learn Log probability Logarithm properties reminder All probabilities in calculation <=1 and multiplying them brings risk of underflow Use log of the probabilities in Probability matrix and calculations Converts back from log Generative Language model Algorithm: Corpus: <s> Lyn drinks chocolate </s> <s> John drinks tea </s> <s> Lyn eats chocolate </s> 1. (<s>, Lyn) or (<s>, John)? 2. (Lyn,eats) or (Lyn,drinks) ? 3. (drinks,tea) or (drinks,chocolate)? 4. (tea,</s>) - always 1. Choose sentence start 2. Choose next bigram starting with previous word 3. Continue until </s> is picked Quiz Objective: Apply sum when calculating log probability instead of product. Question: Given the logarithm of these conditional probabilities: log(P(Mary|<s>))=-2; log(P(</s>|cats))=-1 log(P(likes|Mary)) =-10; log(P(cats|likes))=-100 Approximate the log probability of the following sentence with bigrams : “<s> Mary likes cats </s>” Type: Multiple Choice, single answer Options and solution: log(P(<s> Mary likes cats </s>)) =2000 log(P(<s> Mary likes cats </s>)) = 113 log(P(<s> Mary likes cats </s>))= -112 log(P(<s> Mary likes cats </s>)) = -113 Language Model Evaluation Outline Train/Validation/Test split Perplexity Test data Split corpus to Train/Validation/Test For smaller corpora 80% Train 10% Validation 10% Test For large corpora (typical for text) 98% Train 1% Validation 1% Test Evaluate on Training dataset Test data - split method Random short sequences Continuous text Corpus Training Validation Test Perplexity W → test set containing m sentences s → i-th sentence in the test set, each ending with </s> m → number of all words in entire test set W including </s> but not including <s> Perplexity Smaller perplexity = better model Character level models PP < word-based models PP E.g. m=100 Perplexity for bigram models concatenate all sentences in W → i-th word in test set → j-th word in i-th sentence Log perplexity Examples [Figure from Speech and Language Processing by Dan Jurafsky et. al] Training 38 million words, test 1.5 million words, WSJ corpus Perplexity Unigram: 962 Bigram: 170 Trigram: 109 Quiz Objective: Calculate log perplexity from log probabilities using sum and correct normalization coefficient (not including <s>). Question: Given the logarithm of these conditional probabilities: log(P(Mary|<s>))=-2; log(P(</s>|cats))=-1 log(P(likes|Mary)) =-10; log(P(cats|likes))=-100 Assuming our test set is W=“<s> Mary likes cats </s>”, what is the model's perplexity. Type: Multiple Choice, single answer Options and solution: log PP(W) = (-1/4)*(-113) log PP(W) = (-1/5)*(-113) log PP(W) = (-1/5)*113 log PP(W) = -113 Out of Vocabulary Words Outline Update corpus with <UNK> Choosing vocabulary Unknown words Out of vocabulary words Closed vs. Open vocabularies Unknown word = Out of vocabulary word (OOV) special tag <UNK> in corpus and in input Using <UNK> in corpus Create vocabulary V Replace any word in corpus and not in V by <UNK> Count the probabilities with <UNK> as with any other word Input query <s>Adam drinks chocolate</s> Example Corpus <s> Lyn drinks chocolate </s> <s> John drinks tea </s> <s> Lyn eats chocolate </s> Min frequency f=2 Vocabulary Lyn, drinks, chocolate <s><UNK> drinks chocolate</s> Corpus <s> Lyn drinks chocolate </s> <s> <UNK> drinks <UNK> </s> <s> Lyn <UNK> chocolate </s> How to create vocabulary V Criteria: Min word frequency f Max |V|, include words by frequency ●Use <UNK> sparingly Perplexity - only compare LMs with the same V Quiz Objective: Create corpus vocabulary based on minimum frequency. Question: Given the training corpus and minimum word frequency=2, how would the vocabulary for corpus preprocessed with <UNK> look like? “<s> I am happy I am learning </s> <s> I am happy I can study </s>” Type: Multiple Choice, single answer Options and solution: V = (I,am,happy,learning,can,study) V = (I,am,happy,I,am) V = (I,am,happy,learning,can,study,<UNK>) V = (I,. am,happy) Smoothing Outline Smoothing Backoff and interpolation Missing N-grams in corpus Missing N-grams in training corpus Problem: N-grams made of known words still might be missing in the training corpus Can be 0 Their counts cannot be used for probability estimation “John” , “eats” in corpus “John eats” Smoothing Add-one smoothing (Laplacian smoothing) Add-k smoothing Advanced methods: Kneser-Ney smoothing Good-Turing smoothing Backoff If N-gram missing => use (N-1)-gram, ... ○Probability discounting e.g. Katz backoff ○“Stupid” backoff Corpus <s> Lyn drinks chocolate </s> <s> John drinks tea </s> <s> Lyn eats chocolate </s> Interpolation Quiz Objective: Apply n-gram probability with add-k smoothing for phrase not present in the corpus. Question: Corpus: “I am happy I am learning” In the context of our corpus, what is the estimated probability of word “can” following the word “I” using the bigram model and add-k-smoothing where k=3. Type: Multiple Choice, single answer Options and solution: P(can|I) =1 P(can|I) = 3/(2+3*4) P(can|I) = 3/(3*4) P(can|I) = 0 Week Summary Summary N-Grams and probabilities Approximate sentence probability from N-Grams Build language model from corpus Fix missing information Out of vocabulary words with <UNK> Missing N-Gram in corpus with smoothing, backoff and interpolation Evaluate language model with perplexity Coding assignment!"
  },
  {
    "index": 8,
    "title": "2.4 Word embeddings with Neural Networks",
    "content": " Overview Some basic applications of word embeddings Semantic analogies and similarity village town city country gas oil petroleum happy joyful sad Sentiment analysis Classification of customer feedback Advanced applications of word embeddings Question answering Information extraction Machine translation Learning objectives Identify the key concepts of word representations Generate word embeddings Prepare text for machine learning Implement the continuous bag-of-words model Prerequisite: neural networks Basic Word Representations Outline Integers One-hot vectors Word embeddings Integers Word Number able about ..... hand happy ..... zebra Integers Simple Ordering: little semantic sense hand happy zebra One-hot vectors able about ... hand happy ... zebra “a” able about ... hand happy ... zebra “happy” able about ... hand happy ... zebra “zebra” ..... rows One-hot vectors Word Number able about ..... hand happy ..... zebra able about ... hand happy ... zebra “happy” ..... One-hot vectors Simple No implied ordering happy ... Zyzzyva ~10k-1M+ rows paper happy excited d(paper, excited) = d(paper, happy) = d(excited, happy) Huge vectors No embedded meaning Word Embeddings Meaning as vectors negative positive anger boring rage paper happy excited kitten spider (-2.08) (-0.91) (-2.52) (0.03) (1.75) (2.31) (1.09) (-1.53) Meaning as vectors negative positive abstract concrete rage anger spider boring kitten happy excited paper thought puppy snake (-2.52, -0.54) (-2.08, -0.71) (-1.53, 0.41) (1.75, -0.81) (2.31, -0.54) (0.03, 0.79) (0.03, -0.93) (0.98, 0.57) (-1.53, 0.41) (1.09, 0.57) Low dimension Word embedding vectors 0.123 -4.059 1.891 ~100-~1000 rows “happy” forest ≈ tree forest ≉ticket Paris:France :: Rome:? Embed meaning e.g. semantic distance e.g. analogies word vectors Terminology integers one-hot vectors word embedding vectors word embeddings “word vectors” Summary Words as integers Words as vectors One-hot vectors Word embedding vectors Benefits of word embeddings for NLP How to Create Word Embeddings Corpus e.g. Wikipedia Embedding method Word embedding process General- purpose Specialized Words in context Word embeddings Transformation Hyperparameters Word embedding size Machine learning model words integers, vectors ... “I think [???] I am” Learning task e.g. contracts, law books Self-supervised = unsupervised + supervised Meaning Word Embedding Methods Basic word embedding methods word2vec (Google, 2013) Continuous bag-of-words (CBOW) Continuous skip-gram / Skip-gram with negative sampling (SGNS) Global Vectors (GloVe) (Stanford, 2014) fastText (Facebook, 2016) Supports out-of-vocabulary (OOV) words Advanced word embedding methods Deep learning, contextual embeddings BERT (Google, 2018) ELMo (Allen Institute for AI, 2018) GPT-2 (OpenAI, 2018) Tunable pre-trained models available Continuous Bag-of-Words Model Corpus Embedding method Continuous bag-of-words word embedding process Word embeddings Transformation “I think [???] I am” CBOW “therefore” Center word prediction: rationale The little is barking dog puppy hound terrier ... Corpus CBOW Transformation Creating a training example I am happy because I am learning center word context words C = 2 window window size = 5 context half-size Corpus CBOW Transformation From corpus to training Corpus Embedding method Word embeddings Transformation I am happy because I am learning Context words Center word I am because I happy am happy I am because happy because am learning CBOW Context words Predicted center word Corpus CBOW Transformation From corpus to training Corpus CBOW Transformation Corpus Embedding method Word embeddings Transformation I am happy because I am learning Context words Center word I am because I happy am happy I am because happy because am learning CBOW Context words Predicted center word From corpus to training Corpus CBOW Transformation Corpus Embedding method Word embeddings Transformation I am happy because I am learning Context words Center word I am because I happy am happy I am because happy because am learning CBOW Context words Predicted center word CBOW in a nutshell Source: Mikolov, T., Chen, K., Corrado, G.S., & Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space Cleaning and Tokenization Cleaning and tokenization matters Letter case “The” == “the” == “THE” → lowercase / upper case Cleaning and tokenization matters Letter case Punctuation “The” == “the” == “THE” , ! . ? → . “ ' « » ' ” → ∅ ... !! ??? → . → lowercase / upper case Cleaning and tokenization matters Letter case Punctuation Numbers “The” == “the” == “THE” , ! . ? → . “ ' « » ' ” → ∅ ... !! ??? → . 1 2 3 5 8 → ∅ 3.14159 90210 → as is/<NUMBER>. → lowercase / upper case Cleaning and tokenization matters Letter case Punctuation Numbers Special characters “The” == “the” == “THE” , ! . ? → . “ ' « » ' ” → ∅ ... !! ??? → . 1 2 3 5 8 → ∅ → lowercase / upper case ∇$ € § ¶ ** → ∅ 3.14159 90210 → as is/<NUMBER>. Cleaning and tokenization matters Letter case Punctuation Numbers Special characters Special words “The” == “the” == “THE” , ! . ? → . “ ' « » ' ” → ∅ ... !! ??? → . 1 2 3 5 8 → ∅ → lowercase / upper case ∇$ € § ¶ ** → ∅ 😊#nlp → :happy: #nlp 3.14159 90210 → as is/<NUMBER>. Example in Python: corpus emoji punctuation number Who ❤\"word embeddings\" in 2020? I do!!! Example in Python: libraries # pip install nltk # pip install emoji import nltk from nltk.tokenize import word_tokenize import emoji nltk.download('punkt') # download pre-trained Punkt tokenizer for English Example in Python: code corpus = 'Who ❤\"word embeddings\" in 2020? I do!!!' data = re.sub(r'[,!?;-]+', '.', corpus) → Who ❤\"word embeddings\" in 2020. I do. Example in Python: code corpus = 'Who ❤\"word embeddings\" in 2020? I do!!!' data = re.sub(r'[,!?;-]+', '.', corpus) data = nltk.word_tokenize(data) # tokenize string to words → ['Who', '❤', '``', 'word', 'embeddings', \"''\", 'in', '2020', '.', 'I', 'do', '.'] Example in Python: code corpus = 'Who ❤\"word embeddings\" in 2020? I do!!!' data = re.sub(r'[,!?;-]+', '.', corpus) data = nltk.word_tokenize(data) # tokenize string to words data = [ ch.lower() for ch in data if ch.isalpha() or ch == '.' or emoji.get_emoji_regexp().search(ch) → ['who', '❤', 'word', 'embeddings', 'in', '.', 'i', 'do', '.'] Sliding Window of Words in Python Sliding window of words in Python def get_windows(words, C): i = C while i < len(words) - C: center_word = words[i] context_words = words[(i - C):i] + words[(i+1):(i+C+1)] yield context_words, center_word i += 1 happy because learning Sliding window of words in Python def get_windows(words, C): ... yield context_words, center_word for x, y in get_windows( ['i', 'am', 'happy', 'because', 'i', 'am', 'learning'], print(f'{x}\\t{y}') Sliding window of words in Python for x, y in get_windows( ['i', 'am', 'happy', 'because', 'i', 'am', 'learning'], print(f'{x}\\t{y}') → ['I', 'am', 'because', 'I'] happy ['am', 'happy', 'I', 'am'] because ['happy', 'because', 'am', 'learning'] Transforming Words into Vectors Transforming center words into vectors I am happy because I am learning Corpus Vocabulary am, because, happy, I, learning One-hot vector because happy learning because happy learning Transforming context words into vectors Average of individual one-hot vectors because happy learning because 0.25 0.25 0.5 / 4 I am because I Final prepared training set Context words Context words vector Center word Center word vector I am because I [0.25; 0.25; 0; 0.5; 0] happy [0; 0; 1; 0; 0] am happy I am [0.5; 0; 0.25; 0.25; 0] because [0; 1; 0; 0; 0] happy because am learning [0.25; 0.25; 0.25; 0; 0.25] [0; 0; 0; 1; 0] Architecture of the CBOW Model Architecture of the CBOW model Input layer Hidden layer Output layer “I am happy because I am learning” → V = 5 Context words vector Center word vector x = ŷ = Hyperparameters Word embedding size ... biases weights ReLU biases weights softmax Architecture of the CBOW Model: Dimensions Dimensions (single input) Input layer Hidden layer Output layer ReLU z1 = W1x + b1 h = ReLU(z1) N x V N x 1 N x 1 N x 1 softmax z2 = W2h + b2 ŷ = softmax(z2) V x N V x 1 V x 1 V x 1 V x 1 N x 1 V x 1 Dimensions (single input) Column vectors Row vectors z1 = W1x + b1 z1 = xW1 T + b1 x = N x 1 z1 = b1 = V x 1 N x 1 N x V W1 = x = 1 x V N x V W1 = 1 x N b1 = 1 x N b1 = Architecture of the CBOW Model: Dimensions 2 Dimensions (batch input) Input layer Hidden layer Output layer ReLU Z1 = W1X + B1 H = ReLU(Z1) N x V N x m N x m N x m softmax Z2 = W2H + B2 Ŷ = softmax(Z2) V x N V x m V x m V x m V x m N x m V x m x(1) Context words vectors X = x(m) → matrix B1 = broadcasting b1 → Dimensions (batch input) Context words matrix ŷ(1) Predicted center word matrix Ŷ = ŷ(m) x(1) X = x(m) Input layer Hidden layer Output layer V x m N x m V x m ReLU softmax Architecture of the CBOW Model Activation Functions Rectified Linear Unit (ReLU) Input layer Hidden layer ReLU z1 = W1x + b1 h = ReLU(z1) ReLU 5.1 -0.3 -4.6 0.2 5.1 0.2 ReLU(x) = max(0, x) Softmax Hidden layer Output layer softmax z = W2h + b2 ŷ = softmax(z) softmax softmax ∈[0, 1]K 𝝨=1 ~probabilities happy zebra Probabilities of being center word 0.083 8.5 Softmax: example exp exp(z) 𝝨=97899 / 𝝨 ŷ = softmax(z) because happy learning 0.03 0.612 0.225 0.05 𝝨=1 ← Predicted center word Training a CBOW Model Cost Function Loss Machine learning model Parameters Input Truth Prediction adjust Loss minimize error CBOW Predicted center word vector Actual center word vector Context words vector W1, b1, W2, b2 cross-entropy loss Cross-entropy loss y = ŷ = Actual 0.083 0.03 0.611 0.225 0.05 because happy learning Predicted log -2.49 -3.49 -0.49 -1.49 -2.49 log(ŷ) -0.49 y ⊙log(ŷ) - 𝝨 J = 0.49 I am happy because I am learning Cross-entropy loss because happy learning log -0.04 -4.61 -4.61 -4.61 -4.61 log(ŷ) -4.61 y ⊙log(ŷ) - 𝝨 J = 4.61 0.96 0.01 0.01 0.01 0.01 J(correct) = 0.49 Cross-entropy loss J = -log ŷactual word because happy learning 0.96 0.01 0.01 0.01 0.01 → J = 4.61 correct predictions: reward incorrect predictions: penalty Forward Propagation Training a CBOW Model Training process Forward propagation Cost Backpropagation and gradient descent ŷ(1) x(1) Forward propagation Context words matrix Predicted center word matrix Ŷ = ŷ(m) X = x(m) Input layer Hidden layer Output layer ReLU softmax Z1 = W1X + B1 H = ReLU(Z1) Z2 = W2H + B2 Ŷ = softmax(Z2) Cost Predicted center word matrix Actual center word matrix Cost: mean of losses ŷ(1) ŷ(m) y(1) y(m) Backpropagation and Gradient Descent Training a CBOW Model Minimizing the cost Backpropagation: calculate partial derivatives of cost with respect to weights and biases Minimizing the cost Backpropagation: calculate partial derivatives of cost with respect to weights and biases Gradient descent: update weights and biases Gradient descent Hyperparameter: learning rate ⍺ Extracting Word Embedding Vectors w(1) Extracting word embedding vectors: option 1 Input layer Hidden layer Output layer ReLU softmax W1 = w(V) x = because happy learning w(1) Extracting word embedding vectors: option 2 Input layer Hidden layer Output layer ReLU softmax W2 = w(V) x = because happy learning Extracting word embedding vectors: option 3 (1) (V) (1) (V) (1) (V) W3= 0.5 (W1+W2 T) = W2 = W1 = x = because happy learning Evaluating Word Embeddings Intrinsic Evaluation Intrinsic evaluation Test relationships between words Analogies “France” is to “Paris” as “Italy” is to <?> “seen” is to “saw” as “been” is to <?> Semantic analogies Syntactic analogies “wolf” is to “pack” as “bee” is to <?> → swarm? colony? ⚡Ambiguity Intrinsic evaluation Test relationships between words Analogies Intrinsic evaluation Test relationships between words Analogies Clustering Source: Michael Zhai, Johnny Tan, and Jinho D. Choi. 2016. Intrinsic and extrinsic evaluations of word embeddings Intrinsic evaluation Test relationships between words Analogies Clustering Visualization village town city country gas oil petroleum happy joyful sad Evaluating Word Embeddings Extrinsic Evaluation Extrinsic evaluation Test word embeddings on external task e.g. named entity recognition, parts-of-speech tagging Named entity Andrew works at person organization Extrinsic evaluation Time-consuming More difficult to troubleshoot Test word embeddings on external task e.g. named entity recognition, parts-of-speech tagging Evaluates actual usefulness of embeddings Conclusion Recap and assignment Data preparation Word representations Continuous bag-of-words model Evaluation Going further Advanced language modelling and word embeddings NLP and machine learning libraries # from keras.layers.embeddings import Embedding embed_layer = Embedding(10000, 400) # import torch.nn as nn embed_layer = nn.Embedding(10000, 400) Keras PyTorch"
  },
  {
    "index": 9,
    "title": "3.1 Recurrent Neural Networks for Language Modeling",
    "content": " Neural Networks for Sentiment Analysis Outline Neural networks and forward propagation Structure for sentiment analysis Neural Networks Forward propagation Activations ith layer Neural Networks for sentiment analysis Embedding Positive (1) Negative (0) Neural Networks for sentiment analysis Embedding Positive (1) Negative (0) Tweet: This movie was almost good Initial Representation Number ..... Tweet: This movie was almost good Word able about ... hand happy ... zebra [700 680 720 20 55] [700 680 720 20 55 0 0 0 0 0 0 0] Padding To match size of longest tweet Summary Structure for sentiment analysis Classify complex tweets Initial representation Dense and ReLU Layers Dense layer in detail Outline ReLU function Neural networks Dot product Non-Linear Function Dense Layer Dense layer Trainable parameters ReLU Layer ReLU = Rectiﬁed linear unit Summary Dense Layer ReLU Layer Other Layers Outline Embedding layer Mean layer Embedding Layer Vocabulary happy because learning NLP sad not Index 0.020 -0.003 0.009 -0.011 -0.040 -0.009 -0.044 0.011 0.006 0.010 0.010 -0.018 -0.047 0.050 0.001 -0.022 Trainable weights Vocabulary Embedding 0.009 0.009 Mean Layer Tweet: I am happy Vocabulary happy Index 0.020 -0.003 0.009 0.006 0.010 0.010 0.020 -0.003 0.009 0.006 0.010 0.010 No trainable parameters Mean of the word embeddings Summary Embedding is trainable using an embedding layer Mean layer gives a vector representation Traditional Language models I saw the soccer game Traditional Language Models J'ai vu le match de foot I saw the game of soccer I saw the soccer match Saw I the game of soccer 4.5 e-5 6.0 e-5 4.6 e-5 2.6 e-9 Sequence N-grams Bigrams Trigrams Need a lot of space and RAM Large N-grams needed to capture dependencies between distant words Summary N-grams consume a lot of memory Different types of RNNs are the preferred alternative Recurrent Neural Networks Nour was supposed to study with me. I called her but she did not ____________ Advantages of RNNs want respond choose want have ask attempt answer know Similar probabilities with trigram have RNNs look at every previous word answer RNNs Basic Structure I called her but she did not _______________ answer called her but she did not Learnable parameters Summary RNNs model relationships among distant words In RNNs a lot of computations share parameters Applications of RNNs One to One Real Valladolid Real Madrid Real Zaragoza Real Madrid Atletico Madrid Real Madrid One to Many Brown Puppy Caption generation Many to One very happy Positive Tweet: Sentiment analysis Many to Many faim hungry Encoder Decoder Machine Translation Summary RNNs can be implemented for a variety of NLP tasks Applications include Machine translation and caption generation Math in Simple RNNs Outline How RNNs propagate information (Through time!) How RNNs make predictions A Vanilla RNN ... A Vanilla RNN Summary Hidden states propagate information through time Basic recurrent units have two inputs at each time: , Cost Function for RNNs Cross Entropy Loss K - classes or possibilities Looking at a single example Either 0 or 1 Cross Entropy Loss ... Average with respect to time Summary For RNNs the loss function is just an average through time! Implementation Note Outline scan() function in tensorﬂow Computation of forward propagation using abstractions tf.scan() function ... cur_value = initializer ys = [] y, cur_value = fn(x, cur_value) ys.append(y) return ys, cur_value for x in elems: def scan(fn, elems, initializer=None, ...): Frameworks like Tensorﬂow need this type of abstraction Parallel computations and GPU usage Summary Frameworks require abstractions tf.scan() mimics RNNs Gated Recurrent Units Outline Gated recurrent unit (GRU) structure Comparison between GRUs and vanilla RNNs Gated Recurrent Units “Ants are really interesting. __________ are everywhere.” They Plural Relevance and update gates to remember important prior information Gated Recurrent Unit 1 - tanh Gates to keep/update relevant information in the hidden state Hidden state candidate Vanilla RNN vs GRUs tanh 1 - Summary GRUs “decide” how to update the hidden state GRUs help preserve important information Deep and Bi-directional RNNs Outline How bidirectional RNNs propagate information Forward propagation in deep RNNs Bi-directional RNNs I was trying really hard to get a hold of ___________. Louise, ﬁnally answered when I was about to give up... her him them Bi-directional RNNs ..... Information ﬂows from the past and from the future independently Deep RNNs ........... Intermediate layers and activations 2. Pass the activations to the next layer Get hidden states for current layer Summary In bidirectional RNNs, the outputs take information from the past and the future Deep RNNs have more than one layer, which helps in complex tasks"
  },
  {
    "index": 10,
    "title": "3.2 LSTMs and Named Entity Recognition",
    "content": " RNNs and Vanishing Gradients Outline Backprop through time RNNs and vanishing/exploding gradients Solutions RNNs: Advantages Captures dependencies within a short range Takes up less RAM than other n-gram models RNNs: Disadvantages Struggles to capture long term dependencies Prone to vanishing or exploding gradients RNN Basic Structure She She kicked the ball right into the _______________ kicked the ball right into the Learnable parameters goal Backpropagation through time Same at every step Gradient is proportional to a sum of partial derivative products Backpropagation through time Length of the product proportional to how far k is from t Contribution of hidden state k Contribution of hidden state t-10 Backpropagation through time Length of the product proportional to how far k is from t Contribution of hidden state k Partial derivatives <1 Partial derivatives >1 Contribution goes to 0 Contribution goes to inﬁnity Vanishing Gradient Exploding Gradient Solving for vanishing or exploding gradients Identity RNN with ReLU activation -1 0 Gradient clipping 32 25 Skip connections Introduction to LSTMs Outline Meet the Long short-term memory unit! LSTM architecture Applications LSTMs: a memorable solution Learns when to remember and when to forget Basic anatomy: A cell state A hidden state Multiple gates Gates allow gradients to avoid vanishing and exploding LSTMs: Based on previous understanding Starting point with some irrelevant information Discard anything irrelevant Add important new information Produce output Gates Cell and Hidden States Gates in LSTM tanh tanh Cell state Hidden state Input Output 1. Forget Gate: information that is no longer important 2. Input Gate: information to be stored 3. Output Gate: information to use at current step LSTM Unit Applications of LSTMs Next-character prediction Chatbots Music composition Image captioning Speech recognition Summary LSTMs offer a solution to vanishing gradients Typical LSTMs have a cell and three gates: Forget gate Input gate Output gate LSTM Architecture Gates in LSTM tanh tanh Cell state Hidden state Input Output 1. Forget Gate: information that is no longer important 2. Input Gate: information to be stored 3. Output Gate: information to use at current step LSTM Unit Sigmoid output between 0 and 1 0-Closed 1-Open Candidate Cell State tanh tanh Cell state Output tanh Candidate cell state Information from the previous hidden state and current input Tanh Shrinks argument to be between -1 and 1 Other activations could be used Hidden state Input New Cell State tanh Hidden state Input Output New Cell state Add information from the candidate cell state using the forget and input gates tanh Cell state New Hidden State tanh Cell state Hidden state Input Output New Hidden State Select information from the new cell state using the output gate tanh The Tanh activation could be omitted Summary LSTMs use a series of gates to decide which information to keep: Forget gate decides what to keep Input gate decides what to add Output gate decides what the next hidden state will be Introduction to Named Entity Recognition What is Named Entity Recognition? Locates and extracts predeﬁned entities from text Places, organizations, names, time and dates Types of Entities Thailand: Geographical Google: Organization Indian: Geopolitical More Types of Entities Egyptian statue: Artifact December: Time Indicator Barack Obama: Person Example of a labeled sentence Sharon ﬂew to Miami last Friday. O O B-geo O B-tim B-per Applications of NER systems Search engine efﬁciency Recommendation engines Customer service Automatic trading Training NERs: Data Processing Outline Convert words and entity classes into arrays Token padding Create a data generator Processing data for NERs Assign each class a number B-per Assign each word a number Sharon ﬂew to Miami last Friday. [ 4282, 853, 187, 5388, 2894, 7 ] O O B-geo O B-tim Token padding For LSTMs, all sequences need to be the same size. Set sequence length to a certain number Use the <PAD> token to ﬁll empty spaces Training the NER Create a tensor for each input and its corresponding number Put them in a batch 64, 128, 256, 512 ... Feed it into an LSTM unit Run the output through a dense layer Predict using a log softmax over K classes Training the NER tanh tanh Inputs Dense LogSoftmax Layers in TensorFlow model = tf.keras.Sequential([ tf.keras.layers.Embedding(), tf.keras.layers.LSTM(), tf.keras.layers.Dense(), Summary Convert words and entities into same-length numerical arrays Train in batches for faster processing Run the output through a ﬁnal layer and activation Computing Accuracy Evaluating the model Pass test set through the model Get arg max across the prediction array Mask padded tokens Compare outputs against test labels Evaluating the model in Python def masked_accuracy(y_true, y_pred): mask = ... y_pred_class = tf.math.argmax(y_pred, axis=-1) matches_true_pred = tf.equal(y_true, y_pred_class) matches_true_pred *= mask masked_acc = tf.reduce_sum(acc) / tf.reduce_sum(mask) return masked_acc Summary If padding tokens, remember to mask them when computing accuracy Coding assignment!"
  },
  {
    "index": 11,
    "title": "3.3 Siamese Networks",
    "content": " Siamese Networks. Where are you going? Where are you from? Question Duplicates How old are you? What is your age? What do Siamese Networks learn? I am happy because I am learning Classiﬁcation: categorize things Siamese Networks: Identify similarity between things What is your age? How old are you? Difference or Similarity Siamese Networks in NLP Handwritten checks Question duplicates What is your age? How old are you? Queries Architecture Model Architecture Question 1 Question 2 embedding embedding LSTM LSTM. cosine similarity different same Model Architecture Inputs Embedding LSTM Vectors Cosine Similarity Loss Function Loss Function Loss Function How old are you? Anchor Where are you from? What is your age? Positive Negative Loss Function Triplets Triplets How old are you? Anchor Where are you from? What is your age? Positive Negative Triplets Whether or not a question has the same meaning as the anchor How old are you? Triplet Loss Anchor Where are you from? What is your age? Positive Simple loss: With non-linearity With alpha margin Negative Triplet Loss From the neural network Simpliﬁed You can use any similarity function or distance metric Triplet Selection Random Hard Easy to satisfy. Little to learn Harder to train. More to learn duplicate set: A, P non-duplicate set: A, N Triplet A, P, N Computing The Cost I Computing The Cost Prepare the batches as follows: What is your age? How old are you? Can you see me? Are you seeing me? Where are thou? Where are you? When is the game? What time is the game? b = 4 Computing The Cost What is your age? Can you see me? Where are thou? When is the game? 𝒗1= (1, d_model) 𝒗1= (1, d_model) 𝒗1_1 𝒗1_2 𝒗1_3 𝒗1_4 𝒗2= (1, d_model) 𝒗2_1 𝒗2_2 𝒗2_3 𝒗2_4 How old are you? Are you seeing me? Where are you? What time is the game? Batch 1 Batch 2 Computing The Cost 0.9 -0.8 0.3 -0.5 -0.8 0.5 0.1 -0.2 0.3 0.1 0.7 -0.8 -0.5 -0.2 -0.8 1.0 Computing The Cost 0.9 -0.8 0.3 -0.5 -0.8 0.5 0.1 -0.2 0.3 0.1 0.7 -0.8 -0.5 -0.2 -0.8 1.0 Computing The Cost 0.9 -0.8 0.3 -0.5 -0.8 0.5 0.1 -0.2 0.3 0.1 0.7 -0.8 -0.5 -0.2 -0.8 1.0 Computing The Cost 0.9 -0.8 0.3 -0.5 -0.8 0.5 0.1 -0.2 0.3 0.1 0.7 -0.8 -0.5 -0.2 -0.8 1.0 Computing The Cost II Computing The Cost What is your age? Can you see me? Where are thou? When is the game? 𝒗2= (1, d_model) How old are you? Are you seeing me? Where are you? What time is the game? 𝒗1= (1, d_model) 𝒗1_1 𝒗1_2 𝒗1_3 𝒗1_4 𝒗2_1 𝒗2_2 𝒗2_3 𝒗2_4 Batch 1 Batch 2 Hard Negative Mining 0.9 -0.8 0.3 -0.5 -0.8 0.5 0.1 -0.2 0.3 0.1 0.7 -0.8 -0.5 -0.2 -0.8 1.0 mean negative: mean of off-diagonal values in each row closest negative: off-diagonal value closest to (but less than) the value on diagonal in each row Hard Negative Mining mean negative: mean of off-diagonal values closest negative: closest off-diagonal value diff Hard Negative Mining One Shot Learning Classiﬁcation vs One Shot Learning Classify as 1 of K classes Classiﬁcation One Shot Learning Measure similarity between 2 classes One Shot Learning No need for retraining ! Learn a similarity score! Training / Testing Dataset Question 1 Question 2 is_duplicate What is your age? How old are you? true Where are you from? Where are you going? false Prepare Batches What is your age? Can you see me? Where are thou? When is the game? How old are you? Are you seeing me? Where are you? What time is the game? Question 1: batch size b Question 2: batch size b q1_a q2_a q1_a q1_b q2_a q2_b 𝒗2= (1, d_model) 𝒗1= (1, d_model) 𝒗1_1 𝒗1_2 𝒗1_3 𝒗1_4 𝒗2_1 𝒗2_2 𝒗2_3 𝒗2_4 Batch 1 Batch 2 Siamese Model Embedding LSTM Vectors Cosine Similarity Create a subnetwork: Testing 1. Convert each input into an array of numbers 2. Feed arrays into your model 3. Compare 𝒗1, 𝒗2 using cosine similarity 4. Test against a threshold 𝜏"
  },
  {
    "index": 12,
    "title": "4.1 Seq2Seq and Attention for Neural Machine Translation",
    "content": " Seq2Seq model for NMT Outline Introduction to Neural Machine Translation Seq2Seq model and its shortcomings Solution for the information bottleneck Neural Machine Translation It's time for tea ��🇸 �� C'est l'heure du thé Seq2Seq model Introduced by Google in 2014 Maps variable-length sequences to ﬁxed-length memory LSTMs and GRUs to avoid vanishing and exploding gradient problems Inputs and outputs can have different lengths Seq2Seq model Encoder tea time It's hidden Decoder for thé l'heure C'est Seq2Seq encoder tea time It's for Embed LSTM Embed LSTM Embed LSTM Embed LSTM Tokenized words Word Embeddings Final Hidden State Initial Hidden State Encodes the overall meaning of the sentence Encoder tea time It's Seq2Seq decoder for C'est <sos> Embed LSTM l'heure C'est Embed LSTM l'heura Embed LSTM thé Embed LSTM The information bottleneck Decoder tea time It's for Fixed Hidden State Size A ﬁxed amount of informations goes from the encoder to the decoder Seq2Seq shortcomings Variable-length sentences + ﬁxed-length memory = As sequence size increases, model performance decreases Use all the encoder hidden states? Encoder tea time It's Decoder for l'heure C'est thé Solution: focus attention in the right place Encoder tea time It's Decoder for C'est attention <sos> The model can focus on speciﬁc hidden states at every step Seq2Seq model with attention Traditional Seq2Seq Models Seq2Seq with Attention Greater BLEU is better Performance Traditional seq2seq models Encoder tea time It's for C'est <sos> Embed LSTM l'heure C'est Embed LSTM l'heura Embed LSTM thé Embed LSTM Encoder <sos> Decoder How to use all the hidden states? tea time It's for C'est Encoder <sos> Decoder How to use all the hidden states? tea time It's for C'est Suboptimal use of information Pointwise addition Context Vector Encoder <sos> How to use all the hidden states? tea time It's for C'est si-1 Decoder Weighted sum Weights depend on the previous hidden state in the decoder The attention layer in more depth si-1 Feedforward Network eij Softmax αij Weights used for the sum of hidden states Learnable parameters Context Vector is an expected value αi1h1 αi2h2 αi3h3 αiMhM + ⋯ + Queries, Keys, Values and Attention Outline Queries, Keys, and Values Alignment Queries, Keys, Values Key Value It's [0.5, 0.2, -1.2, ..., ] time [0.2, -0.7, 0.9, ..., ] for [1.3, 0.3, 0.8, ..., ] tea [-0.4, 0.6, -1.1, ..., ] l'heure Query Queries, Keys, Values Key Value It's [0.5, 0.2, -1.2, ..., ] time [0.2, -0.7, 0.9, ..., ] for [1.3, 0.3, 0.8, ..., ] tea [-0.4, 0.6, -1.1, ..., ] l'heure Query Queries, Keys, Values Key Value It's [0.5, 0.2, -1.2, ..., ] time [0.2, -0.7, 0.9, ..., ] for [1.3, 0.3, 0.8, ..., ] tea [-0.4, 0.6, -1.1, ..., ] l'heure Query Similarity is used in for weighted sum Scaled dot-product attention (Vaswani et al., 2017) Queries Keys Values Scaled dot-product attention (Vaswani et al., 2017) Similarity Between Q and K Scaled dot-product attention (Vaswani et al., 2017) Scale using the root of the key vector size Scaled dot-product attention (Vaswani et al., 2017) Weights for the weighted sum Scaled dot-product attention (Vaswani et al., 2017) Weighted sum of values V Just two matrix multiplications and a Softmax! Alignment Weights c'est l'heure thé it's time for tea Queries Keys Similar words have large weights Flexible attention Works for languages with different grammar structures! Bahdanau et al., Summary Attention is a layer that lets a model focus on what's important Queries, Values, and Keys are used for information retrieval inside the Attention layer Works for languages with very different grammatical structures Setup for machine translation Data in machine translation English French I am hungry! J'ai faim! ..... I watched the soccer game. J'ai regardé le match de football. Attention! (pun intended) Assignment dataset is not as squeaky-clean as this example and contains some Spanish translations. Machine translation setup Use pre-trained vector embeddings Otherwise, initially represent words with a one-hot vectors Keep track of index mappings with word2ind and ind2word dictionaries Add end of sequence tokens: <EOS> Pad the token vectors with zeros Preparing to Translate to English ENGLISH SENTENCE:. Both the ballpoint and the mechanical pencil in the series are equipped with a special mechanism: when the twist mechanism is activated, the lead is pushed forward. TOKENIZED VERSION OF THE ENGLISH SENTENCE:. [ 4546 4 11358 362 8 4 23326 20104 1745 8210 9641 5 6 4 3103 31 2767 30 13 914 4797 64 196 4 22474 5 4797 16 24864 86 2 4 1060 16 6413 1138 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] Padding <EOS> English to French FRENCH TRANSLATION:. Le stylo à bille et le porte-mine de la série sont équipés d'un mécanisme spécial: lorsque le mécanisme de torsion est activé, le plomb est poussé vers l'avant. TOKENIZED VERSION OF THE FRENCH TRANSLATION:. [ 7 29587 9 18240 8 7 420 5 3440 2 6 156 39 7941 14 19 5548 2648 562 7 5548 2 23194 18 20114 1 7 5695 18 8865 149 12 137 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] <EOS> Padding Teacher Forcing Outline Training for NMT Teacher forcing Encoder tea time It's C'est Traditional seq2seq models Decoder <sos> l'heure Decoder C'est l'heure Decoder thé Decoder for Outputs Inputs Training seq2seq models Encoder time It's C'est Decoder <sos> l'heura Decoder Decoder thé Decoder for Targets Output C'est chat C'est chat Yes! No... Even worse lol, no Errors from early steps propagate Fluffy duveteux tea Teacher Forcing Encoder tea time It's C'est Decoder <sos> l'heura Decoder Decoder café Decoder for Targets Output C'est duveteux C'est l'heura Yes! No... Yes! No, but not bad Correct sequence of words as input (shifted right) Improves training performance NMT Model with Attention Outline How everything ﬁts together NMT model in detail NMT Model Encoder <sos> tea time It's for C'est si-1 Decoder Attention Mechanism The decoder has to pass the hidden state to the Attention Mechanism Difficult to implement, so a pre-attention decoder is introduced. NMT Model Encoder Pre Attention Decoder Target Sequence Input Sequence Hidden states used as Queries Hidden states used as Keys and Values Attention Mechanism Context Vectors Decoder Prediction Sequence Teacher Forcing Decoder Encoder NMT Model Attention Create copies Input tokens Target tokens Pre-attention Decoder Target ShiftRight LSTM LSTM. Embedding Embedding Prepare for Attention Mask Input Dense LogSoftmax LSTM Discard “Mask” Target Context Vectors Mask Log Probabilities Input Target Context Vectors BLEU Score BLEU Score BiLingual Evaluation Understudy Compares candidate translations to reference (human) translations The closer to 1, the better BLEU Score How many words from the candidate appear in the reference translations? Candidate Reference 1 Younes said hungry Reference 2 said hungry BLEU Score Candidate Reference 1 Younes said hungry Reference 2 said hungry Count: A model that always outputs common words will do great! BLEU Score (Modiﬁed) Candidate Reference 1 Younes said hungry Reference 2 said hungry Count: Better than the previous implementation version! BLEU score is great, but... Consider the following: BLEU doesn't consider semantic meaning BLEU doesn't consider sentence structure: “Ate I was hungry because!” ROUGE-N. Score ROUGE. Recall-Oriented Understudy for Gisting Evaluation Compares candidates with reference (human) translations Multiple versions for this metric ROUGE-N. How many words from the reference appear in the candidate translations? Candidate Reference 1 Younes said hungry Reference 2 said hungry ROUGE-N. Candidate Reference 1 Younes said hungry Reference 2 said hungry Count 1: Count 2: ROUGE-N, BLEU. and F1 score Candidate Reference 1 Younes said hungry Reference 2 said hungry Sampling and Decoding Outline Random sampling Temperature in sampling Greedy decoding Seq2Seq model Words Decoder <sos> Dense Softmax 0.02 0.04 0.1 0.005 0.08 ..... P(wi) Probability distribution over words in target language Greedy decoding Selects the most probable word at each step But the best word at each step may not be the best for longer sequences... Can be ﬁne for shorter sequences, but limited by inability to look further down the sequence J'ai faim. I am __________. hungry I am, am, am, am... Random sampling Often a little too random for accurate translation! Solution: Assign more weight to more probable words, and less weight to less probable words. 0.05 0.3 0.15 0.25 0.25 full hungry the Temperature Can control for more or less randomness in predictions Lower temperature setting : More conﬁdent, conservative network Higher temperature setting : More excited, random network This word is very likely correct. Yawn. Omg, but what if it's this super random word? Beam Search Beam search decoding Most probable translation is not the one with the most probable word at each step Calculate probability of multiple possible sequences Beam search Solution Beam search decoding Beam width B determines number of sequences you keep Probability of multiple possible sequences at each step Until all B most probable sequences end with <EOS> Beam search with B=1 is greedy decoding. Beam search example <sos> B = 2 0.5 0.4 hungry 0.1 <eos> 0.0 P(w1 | “<sos>”) Beam search example <sos> B = 2 0.5 0.4 hungry 0.1 <eos> 0.0 0.1 0.5 hungry 0.3 <eos> 0.1 0.7 0.05 hungry 0.2 <eos> 0.05 I I 0.05 I am 0.25 I hungry 0.15 I <eos> 0.05 am I 0.28 am am 0.02 am hungry 0.08 am <eos> 0.02 P(w1 | “<sos>”) P(w2 | “I”) P(w2 | “am”) P(w2 | “I”)P(“I”) P(w2 | “am”)P(“am”) Beam search example <sos> B = 2 0.5 0.4 hungry 0.1 <eos> 0.0 0.1 0.5 hungry 0.3 <eos> 0.1 0.7 0.05 hungry 0.2 <eos> 0.05 I I 0.05 I am 0.25 I hungry 0.15 I <eos> 0.05 am I 0.28 am am 0.02 am hungry 0.08 am <eos> 0.02 P(w2 | “I”)P(“I”) P(w2 | “I”) 0.1 0.05 hungry 0.6 <eos> 0.3 0.15 0.15 hungry 0.5 <eos> 0.2 P(w1 | “<sos>”) P(w2 | “am”) P(w2 | “am”)P(“am”) P(w3 | “I am”) P(w3 | “Am I”) Beam search decoding <sos> P(w1 | “<sos>”) Decoder <sos> Decoder <sos> Decoder P(w2 | “I”) Decoder P(w2 | “am”) Decoder Select B most probable words B model runs Problems with beam search Penalizes long sequences, so you should normalize by the sentence length Computationally expensive and consumes a lot of memory Minimum Bayes Risk Minimum Bayes Risk (MBR) Generate several candidate translations Assign a similarity to every pair using a similarity score (such as ROUGE!). Select the sample with the highest average similarity Minimum Bayes Risk (MBR) Find the candidate translation that maximizes ROUGE. score between pair of candidates Compare with every other candidate Repeat for every candidate Select the candidate with the highest average Example: MBR Sampling Compute average ROUGE. Summary Compare several candidate translations Choose candidate with highest average similarity Better performance than random sampling and greedy decoding Title Casing in 44- 52 pt. Lato Font [Note: do not include specialization name, course #, week #, etc.] Subtitle 30-38 pt. Lato [Note: the idea with variable title font size is just to ﬁll the space to the degree possible, k thi t i Title: 28 pt Lato @ (x=0.3, y=0.1) in “format options” Use “Lato” font for text in all slides Use “normal” as your default Option to use “light” or “bold” as needed for contrast Prefer bigger fonts and fewer words on slides whenever possible Use font sizes >= 14 pt (keep in mind for images / screenshots with text, ﬁgures, etc., make sure axis labels, captions and other text is at least this size. ) Capitalize ﬁrst word only and proper nouns in titles Note red line down here 👇, subtitles appear below this line so keep all content above it. Use these colors for highlighting / shading behind text Light blue 2 9fc5e8 Light orange 2 f9cb9c Light green 2 b6d7a8 Light gray 2 efefef Light magenta 2 d5a6bd These are standard colors in google slides. If you create content elsewhere, copy the hexadecimal (e.g., paste the value cfe2f3 into powerpoint custom colors to get light blue 2.) Use Light gray 2 as the background for code blocks Colors for shapes / adding a border around text Dark blue 1 3d85c6 Dark orange 1 e69138 Dark green 1 6aa84f Dark Gray 1 b7b7b7 Dark magenta 1 a64d79 For arrows and lines, use the same palette as for shapes or use plain black (#000000). Use line weights of 3 - 8px Images It's ok to include images you ﬁnd on the internet, but they must be open source! Look for Creative Commons Share Alike (CC BY SA). licensing or similar (e.g. Wikipedia license) Include a citation unless you explicitly don't need to (e.g., Unsplash or pixabay images) ← free image of a pufﬁn from unsplash.com Icons Feel free to use Noun Project icons, we have a license! (no citation necessary) User: content@ Pwd: d33pl3@rn Noun Project Icon Examples (make them whatever color you like!) Search “neural network” Search “deep learning” Search “monkey” Figures: Make sure all text in ﬁgures is legible Slide with code Paste your code blocks in the best quality possible with a 14-size 'consolas' font. You can check the size and type of your font with the following block. When using Jupyter Notebooks you can print the notebook to PDF and copy-paste any part of the code you want (that way the code highlighting is copied, but you will need to check font size and type). This add-on in G docs, is an alternative to that process. import numpy as np def some_function(a,b): dot_product=np.dot(a,b.T) return dot_product Math: Use a LaTeX editor Online editors exist but often produce low-res images. We suggest the LaTeXiT app for easy copy/paste of equation images. You might need to install LaTeX if you don't already have it. Include the LaTeX code for all equations in instructor notes (as below) Put variables in italics (default or \\mathit{}) Put words, partial words and “log”, “sin”, “cos”, etc. in non-italics with \\mathrm{} Quizzes Does the video lead directly to a coding exercise? If so think about adding code examples. If a video does not lead directly to a coding exercise, think about how you might incorporate a quiz question. Quizzes can test for retention, transfer or be a prompt to apply some intuition Retention: “identify the 3 major challenges you'll face when working with medical datasets” (after these have just been presented) Transfer: “you just solved problem X, now apply the same methodology to previously unseen problem Y” Intuition: “how might you approach dealing with class imbalance in your dataset?” (before weighting etc. is introduced) Objective: Derive Bayes' rule from the equations given on the last slide. Question: From the equations presented below, express the probability of a tweet being positive given that it contains the word happy in terms of the probability of a tweet containing the word happy given that it is positive Type: Multiple Choice, single answer Options and solution: That's right. You just derived Bayes' rule. Check the ratio in this equation. The equation should not include any intersection probabilities The equation should not include any intersection probabilities Quizzes: example Scripting Write your script in a doc in “seen” and “heard” 2-column table format with links to slides in “seen” and words in “heard” (example script) Indicate animation clicks with “>>” in the script, do not add any extra blank lines (except to offset the “>>”) as shown in the example above. Scripting: Words to Avoid Avoid “We”, “Us”, “Our” in favor of “I”, “My”, “You”, “Your” Examples: “With your dataset splits ready, weyou can now proceed with setting up your model to consume them. Now that weyou have a model, let's it's time to evaluate it using your test set. Avoid “Learn”, “Know”, “Understand” in favor of what learners will actually do Examples: In this course, you will learn aboutbuild convolutional neural network image classiﬁcation models and understand how they are used them to make diagnoses of lung disorders. Now that you know howhave built convolutional neural networks are used to make medical diagnoses, and understand how to usehave created a treatment effect predictor, you will learn aboutapply natural language processing techniques to extract information from radiology reports Scripting Write for the script to be read aloud No parenthetical statements (how to read a parenthesis?) No shorthand, for example: “You're comparing apples/oranges” → “you're comparing apples and oranges” “AKA” → “which is also known as...” Write math as you want it spoken (open to discretion): “Take log(x+1)” → “Take the log of x plus 1” Avoid cultural references, e.g., “Great job you're almost there, it's fourth and goal!” Avoid cross-referencing content “In the next/last course/week/video/lesson...” → “As you've seen before” or “this topic is important in the context of [thing that came before]” Avoid saying “in this video/lecture” as it's redundant, just start the material."
  },
  {
    "index": 13,
    "title": "4.2 The Transformer Model",
    "content": " Transformers vs RNNs Outline Issues with RNNs Comparison with Transformers Neural Machine Translation How are you Comment allez- vous No parallel computing! Seq2Seq Architectures Vanishing gradient Loss of information ........... T sequential steps RNNs vs Transformer: Encoder-Decoder Encoder <sos> tea time It's for C'est si-1 Decoder Attention Mechanism LSTMs Transformers don't use RNNs, such as LSTMs or GRUs Transformers Overview The Transformer Model. https://arxiv.org/abs/1706.03762 Scaled Dot-Product Attention (Vaswani et al., 2017) Queries Keys Values Multi-Head Attention Scaled dot-product attention multiple times in parallel Linear transformations of the input queries, keys and values The Encoder Every item in the input attends to every other item in the sequence Self-Attention Provides contextual representation of each item in the input sequence The Decoder Every position attends to previous positions Masked Self-Attention Encoder-Decoder Attention Every position from the decoder attents to the outputs from the encoder RNNs vs Transformer: Positional Encoding EMBEDDINGS INPUT. suis content 0.84 0.52 0.0001 0.91 -0.42 0.0002 POSITIONAL ENCODING. The Transformer Encoder Decoder Easy to parallelize! Summary In RNNs parallel computing is difﬁcult to implement For long sequences in RNNs there is loss of information Transformers help with all of the above In RNNs there is the problem of vanishing gradient Transformer Applications Outline Transformers applications in NLP Some Transformers Introduction to T5 Transformer NLP applications Text summarization Auto-Complete Named entity recognition (NER) Translation Chat-bots Question answering (Q&A) Other NLP tasks Sentiment Analysis Market Intelligence Text Classiﬁcation Character Recognition Spell Checking State of the Art Transformers GPT-2: Generative Pre-training for Transformer Radford, A., et al. (2018) Open AI BERT:. Bidirectional Encoder Representations from Transformers Devlin, J., et al. (2018) Google AI Language T5: Text-to-text transfer transformer Colin, R., et al. (2019) Google T5: Text-To-Text Transfer Transformer Translate English into French: “I am happy” “Je suis content” Cola sentence: “He bought fruits and.” Unacceptable Cola sentence: “He bought fruits and vegetables.” Acceptable Classiﬁcation Translation Question: Which volcano in Tanzania is the highest mountain in Africa? Answer: Mount Kilimanjaro Q&A *Cola stands for “Corpus of Linguistic Acceptability” T5: Text-To-Text Transfer Transformer Summarization Summarize: “State authorities dispatched emergency crews Tuesday to survey the damage after an onslaught of severe weather in mississippi...” “Six people hospitalized after a storm in Attala county” Stsb sentence1: “Cats and dogs are mammals.” Sentence2: “There are four known forces in nature - gravity, electromagnetic, weak and strong.” Stsb sentence1: “Cats and dogs are mammals.” Sentence2:“Cats, dogs, and cows are domesticated.” 0.0 2.6 Regression T5: Demo Summary Some transformers include GPT, BERT. and T5 T5 is a powerful multi-task transformer Transformers are suitable for a wide range of NLP applications Scaled Dot-Product Attention Outline ●Revisit scaled dot product attention ●Mathematics behind Attention Scaled dot-product attention Weighted sum of values V Just two matrix multiplications and a Softmax! (Vaswani et al., 2017) Queries Keys Values Weights add up to 1 Improves performance Queries, Keys and Values Stack Same number of rows Size of the embedding Je suis heureux Embedding suis heureux I am happy Stack Embedding happy Stack Generally the same Attention Math Weight assigned to the third key for the second query Context vectors for each query Number of queries Size of the value vector Summary Scaled Dot-product Attention is essential for Transformer The input to Attention are queries, keys, and values GPUs and TPUs Masked Self-Attention Outline ●Overview of masked Self-Attention ●Ways of Attention Encoder-Decoder Attention Queries from one sentence, keys and values from another c'est l'heure thé it's time for tea Weight matrix Self-Attention Queries, keys and values come from the same sentence it's time for tea it's time for tea Meaning of each word within the sentence Weight matrix Masked Self-Attention Queries, keys and values come from the same sentence. Queries don't attend to future positions. it's time for tea it's time for tea Weight matrix Minus inﬁnity Masked self-attention math Weights assigned to future positions are equal to 0 Summary There are three main ways of Attention: Encoder/Decoder, self-attention and masked self-attention. In self-attention, queries and keys come from the same sentence In masked self-attention queries cannot attend to the future Multi-head Attention Outline ●Intuition Multi-Head Attention ●Math of Multi-Head Attention time tea it's Multi-Head Attention - Overview c'est l'heure thé Queries Keys for Original Embeddings time tea it's Values for time tea it's c'est l'heure thé for time tea it's for Head 2 time tea it's c'est l'heure thé for time tea it's for Head 1 Linear Linear Linear Concatenation Linear Scaled Dot-Product Attention heads Multi-Head Attention - Overview Queries Keys Values heads Learnable parameters Multi-Head Attention Concat Context vectors for each query Usual choice of dimensions : Embedding size Attention Attention Head 1 Head 2 Summary Multi-Headed models attend to information from different representations Parallel computations Similar computational cost to single-head attention Transformer decoder Outline ●Overview of Transformer decoder ●Implementation (decoder and feed-forward block) SoftMax Linear Add & Norm Feed Forward Multi-Head Attention Add & Norm Output Probabilities Input Embedding Inputs Transformer decoder Overview Positional Encoding input: sentence or paragraph we predict the next word multi-head attention looks at previous words feed-forward layer with ReLU that's where most parameters are! residual connection with layer normalization repeat N times dense layer and softmax for output sentence gets embedded, add positional encoding (vectors representing ) SoftMax Feed Forward Multi-Head Attention Input Embedding Add & Norm SoftMax Linear Add & Norm Feed Forward Multi-Head Attention Add & Norm Output Probabilities Input Embedding Inputs Transformer decoder Positional Encoding SoftMax Input Embedding Explanation Input Embedding happy Positional Encoding Decoder Block <start> Linear The Transformer decoder Positional input embedding Output Vector Multi-Head Attention Feed Forward Feed Forward Feed Forward Add & Norm Add & Norm SoftMax Linear Add & Norm Feed Forward Multi-Head Attention Add & Norm Output Probabilities Input Embedding Inputs Positional Encoding Feed Forward Multi-Head Attention Add & Norm Decoder Block LayerNorm ( + ) Positional Encoding SoftMax Linear Add & Norm Feed Forward Multi-Head Attention Add & Norm Output Probabilities Input Embedding Inputs The Transformer decoder Feed Forward Self Attention Feed Forward (ReLu) Feed Forward (ReLu) Feed forward layer Positional Encoding SoftMax Linear Add & Norm Feed Forward Multi-Head Attention Add & Norm Output Probabilities Input Embedding Inputs The Transformer decoder Feed Forward Self Attention Feed Forward (ReLu) Feed Forward (ReLu) Feed forward layer Summary It also includes a module to calculate the cross-entropy loss Decoder and feed-forward blocks are the core of this model code Transformer decoder mainly consists of three layers Transformer summarizer Outline ●Overview of Transformer summarizer ●Technical details for data processing ●Inference with a Language Model Transformer for summarization Input Output: Summary Positional Encoding SoftMax Linear Add & Norm Feed Forward Multi-Head Attention Add & Norm Output Probabilities Input Embedding Inputs Technical details for data processing ARTICLE TEXT <EOS> SUMMARY <EOS>. < pad> ... Model Input: Tokenized version: [2,3,5,2,1,3,4,7,8,2,5,1,2,3,6,2,1,0,0] Loss weights: 0s until the ﬁrst <EOS> and then 1 on the start of the summary. Positional Encoding SoftMax Linear Add & Norm Feed Forward Multi-Head Attention Add & Norm Output Probabilities Input Embedding Inputs Cost function Cross entropy loss : over summary : bach elements Positional Encoding SoftMax Linear Add & Norm Feed Forward Multi-Head Attention Add & Norm Output Probabilities Input Embedding Inputs Inference with a Language Model Model input: [Article] <EOS> [Summary] <EOS> Provide: [Article] <EOS> Generate summary word-by-word until the ﬁnal <EOS> Pick the next word by random sampling each time you get a different summary! Inference: Summary For summarization, a weighted loss function is optimized Transformer Decoder summarizes predicting the next word using The transformer uses tokenized versions of the input"
  },
  {
    "index": 14,
    "title": "4.3 Transfer Learning and Fine-Tuning",
    "content": " Week 3 Overview Question Answering Week 3 Transfer learning BERT Question Answering Model Context-based Closed book Model Not just the model Model Training Data Model Training Data Transfer Learning! Classical training Training Inference Course Review Model Course Review Model Transfer learning Pre-training Movie Reviews Model Training on “Downstream” Task Course Review Model Inference Course Review Model Transfer Learning: Different Tasks Watching the movie is like ... Model Pre-Training Sentiment Classiﬁcation Training Downstream task: Question Answering When is Pi Day? Model March 14! Inference When's my birthday? Model Umm... BERT: Bi-directional Context Uni-directional Bi-directional Learning from is like watching the sunset with my best friend! context Learning from is like watching the sunset with my best friend! context context T5: Single task vs. Multi task Model 2 Model 1 Model Studying with was ... Studying with was ... T5: more data, better performance Colossal Clean Crawled Corpus ~800 GB English wikipedia ~13 GB Transfer Learning in NLP Desirable Goals Reduce training time Small datasets Improve predictions Transfer Learning! Transfer learning options Transfer Feature- based Fine- tuning Pre-train data Model Labeled Unlabeled Pre-training task Language modeling Masked words Next sentence Train data Model prediction General purpose learning I am “Happy” CBOW Transfer because I am learning Word Embeddings Translation “Features” input Feature-based vs. Fine-Tuning Pre-Train Train a new model Model features prediction Transfer Model Model prediction Pre-Train Model prediction Fine-tune same model on Downstream task prediction Fine-tune: adding a layer Transfer Pre-Training ... Movies Course reviews Data Model Model Data and performance Data Pre-train data Labeled text data Unlabeled text data Labeled vs Unlabeled Data Pre-train data Transfer learning with unlabeled data Pre-Training Model No labels ! Which tasks work with unlabeled data? Pre-train data Model What day is Pi day? March 14 Downstream task Labeled data Self-supervised task Unlabeled data Inputs (features) Create targets (Labels) Pre-training task Self-supervised tasks Input Target Unlabeled Data Learning from is like watching the sunset with my best _______ Learning from is like watching the sunset with my best friend. friend Model prediction Loss Update Pre-training task Language modeling Fine-tune a model for each downstream task Pre Training Model Training on Downstream task Translation Summarization Q & A Model Model Model Summary Transfer Feature- based Fine- tuning Pre-train data Model Labeled Unlabeled Pre-training task Language modeling Masked words Next sentence Train data Model prediction ELMo, GPT, BERT,. T5 Outline CBOW ELMo GPT BERT. Context ... right ..... they were on the right ..... they were on the right side of the street Continuous Bag of Words ... they were on the right side of the street “right” Fully-connected (Feed Forward) neural network “on” “the” “side” “of” Fixed window Fixed window Fixed window Fixed window Need more context? ... they were on the right side of the street... they were on the right side of history. Use all context words The legislators believed that they were on the right side of history, so they changed the law. ELMo: Full context using RNN The legislators believed that they were on the _____ side of history so they changed the law. Bi-directional LSTM LSTM LSTM. “ right” Word embedding for “right” Open AI GPT RNN. Transformer Decoder Encoder GPT Decoder The legislators believed that they were on the _____ Uni-directional ELMo Why not bi-directional? Transformer ... on the right side... Attention Each word can peek at itself! Transformer GPT: Uni-directional ... on the right Attention No peeking! Transformer ... on the right side... Attention Each word can peek at itself! BERT Transformer Decoder Encoder GPT Decoder Encoder BERT The legislators believed that they were on the _____ side of history, so they changed the law. Bi-directional Transformer + Bi-directional Context ... on the ___ side ___ history ... Model “right” “of” Multi-Mask Language Modeling BERT: Words to Sentences The legislators believed that they were on the right side of history. So they changed the law. Then the bunny ate the carrot. Sentence “A” Sentence “B” Next Sentence Prediction BERT Pre-training Tasks ... on the ___ side ___ history ... Model “right” “of” Multi-Mask Language Modeling Sentence “A” Sentence “B” Next Sentence Prediction T5: Encoder vs. Encoder-Decoder Transformer Decoder Encoder GPT Decoder Encoder BERT Decoder Encoder T5: Multi-task Model Studying with was ... How? T5: Text-to-Text “Summarize: It was the best of times...” “Question: “When is Pi day?” “Classify: Learning from is like...” Task type Classify Summarize Question “March 14” “It was alright” “5 stars” Summary CBOW ELMo GPT BERT. Context window FFNN Full sentence Bi-directional Context RNN Transformer: Decoder Uni-directional Context Transformer: Encoder Bi-directional Context Multi-Mask Next Sentence Prediction Transformer: Encoder - Decoder Bi-directional Context Multi-Task More details next! Bidirectional Encoder Representations from Transformers (BERT) Outline Learn about the BERT architecture Understand how BERT pre-training works BERT Makes use of transfer learning/pre-training: ......... A multi layer bidirectional transformer Positional embeddings BERT_base: 12 layers (12 transformer blocks) 12 attentions heads 110 million parameters BERT BERT. pre-training After school Lukasz does his in the library. Masked language modeling (MLM) BERT. pre-training After school Lukasz does his homework in the library. After school his homework in the . Summary Choose 15% of the tokens at random: mask them 80% of the time, replace them with a random token 10% of the time, or keep as is 10% of the time. There could be multiple masked spans in a sentence Next sentence prediction is also used when pre-training. BERT Objective Outline Learn about the BERT objective Understand how BERT inputs are fed into the model Visualize the output Input [CLS] dog cute [SEP] likes play ##ing [SEP] Token Embeddings [CLS] dog cute [SEP] likes play ##ing [SEP] Segment Embeddings Position Embeddings Formalizing the input NSP Mask ML Mask ML Masked sentence A Masked sentence B ... [CLS] Tok 1 Tok 1 Tok M Tok N [SEP] ... Unlabeled Sentence A and B Pair E ' E 1 T ' E ' E N ......... [CLS] T [SEP] T ' E [SEP] BERT. Visualizing the output [CLS]: special classiﬁcation symbol added in front of every input [CLS] [SEP]:. special separator token [SEP] Objective 1: Multi-Mask LM BERT. Objective Loss: Cross Entropy Loss Objective 2: Next Sentence Prediction Loss: Binary Loss Summary BERT objective Model inputs/outputs Fine-tuning BERT Fine-tuning BERT: Outline BERT Sentence A Sentence B Pre-train BERT Hypothesis Premise MNLI BERT. Sentence A Tags NER BERT. Question Answer SQuAD Inputs Summary Sentence A Sentence B Text Question Passage Hypothesis Premise Sentence Entities Sentence Paraphrase Article Summary Transformer Outline Understand how T5 works Recognize the different types of attention used Overview of model architecture Transformer - T5 Model Classiﬁcation Question Answering (Q&A) Machine Translation Sentiment Summarization Text to Text ©Exploring the Limits of Transfer learning with a unified text to Text Transformer. Raffel et. al. 2020 Transformer - T5 Model Thank you for inviting me to your party last week. Original text Thank you <X> me to your party <Y> week. Inputs <X> for inviting <Y> last <Z> Targets ©Exploring the Limits of Transfer learning with a unified text to Text Transformer. Raffel et. al. 2020 Model Architecture Encoder Decoder y 1 y 2 X 3 y 2 Language model y 1 X 3 y 1 y 2 Preﬁx LM ©Exploring the Limits of Transfer learning with a unified text to Text Transformer. Raffel et. al. 2020 X 3 y 2 Encoder/decoder 12 transformer blocks each 220 million parameters Model Architecture Decoder Encoder ©Exploring the Limits of Transfer learning with a unified text to Text Transformer. Raffel et. al. 2020 Summary Preﬁx LM attention Model architecture Pre-training T5 (MLM) Multi-task Training Strategy Multi-task training strategy “Translate English to German: That is good.” “Das ist gut” “cola sentence: The course is jumping well.” “not acceptable” “stsb sentence1: The rhino grazed on the grass. Sentence2: A rhino is grazing in a ﬁeld.” “3.8” “Summarize: state authorities dispatched emergency crews tuesday to survey the damage after an onslaught of severe weather in mississippi...” “six people hospitalized after a storm in attala county” ©Exploring the Limits of Transfer learning with a unified text to Text Transformer. Raffel et. al. 2020 Input and Output Format translate English to German: That is good. Predict entailment, contradiction , or neutral Machine translation: mnli premise: I hate pigeons hypothesis: My feelings towards pigeons are ﬁlled with animosity. target: entailment Winograd schema The city councilmen refused the demonstrators a permit because *they* feared violence ©Exploring the Limits of Transfer learning with a unified text to Text Transformer. Raffel et. al. 2020 Multi-task Training Strategy How much data from each task to train on? Fine-tuning method GLUE CNNDM. SQuAD SGLUE. EnDe EnFr EnRo * All parameters 83.28 19.24 80.88 71.36 26.98 39.82 27.65 Adapter layers, 80.52 15.08 79.32 60.40 13.84 17.88 15.54 Adapter layers, 81.51 16.62 79.47 63.03 19.83 27.50 22.63 Adapter layers, 81.54 17.78 79.18 64.30 23.45 33.98 25.81 Adapter layers, 81.51 16.62 79.47 63.03 19.83 27.50 22.63 Gradual unfreezing 82.50 18.95 79.17 70.79 26.71 39.02 26.93 ©Exploring the Limits of Transfer learning with a unified text to Text Transformer. Raffel et. al. 2020 Data Training Strategies Examples-proportional mixing Data 1 Data 2 Sample 2 Sample 1 Equal mixing Data 1 Data 2 Sample 1 Sample 2 Temperature-scaled mixing Gradual unfreezing vs. Adapter layers Gradual unfreezing Adapter layers ©Exploring the Limits of Transfer learning with a unified text to Text Transformer. Raffel et. al. 2020 Fine-tuning Pre Training Model Fine Tune on Speciﬁc Task Translation Summarization Q & A Model Model Model 218 steps MLM GLUE Benchmark General Language Understanding Evaluation Datasets with different genres, and of different sizes and difﬁculties A collection used to train, evaluate, analyze natural language understanding systems Leaderboard Tasks Evaluated on Sentiment Sentence grammatical or not? Paraphrase Questions duplicates Similarity Answerable Entailment Contradiction Winograd (co-ref) General Language Understanding Evaluation Model agnostic Drive research Makes use of transfer learning Question Answering Transformer encoder Feedforward: Add & Norm Feed Forward Multi-Head Attention Add & Norm Input Embedding Inputs Positional Encoding Feed Forward Multi-Head Attention Input Embedding Add & Norm LayerNorm, dense, activation, dropout_middle, dense, dropout_final Transformer encoder Encoder block: Residual( LayerNorm, attention, dropout_, Residual( feed_forward, Add & Norm Feed Forward Multi-Head Attention Add & Norm Input Embedding Inputs Positional Encoding Feed Forward Multi-Head Attention Input Embedding Add & Norm Transformer encoder Feedforward: Add & Norm Feed Forward Multi-Head Attention Add & Norm Input Embedding Inputs Positional Encoding Feed Forward Multi-Head Attention Input Embedding Add & Norm LayerNorm, dense, activation, dropout_middle, dense, dropout_final Residual( LayerNorm, attention, dropout_, Residual( feed_forward, Encoder block: Context: Since the end of the Second World War , France has become an ethnically diverse country . Today , approximately ﬁve percent of the French population is non - European and non - white . This does not approach the number of non - white citizens in the United States ( roughly 28 - 37 % , depending on how Latinos are classiﬁed ; see Demographics of the United States ) . Nevertheless , it amounts to at least three million people , and has forced the issues of ethnic diversity onto the French policy agenda . France has developed an approach to dealing with ethnic problems that stands in contrast to that of many advanced , industrialized countries . Unlike the United States , Britain , or even the Netherlands , France maintains a \" color - blind \" model of public policy . This means that it targets virtually no policies directly at racial or ethnic groups . Instead , it uses geographic or class criteria to address issues of social inequalities . It has , however , developed an extensive anti - racist policy repertoire since the early 1970s . Until recently , French policies focused primarily on issues of hate speech - going much further than their American counterparts - and relatively less on issues of discrimination in jobs , housing , and in provision of goods and services . Data examples Question: What percentage of the French population today is non - European ? Target: Approximately ﬁve percent Implementing Q&A with T5 Process data to get the required inputs and outputs: \"question: Q context: C\" as input and \"A\" as target Load a pre-trained model Fine tune your model on the new task and input Predict using your own model “Translate English to German: That is good.” “Das ist gut” “cola sentence: The course is jumping well.” “not acceptable” “stsb sentence1: The rhino grazed on the grass. Sentence2: A rhino is grazing in a ﬁeld.” “3.8” “Summarize: state authorities dispatched emergency crews tuesday to survey the damage after an onslaught of severe weather in mississippi...” “six people hospitalized after a storm in attala county” Hugging Face: Introduction Outline What is Hugging Face? How you can use the Hugging Face ecosystem Hugging Face Transformers library Use it with Use it for Applying state of the art transformer models Fine-tuning pretrained transformer models Hugging Face: Using Transformers Pipelines 1. Pre-processing your inputs 2. Running the model 3. Post-processing the outputs Context Questions Answers Hugging Face: Fine-Tuning Transformers Model Checkpoints: More than 14 thousand Datasets: One Thousand Evaluation metrics Tokenizer Trainer Human readable output Tokenizer Checkpoint: Set of learned parameters for a model using a training procedure for some task Hugging Face: Using Transformers Using Transformers Pipelines 1. Pre-processing your inputs 2. Running the model 3. Post-processing the outputs Context Questions Answers Tasks Pipelines Initialization Task Model Checkpoint Use Inputs for the task Sentiment Analysis Sequence Question Answering Context and questions Fill-Mask Sentence and position Checkpoints Huge number of model checkpoints that you can use in your pipelines. But beware, not every checkpoint would be suitable for your task. Model Hub Hub containing models that you can use in your pipelines according to the task you need: https://huggingface.co/models Model Card shows a description of your selected model and useful information such as code snippet examples. Hugging Face: Fine-Tuning Transformers Fine-Tuning Tools Model Checkpoints: More than 14 thousand Datasets: One Thousand Evaluation metrics Tokenizer Trainer Human readable output Tokenizer Model Dataset Name in DistilBERT Stanford Question Answering Dataset (SQuAD) distilbert-base-cased- distilled-squad Model Checkpoints Model Checkpoints: More than 15 thousand (and increasing) Upload the architecture and weights with 1 line of code! BERT Wikipedia and Book Corpus bert-base-cased ....... Datasets Datasets: One Thousand Load them using just one function Optimized to work with massive amounts of data! Tokenizers Tokenizer \"What well-known superheroes were introduced between 1939 and 1941 by Detective Comics?\" [ 101, 1327, 1218, 118, 1227, 18365, 1279, 1127, 2234, 1206, 3061, 1105, 3018, 1118, 9187, 7452, 136, 102] Depending on the use case, you might need to run additional steps. Trainer and Evaluation Metrics Trainer object let's you deﬁne the training procedure Number of epochs Warm-up steps Weight decay ... Train using one line of code! Pre-deﬁned evaluation metrics, like BLEU and ROUGE. "
  }
]
