[
  {
    "index": 0,
    "title": "1 Neural Networks",
    "content": "1 Neural Networks. Introduction to Deep Learning Welcome • AI is the new Electricity • Electricity had once transformed countless industries: transportation, manufacturing, healthcare, communications, and more • AI will now bring about an equally big transformation. What you'll learn Courses in this sequence (Specialization): 1. Neural Networks and Deep Learning 2. Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization 3. Structuring your Machine Learning project 4. Convolutional Neural Networks 5. Natural Language Processing: Building sequence models Introduction to Deep Learning What is a Neural Network? Housing Price Prediction size of house price Housing Price Prediction #bedrooms size Housing Price Prediction zip code !$ wealth !% Introduction to Deep Learning Supervised Learning with NeuralNetworks Supervised Learning Input(x) Output (y) Application Price Click on ad? (0/1) Object (1,...,1000) Text transcript Chinese Position of other cars Real Estate Online Advertising Photo tagging Speech recognition Machine translation Autonomous driving Home features Ad, user info Image Audio English Image, Radar info Neural Network examples Standard NN Convolutional NN Recurrent NN Size #bedrooms ... Price (1000$s) Structured Data Supervised Learning User Age Ad Id ... Click Unstructured Data Image Four scores and seven years ago... Text Audio Introduction to Neural Networks Why is Deep Learning taking off? Scale drives deep learning progress Amount of data Performanc • Data • Computation • Algorithms Idea Experiment Code Scale drives deep learning progress Introduction to Neural Networks About thisCourse Courses in this Specialization 1. Neural Networks and DeepLearning 2. Improving Deep Neural Networks:Hyperparameter tuning, Regularization and Optimization 3. Structuring your Machine Learningproject 4. Convolutional Neural Networks 5. Natural Language Processing: Building sequencemodels Outline of this Course Week 1: Introduction Week 2: Basics of Neural Network programming Week 3: One hidden layer Neural Networks Week 4: Deep Neural Networks Basics of Neural Network Programming Binary Classification 1 (cat) vs 0 (non cat) Red Green Blue Binary Classification Notation Basics of Neural Network Programming Logistic Regression Logistic Regression Basics of Neural Network Programming Logistic Regression cost function Logistic Regression cost function !\" = % &!' + ) , where % * = Given ('(.), !(.)),...,('(1), !(1)) , want !\"(2) ≈ ! 2 . Loss (error) function: Basics of Neural Network Programming Gradient Descent Gradient Descent Recap: !\" = % &'( + * , % + = , 78, !(7) log !\" 7 + (1 - !(7)) log(1 - !\" 7 ) Want to find &, * that minimize 1 &, * & Gradient Descent Basics of Neural Network Programming Derivatives Intuition about derivatives Basics of Neural Network Programming More derivatives examples Intuition about derivatives More derivative examples Basics of Neural Network Programming Computation Graph Computation Graph Basics of Neural Network Programming Derivatives with a Computation Graph Computing derivatives Computing derivatives Basics of Neural Network Programming Logistic Regression Gradient descent ℒ +, ) = -() log(+) + (1 - )) log(1 - +)) Logistic regression recap Logistic regression derivatives Basics of Neural Network Programming Gradient descent on m examples Logistic regression on m examples Logistic regression on m examples Basics of Neural Network Programming Vectorization What is vectorization? Basics of Neural Network Programming More vectorization examples Neural network programming guideline Whenever possible, avoid explicit for-loops. Vectors and matrix valued functions Say you need to apply the exponential operation on every element of a matrix/vector. u[i]=math.exp(v[i]) u = np.zeros((n,1)) for i in range(n): Logistic regression derivatives for i = 1 to n: * += - -(\") log -1 \" + (1 - - \" ) log(1 - -1 \" ) Basics of Neural Network Programming Vectorizing Logistic Regression Vectorizing Logistic Regression Basics of Neural Network Programming Vectorizing Logistic Regression's Gradient Computation Vectorizing Logistic Regression Implementing Logistic Regression for i = 1 to m: * += - -($) log & $ + (1 - - $ ) log(1 - & $ ) Basics of Neural Network Programming Broadcasting in Python Broadcasting example cal = A.sum(axis = 0) percentage = 100*A/(cal.reshape(1,4)) Apples Beef Eggs Potatoes Carb Fat Protein Calories from Carbs, Proteins, Fats in 100g of different foods: Broadcasting example General Principle Basics of Neural Network Programming Explanation of logistic regression cost function (Optional) Logistic regression cost function Logistic regression cost function Cost on m examples One hidden layer Neural Network Neural Networks Overview What is a Neural Network? One hidden layer Neural Network Neural Network Representation Neural Network Representation One hidden layer Neural Network Computing a Neural Network's Output Neural Network Representation Neural Network Representation Neural Network Representation Neural Network Representation learning Given input x: One hidden layer Neural Network Vectorizing across multiple examples Vectorizing across multiple examples Vectorizing across multiple examples for i = 1 to m: One hidden layer Neural Network Explanation for vectorized implementation Justification for vectorized implementation Recap of vectorizing across multiple examples for i = 1 to m One hidden layer Neural Network Activation functions Activation functions Given x: Pros and cons of activation functions sigmoid: ! = One hidden layer Neural Network Why do you need non-linear activation functions? Activation function Given x: One hidden layer Neural Network Derivatives of activation functions Sigmoid activation function !(#) = tanh(#) Tanh activation function ReLU Leaky ReLU ReLU and Leaky ReLU One hidden layer Neural Network Gradient descent for neural networks Gradient descent for neural networks Formulas for computing derivatives One hidden layer Neural Network Backpropagation intuition (Optional) Computing gradients Logistic regression Neural network gradients Summary of gradient descent ;<. >?:(!6 \" , '5A> = 1, BCC<!A:> = DE?C. ) ;<. >?:(!6 $ , '5A> = 1, BCC<!A:> = DE?C). Summary of gradient descent One hidden layer Neural Network Random Initialization What happens if you initialize weights to zero? Random initialization Deep Neural Networks Deep L-layer Neural network Andrew What is a deep neural network? logistic regression 1 hidden layer 2 hidden layers 5 hidden layers Andrew Deep neural network notation Deep Neural Networks Forward Propagation in a Deep Network Andrew Forward propagation in a deep network Getting your matrix dimensions right Deep Neural Networks Parameters ![\"] and \"[\"] Vectorized implementation Why deep representations? Deep Neural Networks Intuition about deep representation Circuit theory and deep learning Informally: There are functions you can compute with a “small” L-layer deep neural network that shallower networks require exponentially more hidden units to compute. Building blocks of deep neural networks Deep Neural Networks Andrew Forward and backward functions Andrew Forward and backward functions Parameters vs Hyperparameters Deep Neural Networks What are hyperparameters? Parameters: ! \" , % \" ,! & , % & ,! ' , % ' ... Applied deep learning is a very empirical process cost ! # of iterations Idea Experiment Code What does this have to do with the brain? Deep Neural Networks Forward and backward propagation"
  },
  {
    "index": 1,
    "title": "2 Improving Deep Neural Networks",
    "content": "2 Improving Deep Neural Networks. Setting up your ML application Train/dev/test sets Applied ML is a highly iterative process Idea Experiment Code # layers # hidden units learning rates activation functions Train/dev/test sets Mismatched train/test distribution Training set: Cat pictures from webpages Dev/test sets: Cat pictures from users using your app Not having a test set might be okay. (Only dev set.) Setting up your ML application Bias/Variance Bias and Variance high bias “just right” high variance Bias and Variance Cat classification Train set error: Dev set error: High bias and high variance Setting up your ML application Basic “recipe” for machine learning Basic recipe for machine learning Regularizing your neural network Regularization Logistic regression min Neural network Regularizing your neural network Why regularization reduces overfitting How does regularization prevent overfitting? high bias “just right” high variance How does regularization prevent overfitting? Regularizing your neural network Dropout regularization Dropout regularization Implementing dropout (“Inverted dropout”) Making predictions at test time Regularizing your neural network Understanding dropout Why does drop-out work? Intuition: Can't rely on any one feature, so have to spread out weights. Regularizing your neural network Other regularization methods Data augmentation Early stopping # iterations Setting up your optimization problem Normalizing inputs Normalizing training sets Why normalize inputs? ! Unnormalized: ! Normalized: Vanishing/exploding gradients Setting up your optimization problem Vanishing/exploding gradients Single neuron example Numerical approximation of gradients Setting up your optimization problem Checking your derivative computation Checking your derivative computation Gradient Checking Setting up your optimization problem Gradient check for a neural network Take ! \" , $[\"],...,! ( , $ ( and reshape into a big vector ). Take +! \" , +$[\"],...,+! ( , +$ ( and reshape into a big vector d). Gradient checking (Grad check) Gradient Checking implementation notes Setting up your optimization problem Gradient checking implementation notes - Don't use in training - only to debug - If algorithm fails grad check, look at components to try to identify bug. - Remember regularization. - Doesn't work with dropout. - Run at random initialization; perhaps again after some training. Optimization Algorithms Mini-batch gradient descent Batch vs. mini-batch gradient descent Vectorization allows you to efficiently compute on m examples. Mini-batch gradient descent Optimization Algorithms Understanding mini-batch gradient descent Training with mini batch gradient descent # iterations cost Batch gradient descent mini batch # (t) cost Mini-batch gradient descent Choosing your mini-batch size Choosing your mini-batch size Optimization Algorithms Understanding exponentially weighted averages Exponentially weighted averages days temperature Exponentially weighted averages Implementing exponentially weighted averages Optimization Algorithms Bias correction in exponentially weighted average Bias correction days temperature Optimization Algorithms Gradient descent with momentum Gradient descent example Implementation details Hyperparameters: -, % On iteration 8: Compute )*, ), on the current mini-batch Optimization Algorithms RMSprop RMSprop Optimization Algorithms Adam optimization algorithm Adam optimization algorithm yhat = np.array([.9, 0.2, 0.1, .4, .9]) Hyperparameters choice: Adam Coates Optimization Algorithms Learning rate decay Learning rate decay Learning rate decay Other learning rate decay methods Optimization Algorithms The problem of local optima Local optima in neural networks Problem of plateaus • Unlikely to get stuck in a bad local optima • Plateaus can make learning slow Hyperparameter tuning Tuning process Hyperparameters Try random values: Don't use a grid Hyperparameter 1 Hyperparameter 2 Hyperparameter 1 Hyperparameter 2 Coarse to fine Hyperparameter 1 Hyperparameter 2 Hyperparameter tuning Using an appropriate scale to pick hyperparameters Picking hyperparameters at random Appropriate scale for hyperparameters Hyperparameters for exponentially weighted averages Hyperparameters tuning Hyperparameters tuning in practice: Pandas vs. Caviar Re-test hyperparameters occasionally Idea Experiment Code - NLP, Vision, Speech, Ads, logistics, .... - Intuitions do get stale. Re-evaluate occasionally. Panda Babysitting one model Training many models in parallel Caviar Batch Normalization Normalizing activations in a network Normalizing inputs to speed up learning Implementing Batch Norm Batch Normalization Fitting Batch Norm into a neural network Adding Batch Norm to a network Working with mini-batches Implementing gradient descent Batch Normalization Why does Batch Norm work? Learning on shifting input distribution Cat Non-Cat Why this is a problem with neural networks? • Each mini-batch is scaled by the mean/variance computed on just that mini-batch. • This adds some noise to the values +[-] within that minibatch. So similar to dropout, it adds some noise to each hidden layer's activations. • This has a slight regularization effect. Batch Norm as regularization Softmax regression Multi-class classification Recognizing cats, dogs, and baby chicks Softmax layer Softmax examples Programming Frameworks Deep Learning frameworks Deep learning frameworks • Caffe/Caffe2 • CNTK • Keras • Lasagne • mxnet • PaddlePaddle • TensorFlow • Theano • Torch Choosing deep learning frameworks - Ease of programming (development and deployment) - Running speed - Truly open (open source with good governance) Programming Frameworks TensorFlow Motivating problem"
  },
  {
    "index": 2,
    "title": "3 Structuring Machine Learning Projects",
    "content": "3 Structuring Machine Learning Projects. Introduction to ML strategy Why ML Strategy? Motivating example Ideas: • Collect more data • Collect more diverse training set • Train algorithm longer with gradient descent • Try Adam instead of gradient descent • Try bigger network • Try smaller network • Try dropout • Add !\" regularization • Network architecture • Activation functions • # hidden units Introduction to ML strategy Orthogonalization TV tuning example Car Chain of assumptions in ML Fit training set well on cost function Fit dev set well on cost function Fit test set well on cost function Performs well in real world Setting up your goal Single number evaluation metric Using a single number evaluation metric Idea Experiment Code Classifier Precision Recall F1 Score Another example Algorithm US China India Other Average Setting up your goal Satisficing and optimizing metrics Another cat classification example Classifier Accuracy Running time Train/dev/test distributions Setting up your goal Cat classification dev/test sets Idea Experiment Code Regions: • Other Europe • South America • India • China • Other Asia • Australia True story (details changed) Optimizing on dev set on loan approvals for medium income zip codes Tested on low income zip codes Guideline Choose a dev set and test set to reflect data you expect to get in the future and consider important to do well on. Size of dev and test sets Setting up your goal Old way of splitting data Size of dev set Set your dev set to be big enough to detect differences in algorithm/models you're trying out. Size of test set Set your test set to be big enough to give high confidence in the overall performance of your system. When to change dev/test sets and metrics Setting up your goal Cat dataset examples Metric: classification error Algorithm A: 3% error Algorithm B: 5% error 1. So far we've only discussed how to define a metric to evaluate classifiers. 2. Worry separately about how to do well on this metric. Orthogonalization for cat pictures: anti-porn Another example Algorithm A: 3% error Algorithm B: 5% error If doing well on your metric + dev/test set does not correspond to doing well on your application, change your metric and/or dev/test set. Dev/test User images Comparing to humanlevel performance Why human-level performance? Comparing to human-level performance time accuracy Why compare to human-level performance Humans are quite good at a lot of tasks. So long as ML is worse than humans, you can: - Get labeled data from humans. - Gain insight from manual error analysis: Why did a person get this right? - Better analysis of bias/variance. Comparing to humanlevel performance Avoidable bias Bias and Variance high bias “just right” high variance Bias and Variance Cat classification Training set error: 1% 15% 15% 0.5% Dev set error: 11% 16% 30% 1% Cat classification example Training error 8% Dev error 10% Understanding human-level performance Comparing to humanlevel performance Human-level error as a proxy for Bayes error Medical image classification example: Suppose: (a) Typical human ................... 3 % error (b) Typical doctor ..................... 1 % error (c) Experienced doctor ............... 0.7 % error (d) Team of experienced doctors .. 0.5 % error What is “human-level” error? Error analysis example Training error Dev error Summary of bias/variance with human-level performance Human-level error Dev error Training error Surpassing humanlevel performance Comparing to humanlevel performance Surpassing human-level performance Team of humans One human Training error Dev error Problems where ML significantly surpasses human-level performance - Online advertising - Product recommendations - Logistics (predicting transit time) - Loan approvals Improving your model performance Comparing to humanlevel performance The two fundamental assumptions of supervised learning 1. You can fit the training set pretty well. 2. The training set performance generalizes pretty well to the dev/test set. Reducing (avoidable) bias and variance Human-level Dev error Train bigger model Train longer/better optimization algorithms NN architecture/hyperparameters search More data Regularization NN architecture/hyperparameters search Training error Error Analysis Carrying out error analysis Look at dev examples to evaluate ideas Should you try to make your cat classifier do better on dogs? Error analysis: • Get ~100 mislabeled dev set examples. • Count up how many are dogs. Evaluate multiple ideas in parallel Ideas for cat detection: • Fix pictures of dogs being recognized as cats • Fix great cats (lions, panthers, etc..) being misrecognized • Improve performance on blurry images Image % of total Error Analysis Cleaning up Incorrectly labeled data Incorrectly labeled examples DL algorithms are quite robust to random errors in the training set. Error analysis Overall dev set error Errors due incorrect labels Errors due to other causes Goal of dev set is to help you select between two classifiers A & B. Image Dog Great Cat Blurry Incorrectly labeled Comments 98 􀀁 Labeler missed cat in background 100 􀀁 Drawing of a cat; Not a real cat. % of total 8% 43% 61% 6% Correcting incorrect dev/test set examples • Apply same process to your dev and test sets to make sure they continue to come from the same distribution • Consider examining examples your algorithm got right as well as ones it got wrong. • Train and dev/test data may now come from slightly different distributions. Error Analysis Build your first system quickly, then iterate Speech recognition example • Noisy background • Café noise • Car noise • Accented speech • Far from microphone • Young children's speech • Stuttering • Set up dev/test set and metric • Build initial system quickly • Use Bias/Variance analysis & Error analysis to prioritize next steps. Guideline: Build your first system quickly, then iterate Mismatched training and dev/test data Training and testing on different distributions Cat app example Data from webpages Data from mobile app Speech recognition example Purchased data Smart speaker control Voice keyboard Speech activated rearview mirror Training Dev/test Bias and Variance with mismatched data distributions Mismatched training and dev/test data Cat classifier example Assume humans get ≈ 0% error. Training-dev set: Same distribution as training set, but not used for training Training error Dev error Bias/variance on mismatched training and dev/test sets More general formulation Addressing data mismatch Mismatched training and dev/test data Addressing data mismatch • Carry out manual error analysis to try to understand difference between training and dev/test sets • Make training data more similar; or collect more data similar to dev/test sets Artificial data synthesis “The quick brown Car noise fox jumps over the lazy dog.” Synthesized in-car audio Artificial data synthesis Car recognition: Learning from multiple tasks Transfer learning Transfer learning When transfer learning makes sense • You have a lot more data for Task A than Task B. • Task A and B have the same input x. • Low level features from A could be helpful for learning B. Learning from multiple tasks Multi-task learning Simplified autonomous driving example Neural network architecture When multi-task learning makes sense • Training on a set of tasks that could benefit from having shared lower-level features. • Usually: Amount of data you have for each task is quite similar. • Can train a big enough neural network to do well on all the tasks. What is end-to-end deep learning End-to-end deep learning What is end-to-end learning? Speech recognition example Face recognition [Image courtesy of Baidu] More examples Machine translation Estimating child's age: Whether to use end-to-end learning End-to-end deep learning Pros and cons of end-to-end deep learning Pros: • Let the data speak • Less hand-designing of components needed Cons: • May need large amount of data • Excludes potentially useful hand-designed components Applying end-to-end deep learning Key question: Do you have sufficient data to learn a function of the complexity needed to map x to y?"
  },
  {
    "index": 3,
    "title": "4 Convolutional Neural Networks",
    "content": "4 Convolutional Neural Networks. Convolutional Neural Networks Computer vision Computer Vision Problems Image Classification Cat? (0/1) Neural Style Transfer Object detection Deep Learning on large images Cat? (0/1) Convolutional Neural Networks Edge detection example Computer Vision Problem vertical edges horizontal edges Vertical edge detection Vertical edge detection Convolutional Neural Networks More edge detection Vertical edge detection examples Vertical and Horizontal Edge Detection Vertical Horizontal Learning to detect edges Convolutional Neural Networks Padding Padding Valid and Same convolutions “Valid”: “Same”: Pad so that output size is the same as the input size. Convolutional Neural Networks Strided convolutions Strided convolution Summary of convolutions & × & image #× # filter padding p stride s Technical note on cross-correlation vs. convolution Convolution in math textbook: Convolutional Neural Networks Convolutions over volumes Convolutions on RGB images Convolutions on RGB image Multiple filters Convolutional Neural Networks One layer of a convolutional network Example of a layer Number of parameters in one layer If you have 10 filters that are 3 x 3 x 3 in one layer of a neural network, how many parameters does that layer have? Summary of notation If layer l is a convolution layer: \" # = filter size $ # = padding % # = stride # = number of filters Each filter is: Activations: Weights: bias: Input: Output: Convolutional Neural Networks A simple convolution network example Example ConvNet Types of layer in a convolutional network: - Convolution - Pooling - Fully connected Convolutional Neural Networks Pooling layers Pooling layer: Max pooling Pooling layer: Max pooling Pooling layer: Average pooling Summary of pooling Hyperparameters: f : filter size s : stride Max or average pooling Convolutional Neural Networks Convolutional neural network example Neural network example Convolutional Neural Networks Why convolutions? Why convolutions Why convolutions Parameter sharing: A feature detector (such as a vertical edge detector) that's useful in one part of the image is probably useful in another part of the image. Sparsity of connections: In each layer, each output value depends only on a small number of inputs. Putting it together Training set (% & , ( & ) ...(% + , ( + ). Use gradient descent to optimize parameters to reduce , Cost , = &+ Case Studies Why look at case studies? Outline Classic networks: • LeNet-5 ResNet Inception • AlexNet • VGG Case Studies Classic networks LeNet - 5 avg pool avg pool [LeCun et al., 1998. Gradient-based learning applied to document recognition] AlexNet MAX-POOL. same MAX-POOL. same MAX-POOL. Softmax [Krizhevsky et al., 2012. ImageNet classification with deep convolutional neural networks] VGG - 16 CONV = 3×3 filter, s = 1, same MAX-POOL. = 2×2 , s = 2 [CONV 64] POOL [CONV. 128] POOL [CONV. 256] POOL [CONV. 512] POOL [CONV. 512] POOL Softmax [Simonyan & Zisserman 2015. Very deep convolutional networks for large-scale image recognition] Case Studies Residual Networks (ResNets) Residual block [He et al., 2015. Deep residual networks for image recognition] Residual Network # layers training error Plain # layers training error ResNet [He et al., 2015. Deep residual networks for image recognition] Case Studies Why ResNets work Why do residual networks work? ResNet Plain ResNet [He et al., 2015. Deep residual networks for image recognition] Case Studies Network in Network and 1×1 convolutions Why does a 1 × 1 convolution do? 6 × 6 × 32 1 × 1 × 32 6 × 6 × # filters [Lin et al., 2013. Network in network] Using 1×1 convolutions ReLU CONV 1 × 1 [Lin et al., 2013. Network in network] Case Studies Inception network motivation Motivation for inception network MAX-POOL. [ Szegedy et al. 2014. Going deeper with convolutions] The problem of computational cost CONV same, Using 1×1 convolution CONV CONV. Case Studies Inception network Inception module Previous Activation CONV CONV CONV CONV CONV MAXPOOL. same CONV Channel Concat Inception network [Szegedy et al., 2014, Going Deeper with Convolutions] http://knowyourmeme.com/memes/we-need-to-go-deeper MobileNet Convolutional Neural Networks Motivation for MobileNets • Low computational cost at deployment • Useful for mobile and embedded vision applications • Key idea: Normal vs. depthwiseseparable convolutions [Howard et al. 2017, MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications] Normal Convolution Computational cost = #filter params x # filter positions x # of filters Depthwise Separable Convolution Normal Convolution Depthwise Separable Convolution Depthwise Pointwise Computational cost = #filter params x # filter positions x # of filters Depthwise Convolution Depthwise Separable Convolution Depthwise Convolution Pointwise Convolution Pointwise Convolution Computational cost = #filter params x # filter positions x # of filters Depthwise Separable Convolution Normal Convolution Depthwise Separable Convolution Depthwise Pointwise Cost Summary [Howard et al. 2017, MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications] Cost of depthwise separable convolution Cost of normal convolution Depthwise Separable Convolution Depthwise Convolution Pointwise Convolution MobileNet Architecture Convolutional Neural Networks MobileNet [Sandler et al. 2019, MobileNetV2: Inverted Residuals and Linear Bottlenecks] MobileNet v1 MobileNet v2 Expansion Depthwise Projection Residual Connection MobileNet v2 Bottleneck [Sandler et al. 2019, MobileNetV2: Inverted Residuals and Linear Bottlenecks] Residual Connection Expansion Depthwise Pointwise MobileNet [Sandler et al. 2019, MobileNetV2: Inverted Residuals and Linear Bottlenecks] MobileNet v1 MobileNet v2 Expansion Depthwise Projection Residual Connection MobileNet v2 Full Architecture [Sandler et al. 2019, MobileNetV2: Inverted Residuals and Linear Bottlenecks] conv2d conv2d avgpool conv2d EfficientNet Convolutional Neural Networks [Tan and Le, 2019, EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks] Andrew Baseline HWDieigdehpeererr Resolution EfficientNet Compound scaling Practical advice for using ConvNets Transfer Learning Practical advice for using ConvNets Data augmentation Common augmentation method Mirroring Random Cropping Rotation Shearing Local warping Color shifting Implementing distortions during training Practical advice for using ConvNets The state of computer vision Data vs. hand-engineering Two sources of knowledge • Labeled data • Hand engineered features/network architecture/other components Tips for doing well on benchmarks/winning competitions Ensembling • Train several networks independently and average their outputs Multi-crop at test time • Run classifier on multiple versions of test images and average results Use open source code • Use architectures of networks published in the literature • Use pretrained models and fine-tune on your dataset • Use open source implementations if possible Object Detection Object localization What are localization and detection? Image classification Classification with localization Detection Classification with localization 1 - pedestrian 2 - car 3 - motorcycle 4 - background Defining the target label y 1 - pedestrian 2 - car 3 - motorcycle 4 - background Need to output #$, #&, #', #(, class label (1-4) Object Detection Landmark detection Landmark detection ConvNet Object Detection Object detection Car detection example Training set: Sliding windows detection Object Detection Convolutional implementation of sliding windows Turning FC layer into convolutional layers MAX POOL FC FC. softmax (4) MAX POOL. Convolution implementation of sliding windows MAX POOL FC FC FC MAX POOL MAX POOL. [ Sermanet et al., 2014, OverFeat: Integrated recognition, localization and detection using convolutional networks] Convolution implementation of sliding windows MAX POOL. Object Detection Bounding box predictions Output accurate bounding boxes YOLO algorithm Labels for training For each grid cell: [Redmon et al., 2015, You Only Look Once: Unified real-time object detection] Specify the bounding boxes [Redmon et al., 2015, You Only Look Once: Unified real-time object detection] Object Detection Intersection over union Evaluating object localization “Correct” if IoU ≥ 0.5 More generally, IoU is a measure of the overlap between two bounding boxes. Object Detection Non-max suppression Non-max suppression example Non-max suppression example Non-max suppression example Non-max suppression algorithm Discard all boxes with $% ≤ 0.6 While there are any remaining boxes: • Pick the box with the largest $% Output that as a prediction. • Discard any remaining box with IoU ≥ 0.5 with the box output in the previous step Each output prediction is: Object Detection Anchor boxes Overlapping objects: Anchor box 1: Anchor box 2: [Redmon et al., 2015, You Only Look Once: Unified real-time object detection] Anchor box algorithm Previously: Each object in training image is assigned to grid cell that contains that object's midpoint. With two anchor boxes: Each object in training image is assigned to grid cell that contains object's midpoint and anchor box for the grid cell with highest IoU. Anchor box example Anchor box 1: Anchor box 2: Object Detection Putting it together: YOLO algorithm Training [Redmon et al., 2015, You Only Look Once: Unified real-time object detection] 1 - pedestrian 2 - car 3 - motorcycle Making predictions Outputting the non-max supressed outputs • For each grid call, get 2 predicted bounding boxes. • Get rid of low probability predictions. • For each class (pedestrian, car, motorcycle) use non-max suppression to generate final predictions. Object Detection Region proposals (Optional) Region proposal: R-CNN [Girshik et. al, 2013, Rich feature hierarchies for accurate object detection and semantic segmentation] Faster algorithms R-CNN: Propose regions. Classify proposed regions one at a time. Output label + bounding box. [Girshik et. al, 2013. Rich feature hierarchies for accurate object detection and semantic segmentation] Fast R-CNN: Propose regions. Use convolution implementation of sliding windows to classify all the proposed regions. [Girshik, 2015. Fast R-CNN] Faster R-CNN: Use convolutional network to propose regions. [Ren et. al, 2016. Faster R-CNN: Towards real-time object detection with region proposal networks] Semantic segmentation with U-Net Convolutional Neural Networks Object Detection vs. Semantic Segmentation Input image Object Detection Semantic Segmentation Motivation for U-Net Chest X-Ray Brain MRI [Dong et al., 2017, Automatic Brain Tumor Detection and Segmentation Using U-Net Based Fully Convolutional Networks ] [Novikov et al., 2017, Fully Convolutional Architectures for Multi-Class Segmentation in Chest Radiographs] Per-pixel class labels 1. Car 0. Not Car Per-pixel class labels 1. Car 2. Building 3. Road Segmentation Map Deep Learning for Semantic Segmentation Normal Convolution Transpose Convolution Transpose Convolution Transpose Convolution weight filter filter f x f = 3 x 3 padding p = 1 stride s = 2 Deep Learning for Semantic Segmentation Skip connection U-Net [Ronneberger et al., 2015, U-Net: Convolutional Networks for Biomedical Image Segmentation] Andrew Conv, RELU Max Pool Trans Conv Skip Connection Conv (1x1) U-Net [Ronneberger et al., 2015, U-Net: Convolutional Networks for Biomedical Image Segmentation] Andrew Conv, RELU Max Pool Trans Conv Skip Connection Conv (1x1) h x w x 3 h x w x # classes Face recognition What is face recognition? Face recognition [Courtesy of Baidu] Face verification vs. face recognition Verification • Input image, name/ID • Output whether the input image is that of the claimed person Recognition • Has a database of K persons • Get an input image • Output ID if the image is any of the K persons (or “not recognized”) Face recognition One-shot learning One-shot learning Learning from one example to recognize the person again Learning a “similarity” function d(img1,img2) = degree of difference between images If d(img1,img2) ≤ - Face recognition Siamese network Siamese network [Taigman et. al., 2014. DeepFace closing the gap to human level performance] Goal of learning Parameters of NN define an encoding ((\" ) ) Learn parameters so that: If \" ) , \" + are the same person, f \" ) - f \" + & is small. If \" ) , \" + are different persons, f \" ) - f \" + & is large. Face recognition Triplet loss Learning Objective [Schroff et al.,2015, FaceNet: A unified embedding for face recognition and clustering] Anchor Positive Anchor Negative Loss function Training set: 10k pictures of 1k persons [Schroff et al.,2015, FaceNet: A unified embedding for face recognition and clustering] Choosing the triplets A,P,N During training, if A,P,N are chosen randomly, ! \", $ + & ≤ !(\",)) is easily satisfied. Choose triplets that're “hard” to train on. [Schroff et al.,2015, FaceNet: A unified embedding for face recognition and clustering] Training set using triplet loss Anchor Positive Negative Face recognition Face verification and binary classification Learning the similarity function [Taigman et. al., 2014. DeepFace closing the gap to human level performance] Face verification supervised learning [Taigman et. al., 2014. DeepFace closing the gap to human level performance] What is neural style transfer? Neural Style Transfer Neural style transfer [Images generated by Justin Johnson] Content Style Content Style Generated image Generated image What are deep ConvNets learning? Neural Style Transfer Visualizing what a deep network is learning Pick a unit in layer 1. Find the nine image patches that maximize the unit's activation. Repeat for other units. [Zeiler and Fergus., 2013, Visualizing and understanding convolutional networks] Visualizing deep layers Layer 1 Layer 2 Layer 3 Layer 4 Layer 5 Visualizing deep layers: Layer 1 Layer 1 Layer 2 Layer 3 Layer 4 Layer 5 Visualizing deep layers: Layer 2 Layer 1 Layer 2 Layer 3 Layer 4 Layer 5 Visualizing deep layers: Layer 3 Layer 1 Layer 2 Layer 3 Layer 4 Layer 5 Visualizing deep layers: Layer 3 Layer 1 Layer 2 Layer 3 Layer 4 Layer 5 Visualizing deep layers: Layer 4 Layer 1 Layer 2 Layer 3 Layer 4 Layer 5 Visualizing deep layers: Layer 5 Layer 1 Layer 2 Layer 3 Layer 4 Layer 5 Cost function Neural Style Transfer Neural style transfer cost function Content C Style S Generated image G [Gatys et al., 2015. A neural algorithm of artistic style. Images on slide generated by Justin Johnson] Find the generated image G 1. Initiate G randomly 2. Use gradient descent to minimize %(') [Gatys et al., 2015. A neural algorithm of artistic style] Content cost function Neural Style Transfer Content cost function • Say you use hidden layer ! to compute content cost. • Use pre-trained ConvNet. (E.g., VGG network) • Let 6[2](9) and 6[2](:) be the activation of layer ! on the images • If 6[2](9) and 6[2](:) are similar, both images have similar content [Gatys et al., 2015. A neural algorithm of artistic style] Style cost function Neural Style Transfer Meaning of the “style” of an image Say you are using layer $'s activation to measure “style.” Define style as correlation between activations across channels. How correlated are the activations across different channels? [Gatys et al., 2015. A neural algorithm of artistic style] Intuition about style of an image Style image Generated Image [Gatys et al., 2015. A neural algorithm of artistic style] Style matrix Let a*,,,- [/] = activation at 2, 3, 4 . 7[/] is n9 [/]×n9 [/] [Gatys et al., 2015. A neural algorithm of artistic style] Style cost function [Gatys et al., 2015. A neural algorithm of artistic style] Convolutional Networks in 1D or 3D 1D and 3D generalizations of models Convolutions in 2D and 1D"
  },
  {
    "index": 4,
    "title": "5 Sequence Models",
    "content": "5 Sequence Models. Recurrent Neural Networks Why sequence models? Examples of sequence data Music generation ∅ Speech recognition “The quick brown fox jumped over the lazy dog.” Sentiment classification “There is nothing to like in this movie.” DNA sequence analysis AGCCCCTGTGAGGAACTAG AGCCCCTGTGAGGAACTAG. Machine translation Voulez-vous chanter avec moi? Do you want to sing with Video activity recognition Running Name entity recognition Yesterday, Harry Potter met Hermione Granger. Yesterday, Harry Potter met Hermione Granger. Recurrent Neural Networks Notation Motivating example x: Harry Potter and Hermione Granger invented a new spell. Representing words x: Harry Potter and Hermione Granger invented a new spell. Representing words x: Harry Potter and Hermione Granger invented a new spell. And = 367 Invented = 4700 New = 5976 Spell = 8376 Harry = 4075 Potter = 6830 Hermione = 4200 Gran... = 4000 Recurrent Neural Networks Recurrent Neural Network Model Why not a standard network? Problems: - Inputs, outputs can be different lengths in different examples. - Doesn't share features learned across different positions of text. Recurrent Neural Networks He said, “Teddy Roosevelt was a great President.” He said, “Teddy bears are on sale!” Forward Propagation Simplified RNN notation Recurrent Neural Networks Backpropagation through time Forward propagation and backpropagation Forward propagation and backpropagation Backpropagation through time Recurrent Neural Networks Different types of RNNs Examples of sequence data Music generation ∅ Speech recognition “The quick brown fox jumped over the lazy dog.” Sentiment classification “There is nothing to like in this movie.” DNA sequence analysis AGCCCCTGTGAGGAACTAG AGCCCCTGTGAGGAACTAG. Machine translation Voulez-vous chanter avec moi? Do you want to sing with Video activity recognition Running Name entity recognition Yesterday, Harry Potter met Hermione Granger. Yesterday, Harry Potter met Hermione Granger. Examples of RNN architectures Examples of RNN architectures Summary of RNN types One to one One to many Many to one Many to many Many to many Recurrent Neural Networks Language model and sequence generation What is language modelling? Speech recognition The apple and pair salad. The apple and pear salad. !(The apple and pair salad) = !(The apple and pear salad) = Language modelling with an RNN Training set: large corpus of english text. Cats average 15 hours of sleep a day. The Egyptian Mau is a bread of cat. <EOS> RNN model Cats average 15 hours of sleep a day. <EOS> &0()* log &'0()* Recurrent Neural Networks Sampling novel sequences Sampling a sequence from a trained RNN Character-level language model Vocabulary = [a, aaron, ..., zulu, <UNK>] Sequence generation President enrique peña nieto, announced sench's sulk former coming football langston paring. “I was not at all surprised,” said hich langston. “Concussion epidemic”, to be examined. The gray football the told some and this has on the uefa icon, should money as. News Shakespeare The mortal moon hath her eclipse in love. And subject of this thou art another this fold. When besser be my love to me see sabl's. For whose are ruse of mine eyes heaves. Recurrent Neural Networks Vanishing gradients with RNNs Vanishing gradients with RNNs Exploding gradients. ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋯ Recurrent Neural Networks Gated Recurrent Unit (GRU) RNN unit GRU (simplified) The cat, which already ate ..., was full. [Cho et al., 2014. On the properties of neural machine translation: Encoder-decoder approaches] [Chung et al., 2014. Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling] Full GRU The cat, which ate already, was full. 5̃\"#$ = tanh((>[ 5\"#*+$, -\"#$] + />) Recurrent Neural Networks LSTM (long short term memory) unit GRU and LSTM !̃#$% = tanh(,- Γ/ ∗ !#$12%, 4#$% + 6-) GRU LSTM. [ Hochreiter & Schmidhuber 1997. Long short-term memory] LSTM units !̃#$% = tanh(,- Γ/ ∗ !#$12%, 4#$% + 6-) GRU LSTM !̃#$%. = tanh(,- =#$12%, 4#$% + 6-) [Hochreiter & Schmidhuber 1997. Long short-term memory] !̃#$% = tanh(,- =#$12%, 4#$% + 6-) LSTM in pictures forget gate update gate tanh output gate tanh softmax softmax softmax softmax * ---- Recurrent Neural Networks Bidirectional RNN Getting information from the future He said, “Teddy bears are on sale!” He said, “Teddy Roosevelt was a great President!” He said, “Teddy bears are on sale!” Bidirectional RNN (BRNN). Recurrent Neural Networks Deep RNNs Deep RNN example NLP and Word Embeddings Word representation Word representation V = [a, aaron, ..., zulu, <UNK>] 1-hot representation Apple Orange I want a glass of orange ______. I want a glass of apple______. King Woman Man Queen Featurized representation: word embedding Apple Orange King Woman Man Queen I want a glass of orange ______. I want a glass of apple______. Visualizing word embeddings fish dog cat apple grape one orange three two four king man queen woman [van der Maaten and Hinton., 2008. Visualizing data using t-SNE] NLP and Word Embeddings Using word embeddings Named entity recognition example Sally Johnson is an orange farmer Robert Lin is an apple farmer Transfer learning and word embeddings 1. Learn word embeddings from large text corpus. (1-100B words) (Or download pre-trained embedding online.) 2. Transfer embedding to new task with smaller training set. (say, 100k words) 3. Optional: Continue to finetune the word embeddings with new data. Relation to face encoding [Taigman et. al., 2014. DeepFace: Closing the gap to human level performance] NLP and Word Embeddings Properties of word embeddings Analogies Apple Orange King Woman Man Queen Gender Royal Age Food [Mikolov et. al., 2013, Linguistic regularities in continuous space word representations] Analogies using word vectors fish dog cat apple grape orange one three two four king man queen woman Cosine similarity Man:Woman as Boy:Girl Ottawa:Canada as Nairobi:Kenya Big:Bigger as Tall:Taller Yen:Japan as Ruble:Russia NLP and Word Embeddings Embedding matrix Embedding matrix In practice, use specialized function to look up an embedding. NLP and Word Embeddings Learning word embeddings Neural language model I want a glass of orange ______. want glass orange [Bengio et. al., 2003, A neural probabilistic language model] Other context/target pairs I want a glass of orange juice to go along with my cereal. Context: Last 4 words. 4 words on left & right Last 1 word Nearby 1 word NLP and Word Embeddings Word2Vec Skip-grams I want a glass of orange juice to go along with my cereal. [Mikolov et. al., 2013. Efficient estimation of word representations in vector space.] Model Vocab size = 10,000k Problems with softmax classification How to sample the context #? NLP and Word Embeddings Negative sampling Defining a new learning problem I want a glass of orange juice to go along with my cereal. [Mikolov et. al., 2013. Distributed representation of words and phrases and their compositionality] Model Softmax: ! \" # = context word orange orange orange juice king book target? the orange orange Selecting negative examples context word orange orange orange juice king book target? the orange orange NLP and Word Embeddings GloVe word vectors GloVe (global vectors for word representation) I want a glass of orange juice to go along with my cereal. [Pennington et. al., 2014. GloVe: Global vectors for word representation] Model A note on the featurization view of word embeddings minimize Σ Σ ( )*+ ,* 2 - log )*+ King Woman Man Queen Gender Royal Age Food NLP and Word Embeddings Sentiment classification Sentiment classification problem The dessert is excellent. Service was quite slow. Good for a quick meal, but nothing special. Completely lacking in good taste, good service, and good ambience. Simple sentiment classification model The desert excellent The dessert is excellent “Completely lacking in good taste, good service, and good ambience.” RNN for sentiment classification Completely lacking in good .... ambience softmax NLP and Word Embeddings Debiasing word embeddings The problem of bias in word embeddings [Bolukbasi et. al., 2016. Man is to computer programmer as woman is to homemaker? Debiasing word embeddings] Man:Woman as King:Queen Man:Computer_Programmer as Woman: Father:Doctor as Mother: Word embeddings can reflect gender, ethnicity, age, sexual orientation, and other biases of the text used to train the model. Homemaker Nurse Addressing bias in word embeddings 1. Identify bias direction. 2. Neutralize: For every word that is not definitional, project to get rid of bias. 3. Equalize pairs. [Bolukbasi et. al., 2016. Man is to computer programmer as woman is to homemaker? Debiasing word embeddings] Sequence to sequence models Basic models Sequence to sequence model Jane visite l'Afrique en septembre Jane is visiting Africa in September. [Cho et al., 2014. Learning phrase representations using RNN encoder-decoder for statistical machine translation] [Sutskever et al., 2014. Sequence to sequence learning with neural networks] Image captioning A cat sitting on a chair MAX-POOL. same MAX-POOL. same MAX-POOL. Softmax [Mao et. al., 2014. Deep captioning with multimodal recurrent neural networks] [Vinyals et. al., 2014. Show and tell: Neural image caption generator] [Karpathy and Li, 2015. Deep visual-semantic alignments for generating image descriptions] ⋯ Sequence to sequence models Picking the most likely sentence Machine translation as building a conditional language model Language model: Machine translation: !\"#$ Finding the most likely translation Jane visite l'Afrique en septembre. /('\"&$,..., '\"*,$| %) Jane is visiting Africa in September. Jane is going to be visiting Africa in September. In September, Jane will visit Africa. Her African friend welcomed Jane in September. arg max Why not a greedy search? Jane is visiting Africa in September. Jane is going to be visiting Africa in September. Sequence to sequence models Beam search Beam search algorithm jane september zulu Step 1 Beam search algorithm jane september zulu Step 1 Step 2 Beam search (4 = 3) in september jane is jane visits in september 0('\"&$, '\"9$| %) jane visits africa in september. <EOS> jane is jane visits Sequence to sequence models Refinements to beam search Length normalization arg max ' () *+,- ., *+0-,..., *+,20-) arg max ' 7log ) *+,- ., *+0-,..., *+,20-) 7log ) *+,- ., *+0-,..., *+,20-) Beam search discussion Beam width B? Unlike exact search algorithms like BFS (Breadth First Search) or DFS (Depth First Search), Beam Search runs faster but is not guaranteed to find exact maximum for arg max ' )(*|.). Sequence to sequence models Error analysis on beam search Example Jane visite l'Afrique en septembre. Human: Jane visits Africa in September. Algorithm: Jane visited Africa last September. Error analysis on beam search Human: Jane visits Africa in September. (+∗) Algorithm: Jane visited Africa last September. (+.) Case 1: Beam search chose +.. But +∗ attains higher / + % . Conclusion: Beam search is at fault. Case 2: +∗ is a better translation than +.. But RNN predicted / +∗ % < / +. % . Conclusion: RNN model is at fault. Error analysis process Jane visits Africa in September. Jane visited Africa last September. Human Algorithm / +∗ % / +. % At fault? Figures out what faction of errors are “due to” beam search vs. RNN model Sequence to sequence models Bleu score (optional) Evaluating machine translation French: Le chat est sur le tapis. Reference 1: The cat is on the mat. Reference 2: There is a cat on the mat. MT output: the the the the the the the. Precision: Modified precision: [Papineni et. al., 2002. Bleu: A method for automatic evaluation of machine translation] Bleu score on bigrams Example: Reference 1: The cat is on the mat. Reference 2: There is a cat on the mat. MT output: The cat the cat on the mat. the cat cat the cat on on the the mat [Papineni et. al., 2002. Bleu: A method for automatic evaluation of machine translation] Bleu score on unigrams Example: Reference 1: The cat is on the mat. Reference 2: There is a cat on the mat. MT output: The cat the cat on the mat. [Papineni et. al., 2002. Bleu: A method for automatic evaluation of machine translation] Bleu details !' = Bleu score on n-grams only Combined Bleu score: 1 if MT_output_length > reference_output_length exp (1 - MT_output_length/reference_output_length) otherwise [Papineni et. al., 2002. Bleu: A method for automatic evaluation of machine translation] Sequence to sequence models Attention model intuition The problem of long sequences Jane s'est rendue en Afrique en septembre dernier, a apprécié la culture et a rencontré beaucoup de gens merveilleux; elle est revenue en parlant comment son voyage était merveilleux, et elle me tente d'y aller aussi. Jane went to Africa last September, and enjoyed the culture and met many wonderful people; she came back raving about how wonderful her trip was, and is tempting me to go too. 10 20 30 40 50 Sentence length Bleu score Attention model intuition jane visite l'Afrique en septembre [Bahdanau et. al., 2014. Neural machine translation by jointly learning to align and translate] Sequence to sequence models Attention model Attention model jane visite l'Afrique en septembre [Bahdanau et. al., 2014. Neural machine translation by jointly learning to align and translate] [Bahdanau et. al., 2014. Neural machine translation by jointly learning to align and translate] Computing attention +\",,,.$ [Xu et. al., 2015. Show, attend and tell: Neural image caption generation with visual attention] +\",,,.$ = amount of attention /\", $ should pay to )\",.$ Attention examples July 20th 1969 1969 - 07 - 20 23 April, 1564 1564 - 04 - 23 Visualization of +\",,,.$: Audio data Speech recognition Speech recognition problem audio clip transcript “the quick brown fox” Attention model for speech recognition CTC cost for speech recognition [Graves et al., 2006. Connectionist Temporal Classification: Labeling unsegmented sequence data with recurrent neural networks] (Connectionist temporal classification) “the quick brown fox” Basic rule: collapse repeated characters not separated by “blank” Audio data Trigger word detection What is trigger word detection? Amazon Echo (Alexa) Baidu DuerOS (xiaodunihao) Apple Siri (Hey Siri) Google Home (Okay Google) Trigger word detection algorithm Conclusion Summary and thank you Specialization outline 1. Neural Networks and Deep Learning 2. Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization 3. Structuring Machine Learning Projects 4. Convolutional Neural Networks 5. Sequence Models Deep learning is a super power Please buy this from shutterstock and replace in final video. - Thank you. Sequence to sequence models Transformers Intuition RNN GRU LSTM. Increased complexity, sequential Transformers Motivation • Attention + CNN • Self-Attention • Multi-Head Attention [Vaswani et al. 2017, Attention Is All You Need] Transformers Intuition Sequence to sequence models Self-Attention exp(𝑒<𝑡,𝑡'>) 𝑇𝑥 exp(𝑒<𝑡,𝑡'>) RNN Attention Self-Attention Intuition 𝐴(𝑞, 𝐾, 𝑉)= attention-based vector representation of a word en septembre Jane visite l'Afrique Transformers Attention exp(𝑒<𝑞∙𝑘<𝑖>>) Σ𝑗 exp(𝑒<𝑞∙𝑘<𝑗>>) [Vaswani et al. 2017, Attention Is All You Need] en septembre Jane visite Self-Attention l'Afrique exp(𝑒<𝑞∙𝑘<𝑖>>) Σ𝑗 exp(𝑒<𝑞∙𝑘<𝑗>>) Query (𝑄) Key (𝐾) Value (𝑉) [Vaswani et al. 2017, Attention Is All You Need] Sequence to sequence models Multi-Head Attention Jane visite l'Afrique en septembre V, Did what? V, When? V, Who? Multi-Head Attention Multi-Head Attention [Vaswani et al. 2017, Attention Is All You Need] Sequence to sequence models Transformers <SOS> 𝑦<1> 𝑦<2> ... 𝑦<𝑇𝑦-1> 𝑦<𝑇𝑦> Feed Forward Neural Network Jane visite l'Afrique en septembre <SOS> 𝑥<1> 𝑥<2> ... 𝑥<𝑇𝑥-1> 𝑥<𝑇𝑥> <EOS> Masked Multi-Head Attention Encoder Decoder Transformer [Vaswani et al. 2017, Attention Is All You Need] Multi-Head Attention Multi-Head Attention Feed Forward Neural Network Jane visits Africa in September Add & Norm Add & Norm Add & Norm Add & Norm Add & Norm Softmax Linear + Positional Encoding Details"
  }
]
