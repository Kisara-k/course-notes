[
  {
    "index": 1,
    "title": "1 Trustworthy Generative AI",
    "content": "Welcome to my course on trustworthy generative AI. And this course is all about how can we use generative AI large language models like ChatGPT or Claude or other tools to do interesting things. To do really valuable work, to increase our productivity but to do it in a way that's trustworthy, where we know that we can trust the outputs. We can reduce the chance that we make mistakes because of it. Now, people have sort of misconceptions about how to use this technology effectively. Some people just don't believe you can use it effectively at all. It just lies. It makes mistakes all the time. Hallucination, hallucination, God we cannot get away from that word. Hallucination is a feature. It is part of the tool that makes it amazing for all kinds of tasks like brainstorming or dreaming up stories or doing all kinds of interesting things. It's not a bug, it's a wonderful part of the tool. But if we use the tool inappropriately, hallucination becomes a major problem. So we have to think about and understand how do we approach this new, fundamentally unique capability in computing and use it in a way to solve real problems. And there's all kinds of problems we can solve. And part of the magic to use it appropriately, to use it in a trustworthy way, is selecting the right problems, formulating them in the right way, prompt engineering them correctly. And that's what we're going to talk about in this course. Now, let's start off by just kind of assessing and looking at sort of the dimensions of generative AI. So I'm going to ask ChatGPT or GPT-35, tell me how much wood could a woodchuck chuck if a woodchuck could chuck wood? I can't even say it. It says tongue twister. Now, what does it say? According to a popular tongue twister, a woodchuck could chuck wood up to a certain limit. However, in reality, woodchucks are groundhogs who primarily burrow in soil rather than chucking wood. Therefore, they cannot actually chuck wood and there is no specific amount or limit to how much wood a woodchuck could chuck. I'm still untwisting that my tongue from this one. Okay, now, what happened? Well, first off, let's stop and take a minute and appreciate the sophistication of what just happened. I asked it a question that prior to the release of these models, we never could have gotten an answer of that sophistication now part of what it says is probably true. Now, I don't know because I'm not an expert on woodchucks. I don't have the expertise to evaluate it, but it looks correct. It says, however, in reality, woodchucks are groundhogs who primarily burrow in soil rather than chucking wood. So one thing we get is we get text generated. One thing that we get is a lot of reasoning or truth about the question. I asked it, how much woodchuck a woodchuck could chuck? It says, according to a popular tongue twister, a woodchuck could chuck wood up to a certain limit. So it's actually reasoning, right? It's saying, look, your question itself implies that there is a limit, and I'm going to point that out. Super sophisticated reasoning, that's a unique capability. Now, some people are going to just read this and say, what does that mean? It's just all nonsense and gloss over and fundamentally fail to appreciate the incredible nature of that output. It's saying that question, the tongue twister itself implies a limit. That's really sophisticated reasoning. That's something that we just could not do before. Now that we're appreciating, that we see it also goes on and produces something that's probably true. I don't know for certain, but it looks true that woodchucks are groundhogs who primarily burrow in soil rather than chucking wood. And then therefore they cannot actually chuck wood and there is no specific amount or limit to how much a woodchuck could chuck. That might not be true, I'm sure there is a limit to how much they chuck, if you think about it. They probably have a limited lifespan. They can only chuck so much per day if they could. So that's probably not fully accurate. Let's look at this again. Write four paragraphs about wood chuck trees and use the Chicago citation format. It goes through and it says, woodchucks also know as groundhogs. It goes through, gives me all this stuff, gives me some work cited. I don't even know if that exists or not. It might exist, it might not. It might have just generated something that looks like a link to the Iowa Department of Natural Resources from 2019 or it might actually be one. We don't actually know. It wrote me a whole bunch of stuff about wood chucks. This is kind of amazing. I can use it to do creative writing. I can try to do it to use factual writing, but I can't actually assess wood chucks. So I don't know if it's correct, but it'll do whatever we want. And notice also how I ask the question matters a lot and this is going to come back to the trustworthy part, the prompt engineering part. I said write four paragraphs about wood chuck trees and use this Chicago citation format. Now, the fact that I said write four paragraphs about this, I might have been looking for it to write truthful paragraphs, but it may interpret my request as being just write something. It doesn't have to be truthful, just creative writing. The way that we word it, the way that we interact with it is really important because a lot of times it's trying to provide guardrails and say, hey, I can't answer that, I don't know this. But if we get creative or we go off a little bit in our wording, sometimes we can get it to generate some text. In this case, it generated a bunch of text, generated a bunch of references and references. We hear people talk all the time, it can't generate references. Well, that's not really true, it can if you use the tool in the right way and we'll talk about that later. But if we use it in the wrong way, like here, we could get something generated that's totally incorrect, where it's not really differentiating between fact and fiction. And maybe it thinks it's supposed to be producing fiction for us and we think it's supposed to be producing fact. And so just the fact that we're getting confused and now I can go and I can say now write a poem about wood chucks, and it writes a whole poem about wood chucks. So in this one conversation we've mixed a lot of things together. It's got reasoning, it's understanding really complex things, showing the power of computation and the reasoning that it can do. It can generate things that look like facts, but might not be that you have to be able to assess and have the expertise. It can go and generate completely creative things where hallucination is a feature. The fact that it can hallucinate all these ideas for a poem that didn't exist before, that's a feature, not a bug. But we have to start thinking about this thing that has all this complexity to it and start really sort of breaking down how do we use it? How do we interact with it, give it statements that give us output that we can trust and rely on? What are the tasks that we can give it that when we get output, we can check and make sure they're correct? What are the types of things that are good ways of using it versus not very good ways to do it? Both for the world and make getting things right and not making more mistakes, but also for us as humans and knowing that. We're not going to go use it in some way that's going to replace us or create a problem for us. And so we're going to explore kind of all these issues and look at how do we take this complicated tool, this generative AI tool, but reframe problems in ways that it can really, really help us. And give us results that we can trust and rely on? What are the types of things that we can do where we don't have massive risk and it's hugely valuable. We'll explore all these things. We'll look at the prompt engineering behind it, we'll look at the appropriate use cases. And we'll look at how do we use generative AI in a trustworthy way to solve real problems that have real impact and really matter in the world. And allow you to unleash your creativity, to scale up the types of things that you can do and solve and do it in a way that you can rely on the answers.\n\nOne of the most common inappropriate uses of these tools is to think of them as sources of facts. Now, one of the common mental models that people have of a text box that you go and type things into is an Internet search. So people are used to going and saying, give me information about this or ask this question and getting search results back and then having a belief in the correctness the soundness of those facts. But we should not think of Generative AI as a tool that's going to generate facts. Now, there are all kinds of interesting architectures where you can get it to help you get to facts, but we have to think of it differently. If we just go to the tool and we ask it for us to generate facts, that is not what it was designed for. People talk, you can't trust the sources that it generates well, that's not the purpose of the tool. The purpose of the tool and the way to think about it is sort of like a computational engine. It's an engine that can help you to do all kinds of interesting things in computing and to do all kinds of interesting natural language analyses and that you can build really powerful capabilities on top of. But it itself is not necessarily going to give you correct facts. Now, I'm going to give you an example of why you should not use it as a just fundamental source of facts. You have to understand that it's always going to make mistakes in anytime you're using it as just a generator of facts. Now, you can have it navigate you to facts or help you navigate to facts and notice the word navigation, not generation. We want to navigate to real sources, not generate potentially fake sources, that's key. Now, what does it mean to generate and use it in a way that generates facts that are potentially incorrect? Well, if I go and I say, tell me about Jules White at Vanderbilt University. It comes back and says, Jules White is an assistant professor of computer science at Vanderbilt University, a prestigious research institution located in Nashville, Tennessee. Well, that sounds pretty good, right? I think you probably know from this video in the subtitle or the banner down at the bottom that I'm at Vanderbilt, so it must be true, right? He joined Vanderbilt in 2011 and has made significant contributions to the field of computer science and software engineering. Man, I love this, I want to feel like I made significant contributions. I don't know one way or the other, but let's think about the facts so I could read the whole thing. But I'm going to follow up with a question. Please list the facts that the veracity of this output depends upon. And it comes back and it says, the veracity of the output depends on the following facts. Jules White is an assistant professor of computer science at Vanderbilt university. This is false, now, this is false. And it could be that it saw that and had a source that said, I was an assistant professor, which is absolutely possible that it was trained on something. And this shows you one thing. Its data could be out of date if you're not giving it information to reason about it's using what it was trained on. And I could be an assistant professor based on information it was trained on. I'm a full professor at Vanderbilt, so this is incorrect. Vanderbilt University is a prestigious research institution located in Nashville, Tennessee. I'll call that correct, Dr. White joined Vanderbilt University in 2011. That's incorrect, I didn't join in 2011, and there's no way it found that. It just generated it, now, the point behind this is this is a generation of facts that are untrue. So you can't generate facts, what you can generate is text. And that text may have a lot of significant reasoning in it, and you can rely on that reasoning to do useful things for you. But you shouldn't just rely on the generation to produce fact. If you really, really care about facts and sources, this is not the way to use it. It is not an Internet search that you go and type in and you're getting back results. Now, there are ways of using it to help enhance Internet search. So when you see an Internet search, what they're really doing when they enhance it is they are taking and retrieving documents that are relevant. They are giving all of that information to the generative AI in a prompt and saying, summarize these results. They're not saying generate a bunch of new results, they're saying, summarize this data that I'm giving you with respect to this question. They're basically using it as a computational engine to summarize outline, to help answer the question with respect to facts that they've given, it not with respect to something it was potentially trained on. Another way you can think about it is like, you can imagine going into an app, right, that has a bunch of medical information. And you could imagine going into the app and asking, like, tell me about let's say it's a medical information app. You tell me about my last doctor's visit. Now, it could generate a bunch of text that tells you a bunch of facts about your last medical visit that are totally wrong and fictitious. Or you could use it to try to navigate you to the place in the app that has the actual fact. To navigate you to the screen, to say which of these screens would have that information that the user has requested, and to try to navigate the user there by taking them directly to that screen. And so in that case, it's not generating facts, it's taking you to where they are, it's navigating you there. That would be an appropriate use case, is to try to navigate the user there. Now, if you get them to a screen and it doesn't have the information they want. It's not that you've generated a bunch of false facts, you've taken them to other facts, they just happen to be the facts that they didn't want. And so, worst case, they have to go and find it. But if you take them to the screen and the information they want is there, you've navigated them to real facts. So navigation is different from generation. You have to rethink how you're using the tool and use it in ways that make sense, use it to reason and navigate with real facts, not to generate false facts. So if you're using it to generate facts, like in this example where I generated facts about Jules White and they're incorrect, that doesn't make sense. Now, if I said, here's a bunch of information about Jules White, summarize this information, that's probably going to give you pretty reliable facts. Or even better, if you give it a very limited set of things and say, which of these pages may have facts about Jules White? And it gives you an answer, and then the user goes and looks at it and can assess if those are facts about Jules White or maybe there are facts about another faculty member. Even better, you want to use it in a way where you put guardrails on it and you're not trying to generate random, arbitrary stuff. And then assuming that it's going to be factual, because there's a good chance it's not going to be perfectly factual.\n\nOne of the things we have to ask when we're trying to determine, is generative AI an appropriate tool to solve the problem is how much does it matter if the answer is perfectly correct or not? If we really care about correctness, then one of the very fundamental things we want is we want it to be easy to check if an answer is correct or not. Because often when we're going and exploring, can we solve this problem with generative AI? We have to be able to test it and understand are we correct or not. We also have to be able to go and check when we begin using it in the wild and the real world to solve problems if it's actually doing the right thing. One of the fundamental things is if we really care about correctness, we have to be able to check correctness. If we don't care that it's perfect and then it doesn't really matter so much. But if we really care about perfect answers, we really want it to be something where we can check the correctness. Let me give you an example of what this looks like. Now it turns out I'm going to show you an example where it's not something that we can solve with just ChatGPT or GPT35 we can actually go and solve it with tools like advanced data analysis by formatting the problem differently but I'm just going to show you an example where it doesn't get it right, and the fact that we can easily check it is a real important thing for us. Hopefully you're familiar with the Wordle game. Basically, the idea behind this game is you have a five letter word that you have to guess. You have five tries to guess it. After each try, you will know what letters you've guessed are correct, which letters you've guessed are correct and in the right position, and which letters you've guessed are incorrect. Then based on that, you'll be able to, through process of elimination, hopefully guess in a number of tries the correct word. Now I've got a partial wordle problem. I know that I need to guess a five letter word where the second letter is E, and the last letter is T. Through a series of guesses, I have eliminated a subset of the letters. I've eliminated the letters R, Y, U, S, and a whole bunch of other things. Now I'm going to phrase this as a prompt for GPT35. I'm basically going to ask it, I need you to think step by step how to solve this problem. I need you to find a word that fits the pattern. BLANK,E blank T. The word cannot include any of the letters, and I have these highlighted letters. The word is a standard word in the English language. Explain step by step how to solve problem for possible words and in your plan and what is the word. Now I Probably could prompt engineer this all kinds of different ways. But it turns out it doesn't really matter how I prompt engineering. This is not the best use of ChatGPT, but the fact that I can fact check it is really important because think about it, it's really easy to go and apply the rules of this game to any answer it gives us to reverse engineer, is it correct or not? Now, it isn't easy to generate solutions necessarily, but it's really easy to fact check them. If we go down here, it gives us an answer at the very bottom which is beset. Is that a correct answer or not? Well, it's really easy to check because we can just go back and say, okay, does it match the first pattern? Does it have an E in the second position, and a T at the end? Well, yes, it does. Now, does it follow the rule of the prohibited letters? Does it have R, Y, U, I O, P, A, S well we know it has an S. It's absolutely wrong. That took 2 seconds. I can go, in fact, check it and see if it's right or wrong, but it didn't really hurt me a whole lot to try it, because generating an answer was cheap, but also checking the answers was cheap. This is really important. I very quickly determined that it got the answer wrong. Now I could go and generate a bunch of different times. I could prompt engineer a bunch of different ways, I can see if I could get it generating correct answers, or I could go and use the right tool. Which in this case would be something more like advanced data analysis to solve this problem. But the key, if I really care about correctness, I want it to be easy to check for correctness. You could have it generate solutions to a crossword puzzle. They'd be easy to check and have some basic consistency checks. Are they correct or not? Are all the words crossing and have the right letters? Are they have the right number of letters? Are the reasonable answers to those questions? Those types of things. You could go and check all things. But the key is, if you're going to use it in a place where being right or wrong really matters, you really want it to be cheap to check the answer for correctness. You want it to be like the crossword or the wordle where you can look and say that's a right answer, I know it immediately, or as in this case, you can quickly identify that that's just not going to work. Make sure that if you're going to use it, it's an appropriate use if you require correctness, that you can check the correctness very cheaply, very easily, and you're not going to be struggling to check correctness.\n\nOne of the really important things when we're working with a use case for generative AI is we come up with examples where if we really care about correctness that it's easy to check if the answer is correct or not. Or another way to think about it is, is the process and cost to check the correctness of the answer lower than actually producing the answer in the first place ourselves? What we want to look for is places where it can generate answers for us, and the cost for us to check the correctness of them if we care about correctness, is very, very low. Now, I'm going to show you an example where this is not the case, and this is what I would describe as an inappropriate use of generative AI. So I'm going to go into Chat GBT and I'm going to ask it, how might you translate computer science into Babylonian cuneiform? Now, why is this an inappropriate use? Why does this not fit that model that I just talked about? Well, for one, I'm not an expert on Babylonian cuneiform, I have no idea. And it comes back and it says translating modern terms into ancient languages like Babylonian cuneiform can be challenging, as these ancient scripts were not designed to represent concepts specific to our modern world. However, we can attempt to provide a transliteration of the term computer science into Babylonian cuneiform using some creative adaptation. Please note that the resulting transliteration may not have a direct equivalent meaning, as the concept of computer science did not exist in ancient Babylon. It's more an approximation of the term. Additionally, since this is an ancient script, different variations and stylistic choices can be made when writing the translation. Here is a possible transliteration. Now, what do I do with this, right? I'm not an expert on Babylonian cuneiform. I have no idea if transliteration at all makes sense in this case, if it's appropriate based on cuneiform or culturally or any of these other things. And now I've got a bunch of symbols that look like cuneiform printed out on the screen. What do I do with this? Well, I would have to go and try to fact check it. Well, how would I fact check it? I'd probably have to go to the library. And after going to the library, I quickly discover probably that it takes vast expertise to be able to go and transliterate computer science into Babylonian cuneiform. So then I'd probably have to go find a real expert, an actual human being, and I'd have to talk to them. And I would say, does this make sense? Is this a good translation? And by the time I've done that, I'm basically in the place where I should have just gone to the human expert in the first place and asked them. So if I really, really care about correctness of the answer, but I can't easily tell if the answer is correct or not, that is a very bad use case probably. So is this example of Babylonian cuneiform? I don't know if this is correct or not. And the effort that it's going to take me to determine if it's correct or not is going to be equivalent to me actually appropriately finding the answer in the first place, which is going and talking to the human expert and getting the real answer and getting the real nuance and discussion behind it and getting the richer context. So although I may be able to try to solve the problem, in some cases, it may not make sense if I really care about correctness and I have no easy way of checking the solution. And so this is a great example of something that you don't want to do with generative AI, a totally inappropriate use case. So if your use case really requires correctness and you can't easily check the correctness, you shouldn't be going forward with it.\n\nA really great appropriate use of ChatGBT and other generative AI tools is basically to help people overcome roadblocks, to start thinking. Basically to generate content or answers where it doesn't matter if the answer is right or wrong, because the real point of this is to help get the human thinking, to help the person start thinking about what are pathways forward, or to create that first draft of some writing that they can go and iterate and edit and improve and do something with. So in these cases where right or wrong isn't really what it's about, really the purpose of it is to be some seed of inspiration for the human. This is a great use case. Now I'm going to give you an example of what this looks like. So I'm going to think about, I'm planning a faculty workshop where I'm going to invite a bunch of participants, they're going to go and register somehow and then I need to distribute Zoom links to them. How am I going to go about solving this problem? Now, I could go and think about individual ways, but what I want is some additional ideas, some fodder for me to go and think about what is the right way to solve the problem and analyze it. So I'm going to ask ChatGBT, give me ten possible ways that I could allow people to register for my workshop and then distribute Zoom links to them. Give specific and actionable plans that include concrete products, services and tools I could use. So what does ChatGPT say? It says, certainly, here are ten possible ways that you can allow people to register for your workshop and distribute Zoom links to them. Now, notice, it doesn't matter if it's right or wrong. Worst case, I'll look at one of these and I'll say, is that even a product? Is that real? Will that really work? And I can start thinking about and exploring and probably I'll figure out what is and is not real, but it's going to start me off in the exploration process. Now it turns out all of these are pretty good. So Eventbrite, Google Forms and Mailchimp, WordPress with WooCommerce, JotForm, Typeform with Zapier, Mail hippo, Facebook Events, Mighty Networks, Constant Contact, Custom Website with Stripe. Now, most of these I didn't even know would be a good solution for this. I've heard of most of them, but I've never even realized that I could probably go and use them for this purpose. Now, certain ones I've used before and so like Google Forms, I've used Google Forms before, but I've never thought about connecting it to Mailchimp. So really what's beautiful about this is I'm now informed in a different way. And maybe some of these don't work out, maybe they're not perfect, but the key thing is it's giving me a jumping off point to go and explore and it doesn't matter if it's perfectly right or wrong. Now, let me give you an example of doing this again. Well, now I've got all these different options, but I really don't necessarily know right off the bat how to rank them, score them, sort them based on what I'm trying to achieve. So I'm going to go in and I'm going to refine the output, I'm going to take this potentially completely incorrect answer and I'm going to start refining it and working with it to help me think through maybe how to approach it. Turns out I think it's a pretty good answer that it started off with, but I'm going to help it sort it for me. So score these based on least effort, and most, sorry, lowest cost, least effort, and most automated. And it's going to go and resort them based on its perception of lowest cost, least effort. So Google Forms and MailChimp. Well, Google Forms, I knew that was low cost because I've used it over and over, but there's also lots of other things like Facebook Events. I never really thought about doing this with Facebook Events, and that could potentially work. Or WordPress with WooCommerce, well, that might be low cost, but that seems like a lot of effort for me to go and set up. And we actually see that reflected because Facebook Events, Eventbrite, and Google Forms with MailChimp or its perception of least effort. Now, I don't know if I completely agree, just kind of with a cursory understanding of how these work, if these really fall into least effort, but it is sort of helping at a high level for me to sort through this and think through it. And then most automated, we've got another set of categories here with Eventbrite, which probably makes sense. It is probably custom-designed for this use case. Now, the cool thing about this is I don't necessarily know if these are exactly right or not, but that's really not the point. I'm seeing lots of options. I'm starting to think about it. I'm starting to think about what questions, if we really look at up here, what we're seeing is it inspired questions for me of like, okay, well, here's the options, here's my follow on questions. It's getting me thinking, it's getting me asking questions about each of these groups of tools and how we might go and think about sorting them. That's the key part. It's helping to sort it. And then, I'm saying, okay, now let's think. Once we've got these new sortings, propose to me some possible new ideas, new solutions. Now, they're not totally new, but it's basically saying, okay, based on your criteria, here's what I'm going to choose for you. And it gives me Google Forms and MailChimp, JotForm, Eventbrite. Now, it's interesting because I actually did this one time in order to come up with using Google Forms, but I didn't think of using MailChimp when I did it. And going forward now, I would think to use those together, and I would go and investigate it. So I think, well, okay, what else can I learn about this? Again, it's not about being right or wrong or perfect, it's about helping me to think through it, to begin thinking and exploring. So I'm going to take it, and I'm going to ask it to draw the workflow. So, I know about Google Forms, but I didn't know about Google Forms plus MailChimp. So what would that workflow look like? What would I do in order to make that work? So I ask it to draw a mermaid diagram of the workflow for Google Forms plus MailChimp that explains these steps that I will take. Now, note I'm using a custom user interface that I may talk about later at some point. Your chatGBT out of the box can't necessarily draw mermaid diagrams, but it shows you the potential of generative AI to help in these ways, to do things like generate any answer. So it generates the potential workflow for me. I would create the Google Form, embed it on a website or share a link, attendees would submit the registration form, Google captures attendee responses, I would set up MailChimp integration, create a new MailChimp list, mail Google Form fields, or map Google Form fields to MailChimp fields, configure the confirmation email, and then include the Zoom link and do all this. So now I know what the process roughly is. Now, it may not be exactly that, but I have a high-level understanding. Now, the core of this is it doesn't matter if it's perfectly right or not, because the real value for this was for me to start thinking. It was for me to engage, it was to expand my horizon of what possible ideas were. And this is a great use case for brainstorming, thinking through things, but it doesn't need to be the end. So whatever I do, this would be a jumping off point for me that then to go and explore and see, will Google Forms with MailChimp really work as advertised and described here, or maybe not? I'm still going to go and follow up, I'm still going to fact-check, verify, but it gives me different potential options to go and explore, even if they don't pan out. So, it's about inspiration, it's about thinking, the idea does not have to be perfect. And this is a great use case. It's a very, very appropriate use case for these types of tools.\n\nIt's really important that we talk about risk when we're talking about generative AI and appropriate use. Because we really want to think carefully and make sure that we don't use generative AI for tasks where there's huge risk to human life, to your reputation, to your whatever it is, where there's some hugely negative impact, particularly to a human being, if it gets the wrong answer. Because it's very, very possible that it could generate something that isn't correct. So we have to think about what does that mean from a perspective of how to appropriately use it. So I'm going to give you an example of something that's really risky, right, to go into the tool and say, is it safe for me to take this action? In this case, I'm saying, is it safe for me to take Tylenol? That's a risky action, right? If I have some particular medical conditions, is it going to be okay or not? Well, if it makes the wrong decision, then I could end up really hurt. And that's not the type of thing that we want to do. That's an inappropriate use of this type of technology and tool because it has huge risk for humans. So we want to really make sure that we steer away from these types of things. Why else is this a risky and inappropriate use? Well, one, it's risky because it doesn't know much about me, like other than what I've given it in this prompt, which is nothing. I've just said, is it safe for me to take Tylenol? And it's risky because it doesn't have information about me. And if it lacks fundamental information that it needs to make a decision, then it could end up telling me to take an action that hurts me. And so we have to realize it's really, really important that we recognize that, one, these models rely on what we give them. And if we make mistakes and don't give them enough information or aren't specific enough about what we want, they could create an output that has a flaw in it and that could lead to something happening. Now the question is we want to make sure that when we use these tools, if we make a mistake, if we don't give it enough information, or if it makes a mistake and hallucinates something that that doesn't lead downstream to someone being hurt. So in a situation like this, how could we use it appropriately and take away the risk while still providing value? Well, let's think about you're going to go into your doctor to ask this question because the doctor has the information and they have the expertise to basically answer this question. But what might be valuable is to think about what questions should you ask your doctor or what things might you want to communicate to your doctor for them to come to a decision or to help you understand the decision or to help you understand what is or is not okay. So, for example, we could go into generative AI, and instead of saying, is it safe for me to take? Instead we could say, what are some questions I might ask my healthcare provider before determining if Tylenol is safe to take given my health conditions? That's a completely different use because it's all about helping me to communicate with my healthcare provider. It's not about trying to determine if there's risk or not, if it's okay or not. It's just about helping seed me and helping me getting a conversation, going to start a conversation. So it gives me specific questions. Given my specific health conditions, is it safe for me to take Tylenol? That would be a question that you could ask your doctor. And for example, if they weren't aware of conditions, they might say, what do you mean specific conditions? You have some conditions that maybe I had to haven't gotten the information about that. Maybe asking the question helps them to realize that they don't have all of your information that they need. They thought they did, but you said specific conditions, and their chart doesn't say anything about that. Maybe there's something that they aren't aware of, and maybe they ask follow up questions. Are there any interactions between Tylenol and my current medications? And the doctor could look or explain all of these things are a valuable way that you could potentially use it. That take away the risk. So when you start thinking about UNICEF, generative AI, you have to ask yourself, am I using it a way that looks like the first case where I'm asking it to make a decision or to do something that if it's wrong, it could be profoundly hurtful to a human being or profoundly negative impact on our world? Or am I using it in a way where even if it gets it wrong, there's no harm, no foul, right? In this second example, let's say it generates a bad question, right? Well, what's the worst case? Your doctor just says, you don't need to worry about that, or that question doesn't really make sense in your situation. There's not really a big problem here, because at the very least, it's started a dialogue, it's created a conversation, and then they could say, but this is what's important. Or these are the other things you should consider that might spark a whole discussion. In fact, it's very unlikely that there's risk in this at all. If the risk is there, it's very, very low. Maybe the risk is that you're worried about your reputation and that you might ask a question. Although, I tell students all the time there's no such thing as a bad question. And I think that I really do think that's true. At the very little least, if you read the questions as a human being and you decide that it's an okay question to ask, it probably is. So the risk is very low that you're going to read something there and then ask it to your healthcare provider, that's going to cause a major problem. So you want to think about use cases that put the human in the loop, that take away the potential for harm to be done downstream, but also provide sort of value to human beings, like in this case, where it's potentially generating a conversation. That's what appropriate use looks like, that appropriately accounts for risk or shifts the use of the tool in a way to eliminate the risk. And that's what we want to do is we want to avoid these examples that look like the first where if it makes a mistake or we don't give it enough information or we word things in properly or we're not completely clear that some huge negative impact could come out of it.\n\nPart of what's appropriate really depends on you, and is the use case going to benefit you personally as a human being? Now I want to talk about what I mean by this and I'm going to go back to the classic thing that we heard so much of when ChatGPT came out, which was there was all this concern about, everybody's going to cheat on quiz questions, everybody's going to cheat on tests, we're never going to write essays again. I want to talk about and bring some nuance into that discussion because it really, unfortunately, it doesn't provide the appropriate nuance, and we just think about it as like, don't use it to answer quiz questions, don't do it, use it that way. Well, yeah, I agree with that. But then it also paints this picture that we can't use it in other ways to support our education and learning that are really valuable. I'm going to give you an example of how you can hurt yourself by using the tool. Let's imagine I've got to quiz and I've got to answer the question, explain the transformer architecture and how attention works. I've got that on some computer science quiz , I have no idea, I forgot to study, I copy and paste that answer or that question in, it generates a bunch of text. I don't understand any of it, I take it and I turn it in. None of that benefits you, that's a terrible use case. It's a completely inappropriate use because it hurts, one you're learning, two it puts you at risk because you may now be accused of cheating because you are, and unless they've explicitly told you, go take this question that you know nothing about and answer it with ChatGPT, and come back to us, unless they've told you to do that, you're probably cheating, it probably provides you no value. Now, how could we take this same thing and use it to provide value? Well, let's imagine I'm trying to learn about transformers and understand them , and understand attention. Well, then it's perfectly appropriate to put that in there and to ask as a starting point to explain it. Now, it may not explain it perfectly and make errors in its explanation , that's potentially possible. That's another part of it, is I can go and use it as a jumping off point, a beginning of my exploration, a first draft of my understanding. But I'm always going to have to go beyond. I'm always going to have to take what it does and use it as a framework to now fact check, edit, revise, improve upon. If I go and I just cut and paste this in there, generate an answer, turn down in a quiz question, totally inappropriate use. If I use it as a jumping off point for my own education, it can be really helpful, especially if it's a jumping off point for thinking about things that may not be easily accessible. For example, let's say I have to go and explain transformers, and I'm going to be explaining it to a group of financial professionals, financial service professionals. I may not fully understand what they know about the domain or what they don't know, and I'd like to use analogies and terminology that are going to be more familiar. I could go and brainstorm and explore the concept of how do I teach this? I would go and ask, \"How might I explain transformers to financial service professionals using analogies from their domain?\" I'm doing something very similar to the first example, but now I'm doing it in a way that benefits me. I'm not trying to cheat on an exam, I'm not taking away who I am as a human, I'm trying to deepen my understanding. It comes up with a number of different analogies to explain the concept of attention. Now, this is useful to me. Analogies may be imperfect by design, and I can go and talk about it. It can spark conversation and interesting things, it's going to help me as a human being. Now, I might also be struggling with the concept of transformers as a student and I don't want to tell students, don't go and look at it, don't use it because it can be a really valuable learning aid for them. I could go in and I could say,\" I'm struggling to understand the concept of attention and transformers. Could you give me some simple step by step examples of how it works?\" It's great here, it comes up with all these step by step examples. They may not be perfect, but they help me to start to get a sense of what are all the steps. We have this concept of query vectors, key vectors, value vectors, and transformers are the underlying model behind all these large language models. Now I could start going exploring it and understanding how might ChatGPT or GPT-4 work underneath the hood. But I might also look at this and I might say, \"Hey, that explanation doesn't help me. Go and explain it to me, but don't assume a background in statistics or linear algebra.\" So it goes and reformulates all of its descriptions. These are examples of appropriate use of the tool to further my education. The first one is taking away from me, it's not helping my education. This is some of the nuance you have to think through when you're using the tool. Are you using the tool in a way that benefits you as a human, and particularly benefits you in the long run? Are you learning from what you're getting out? Are you viewing it as a draft to inspire ideas that you're going to go and follow up on as a beginning of the journey rather than an end? This is the beginning and the end of the journey. I'm asking the question, I'm generating an answer, I'm doing nothing to improve on it, and I'm cutting and pasting it into the quiz; exactly the wrong use case. Down here, I've got the beginning of a journey. I've got the generation of a bunch of analogies in which I'm going to take and think about, maybe refine, improve upon, think about how I'm going to talk through them, and then I'm going to actually go and present them to the financial professionals and do this explanation. It's a beginning of a journey, and it's a beginning of a journey that I then edit on. This is one of the really important things. For this tool to be really valuable to you as a human, you want to think of everything that comes out of it as draft. It is your job to fact check it, revise it, edit, and improve upon it, so that it's helping you, it's boosting you, but it's not replacing you. If we go back to the first example, if you are a student and you're thinking, \"Well, why don't I just copy and paste on my quiz questions, and cheat my way through?\" I'll tell you the number 1 reason why. If you cut and copy it and paste, and that's all you can do is copy and paste your quiz question, you can be automated out of a job. Somebody can easily go and just create some automation to copy and paste the quiz question or the equivalent question or task into this tool and generate the answer and there's no need for you. Unless you are taking the output and viewing it as draft and editing, improving and doing something else, you're doing yourself a disservice because you're putting yourself into a position where you're not going to be replaced by AI, you're going to be replaced by somebody using AI. Because that person using AI is going to build a workflow that automates that step and eliminates the need for you to manually copy and paste. When you're using these tools, part of appropriate use is thinking, how does it benefit you as a human? If you're not using it in a way where you're taking the output, you're learning from it, you're using it as a draft or a beginning, that you edit, you improve upon, that supports you in going further, you're not using the tool appropriately and you're using it in a way that's going to lead long term to negative outcomes for yourself. One of the most important aspects of appropriate use is you use it in a way that benefits you and you're learning, and that is a jumping off point for you to improve upon what it has produced."
  },
  {
    "index": 2,
    "title": "2 ACHIEVE Framework",
    "content": "The ACHIEVE Framework We should approach AI with a focus on augmented, not artificial, intelligence. Augmented intelligence is about using AI to enhance human abilities rather than replace people. The goal is to create an \"exoskeleton for the mind\" - tools that amplify our creativity, productivity, and problem-solving skills.  Augmented intelligence keeps humans firmly in the loop. It provides aid and assistance, but relies on human guidance, oversight, and improvement. This ensures people remain integral to the process rather than being automated away. Augmented intelligence combines the strengths of humans and machines to help us think more critically, get outside our biases, inspire new solutions, and accomplish great things we couldn't on our own. It expands what is humanly possible. This collaborative approach recognizes that there are unique capabilities possessed by both humans and AI. The key is utilizing AI in a way that augments and complements our distinctly human skills and spark.  The Achieve framework provides an approach to using Generative AI in a way that augments human intelligence rather than replaces people. The goal is to enhance creativity, productivity, and problem-solving by combining the strengths of humans and AI.  The key aspects of the Achieve framework are:  Aiding human coordination - Use AI to help teams coordinate better by summarizing discussions, clarifying ambiguities in plans, and identifying potential conflicts.  Cutting out tedious tasks - Automate repetitive and mundane work so people can focus their time on more meaningful and engaging activities.  Helping provide a safety net - Have AI review work products to catch mistakes and errors a human could miss.  Inspiring better problem solving and creativity - Interact with the AI output to stimulate new ideas and critical thinking. Treat it as a starting point, not the final product.  Enabling great ideas to scale faster - Use generative capabilities to rapidly prototype and expand on creative concepts in ways not humanly feasible.  The Achieve acronym summarizes this as:  A: Aiding human coordination  C: Cutting out tedious tasks  H: Helping provide a safety net  IEV: Inspiring better problem solving and creativity  E: Enabling great ideas to scale faster  Adopting this augmented intelligence mindset ensures humans remain integral to the process rather than being automated away. The key is leveraging AI's capabilities while still valuing, guiding, and improving upon its work. Used appropriately in this manner, generative AI can transform what individuals and organizations can accomplish.\n\nI'm passionate and excited about a particular vision of where we're going with generative AI. It's not artificial intelligence that tries to replace and supplant people, but it's about augmented intelligence where we amplify and augment human creativity and problem solving skills where we basically give people new capabilities. It's like an exoskeleton for the mind that can amplify that human spark within you. Enable you to create and do more interesting things than ever before. But not something that's going to take away who you are, but allow us to spend more time thinking critically. Creating new and exciting things that we couldn't create before. Have more time to interact with each other and be human. That's what I think the real potential of this is, and not going and replacing people and I think it's really important that we approach generative AI from that standpoint. So I'm going to give you a framework to think about how to appropriately use generative AI. The framework that I use is called ACHIEVE. This is a framework that I created to help think about what is the right way to use it to achieve this vision of augmented intelligence, as opposed to something that's going to take away from us by trying to replace us with this artificial substitute. We've seen how artificial things of all kinds don't really substitute for the real thing. ACHIEVE is about a couple of things. It's about how we use and think about ways of leveraging these new capabilities. One, to aid in human coordination and I'm going to show what each of these things means in a minute. To cut out tedious tasks, we can spend more time thinking, reasoning, and interacting with other human beings and less time on these things that honestly, probably people don't really enjoy. To help provide a safety net so that it can help us catch mistakes. It could be as simple as catching a mistake and spelling to something more complex, like we'll see in a minute. Then the IEV, I give it three letters because it's so important and this is the one that I think really, really matters when you're using these tools and approaching them, it's all about inspiring better problem solving and creativity within the human. You want the human thinking when they're using the tools, you want them getting excited and you want it sparking their imagination, you want them being an integral part of this thing. Not something that's sitting back and letting it do all the work. Then finally, we want to enable great ideas to scale faster by doing all kinds of things. Everything from the Pokemon, meal plan that I then turn into software and games like on the fly in a conversation, we want to enable scaling up and doing crazy things like that and these tools will allow us to do all of these things, scale up great ideas really quickly. So I'm going to give you an example, and I want you to imagine we're going to design a workshop, and then we're going to host this workshop and it's going to be a faculty workshop at a university where we're bringing in a bunch of participants and we have to solve and deal with a bunch of different issues from how do we communicate with the participants, to how do we plan the workshop, to dealing with unexpected issues and like childcare, to how do we follow up with them and communicate afterwards and be engaging. We're going to see a lot of real world problems that we see and we're going to use generative AI within the ACHIEVE framework to go and solve and make an impact. Let's start with aiding human coordination. Now we don't think of these tools as something that can help human beings, but they can be really helpful in helping us coordinate with each other. I've gone and I've recorded the first meeting where we were planning this workshop and I've captured notes from it. Maybe this is a virtual meeting where you just recorded the raw transcript or maybe you took notes and type them in. Now I'm going to tell it, act as my personal assistant, read the following meeting transcript and provide me a summary of the key points of discussion, and it goes through and reads it and that summary could be really valuable in itself. Meeting summaries are great, but I'm going to do something more important with it for human coordination. I'm going to tell it, list any ambiguities in the plan that the team should address. How often as human beings do we build plans but then they're ambiguous and we don't coordinate well, because one person interprets something a different way than another person and then that leads to issues. That's a really important thing to be able to do. You can imagine sitting at the end of a meeting just putting in the meeting notes and saying, hey, what's ambiguous about this? Let's talk about it or taking in a specification or plan that was written in advance and getting feedback on what might be ambiguous. Now let's say it's wrong. Well, what is the harm in that? Nothing. In fact, it's good because as a team, we will then talk about. Here's what it said is ambiguous, do we think it is or not and why? And then we've basically gotten more insight onto what we're doing and we can make sure that we are not missing something. We're getting a second pair of eyes on our human coordination. It says, for example, the exact number of registrations for the July workshop is not provided in the transcript. This information is critical for planning and logistics. Then it says, there's no clear deadline for when John should provide his draft for review, which could lead to delays or miscommunication. It's going out and actually reading the notes and the plan and then coming up with really concrete things that aren't clear from the plan. Now what else can we do with it? Well, we can think about who should follow up on it. So we can say for example, who should follow up on each of the listed ambiguities, and if we have a transcript where it's actually recorded who said what, it can tell us, for example, the number of July workshop registrations, which it said was an ambiguity. Sam Gray, who was directly asked about this, should provide the information to Daniel Brown. Or if we look at the deadline for the draft review, Reema Mistry who initially requested the update, should set a clear deadline for John Burke. So it's beginning to help us as a team coordinate better and think through ambiguities and way of solving the problem. Let's talk about the C, the cutting out tedious tasks. We spend so much time everyday on things that aren't maybe the best use of our critical reasoning and creativity skills as human beings and honestly, a lot of times they're tasks that make jobs hard to deal with and make people want to leave. So how do we use it to cut out tedious tasks? Well, in the planning of this workshop, we've created a survey and we've allowed people to in free text write, how are they using ChatGPT today and anything they want. Now, as the person who's going to teach this workshop, I could go and manually try to cut and paste that information into different segments of the audience and try to figure out who's using it different ways and do all that analysis, but that's a lot of work. So I'm going to upload the survey responses, I'm going to say this is the list of people that registered for my workshop on prompt engineering and ChatGPT, describe the data in this file. Then I'm going to follow up and I'm going to say, the registration had, how are you using ChatGPT, Bard, Bing, etc, as a question. Can you turn this into a list of usage and then group the responses underneath your types? I'm asking it to look at that free text response and categorize or group people together and group their responses under these different categories. It comes back with categories like, not using it yet, not using it at all, and it groups all the different responses under different categories automatically. Now, I could have copied and pasted and come up with all these categories and done it manually, but I want to spend my time thinking about my audience and thinking about how best to communicate with them. Then I can go and I can think about, well, let me think of other ways of grouping and I can quickly have it regroup everything so that I can look at it in different ways. This is one of the real powers of the tool as I can quickly go and look at different groupings. Before it would have taken so much time, so much tedious cutting and pasting and reorganizing that I wouldn't have explored many different ways of segmenting my audience and understanding them, but now I can just say, give me three more possible groupings and it says, okay, well here's a grouping by specific task, not using it, writing and communication, coding and development, research and teaching. I could then follow up and look at those and look at the individual responses and have it shape all of that. But the key is it gives me more time as a human to think about how I'm going to interact with these other human beings. It's taking away that tedious task, so I have more time for the thing that really matters. Let's talk about help providing a safety net. This is something really powerful that we can do. We have to worry about as humans, making mistakes all the time. We'd like to try to make it less likely that we would or that we can catch them. One of the things that I'm thinking about is I'm going to go and present this material and I presented it to a different audience previously. Maybe I presented it to a bunch of computer science professors, and so they're going to have different terms that they're going to be familiar with. I don't want to start my presentation and realize I'm using all these terms that I haven't defined and make the mistake of losing my audience in the first 10 slides. I upload my 120 slides to ChatGPT Advanced Data Analysis, and I say, read my first 10 slides and tell me any technical terms that I used that I haven't defined and an audience of manufacturing business executives might not be familiar with. It comes back with, for that particular audience, here are possible terms. It's providing me a safety net in terms of my presentation. Now maybe I won't lose my audience because it's giving me second sight on what is in there that maybe I haven't defined, that maybe I need to talk about and explain in more detail because my audience has changed. Let's look at something that we can do in terms of coordination and a safety net. This is a huge problem when we're building software, but it's also a problem in other domains. That is, we're working on a problem. We have two teams that go and work independently on different parts of the problem. But when the teams come together, they realize they've made conflicting decisions, and in software the miss means that we can't integrate our two pieces of the problem. Our two pieces of the software don't work together. Now we've just wasted all this time and effort and money where in the case of this workshop, maybe we have two teams working on, one on logistics and one on the overall planning, and they've made conflicting decisions and that could be a problem. Let's try to solve that problem. I say I'm going to upload the second team's meeting notes. And I say a second team working on logistics for the workshop just met. These are their meeting notes. Please point out any potential conflicts and decisions made by this team and the other team. It comes back and it says venue size and layout. The logistics team suggest using the university's main auditorium for the sessions, which conflicts with the initial plan to conduct the sessions in a smaller setting, presumably in Conference Room B. Or IT requirements. The logistics team list of necessary equipment includes a high resolution projector, multiple wireless microphones, a good sound system, and several laptops. This contradicts the initial team's IT requirements, which were likely based on a smaller room set up. Now it's helping us catch not only mistakes in our individual things like presentations, but also coordination, potential mistakes that could be really impactful and helping us, do a better job as humans, and I love it when my mistakes are caught before they become a problem. Like before I'm giving the presentation or before we realize we've scheduled the wrong room. Now let's talk about the IEV. This is the most important piece from my perspective, and it's all about inspiring better problem solving and creativity with humans. Now we hear all the time about how AI is biased, but we as humans are incredibly biased. We actually produced the biased data that led to the training of AI. But what's interesting is it can help us overcome some of our own biases if we use it correctly. We have biases in problem solving, like we know one way of solving a problem. So we automatically try to solve every problem that way. We have bias in confirmation, we have confirmation bias on data. We're going to look at it and we're going to try to see what we want to see in it. So one of the uses of generative AI is to help us think outside the box, to be more creative and inspire new ways and new solutions. We've got an interesting problem with our workshop, and that's that we need to provide childcare. We've looked at using our audience segmentation from our free text forms, and we've realized it's all fourth graders and it turns out they're all interested in Pokemon. Can we build some interesting content that goes along with what their parents are learning, but is appropriately targeted to this younger audience. We say, please provide three different 90-minute lesson plans for teaching large language model concepts to fourth graders using Pokemon examples and hands on exercises. It comes back with different lesson plans, like what is a large language model and how it works. Imagine a game where you describe a Pokemon, and I have to guess which one you're thinking of. The better your description, the easier it is for me to guess, highlight how a large language model is similar. It tries to guess the next word based on the words that have already seen. What it's doing is it's then giving us ideas that we can go and inspire. We're not going to just take them, we're going to use them as draft output that we as humans respond to. We edit, we improve on it. In fact, that's one of the most important things that you as a user of this need to know is that, you have to engage with it. You have to take it as draft and you have to improve and iterate and work on it. You have to inject your thinking and creativity into the process, otherwise you're going to be automated out. Realistically, if all you're doing is copying and pasting quiz questions and getting the answers to cheat, you're not going to be employable long term. But if you're somebody that goes in and engages with the tool in creative and interesting ways. If you view the output as draft and you're going to edit and refine it and improve on it, that is the philosophy that you need to have with the tool, and that's the philosophy we need to encourage with people is, don't take it for face value. Take it and really look at it. Really understand if there are flaws. Really do your work to improve it. Then get really creative and insightful and use it to solve bigger and better problems. Now let me give you another example of this. I've got these 120 slides and I'm going to upload them and I'm just going to ask it to summarize them. But then I'm going to do something that's really valuable. After this summary comes in, I'm going to go and say, act as a skeptic of everything I say in this presentation. Find flaws in my assumptions, assertions, and other key points and then generate ten hard questions for me, and what does it do? It comes back with, here's the assumptions you're making in your slides. You have the assumption that ChatGPT and similar tools will bring incredible increases in productivity. Question, isn't it possible that these tools might also introduce new challenges and inefficiencies, such as the need for additional oversight or the risk of misuse. That's a great question and I should address it, and by going in and using the tool, it's inspiring me to think more critically, to get outside the box of my perspective on this particular content and start improving and thinking, it's not replacing me, it's making me think. Then I respond and I think about how to address that question. That's a fundamentally new capability. Now let's look at the E. Enabling great ideas to scale faster. Now we've seen a lot of examples of wild new capabilities and things we can do, but I want to talk to you about this one. I'm imagining at the end of this workshop, I want to go and I want to send personalized emails to every single person to get them excited about ChatGPT or Generative AI, or Cloud, or whatever it is that I want. I need some way of engaging that audience, I come up with this idea. It would be awesome if in every one of my e mails there was a custom prompt that was specific to that person's department. What they do that showcase something that you could potentially do with ChatGPT. But for me to go and try to scale that idea up would be really hard, right? To think of prompts for every single department and have the domain knowledge to me and make them even close to relevant is not going to be easy. How do I do that as an individual person? Well, using a tool like this, I can scale that idea up. I say I'm going to create customized post workshop emails for every attendee, I'm just following up in the conversation. I want you to create a list of interesting ChatGPT prompts and ways of combining large language models with each discipline or department. Basically, I'm telling it generate prompts that people could then take out of those e mails, copy and paste into ChatGPT, and see it doing interest, something interesting and relevant to them to help get them excited to go and engaged. But also notice what's happening. We're asking ChatGPT to come up with ideas for how other people could talk to itself. How would people go and talk ChatGPT? It's proposing ways that you could talk to it. That's a wild new capability that's trans-formative and and it comes back and it has a whole bunch of different ideas and I'm going to zoom in on two of them. Here's the marketing prompt, that somebody from marketing could cut and paste, design a comprehensive marketing campaign for launching an eco friendly product incorporating digital and traditional channels. Now they could copy and paste that out of the e mail and see it doing things that are relevant to their domain or alumni relations outline a year long engagement strategy to reconnect with alumni including events, communications and collaboration opportunities. They could copy and paste that out of the e mail. Now if I had to go and think of all those ideas from scratch and write them all down, I couldn't scale this idea. Now the important thing was the idea of the template that I had in my idea of how I would do this and then I'm using that template to scale up. It's all about that ability to go and scale things up, it's all about augmented intelligence, not about replacing people, not about this artificial substitute. It's about that exoskeleton for the mind and using it through the achieved framework to augment, amplify our creativity and problem solving skills. We can put that exoskeleton on and do amazing, amazing things. What's coming next is the tools. Now we've done all of this so far with basically cutting and pasting in and out of a chat interface. But the future is going to be in these tools that integrate Generative AI into the tools we use in the workplace every day. They're going to transform what we can create, how fast we can create, how big of problems we can solve, how creative we can be. It's going to give a totally new medium for expression and how we access computing is going to fundamentally change and the future of computing is Generative, AI as our interface to it, of being able to radically change what we can do and how we can control these things. When you're thinking about how do we engage with this, you have to think about it from this augmented intelligence perspective. Think about the achieved framework and how can you go and apply that framework in your discipline and your domain to do things that benefit people, that give them that exoskeleton for the mind, not that replace them in some artificial way.\n\nFiltering is an operation, it's a very appropriate use case for generative AI. I'm going to show you what I mean by filtering. Now, before I start this filtering discussion and get deep into it, there's a prerequisite here. That is we're filtering information that this user should have access to that's appropriate to provide this user. There's a fundamental question that becomes before this, which is we're going to filter in some other way to make sure that the information that's going in is only relevant things that are appropriate from a security or whatever perspective for this user to have. we're not doing this as a first line of filtering, as in deciding is it okay or not for this person to have access to these social security numbers and letting generative AI decide, no, we're not going to do something like that. What we're going to do is we're going to filter for other reasons to support human reasoning. What we're going to do is we're going to give them information. We're going to give human information that we would give it to them anyways. But we're going to help make it more efficient to reason about and work with by performing filtering with generative AI. I want to start off this whole thing with Cove audit from that perspective is we're not using generative AI to decide if this is going to violate somebody's privacy or something like that to give the information to the user, but what we're doing is we're using it to help enhance, augment the user's reasoning with information that's appropriate for them to have. Let's take a look at what that looks like now. People tell me all the time, and I've seen it over and it can't cite anything. Generative AI, you have to be careful. It generates fake citations. It can't cite anything. Now, I'm going to start off by pointing out that that's not actually accurate. That's inaccurate statement. If used incorrectly, generative AI cannot cite where things came from, cannot provide citations. I'm going to show this as a very specific example. I'm going to give generative AI a bunch of my citations of journal papers I've written. These are all citations. then I'm going to say which of these papers might be related to health care? It's going to go through and it says the following papers might be related to health care and it gives me a list of my papers. These are essentially citations. Now this is the key is when we hear these statements like it can't provide citations, it can't do that. What we're really hearing is I'm using generative AI in some inappropriate way. Now, it probably can do some of the things that people are talking about. In this case, it is giving us citations. It's just that you can't go and give it a prompt for, give me random citations related to some topic. But if you give it a bunch of citations, it can filter them very effectively and give you back citations that are relevant to the question that you're asking. If you give it information you ask it to filter, it can cite back to the original information and tell you which of the new items are relevant. Now notice this is a filtering operation. One of the important things about this is a, being able to go back a filter means that we are taking the original information and we're reducing it in some way. One of the things we want to think about is what we talked earlier, that is we want to be able to check the correctness of the output really easily. so the way that we do that is whenever we're doing a filtering operation of some kind to make sure that it's appropriate, we want to go and have identifiers of some kind in the original information so that when it filters it down to a smaller list, we can look at the identifiers for what's left and map them back and make sure that they really did exist in the original list. This is why it's a really appropriate use case is because the fundamental operation is we're going to end up with something that's a subset of what came before of what we gave it, and so we can always go and check are all the things that are left in the subset things that we gave it. The other important thing is we're going to do this to augment human reasoning for tasks where the downstream impacts aren't catastrophic, like making an incorrect health decision. On the other hand, we could say filter for papers related to health care to help somebody figure out what things that I've written might be related to health care. that would be an appropriate use and it'd be perfectly fine to do that and filter. We have one dimension which is we want to make sure the downstream task, if I reason incorrectly as a human, that I'm not going to go and do something really bad. We want to be careful in those use cases. Two, when we do this, we want to do it in a way that it's easy to fact check. In this case, when we filter down, we get a list of papers and we can easily go and check those and see if they appeared in the original list. Now there's another way we can do this really easily. We can put line numbers or page numbers into things and give it text with all this. I've got a bunch of line numbers attached to all this text, all these numbers or line numbers, because these lines wrap around, it doesn't look like individual lines, but it's a bunch of line numbers. Now I can ask it, show me all of the lines that explain what a prompt is and include the line number from above. Notice what I'm doing. I'm asking it to filter the information down and then I'm telling it to tell me and make sure and include the original line number, the identifier of that, so that I can go and check it for consistency and correctness. What we get down here is it gives us all the line numbers in the original text, where we talk about prompts in some way and what they are and explain them. Now I can go and look and I can fact check it and the fact check is easy. The fact check doesn't look like going and checking Babylonian cuneiform anymore. It just looks like is this on line 17 of the original text? Is this on line 20? Let's check line 17. Writing clearly is also important for conversing with generative AI. We could go and look at line 17 and it says, writing clearly is also important for conversing with generative AI and so we could easily go and fact check that. We could probably also automate the fact check, so that the human doesn't even have to do it and we can detect problems before they occur. Now, we can also do fact checking even if we don't have identifiers. A filtering operation is something that's eminently checkable. And this is what makes it such a safe, appropriate action is that we can go and check that everything it gives us back is something that originally filtered. Now it gets a little bit trickier if we start doing operations like summarization, which is a filter, but it's a filter plus some transformation. Similarly, if we go in and have it filter and explain the information, that also gets a little bit more creative and what it can do. But there are ways to still help ground that, particularly when we're supporting human reasoning. I'm going to give you an example of these. I think of these as variations on filtering, summarizing, and explaining from these things. Now, they're not perfectly that way, but I'm going to lump them in here because I think it helps in thinking through the framework. And also because it shows you how using this concept of referencing and being able to go back to the source or traceability, back to the original information you gave it as really important. Summarize what a prompt is underneath each sentence of your summary, provide a quotation, including the original line number from the original text. Now this is a different type of summary. I'm filtering information down and I'm summarizing it, but I'm building in that self check that we talk about that's so important. And now what we see is a prompt is a message that a user sends to a generative AI tool which provides a request or instruction for the AI to generate an output. That's the statement that it's making and then it tells us where it's getting its basis for that statement, which is right here. Line 5, the messages that you send to the generative AI are called prompts, and the messages that the generative AI sends back are called outputs. I could go through line by line now of what it generated in the summary, and it gives me supporting quotation. Line 7 and line 12 for this one, and it gives me the text. One thing I could do is I could check do all of those supporting quotations actually exist? Did it get them right? Then as a human, I can look at it and say, do those statements appear to support that summary? In this case, it is probably correct here. It's really important to think about filtering is a very valuable operation, it's a very appropriate operation. Summarization, explanation, and other things that are related to that can also be appropriate if we're doing a couple of things. One, we are not giving the user information to summarize or filter, or do something with that they shouldn't already have access to. Two, we are going to preserve traceability to the original information. That is, things like line numbers, identifiers, quotations, things where we can go and look at what we're getting out and map it back into the original data. Those are things that are all really important. Then finally, we're going to avoid certain types of tasks directly. For example, if I'm going to try to tell you if you should make a particular medical decision or not, or I'm going to tell you to take a particular action or not. We want to be very careful with those things. And probably the more appropriate thing to do is to tell you who to talk to and what to ask them in order to discuss that decision. We're going to take you to humans now if it's something like this where we're supporting you doing research and learning and education about generative AI, perfectly appropriate to filter that information, summarize it, provide supporting quotation, so I can go back to it. But notice I'm using it in an appropriate way. I'm not just going and saying, tell me what generative AI is and hoping that it's correct. Or say tell me what generative AI is and give me citations. That's not a filtering operation because we didn't give it the original source material. That's what's different about a filter. We give it the source material. We have it basically filter out things from the source material. We get a subset of it, and that subset should be checkable back to the original. And that's what's so important about the filtering operation, is that it's eminently checkable.\n\nIdeation is a great use case for generative AI, it's very appropriate and it's something that really puts the human at the center. Because the goal of ideation is to inspire the human to think of things, to think outside of the box or their narrow sort of limits on reasoning. And try to come up with ways of helping them to see new ways to explore a problem, new avenue use to try to solve something that they're stuck on. New things to try out things that basically spark their creativity, their imagination. This is a great use case of it and what's one of the things that I find most fun about using generative AI. So what are examples of what you might do? Well, you could do a lot of things. You could do things like proposing a middle ground in a negotiation, looking for ambiguities in a plan. Thinking about and trying to understand how somebody might react to a presentation or a paper that you're giving, and thinking through it from their perspective, or thinking through and trying to empathize. What are the emotions that somebody in this situation might be going through, getting it to generate ideas, all kinds of things that we sometimes aren't that great at as humans. Now I'm going to do some sort of sillier, more fun things. I'm going to start off with a simple idea, generate five ideas for how I could pickle a banana. I don't even know if I can pickle a banana, but it looks like it's possible from this. Now after looking at this, spicy pickled banana slices, curry pickled bananas, sweet and sour pickled bananas, savory garlic pickled bananas, cinnamon pickled bananas. Some of these sound exciting, some of these don't. I don't really know if you can pickle a banana or not or if it's just going to dissolve into complete mush, but it's probably pretty low risk to ideate around this idea with it. So, worst case, I do some curry pickled bananas and a bunch of vinegar, water, sugar, salt and curry and it doesn't work out. But more likely what I would do, is take this idea of curry pickled bananas and then I would go and search the Internet to try to find actual recipes for it that give me enough detail to try out. But if I didn't do that and I stuck a bunch of bananas in a jar with curry and pickles and vinegar or whatever I'm going to do in this, I'm probably just going to get a not so tasty thing out of it. Now, you could be careful if it was starting to talk about meat and things that might have food safety. Maybe one want to be a little more careful here. Another example, you're thinking about a class, I teach a generative AI class around software engineering. So I could say give me six interesting projects for my software engineering regenerative AI class. It comes up with a bunch of different ideas. None of them are that particularly interesting to me. I just throw them all away and move on. And this is one of the great things is it's cheap to generate ideas. Doesn't matter if they're not great, we just look at them as a human and move on. But that's also part of this is as the human we want to be evaluating it. We are the one that's supposed to be getting inspired. We're not supposed to be taking it and taking whatever it gives us and letting that be our sole work product, but we're supposed to be using it to drive our ideas. Now I look at these and I'm like, none of these are right for my class. I could go and follow up, craft the prompt differently, do some more ideation to try to get there, but I'm just going to throw them away for now because they're not really that interesting. Then I thought it would be kind of fun to mess around with diagramming and I've seen some really funny videos recently on YouTube related to parenting. I'm a parent, obviously, and so I'm going to say create four interesting mermaid diagrams, visualizing different funny aspects of being a parent. Each diagram should have at least six elements. Well, okay, we've got parent sleep, laundry, endless chores, tiredness, keeping up with the kids. That's kind of funny, kind of humorous. I don't really connect with that one that much. This one I connect with a little bit more. You want your child to eat their vegetables and it's offer, offer a treat, threaten, yell or scold toddler, bargain one more bite for some cartoon or surrender, give up and let the toddler eat junk food. And honestly, this sounds like far too close to home on the things that I see happening both at my house and with other parents. Diagram toys, toys, toys, your child has too many toys. They can go to Goodwill or another charity. Group by type, switch up toys and rotation, put rarely used toys in storage and then bring out on special occasions. These are all the types of things that I've seen going on. Parent brains, they have all kinds of things there. But the kind of the key behind this is it's not about going and trying to replace me. It's about going in, generating something that I respond to, that I think about, that I have a reaction to, that then takes me in a new direction. And so it's a very appropriate use case when we're using it for ideation to support human inspiration and creativity. But it's not appropriate if our goal is to just take this and use it and not think about it as a human. And that's really more about you as the human being and how you're using the tool. If you're not using it to inspire you, then you're not really doing ideation, you're really just using it to generate answers for you and replace you. But if you're using it to think about the output and to get expired and to react to it and to have emotions because of it, that's an appropriate use case. And it's probably pretty safe use case for you.\n\nNavigation is something that I don't see talked about nearly enough with generative AI, because it's a great way of helping to eliminate or mitigate this problem of generative AI potentially producing inaccurate information. So what does navigation look like? Well, I want to show you some examples of navigation and why it's a really appropriate use case that makes a lot of sense when we're going to deal with particularly sensitive information. So there's lots of sensitive information where we can handle it appropriately with generative AI if we're focused on navigation. So let me show you what I'm talking about. So I've got a list of screens here for a mobile app for healthcare. So at Vanderbilt here, if you go to the medical center, we have something like the My Health at Vanderbilt app. Now, you can imagine you're a patient who comes into this app and you're looking for some information. And today you would probably go through and click through a bunch of screens to try to find that information. Now, another way you might be able to go and do this, that would not be an appropriate use. I think this would be an inappropriate use probably, is to have a patient be able to just go in and type in free text and say, hey, I want to know my last blood pressure. And you might get a hallucination, you might not, you could put all kinds of guardrails to try to ensure you don't. But that's probably not an appropriate use case because you're generating the information as text, right? And there's some trickery there to try to figure out is it completely right or not? It could be difficult. Now, blood pressure might be an easy one to check, but it could get really complex to do that. So we need to think about how do we deal with those sort of like that type of information that is very problematic if we get it wrong. But we still want to help users access it and find it and do things with it and that's where navigation comes in. We're going to take a little bit of the flexibility away in the power of generative AI, but we're going to do it so that things are safer. So we've got all these screens of our mobile app about healthcare. Now, how do we use navigation, generative AI navigation? Well, what we're going to do is we're going to say, whenever I ask you a question, don't answer, instead, tell me the screen that it is on. So what we want to do is, instead of trying to tell you what your blood pressure is, we want to tell you what screen in the mobile app the blood pressure is on. Or even better, the underlying generative AI just immediately transitions the app onto that screen. It doesn't generate the information, the information is already there, it just automates the navigation process for the user to take them to where that information is that's what we want. So now we can say, what is my blood pressure? And it says, I'm sorry, but I cannot answer that question as it is not on any screen in the mobile healthcare app. So that's great, it can tell us what's not in the app, that would save me a lot of time and effort as a user. And in fact, that could be one of the most frustrating things, is you're looking for something that just isn't there and you'd take you a long time to figure out of clicking around and knowing that it's really not there. Or if I say, when is my next appointment? It says, the information about your next appointment can be found on the appointment scheduler screen in the mobile healthcare app. Or even better, it could have just directly opened up that screen and navigated me there and popped it open and then I could have seen my appointment. Now, notice what it's not doing, it's not generating the information about the appointment and potentially getting it wrong, it's generating an idea of where in the app that information is, it's generating a reference to some screen. Now, let's think about what happens if it gets it wrong and it takes us to the wrong screen, we're just going to look at it as humans and go, there's no calendar information there and the error is a lot more minor. It just becomes a frustration rather than a major problem, if it tells you the wrong appointment time, that's a big deal. If it takes you to a screen that doesn't show you an appointment, you're just going to look at and be like, well, that was useless, it didn't help me, but it's not hurting you, so it's an appropriate and much safer way. Now, the reality is it's probably going to be able to find it fairly effectively and most of the time it's probably going to do a pretty good job, but if it doesn't, it's not catastrophic. Even though we're dealing with sensitive information, navigation is much, much safer, sort of fundamentally, because at the end of the day, the human is going to look and you're going to say, that's the information I want, or It's not the information I want. But if it's just generated something, the human can't look at it and say, that's fake, that's not the appointment time, my appointment was at 9:00 not 9:30. A human is not going to be able to easily see that, but they're going to be able to look and say, look, that's not an appointments, that's blood pressure. Or look, that's not appointments, that's the name of my foot doctor. No, navigation is much safer. Or if I go in and I say, what are my recent lab results? That would be something that would be horrible to just generate with generative AI, because it could generate all kinds of values. And particularly some inexperienced user looking at they're going to have no idea what those values are and if they're correct or not or within reasonable ranges. But if we say, what are my lab results? Then it can say the recent lab results may be accessible through the health tracker screen. But then it also says if the lab tests were recorded in the app, however, if you were not recorded your lab results in the app, the app will not be able to provide you with that information. So it could also take you there and if you don't see you, it might be able to explain why that might be the case. We might even not want that feature, we might just want to take them to the health tracker screen. But you can kind of see how if when we adjust the problem, when we think about solving it differently, when we think about solving it in terms of navigation, it becomes much safer. Because at the end of the day, navigation is about taking users to the information and users are going to be able to look and say, that's not the information I want. Or probably not, or there may be some cases where we want to be careful about navigation, but in general, that's going to be a pretty appropriate, fairly safe use case for it. On the other hand, if we are generating information, trying to show it to the user, we're trying to help the again, at the end of the day, we're trying to help the user get the information they want. But if we're generating the information they want, there's a lot higher chance that we're going to make a mistake than if we navigate them to where it is. And if we're navigating them, it can't produce something that's not already there, so if it's wrong in the app, well, the app already had it wrong in the first place. It's not the fault of generative AI, or if it's not there and they see that it's not on the screen they've taken it to, we haven't done something terrible, we've just taken the wrong screen in the app. It's like if you had a friend and you said, hey, where's that in this app? And they say, I think you go and click that menu and then that menu, and then they're like, it's not there. Well, it is on my app. Sorry about it. Well, no harm, no foul, not a big deal. So navigation is a really appropriate use and it's a great way, particularly when you're dealing with sensitive information, use the existing system. But just show them where it is in the existing system because a lot of those existing systems aren't the most user friendly. Or we may not understand where to find things, but we know how to ask and then it can help translate our ask into finding it in the actual system.\n\nOur expertise is critical in determining what's an appropriate use of generative AI and what isn't and making sure that we're using it in a safe way. And our expertise is going to determine the range of things that we can do with it. A lot of things that we've seen are really focused on how do we go and explore some information where we can't easily go and assess whether it's correct or not. But when you as a human have the expertise to look at the information and know if it's correct or not, it's a really powerful tool and it's a completely different ballgame. Because as my friend Bob Higgins likes to say, it takes you to 50, 80% really, really quickly on a lot of problems you want to solve but only if you have the expertise to evaluate the output and determine if it's correct. Everything that comes out should be considered draft but if you've got the expertise to take that draft and iterate on it, improve it, catch all the mistakes, then it's absolutely an effective tool to use and an appropriate way of using it if you've got the expertise to evaluate its output. Let's take a look at an example of this in some different ways that we may have the expertise to evaluate what it's doing. Let's start off with one of the most basic ones, which is I'm going to use it to help me write some content. I say, please generate a paragraph describing prompt patterns using the following points focused on documenting reasonable patterns for describing. It goes on and it gives some other points and then it generates a paragraph of text. Now, I have the expertise to evaluate this text and determine if it's correct or not. I have the expertise to determine if it's facts. Well, one prompt, patterns are like my thing. It's something that I'm deeply engaged in in building research around and publishing papers on so I know this topic really well and I can assess it. Second, I'm asking it to express my ideas. I'm asking it to write something for me that is based on my ideas. I'm giving it the bullet points that I want to go into the writing. You see right here, I've got all these bullet points that I want it to incorporate into the writing right here. Now that's my material, those are my ideas, and I have the expertise to then go down here and read and see, did it accurately express the ideas that I wanted it to express? Did it convey things correctly? I have the expertise because I'm giving it the ideas and I'm telling you this is what I want you to express. I have the expertise to evaluate if it is accurately expressing my ideas. I also have the expertise in the subject matter, and so this is a totally appropriate use. Now if I don't know anything about prompt patterns, and these aren't my ideas that I'm injecting into there, I don't have the expertise and it's an inappropriate use. Expertise is really critical. A lot of avenues open up if it's in your area of expertise. It suddenly becomes a productivity tool. If it makes a mistake, that's okay, because I have the expertise to evaluate it and fix it. Let's take another look at an example. Let's say that I want to know something about, why do they build buildings with steel rather than aluminum. I ask, please tell me about what is relevant to research if I want to know why steel rather than aluminum is used in structural columns for large buildings. Now notice what I'm doing. I'm not asking it for the answer, although it is going to give me an answer. This is an important point here is I'm asking what I could research, what can I go and look at to help find out this question? It's again, I'm starting to use it again in that navigation way but I'm also using it to I have the expertise to evaluate and go and look at and follow up on these ideas. That's the key, is it's giving me a jumping-off point that I have the expertise to go and follow up on. I have the capability to go and drill down on these ideas and know, for example, I know how I would go and find a steel is stronger than aluminum. Well, one way I could do it is I could go Google it. I might go Google that and find out, well, no, that's not necessarily true, or there may be more nuance to it than that. I could go Google that. Or if I wanted to know, hey, cost, is this a major factor in it, I might be able to go Google on the Internet, I might be able to go find a book, or I might be able to go and talk to somebody in civil engineering or mechanical engineering. I could go talk to friends that I have in industry and I could find out how to drill down on this. But it gives me a road map, a starting point, and I have the expertise to evaluate the roadmap, to use it, to take it, and to use it to go and find the things I need, and to evaluate if they're correct or not. I have the expertise to take things that it's providing and use it as a starting point for research, of going and looking for truth. Now, I'm not using it as the truth, I'm using it as my jumping off point and I have the expertise to run the process on this. It doesn't have to give you the facts right there. It just may need to give you a jumping off point to start a process, and part of your process is going to be in turn to determine if things are true or false. I have the expertise to go and do that in this case. Then I want to give you another example of where we can use it. We can use it to combine different concepts or different areas where we have expertise. In this way, I'm going to take some two things that are very distinct and different that might not be easy for me to think of how to combine and I'm going to combine them together. But I still have the expertise to evaluate the output, so I'm going to ask it, give me some ways that software architecture and design is similar to making pottery on a wheel. I do both of those things. I do software architecture and design and I make pottery on the wheel. Now it generates a bunch of interesting things for me. Planning and prototyping. Just as a potter uses a prototype to create a finished product, software engineers use planning and prototyping to create successful software designs. Well, that's true. I can evaluate that that's reasonable statement. Before starting to work on the software, designers first plan and prototype the software design to ensure that it meets the intended goals and requirements. Well, hopefully, they do that, not always. I could say that's mostly true, it just depends. Sometimes it's just ad hoc and on the fly and there's all these different statements. It would take me a long time probably, to think about how do you merge these two very different concepts together. When I use the tool to merge them together, one, the creativity, the idea of that I should even merge these totally dissimilar things came from me, now I'm merging them together, I'm getting some new output, but I totally have the expertise on both topic areas to begin assessing, is this correct or not? Another example I could go and I could say, give me some ways that software architecture and design is similar to making 17th century tapestries. Now, I know nothing about 17th century tapestries, but I do know about software architecture, so part of it I'll be able to go and assess, the other part I'm going to have to follow up on. But I have the expertise to go and find the right people, find the right resources to evaluate different claims. If I don't, then I shouldn't do this, but I think I could go and follow up on the other piece, and I have a lot of expertise in the first piece, so I could evaluate those parts that are related to the software, I could evaluate right there, and what I'm looking at, and then the other part about tapestries, I could go follow up on that and probably find out if that's true or not. Now, if I can't, I don't have the expertise, I shouldn't be doing it so I can mix things together. That's another great use case. Now a final thing I want to emphasize is that the safety of a lot of operations and the safety of a lot of tasks is directly dependent upon your expertise. Now if you are a medical doctor, the example that I showed earlier about taking Tylenol, maybe you can answer that question directly. Maybe it's totally safe for you to ask that question, because the purpose of you asking the question is to look at its answer and assess it. It's really fundamentally important what expertise you have in determining if it's appropriate use of the technology that's going to give you a trustworthy result. Now here's an example from my domain that could be highly risky to you if you are not a software engineer and you don't have the requisite expertise to evaluate. I'm going to say I need a sample Python program to automate deleting unused directories in my downloads folder, please show me the sample. Now if you don't have the expertise to do this and read this code and you take this program and you execute it on your machine, you are doing something extremely risky. You don't know if that code is correct or not, you don't know how it decided, what unused directories means, it might have come up with something that you didn't intend. It might have come up with its own definition of that, that's not what you wanted. If it translates that into code and you can't read it and you can't easily assess it. Now you could go and talk to a software engineer and say, is this appropriate or not? But if you yourself are planning on running it and make taking action based on it and you don't have the expertise to assess it, that's a problem. On the other hand, I can read this code, I can look at it, and I can say, hey, it looks fine, or no, it doesn't look fine, it doesn't express what I'm trying to capture or do, and so it's actually a totally appropriate and safe use for me because I have the expertise. This is one thing that's really useful and important to know. Is you really have to honestly assess your expertise and know it and in those areas, go fast, move fast, do all interesting things, but know that it's producing a starting point for you to take your expertise and check it, take it further, improve upon it, do more. The expertise of the user is fundamentally important and a lot of people talk about all these things, all these mistakes it makes, all the other things that does wrong, except that if you're an expert on that, it doesn't matter how many times if it gets 80% of it right and 20% wrong, because getting 80% right and starting off that far ahead of the game can be a huge benefit. Now on the other hand, if you're an expert and you know that it's going to be very hard to spot mistakes that it makes, even in your area of expertise, then absolutely do not go there. But if you're an expert and you know that you can look at it and you know that you can accurately determine what is correct or not, just the same way that if you had somebody working beneath you, a more junior person who was producing the initial draft of the solution and handed it off to you that you could accurately edit and check and improve upon what they did, then fair game go for it. That's the way to think about it. Like all the time, we rely on other people to give us starting points for things that have errors in it, and then we as experts, go and improve and refine and work on those things, and we need to start thinking about it in that context. If you wouldn't go to a lawyer and have them write 80% of the contract and hand it off to you and have you finish it, then you shouldn't be generating the contract and using it as it is. If you don't have the expertise to take it the rest of the way to fact check it and approve it, it's not the right place. But if you would and you would do that normally, I'm perfectly comfortable with somebody handing me a Python program. I'm perfectly comfortable if it's a large Python program, and I need to go and dissect it and understand it and test it and work with it, I'm comfortable doing that and I deal with that all the time. I have other people who are handing off code to me or I'm doing a code review, or I'm starting with some open source project that somebody else built and I need to go and refine, and edit and work with it, and that's okay, because that's within my domain of expertise. Expertise is really critical in thinking about what's appropriate and what's not. Honestly assessing what we are experts in, what we would be willing to do and what I would be able to review if it came from another human being, is another really important thing, because if we think about it like that, would I be able to spot the mistakes? Will I be able to improve on what's there? If the answer is yes, then fair game. Go and use your expertise, use it as a great accelerator, a great jumping off point, a place that you can go and not have to start from zero, but be way ahead."
  }
]