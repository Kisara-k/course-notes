[
  {
    "index": 1,
    "title": "1 Agentic AI Concepts",
    "content": "Agentic AI, what is it, and why is it so exciting? Why is it that I get up in the morning and I'm just like absolutely amazed at what's possible now? And I'm also really amazed at what it can do for me in my life and the way that I can personally engage with agentic AI. Now, one of the interesting things is a lot of this stuff gets talked about in really complicated ways, but you right now can go and build agents to help you with life. And that's what I'm going to show you in this course is how to build these amazing things that can just do so many simple tasks that take so much work and effort. So I'm going to give you a quick example and demo of what you're going to learn to build in this course and be able to do. You're not only going to be able to build this, but you're going to understand exactly how it works, why it's amazing, and how to go and build new ones to do cool things for you. So I'm going to start off by I'm going to go and give this thing a receipt for a travel expense. So I've got a bunch of travel expense receipts that I've collected and I need to go through and get its help going and cataloging them. So how do I go about doing that? Well, I'm going to give it the expense, and I'm going to say, please add this expense. And notice I'm not subscribing to a travel expense program, I'm using a GPT. So it's looking at it, looking at the receipt. It's going out to Zapier, which is going to be one of the keys to what we're going to do is we're going to do automation through Zapier to work with our email, our documents, all kinds of cool things. In this case, we're going to be working with a spreadsheet of expenses and we're going to go and do some travel expense reporting. So I'm going to confirm and let it go ahead and run this action. So it's starting off and saying, I'm going to check Zapier to see what actions are available that I can go and take. It's now going to go and talk to a travel expense expert. This is another agent that is an expert in travel expenses, and it's going to go talk to that agent and see if the expense is complete. The other agent comes back and says, okay, well, this part's correct, the vendor is unknown. Could be acceptable if more detail is provided, and the date is missing. So let's go and tell it that a missing information. It was Uber on 7/23 at 8am. And I'll go ahead and add a business justification. I'll just say for a trip to the airport. So it says, okay, let's take that information you give me, let's check it. Now it updates it, and it said, okay, now I'm going to check your current list of travel expenses to make sure that this isn't a duplicate of anything. So it's going to reach out to my travel expenses and I'm going to show you what that looks like. That's over here. I've created a simple spreadsheet that I've given it access to with all of my travel expenses and it's going to help me build this list. Now if we go back to it and we go over here, we'll see. It's saying, okay, let's update the expense entry with the provided details. And it gives the details, it fetches my current travel expenses. And it says, since this expense is already recorded, would you still like to add it? So it's gone and it's identified that, whoops, maybe you're already adding a duplicate expense. Do you still want to do it? And I'm just going to go ahead and tell it. Yeah, please go ahead. So now it's going to create a new travel expense for me in a second. So let's see what it does. Okay, it's got the travel expense details. It's adding the travel expense. It's going out to Zapier. It's confirming that we want to do this. And I'm going to say, yes, I want to do that. It's talking to Zapier, which is going to help us with all this automation and working with all of our different accounts, Google, other places. Now it says it's been added. It gives us an updated list of the travel expenses. And now we see the new travel expense for Uber. Let's go check it out over here. And we see now we've got a new travel expense down here that's been added. So we've got the new travel expense added and we've got it listed over here as well. Now it's asking me, would you like to send an email to your amazing admin for expense reimbursement? And I'm going to say, yes, please. So now let's get help drafting that email, drafting the reimbursement email. And this is again actually reaching out to another agent that is an expert in drafting emails, particularly for reimbursement. Ask me to confirm, I'm going to say yes. And now what's going to happen? Well, it's talking to that other agent right now through Zapier, which is pretty amazing. We've got multiple agents working on our behalf already. And now let's see what happens to my super admin. Can you please create an expense report for me based on the following. It's got all of my expenses with the dates and the information, best, Jules. And it asked me if I'd like to send it. Yes, I would like to send that email and get reimbursed. Okay, it's going to go off, it's going to start sending the email. Just give it a moment here. We'll confirm it. I love this so much. It makes me so much, so happy, so much joy. I give it anything. I give it an image of a receipt that's partial and incomplete, it fixes it. I give it like text. I speak into it and I tell it whatever it is. Now it's doing something important. It's asking me if I want to review and confirm the email. So I'll go and review it. It's created the email for me. I can now go and review it. And if I want to do that and go ahead and send it, I could. I'm not going to send this email because this isn't a real email address that it's going to. But you can see this is the amazing stuff that I'm going to teach you how to do. And it's based on agentic AI principles, which sound really complicated. They get talked about in the most complicated ways possible. And the truth is, is that you can understand all these principles very simply, and I will take you through them step by step. And by the end of it, you will be able to build this travel expense assistant. And you will have the knowledge to go and build the next assistant that's going to transform your life. Be it for cooking or helping you with your email, whatever it is, you're going to be able to build it. Because we're going to go and break down all this complicated agentic AI stuff into really simple things that you can do with OpenAI GPTs and Zapier. And a fundamental understanding of the concepts, which are not crazy complicated. They're just fun, natural language, exciting stuff. So welcome to this journey with me. We're going to go and build some amazing things together.\n\nSo what is Agentic AI? Well, I want to take a look at this with you and help you give an intuitive sense of what this is all about and why it's sort of fascinating. Now, what Agentic AI is, is basically AI that can go and act with agency. You think of it as autonomy, it can go and try to problem solve and react to different situations. And you've probably already seen it in your interaction with generative AI, but you probably weren't thinking of it that way. Now, Agentic AI is really putting this into practice and creating generative AI or other systems that can go and interact with the real world, take actions, see the result of those actions, and then respond based on that. So we're going to go and simulate this whole process. Well, it's not fully simulating it because we're going to kind of build an agentic system that's going to have a manual component to it. Meaning that as a human being, I'm going to help the agent accomplish actions and respond to situations. And so we're going to build an agentic system that has a human in the loop where the human is helping create that agency that it needs. It's helping carry out the actions. And that's one of the key things that we start seeing when we have these agentic systems as they do things for us sort of autonomously. So here's my prompt to get this started. You're going to help me cook whatever I need. Since you can't directly lift pots or pans or get to the fridge, you will tell me the steps and I will perform them. We will go one step at a time, ask me for what I want to cook. And this is a prompt that's essentially kicking off an agentic system where I'm going to follow the instructions that it tells me to follow. And then whenever it tells me there's some action that needs to take place, I'm going to take that action and then I'm going to tell it what happened and it says, okay, great, let's get started. What would you like to cook today? And I'm going to start off with something fairly simple. I'd like to cook a dish that combines the flavors of Ethiopia, Uzbekistan, while being keto friendly and up and under to 1000 calories. And it says, that sounds like a fun and unique challenge. How about we make a keto-friendly Ethiopian-Uzbek fusion dish featuring a flavorful marinated meat served with a side of spiced vegetables. Here's a plan. And it's starting to build out. Here's how I'm going to go and accomplish this. And it says, okay, here are the ingredients you will need. Lists all the ingredients. And then I want to draw your attention down to. This is instructions at the bottom. This is where it's telling me what to do. Here is the task to go and perform. And it says, marinating the meat. Mix the berbere spice, ground coriander, cumin, seeds, turmeric, fenugreek, paprika, etc. Coat the meat with the spice, ensure it's covered. Let me know when you have the ingredients ready and if you have any questions. So it's telling me, go and do this and tell me when it's done. And so I come back to it and I say, okay, meat is marinated. I did that. And now what does it do? It responds and it says, okay, great, let's move on to the next step, cooking the meat. Heat one tablespoon of olive oil or ghee in a skillet over medium heat. Once the oil is hot, add the marinated meat to the skillet. Cook for about five to seven minutes. Now it's giving me instructions, it's really telling me a task to go and perform. I'm going to go cook the meat is the task, but it's breaking that task down into sub steps that I'm going to go and perform. But you notice what's happening. It's telling me what to go and do. It's giving me the instructions to perform. It has the choice and what gets done and when, and I'm carrying out its instructions. That is it having agency, it's choice over what's happening. And then it says, let me know when the meat is cooked and resting and we can proceed. And so I come back and I want to point out the agentic aspect of this, the fact that it can react and have agency and choose what to do and respond to all kinds of things, including something like, no, it's starting to burn. So I tell it like, hey, here's what's happening with the meat, it's starting to burn. And I don't tell it what to do. And it says, if the meat is starting to burn, reduce the heat to medium or medium low immediately. You can also add a splash of water or broth to the pan to deglaze it and prevent further burning. So it is responding and adapting. Now, I'm simulating all of this. We're not automating it yet. What we're doing is we're seeing and building up intuition about what is about, and this is it happening. Except that rather than fully automating it as a human being, I'm going and performing its actions. So it's a collaboration between myself and the generative AI to get this dish cooked. So it tells me what to do, how to handle the potential burning meat, and then it says, let me know when the meat is done and resting, and we'll move on. And say, I go back and I say, okay, the meat is done. So I'm giving it feedback on its actions. I've completed the action you requested, it's ready. It comes back and says, okay, let's move on to preparing the vegetables. And again, let me know when the vegetables are ready. So it's giving me a task, I'm going to performing the task, and then I'm coming back and saying, okay, task is done. And then it comes up with the next task. And it says, now, in this case, it asked me if I forgot about this, I gave it a little curve ball. It said, let me know when the vegetables are ready. And I said, no vegetables, but I have durian fruit. So this is a little bit of a curve ball for it. How will it respond to, no, there's no vegetables? You asked me for vegetables. I don't have any, but I do have durian fruit. You have to deal with it. How does it respond? How does the agent handle that? And it comes back and it says, okay, well, it's quite different from vegetables, but I'm going to adapt. And so now it tells me how to prepare the durian fruit because it says, well, I want a path forward. I want to try to reach this goal of having created the dish. I don't have access to the resources that I thought. So I'll have to adapt and come up with some other way of solving this. So it adapts, says, okay, now here's how you're going to prepare the durian fruit. I have no idea if this works. This is not a dish I'm cooking in my house. And I tell it, it's done, it comes back, and it says, great, now here's how you go and plate the dish. Now, what happened here? Well, we flipped the script, right? We, instead of going into the generative AI and telling it, do this, do this, plan this, do this, I said, here's what I want you to accomplish. Here is the process you're going to use to accomplish this. The process is you're going to go one step at a time you're going to tell me what to do. I'm going to carry out the action for you and then I'm going to tell you when it's done, basically. And then you can go and respond and do the next thing. And what we see is, is that as we go and change our responses, for example, when we threw at this curveball over the durian fruit, it responds and adapts. And that's a critical thing with the Agentic AI, is it's nothing fixed. We think of all our computer systems that we've built for years and they're very rigid and fixed. Like if you take a computer system and you say, no vegetables, durian fruit, it falls apart. It has no idea what to do. And this is where the magic of Agentic AI starts to come in, is it sees, okay, I don't have vegetables, I've got durian fruit, I will do my best. I'm going to try to figure this out. And that's what we're looking for, is that ability to see things and respond to them, and then choose the next action based on what happened. Or like when the meat was getting ready to burn, adapting the plan and say no, okay, here's what you're going to do instead. You're going to turn down the heat and that's okay. Now tell me when you're got, you're done with that step. And so it's dynamically altering the steps based on the feedback that it's receiving me. And that is the key to agentic AI that we're going to be working with is its ability to ask for an action to be performed, because it's never going to be directly performing it. It's going to be specifying what it wants done. And then either a human being is going to go and do it, or an existing software or computer system that we know how to build is going to go and take that specification and carry it out. That's the key. Now, does it matter if it's a human being or computer system that's doing this? Not fully, the key difference that we're gonnasee is that if it's a human being, we are much more error tolerant. If it goes and it gives us the tasks that it once performed in many different formats, we can handle that and adapt because we're human beings. But computer systems aren't so good at doing that. So when it begins interfacing, when magenta begins interfacing with all the tools and computer systems that we use on a daily basis, if it decides to say, hey, yo, go create this recipe. And it says that to a spreadsheet. The spreadsheet is going to say, I have no idea what you're talking about because it doesn't work with yo create this recipe. It works in columns and numbers and values, and so it's in a much more rigid format. So part of the magic of Agentic AI is figuring out how do we bridge this gap between how it sort of intuitively and fluidly specifies what it wants done, and this rigid world of computer systems that we've built where everything has to be exact and perfect. Boy, we know that generative AI loves to explain itself and gives us all kinds of remarks and details and comments and all those types of things are really problematic when we start working with real computer systems. Now, it's fine in this example like I have, where I'm acting as the automation, I'm going in and listening to what it's saying, and then responding to it because I'm a human being and I can adapt as it can adapt. I can adapt to it giving comments, I can adapt to it explaining itself. I can adapt when it's short and sweet about what it wants. And a lot of what we're going to have to do is the prompt engineering around how do we interface our existing computer systems with these things that are very talkative and adaptive and all of these other things they do and aren't as rigid as we're used to. So I look forward to exploring this more with.\n\nLet's look at the basis of this ISO gentic systems. How do we kick off and get generative AI to go and work this way? There is a prompt engineering pattern, and I've talked about it in my first class, Prompt Engineering for Chat GPT, and a paper I did. But I think of this as the basis for all this stuff, which is a flipped interaction, where rather than us telling the system what to do, rather than us asking it questions, it goes and tells us what to do, it asks us questions, or really it goes and ask existing computer systems questions. It runs queries and databases to collect information it needs, or it goes and tells those systems to run actions that it needs run in order to accomplish some goal that we give it. When I want to take a quick look at the flipped interaction pattern and talk about how this works. The basic structure of this pattern is, we go and tell it that you're going to ask me questions at one at a time in order to get enough information to suggest a restaurant for me to eat at in Nashville, Tennessee, tonight. You can think of this in this basic pattern as, ask me questions one at a time. One at a time is often the best for gentic systems. It doesn't have to be one at a time, but we'll talk about this in a minute. Go and ask me questions, or tell me what to do one step at a time. The goal of this is we want to create a situation where it goes and creates things that need to be done, tasks that need to be performed, or actions that need to be taken. Then the system will go and execute those actions on behalf of the agent and tell it what happened. This rough pattern of prompting, ask me questions one at a time, tell me what to do one step at a time, creates this environment where it's being told, you're in the driver's seat, go and do it. Then at the bottom I say ask the first question. This is a trick that works really well in lots of systems to make sure that we start off with one question only, or we say, \"Tell me the first step,\" so that we start off with one step only. We want to create a powerful pattern of behavior where it goes off and it knows what to do. It was going to do this one step at a time. Let's look at our prompt. Ask me questions one at a time in order to gather enough information to suggest restaurants for me to eat at in Nashville, Tennessee, tonight. Ask the first question. It comes back, what type of cuisine are you in the mood for tonight? I'm going to come back and I'm going to say Tex-Mex. Now I've given it an answer, and it has to adapt, and part of the adaptation is choosing the next question to ask me. That is wildly powerful. That simple thing of looking at what information it was given, and then choosing the next question in response is critically important, because the same thing happens in a task. It has for a task to be run, it looks at what happened, and then it adapts what the next task is going to be based on what happened. Same thing with questions. I say Tex-Mex, then it says, \"Do you have any dietary restrictions or preferences?\" I say no. It adapts the next question then, because if I had said yes, it probably would have gone off and then asked me questions about them. But since I said no, it said, do you prefer a casual dining experience or something more upscale? I say, kid friendly. I just give it some feedback. Notice I'm not constraining my responses to any type of format. I'm giving it very free-flowing. It's having to adapt, not only to the information I'm giving it, but the format that I'm giving it the information. It's very fluid in what it's doing. Would you like a restaurant with outdoor seating or indoor seating, or does it not matter? I say, \"Well, it doesn't matter, it's raining, so I'm probably not going to eat outside.\" Then it goes further and it says, \"Do you have a specific preference for a specific area?\" I say near Vanderbilt. Great, how far are you willing to travel? I say, \"Well, a short drive, 5-10 minutes. Then it says, \"Got it. Do you have any preference for a restaurant that takes reservations? Are you okay with walk-in?\" I say walk-in's fine, but I don't want to have an insane wait. You notice this is totally fluid. Each time I respond to one of its questions, it's adapting. That is the agentic part beginning to happen. That is the agentic magic. It's also adapting to all the different formats and ways that I can specify my input. This is one of the important parts of large language models, is that they can handle input in so many different formats, so many different ways of expressing the information and the idea without us having to go and build some complex thing to be able to handle all these crazy ways that I'm responding. Then after I give it the walk-ins thing, it says, \"Based on your preferences, I recommend Rosepepper Cantina.\" I go and Google Map this, and I'm like, well, right now from my house that's actually about 13 minutes with traffic, slightly farther than I want to drive. I just go back and I say, \"Hey, Google Map says it's 13 minutes away, feeling a little lazy for that trek.\" It comes back and it says, \"In that case, go to SATCo.\" SATCo happens to be right across the street from Vanderbilt. Now that's something really powerful and valuable that I can go and employ to help me solve a problem. Now I can solve a problem like this. Like where should I go? Help make a decision. Help guide me as a human being, and it's collecting information from me. There's no difference between it collecting information from me and it collecting information from a database. It's just the interface to collect that interface changes, or interface to collect that information changes. Asking the human being is much easier, because the human being can adapt to whatever it says. The difference is when it goes and collects information from a real computer system like a database, it's going to have to speak the language of that database. It's going to have to speak the language of that computer system, so it's going to have to translate the goal to whatever the target language is. if you think about something that these models and large language models are supremely good at, it is translation. Absolutely good at translation. Excellent experts at language. That piece is again one of the huge benefits, is that we go and express what we want in our human language, it decomposes it into a plan to go and accomplish it. It interactively begins executing its plan, and when it's a human on one side, it'll ask it in human language. But when it needs to go and it needs to talk to a database, it can go and begin speaking that database language. Just like in Star Wars. They had protocol droids that spoke all these different languages and could communicate with all these different systems. They acted as the interface to those systems. That is what generative AI is doing. Agentic AI is working very similarly, but rather than just asking it a question and us really directing everything one step at a time, agentic AI, we're giving it the ability to go and perform many different steps and adapt. It chooses the step, we perform it. It gets information back about what happened as a result of the step, and then it adapts, and that continues on and on.\n\nNow, something amazing about these models and these patterns is they don't have to just stick with language. These systems can interpret so many different types of information. It doesn't just have to be human language. And this is a key thing that we have to understand is we can go give it all kinds of information. So when we use the flipped interaction pattern where we have it go and ask us questions or ask the database questions or tell us what to do, or tell the computer system what to do. It doesn't have to just get feedback in terms of language. It doesn't just have to be our human language. It can use all kinds of modalities of communication and we're going to take a look at an example of that. And that example we're going to provide is photographs. So here is my flipped interaction prompt. I need you to help me diagnose why my computer isn't connected to the Internet. We need to diagnose it one step at a time. Each step you can ask me to do something and then take a screenshot to verify that I did it correctly. Always start by asking for a screenshot of my system to know what you are working with. What is the first step? So again, I'm using this interaction pattern where I'm describing that I wanted to go step by step solving the problem. I'm getting it to ask me for the first step, and I'm telling it this time, the feedback is primarily going to be screenshots of what took place. So you're going to tell me to do something. I'm going to do that thing on my computer, and then I'm going to show you a screenshot of what happened. Okay, so it says, the first step is to get a screenshot of your current system to understand the environment we're working with. So it's asking us for the current state. So I go and I take this picture of my laptop, this screenshot of my laptop. I upload it as my response. I'm not uploading text anymore, I'm uploading a screenshot of my computer. Absolutely insane that I can do this now. It says, great, thank you for the screenshot. It looks like you're using a Mac. Let's proceed with the next step to diagnose your Internet connection. Now, notice what it did, right? It looked at this screenshot and said, hey, that looks like Mac. So it knows about my computer from the look of what it sees in the screenshot. It's not just me saying I have a Mac, this version or this information it's looking at and collecting rich information from that screenshot and then adapting based on what it sees in the screenshot. Okay, check your Wi-Fi connection. Click on the Wi-Fi icon in the menu bar at the top right of your screen. Make sure Wi-Fi is turned on. Check if you're connected to a Wi-Fi network. And then it says, please take a screenshot of the Wi-Fi drop down menu showing the available networks in your connection issue. So I go and do what it says. Now, I didn't have an actual Internet connection issue, but I just wanted to get it to walk through and see if it could diagnose because I'm claiming that I have one. And so I wanted to see if it can get to the point where it actually understands that, no, I don't actually have an issue. And so I go and I click on that, I take a screenshot of it, I give it back to it, and it says, thank you for the screenshot. It appears that you're connected to the Wi-Fi network Home 699 E. You notice it's actually identified the Wi-Fi network that I'm on from the screenshot. It doesn't require me to tell it, it simply collects the information that it needs from the feedback that I'm giving it. Okay, let's proceed with the next step. Check network preferences. Click on the Wi-Fi settings option at the bottom of the Wi-Fi drop down menu in the network preferences window. Make sure that the Wi-Fi connection status shows as connected and has an IP address assigned. And then it says, please take a screenshot of the network preferences window. So again, telling me an action to take, telling me to give it back a screenshot. I go and take a screenshot of that that you wanted to see, and it says, okay, thank you for the screenshot, but it looks like your Wi-fi connection has an IP address assigned to it and a router address. This suggests that your computer is connected to the Wi-fi network. So it's established, it's starting to diagnose the issue step by step. First it's establishing that I'm connected to the Wi-fi network and that that looks okay, now it's going to go and say, okay, well, what's next? If that's not the issue, what is the next step? And now it says, okay, well, we need to check Internet connectivity, open the terminal application. So now we're getting more sophisticated you can find it by searching for terminal. In spotlight, it gives me the command to run, and then it says in the terminal, type ping 8.8.8.8 and press Enter. Now notice what it's done. It now needs to get a command run on the system. And so it's translated that high level task of check Internet connectivity into a command, a language that the computer understands. So it is doing that translation step. It's not only identifying what needs to be done, but it's identifying how to translate that specifically into the language that it can use to communicate with the computer to find out the information it needs. And then it says, take a screenshot when you're done. Now, I didn't take a screenshot of it. I actually just copied and pasted it back into the prompt. I should have probably done it as a screenshot, but I wanted to see how it would respond if I suddenly throw it a curveball by not giving it a screenshot anymore, but actually giving it the text. And this is the raw text that was output by running that command. You notice this is all, this is something that makes sense to me as a programmer. If you are in IT, it may make sense to you. If you are not into this stuff, you're going to look at this and you're going to say it looks like a foreign language and it is. It's a language that many people understand how to speak, but if you don't know this language, you don't know how to speak it. It looks very foreign, but it understands how to then translate this back into meaning. And it says, the ping results show that your computer can reach Google's DNS server with no packet loss, indicating that your Internet connection is working. So it's translating. Here is the raw information in that language that I see and it's translated into meaning in terms of the problem that it's solving. And then it goes on and tells me the next step to run. And it would keep going, trying to help me diagnose the supposed Internet connectivity problem with my computer that I don't actually have. But you see how it's adapting. It can adapt to many different types of responses. That flipped interaction where we say, here's the pattern, go do one step at a time, I'm going to give you the result. Every time it takes what it gets back, it responds, it adapts, it translates, it can interface with different systems. Now we're still using manual interfacing. We're having a human provide the interface. But rather than interface to the world of cooking, we're now interfacing to the world of machines and to world of computers, which is eventually where we're going to go with these agentic systems is. We are going to allow them to directly talk to those machines in order to carry out tasks, in order to obtain information that the agent needs in order to reach some overall goal.\n\n>> Now, agents don't just have to go and do everything one step at a time. They can actually go and build rich plans. And this is one of the capabilities that really underlies all this, is their ability to take a goal that you're trying to achieve, some high level task, and decompose it into a series of subtasks or goals in order to form a plan to accomplish it. And that is a fundamental capability. Now, one way that we can have agents work is we can have them go and interactively, one task at a time, go and take steps and perform actions and respond. And that's a really powerful way to do it. Another way that we can do it is the complete opposite. We can have it build out the whole plan in advance and then go and send it off to be executed. Now, we haven't gotten to the point of automating things yet, so let's do the planning where a human being is going to go and execute the whole plan. So here's my goal. Plan a four day trip to Nashville that involves going to several different neighborhoods. And it goes and it breaks down my task. It says, okay, I need four days worth of activities. They need to go to different neighborhoods. And it goes day one, downtown Nashville, breakfast at Pancake Pantry. I'm not sure this is accurate on neighborhoods, depending how you classify downtown Nashville. I think if downtown Nashville is not including the Pancake Pantry, I think of that as Hillsboro Village. But, you know, it did its best. And that's another thing, is it's not always going to be perfect. And when we go step by step, it has more ability to adapt and correct that. For example, it could have said, go to pancake pantry. I could say, that's not downtown according to my definition of downtown. It could have said, well, you've got a different definition of downtown than me. And it could have adapted its plan. And when we go and we generate the whole plan like that, it's in one go. It doesn't have the ability, necessity to adapt between steps. This is a key difference. And when we think about planning, if we do it step by step, full adaptation between it. But there's some complexity there, right? Because it may forget steps or it may lose track of what it was trying to do or something like that. On the other hand, we can go, we can have it so it never loses track of what it's going to do. It can build out the full plan in advance, and then it just gets sent off to execute multiple steps. And it doesn't have to remember what it said or anything like that makes it an advance. But if there's a flaw in the plan, it doesn't get to adapt the plan while it's being executed, and that's a key difference. Do we want it in the loop while the plan is executing in between every step, getting to decide what happens next? Or do we want it to be fully baked, and the plan executes completely separately from the agent and we lose the ability to respond if there's a mistake between steps? But we may gain things in terms of efficiency, costs, speed. It may be more repeatable and predictable. So if we put an agent in between, just like any other large language model response, it may tweak what it does from one time to the next. We may not want that. And so we're always going to be balancing these two ways of going and accomplishing tasks. Either it's directly in the loop between every step, or it builds out a fully baked plan in advance, and we go and execute that whole plan, very repeatable. But if there's mistakes or problems, we can't adapt easily, or maybe we want to be somewhere in the middle. So here's the rest of our four day plan for Nashville. Now, I'm going to go back, and I'm going to say, okay, my son loves BMX bike racing. Help me plan a road trip from Nashville to New York City. I want to drive about five to 6 hours per day. Every day needs to stop in a city with a BMX track that he can ride that evening, plan my route, and it goes through, and it says Nashville to Louisville, Kentucky, that's a good choice. There's a great BMX track there. It's not too far. Then we go to a few more days in, and we get to Hummelston town, BMX. And I could not find any information about the existence of a BMX track named Hummelstown BMX. Maybe it exists, maybe it doesn't, but I sure don't seem to see that it exists. It looks like something fictitious. And this is a challenge when we begin thinking about plans, is that if we build fully fixed, fully fleshed out plans in advance, we always run the risk of hallucination. That is, it dreams up something in the plan that looks good but isn't real, and there's all kinds of ways that it can hallucinate things. In this case, it's hallucinating a stop, because it's like, well, I need to stop. It knows that there's a Hummelstown, Pennsylvania, so it says, well, I'll stop there and I'll just dream up a BMX track to be there because we haven't constrained it enough. There's no way to easily do this. Now, luckily, I'm fact checking before I drive on this road trip, and so I can capture that. But that's going to be an issue that we're going to deal with. Now, how do we overcome issues like this? Well, so far, what we've done is just given it sort of open ended prompts, but we can start to add in information. Upfront we can give it information that we want it to work with, constraints that we want it to work with, to try to bound what it does and what it reasons about. And this is really, really important when we start thinking about agents information, giving the agent access to the information it needs to accomplish the task is critically important. Now, some cases, it can go and fetch the information for itself if we explicitly tell it to do that and when to do it. But if we don't give it the ability to go and fetch information, we need to give it all the information to accomplish the task and not just assume that it'll have access to everything. So in this example, I went back and I took that prompt, and I copy and pasted the list of every BMX track in Canada, in the United States, and I dumped it into that original flipped interaction prompt. And then I give it the exact same prompt. Basically, my son loves BMX bike racing. Go plan this trip. And it comes back, and now it's actually correct. It's found real tracks, and we have real information to work with. And all of these stops appear correct to me. I went and did some fact checking. I was able to find all of these different tracks. We also are roughly within the hourly limit that we set. Maybe it's a little too conservative on how far we drive each day. Maybe I should go and tell it, well, you need to drive farther, and it could adapt the plan. But you see, now we're getting a plan fully baked. I'm not going to be driving along and saying, yo, there's not this BMX track here. Like, tell me where to go instead, and driving somewhere else, because that doesn't make sense for planning a road trip, right? I'm going to go and execute its plan, but I'm not going to have it in the loop. And so that's a different sort of style of interaction. And we have to figure out when we start building these systems of, like, how many steps do we want to have it string together, and then we go and execute on its behalf and bring back the result from, or do we ever want to bring the result back? All of these things are important and they will determine how repeatable the process is. The more we insert the agent into the middle of the process, the less repeatable it's going to be. The probably slower it's going to be, probably the more expensive it's going to be. The more we lean towards a fully baked plan, the more repeatable the plan is probably going to be, because there's not any adaptation between. But at the same time, we're going to have to be very, very careful to try to minimize the number of errors in the plan, because if an error happens, it doesn't get to help us and adapt.\n\n>> One of the things that people overestimate when they start working with generative AI is how much generative AI knows because it's so convincing. They often assume knowledge that it doesn't actually have. They assume that it knows all kinds of surrounding information about what they're doing. And also we tend to underestimate how much context is actually needed in order to perform a task. Well, I want you to go back and think about it. When you're building a genetic AI is think about if you tried to get an intern to do something for you. It's day one, they've shown up, they've been at your workplace like one minute, they know nothing about your culture, they know nothing about the layout of your office, they know nothing about your systems in particular. They just have lots of general knowledge about the world, but not about your stuff, not about how you do things, not about your computer systems, not about your goals in life, not about what gas station you stopped at when you were driving to work and what your favorite food is and what you like and don't like, and travel planning. All these things are missing. There's so much information that is fundamentally missing, and it's really critical that we fill that information in whenever we go and get a genetic AI to perform a task. So I want to just kind of give you a sense of like the wealth of information that we need to provide in order to perform a task. So the other day my son came in and he had found this old Nintendo Wii that we had in the closet, and he said, hey, I want to hook this up to the TV. And we pulled this thing out of the box, and lo and behold, it doesn't have an HDMI hookup. And I thought, what on earth am I going to do? I don't even know if my tv can accept these types of inputs. And now I go and I try to get to my TV and it's mounted on the wall, and I feel behind it and I feel more than one of those inputs, there's more than three of them. So now I've got a problem because I feel five. There's only three cables and I'm trying to reach behind blindly and figure out what to do. So finally I broke down and I reached out, I took my phone in, I took a picture behind it, and now I've got information about the tv. Now, I then decided, okay, let's see if I can get ChatGPT to go and solve this problem with planning. And I said, explain how to plug these into the TV using the ChatGPT app. And then it came back and it said, okay, here's where they go. So it built a plan for me to go and connect cable by cable into the tv. Now, the key behind this was that I gave it information about the world. I didn't just say ATV, I didn't say my tv, because my tv is not specific enough unless it knows my tv and has information, detailed information about it. And even if I said my tv and model, I'd still have to know that it even had information about what the panel looked like and the layout of all of those inputs. There's so much information that's needed to accomplish that task. I also have to show it what I'm working with in terms of wires and the exact colors and how many. And there's so much information that's required in order to perform the task. Now, in this case, I'm giving it really specific information about the world around it. And that's the thing, is, it needs to know the information about the world, the system it's working with, the problem it's solving. It needs a wealth of information to do a good job. Just like that intern on day zero needs information to go and solve the problem. Now, let's think about this coffee maker. Explain to me, step by step, how to use this. Now, if I just said a keurig, it may be able to solve it. If I say, explain to me how to use this coffee maker, well, coffee maker is not enough if I don't give it the photo. But the photo is really rich information. One, it says Keurig across the front of it. Two, it's got information about the shape of this thing, what may be the options to use it. And it goes in and breaks it down, and it says, here's what you do. You plug it in, you power it on, it's planning, it's taking in information about the state of the world, then it's deciding what to do. And whenever we begin building these systems, we have to think very carefully about, are we giving the system enough information to perform the task? Does the agent really know all the details and complexities and nuances in order to get to where we want it to go? And we also have to think about when things don't go right. How do we update its knowledge of the state of the world or something changes? How do we update its knowledge of the state of the world? And all of this has to go into it, because as it's going and executing a plan or telling us to execute a plan, something in the world may change that may force its plan to adapt. And if we're not feeding it in information continuously to make sure that it's aware of what's happening around it, it will go and make incorrect decisions. So, if you tell it to go create a travel reservation, and then simultaneously, your wife goes and creates a travel reservation independently and you don't update it, it may go and create a duplicate, and that's not what you wanted. And so you have to think about it like that, intern. You have to not only tell it enough information to go and solve the task, but you have to keep it up to date with all the information that it needs. And that means continuously updating it throughout the process so that it knows what it needs to do, but also it needs to know what happened that may affect what it's currently planning to do.\n\nHow do these agentic systems go and interact with the world? Well, we typically have them interact with the world through tools or actions that they can take through those tools. What we're going to talk about now is this concept of a tool or an action that an agent can go and use to try to accomplish a task because most of the time we don't want it to just dream up whatever it wants to do in order to accomplish the task. For example, if we're going to plan a trip, and it tells us that, well, you need to go and plan a trip that involves flying, and we don't want to fly, that doesn't make any sense, so we want to tell it, what are the allowed methods of transportation? Or if we tell it that it can go and collect information from us about what's happening as we're trying to figure out that Internet connection on our machine, if we tell it it's allowed to use a screenshot as a way of collecting information, that's one method that it could do it. If we don't want to take screenshots, we might say, well, your only tool is to ask me. Your only action is to ask me what happened for me to report back. We can think about giving them different tools to go and accomplish tasks. Now, I'm going to go back to this cooking example, and we're going to equip our agentic AI with some tools. Here's what we're going to say. These are the only tools available to you. Pan, you have a one quart sautee pan, a skillet, a large cast iron skillet, a wood fire, and I say a fire burning using oak. You're going to help me cook whatever I need. Since you can't directly lift pots and pans or get food out, you will tell me the steps, and I will perform them. We will go one step at a time, ask me for what I want to cook. Now what I'm doing is I'm giving it, here are the tools that you have available at your disposal to cook this. It's not the universe of cooking utensils and devices. You can't go and say, well, I want an Instant Pot, and I want a microwave, no, you've got a wood fire, a skillet, and a pan, that's it. Now, it has to figure out how does it solve the problem with the tools available to it. These are the tools in the prompt, and you very commonly see something like this done more sophisticated ways, more programmatic ways. Often these tools are described using languages that are more innate to computers, but this is basically a rough approximation of this, and what we'll see is it can clue in on this and do it. This is where we're telling it, these are the tools available to you. We could also say, these are the actions available to you, like, put pan on fire, things like that. It's a different way of approaching it, but we're going to take a look with the tool approach. It says, what do you want to cook today? I go back and I say, I want to cook pizza on the wood fire with a skillet or a sautee pan. It says, it's a fantastic choice , gather your ingredients. I've got my first task to go and perform, so I go and grab all the ingredients, and I say, I'm ready. Then it comes back, and it says, prepare the dough. It tells me what to do to prepare the dough, and again, asking me when I'm done. I say, I'm done. Then it comes back and it says, prepare the wood fire, and tells me how to get the fire prepared. Notice it's using a tool that I have access to that I've told it it can use, and then it tells me how to put the cast iron skillet on it. Tell me once the skillet's preheated. I say, it's done. Comes back, and it says, now let's continue. Cook the dough. Carefully brush a thin layer of olive oil on the preheated skillet. Again, it's solving the problem with the tools I gave it. I have constrained its universe of options and actions and things it can do down to this set of tools that I need to accomplish the problem. Because what's common in the real world is we don't get to just do whatever we want, we have limited resources, limited tools or things that we have to solve the problem with. An agentic AI has to be able to do the same thing. It can't go and say, you want to do X with a computer, therefore, I will reach into the world's fastest supercomputer and do that. No, you don't have access to it. You don't get to do that. We have to be able to put constraints on what tools it can use to solve the problem, and that's how we're doing it here in the prompt. Then I say, I'm done with that step, comes back and it says, now add the toppings. Again, we're still using, roughly, the tools that we've got. Keeps going, place the skillet on the wood fire, gives me some different information. It's roughly constrained to all the tools that I've basically told it to use, and it's solving the problem. This is how we go about, begin to think about getting our agent to solve the problem, but not to just solve it arbitrarily, but to solve it in a way that is bounded in terms of, when we think about our computer systems, they have limited actions that they can perform. They are constrained. They are rigid. We have to start taking agentic AI, and we have to start putting bounds on how it interacts with those systems by giving it a concept of a tool or an action that could be performed with those systems, and we have to bound those tools and actions to what those systems can actually perform. It can do all kinds of things. It can dream up all kinds of ways of solving the problem. It could go and tell a human all kinds of crazy ways of solving the problem, and a human could go and probably perform many of those ways of solving the problem without being told explicitly to be bounded in some way. But the moment we begin putting these agentic AI systems in contact with something like our customer relationship management system, our CRM system, or in control over our email, or in control over whatever other system it is, those systems are going to be very limited in what they can do and the actions they can take, and so we're going to have to express those tools or actions that bound the capabilities of those systems to the agentic AI. Now, do we call this a tool or an action? There's no hard and fast rule to this, I would say. Many times action is the better way to think about it. I'm going to go and say, these are the explicit actions that you can take, particularly when we start talking about interfacing with computer systems. Computer systems that often makes sense to talk in terms of actions because there's such finite number of things that you can ask the system to do, and so it's easier to spell it out as a series of actions. If you're talking about interfacing with something as open ended as a human being, thinking about expressing it in terms of tools often makes more sense because human beings can perform many different actions with a tool, and they can flexibly adapt, and so if the agentic AI says, go and perform this action with this tool, a human will know how to do that. Or if it says, use this tool to do this, a human can go and fill in the blanks. But when we're interfacing with our rigid computer systems, they can't do that, and so often we have to describe each individual action, or operation, or instruction. Think about like CPUs, and now they have a limited number of instructions they can perform, and we have to start thinking about defining it more explicitly in terms of actions. Which one do you use? It depends on the context, depends on what you're trying to do, but you can think about expressing it both ways. There's also other ways that you could do it as well, but these are two of the most common ways to do it.\n\nSo when we start describing tools and actions to the system, sometimes it's really intuitive, right? If you ask generative AI to use a pot or a skillet, it understands what a pot and a skillet are. But if we start to ask it to interface with our computer systems where we've built all this crazy stuff, we have to start thinking about how do we describe the capabilities of the tools or the actions within the system. That is the moment we move outside of concepts and tools that are well known to like all of these custom computer software tools that we built that may have similar names but do and mean completely different things depending on which computer you're working with. Well, now we have to start thinking about how do we deal with that problem with agentic AI? How do we describe to it the capabilities of the tools that we're giving it access to. So let me give you an example of this. So we built a flipped interaction prompt again, and we're going to describe the tools at the disposal of the agentic AI. Except the agentic AI in this case is going to help us escape when we're trapped in an alien spaceship. So we say, tools, the X 155 tool, and we have a dash, and we say it prepares alien pizza, the Q 63 tool, and we have a dash, and we say it opens a dimensional portal to configurable destination. And then we have the l199 tool, and it causes ship to play Beatles music on a loop. And so we now have a set of tools that we can use to potentially escape the alien spaceship. Now, you notice the names for these tools don't make any sense anymore, right? These aren't tools that we know about. We've never seen these tools before because they're on this alien spaceship. But we want to get the agentic AI to help us, so we tell it what we know about the tools. We don't rely on the name alone to explain the concept. And then we say, okay, I'm trapped in an alien ship, you're going to help me escape. Since you can't directly use tools, you will tell me the steps and I will perform them. Each step must use a tool. We will go one step at a time. Tell me the first step. And so now I'm giving it, here's the list of these alien tools at our disposal, here's what they do. Now. Help me one step at a time. Use the tools to escape the alien ship. How does it do? Step one, prepare an alien pizza. Use tool x155 to prepare an alien pizza. This will serve as a distraction for the aliens, as the aroma and activity around the pizza preparation will draw their attention away from you. I'm not sure if this was my first step that I would use, but I like its thinking here, it's going to distract the aliens. Now, I didn't specify if they're friendly or not. That may be learning culturally that we tend to view aliens as something scary. I guess if I say I'm trapped in an alien spaceship, maybe there's some context there that it may not be a good thing. Then we come back and let's look at this again. So we just gave it x155, but the dash told it, this thing prepares pizza. And that's how it knew on this step one that he could go and prepare pizza. The description of the tool was critical. And in our computer systems, we've built the equivalent of alien tools. All these different custom little software things. And every time we want to work with one, we have to give a really clear and accurate description of what that tool's capabilities are and how you work with it and interact with it. Very, very important. Then goes on and it says, step two, open a dimensional portal. Use tool Q 63 to open a dimensional portal. This portal will be our escape route, leading to a configurable destination. And then it goes on and it says, and use tool l199 to cause the ship to play Beatles music on a loop. This will create an additional layer of distraction, making the aliens focus on the music and further reducing their attention on your escape. So what we've done is we've created new tools that it knew nothing about. We've taught it what those tools do in our prompt. And just to go back and review, all we had to do was we had to go and give it up front. Here's the tools available to you, but not only the tools that are available to you and their names, but what they do. So naming is important, but also the description of what the tool does can be really, really important. Now, how much does tool naming play a role in this? Well, tool naming is a critically important thing. And in fact, the name can be make or break when we're trying to build agentic systems and see how well they use tools. It really, really has to understand what that tool is. And if we can think of a great name for the tool, a very rich and descriptive name, often it'll know how to use it more effectively. We can create crazy tool names like I just did, and then tell it what those do. But often we want both really great names and really good descriptions, but I'm going to go back and I'm just going to drive this home. Now, originally when I did these tools, I had to go and give it descriptions of what they are because the names were bad because they'd never heard these names before. Now what I'm going to do this time is I'm going to go back and I'm going to rename these tools and I'm going to give them names that are descriptive and make sense for what the tool does. So I've renamed my tools to makeAlienPizza, openDimensionalPortal and playBeatlesMusic. And now I tell it I'm trapped in the alien spaceship, what do I need to do? And it goes through and it says, you know, make alien pizza. Same first step, solves the problem the same way, says, go make alien pizza to distract. I don't know why making alien pizza to distract is an intuitive thing, but apparently it thinks it is. Now, let's look at what happens if I change the names of these tools and I don't give a description. So I've got mkpz, sort of like make pizza. I've got odprtl, open dimensional portal, and I've got pbm, which now I've kind of forgotten what it was. Play Beatles music. So I've got these tool names, not very intuitive, they're kind of abbreviated, short. They also look like a lot of things that we've created in computer systems as short abbreviations where we, as the users of those systems, have institutional knowledge that that short little abbreviated name means this. And I can expand it out of my head because I know what it is, but it will not have that institutional knowledge. It will not know what those things are. Now, sometimes it may if they're common things or it's really clear, but often it will not. And so you have to be careful when you start thinking about tool naming, that it's going to be able to understand it. So when I change the tool names and I've now taken away the descriptions like I did before, now it says, okay, first step is to use the mkpz tool to create a detailed map of the alien ship's layout. So it's lost its understanding of the mkpz tool being the make alien pizza tool. And so this kind of helps you understand that it's really, really important that we go about thinking about tool naming, and it's also sort of lost the odprtl. It says, okay, open a portal that will help you move to another part of the ship. So it doesn't have as much context that this is an interdimensional portal, it can take me to any location. And so it's just saying, well, maybe this is just to move around the ship. And then if I go back and I think about, you know, creating tools and building things up, naming becomes really, really important. But also the mixture of tools becomes important. Because, for example, when I go and I say here's a bunch of tools, and I have sort of distracting things and environment, it may not get it right. So in this case I renamed the tools. They all make sense, but I didn't really give description about how you might use them. I didn't really say, here's some tools that you might want to use, but you may not need all of them. And so even how you present the tools, the ordering, the language, you give it around it, like does it have to use all of them or none of them? Are they optional? And as much context as we can give is really important. So in this case, I go in and give it the tools, pliers, scooter, and escape hatch. And I would expect that the first thing it would say would be escape hatch, but instead it says, you know, we need to locate and access the escape hatch, which is great, but then it assumes that the pliers are there for a reason. And it says, okay, use the pliers to loosen the bolts or screws. And I never said there was bolts or screws. So there's missing contextual information, there's missing description and what the tool is used for or why I might want to use it, or that I have to use all of them or none of them. And so we really, really have to think carefully about all of these issues when we begin building agentic AI. Something as simple as the name can make or break these systems, the name of a tool, the description of what a tool does. Many people go and they build these systems and they're like, it doesn't work. The generative AI is so bad when reality is, all you need to go is tweak the name of the tool, the surrounding contextual information you give it about the tools, how you might put the tools together, how many of them have to be used, do they all have to be used or none of them? All these types of things become really, really important details when we start building agentic AI systems.\n\nWe've got our Agentic AI going and step by step, trying to solve problems or build out plans in advance. We've got it so that it knows how to be bounded by tools, and we're figuring out how to give it the right information. Let's talk about one of the critical steps. As it goes and uses tools, we're going to have to tell it the outcome of using that tool. That is, it cannot directly observe the result, and so we need to give it feedback on what the result was. Now, the way that we've been doing this, and we've shown it over and over, but I haven't explicitly talked about it as much is we have been giving it a prompt. It goes and tells us what to do. We go and perform that action, and the next prompt that we're giving it is the result of the action. This is critical. The result of applying that tool or performing that action is the follow up prompt that we give it. At the beginning, what we're doing is we're giving it the goal. Go do this, and I want you to do it one step at a time and adapt. It then goes and tells us the first step to perform, and from that point on, as the human being or the system or whatever it is, what we're feeding back to it is not necessarily human questions, but it is the result of actions from tools. Just like we would have in a conversation, what we can imagine is a conversation where a human maybe kicks it off. Then it goes and decides an action that needs to be run. But then the rest of the messages in the conversation where a human would normally be responding, where we were responding and telling it, hey, that cooking action is done. Instead, what we're going to do is we're going to insert the result of the action that it told us to run. Now, we did this implicitly along the way in many cases, but I want to make this really explicit. It's going to say, go and run this action, and then we're going to take it, and we're going to give it a prompt as a follow up that is the result of the action, so we can see what happened. In response, it will tell the next action to run, and then we will go and take the result of that action and put it in as the next prompt, which will be the result of running the action. What we're doing is, we are thinking about, like, where a human being went in and said, help me figure out what restaurant to go to for dinner. Then it kicked off this process where the human being was giving it information, and then it was asking another question. The human being was giving it information, and then it was asking another question. We're going to do something very similar except that it's going to specify an action rather than a question. Then a computer system is going to go and run that action, and we're going to take the result of that action, and we're going to put it in as the next prompt. Then it's going to side the next action based on the result. Then we're going to go and run that action in the computer system, and then we're going to put it in as a result of the next prompt. I'm going to give you a simulation of this in the context of cooking again, because we love steps and planning. With cooking, it's all somewhat intuitive, and we're going to talk about it in terms of a microwave. What we're going to do is we're going to take the microwave, and we're going to break it down. I've called it tools. It probably would have been better to name this actions. But you bear with me on my subtle inaccuracy here in the prompting because it'll still work out just fine. I've told it, here are your tools. Microwave get current time, microwave reset time, microwave increase time, microwave stop, microwave start, microwave open door, microwave close door, insert food in microwave. These are names that start to look like what computer systems speak. These are the names that the programmers types of names that we go and create. If you were creating a Python program, you might create variable names that look like this or function names that look like this, could possibly be. You're probably not going to prefix them with microwave like this, but they might look something like this. What we're doing is we're starting to move the tools and action descriptions closer to the interfaces that our real computing systems work with so that it can start thinking about breaking down and solving problems by specifying actions that look like what it could actually go and run. Now, in this case, we're imagining we've got this biker wave that's computerized, and it can go and run some actions. I say notes. You must set the time on the microwave to start it. You must work independently without input from me. I'm giving it some additional context about how these actions work and some baselines that it needs to know. You have to set the microwave time in order to apply start. If you want to use this action, it depends on another action. That type of information critically important, and it may not be able to infer it. You're going to help me cook food in the microwave. You can only use the provided tools and refer to them by their name. You must complete one step at a time and wait for a description of the outcome. Tell me the first step to reheat a quesadilla? It comes back, and it says, you need to insert the quesadilla in the microwave. Use the insert food and microwave tool to place the quesadilla inside. Then I tell it, here's the result. Food and microwave. This interaction is starting to look much closer to what we could have it do to interface with the real computer system. It then comes back, says, microwave increase time. Then I tell it result, time increased by five seconds. Now, notice the first time, I didn't tell it how much time that we will get an increase. If we go back and we look at the actions, it's just microwave increase time, but there's no specification of how much time. Now, this is where the outcome of applying the tool is becoming critically important because I'm telling I did that. I pressed that button, or I ran that action or I applied that tool, however you want to talk about it. The result is that the time increased by five seconds. This is the first instance where it now knows how much time it's going to get an increase by. Now it has to see that, 1-2 minutes, that's not going to cut it, so now it has to adapt. It comes back and it says, continue to use microwave increase time tool to set the total reheating time. If you want to reheat for one minute, you'll need to increase the time for 60 seconds, use the tool incrementally until you reach the desired time. It's still interfacing on a human level. I go back and I say, result, time increased to 60 seconds. It says, perfect. Now, it says, use the microwave start tool to start the microwave, and I'm just going to throw a little curveball in there for it. I say error door open. It comes back and it says, it looks like the door's open. Use the microwave closed door action. Now I go back and I say result, the door is closed. Notice, every time I'm giving a result, I'm giving it more up to date information about the state of the world around it so that it can make an appropriate decision. When I threw it this curveball and I said, error door open, I'm telling it, you asked me to perform this action. I couldn't do it. But critically, I'm telling it information about why I couldn't do it. I'm giving it information that needs to respond appropriately. When we start building agentic systems, the error messages are critically important that these are very rich and thoughtful error messages. We need to think about, I want to make it so darn clear with my error message, like, what is going on and why? A lot of times we neglect these things in computer systems. We go and we just rely on incomprehensible error messages, and we're just like, programmers will figure it out, IT folks will figure it out. It actually really matters. We want to think carefully about it. Now, sometimes these systems can get really smart, and they can just dissect all of those messages without us going and doing it for them. But sometimes we really need to be thoughtful in the feedback that we give. Error door open, it says, result door closed. Then just one more little note on this. If I had said error 32, and it doesn't know what error 32 is , it's totally hosed. No way it's going to be able to figure out what to do. It's really critical that if it doesn't know an error code, that I translate that error code into something that it will understand the meaning of. If it knows the error codes, who cares? Give it to it in numerical representation, it'll be just fine. Tell it the door is closed now. It says, now, start the microwave, and I tell it, the result the microwave is running. Then it basically comes back and it says, when it's done, use the microwave open door tool to open the door and remove your quesadilla. You see how we've brought it closer to our computer systems. We started to name things in a way that it can figure out and apply these actions or tools. But also, what we're doing is we're now starting to replace human feedback with the result of the tool. Now, the result of the tool right now is me faking it. But what will happen long term is that rather than me faking it, the actual computer system will provide the message back to it. You asked to run this tool, this is what happened. It will then respond, decide the next thing for the computer system to do and go through a chain of these things. The feedback will be critically important that we give it. We must tell it the result of its action so that it understands and can interpret the state of the world around it or the state of the system that it's interacting with, those feedback messages are absolutely critical. The quality of them, the quality of the error messages can make or break these systems. If you go and you create a system and you're like, it's making mistake and mistake, and it's just compounding these mistakes, you probably want to go and look and think about how do you give a really intelligent error message back? Because often if you tweak the error message, suddenly, it'll be able to get out of its loop. There's probably some missing information that it needs to know that if you just gave it in the error message, would help resolve everything.\n\nI want to take you back to that intern analogy again, and I want to think about the different ways that we teach someone to perform a task, or we instruct them to perform a task. And one of the ways we do it is with explicit instructions. We've been doing that over and over. We're telling the agentic AI, here's what you're going to go and do. You're going to do it step by step. But one of the ways that we teach, for example, interns in the real world is we go and show them examples of the problem being solved correctly. So if we want them to write a report, we say, here's the report. Here is my thought process and how I put together this report. Here's the final result. Or we want them to go and diagnose an issue in software, if they're a developer, we say, here was the feature I was trying to implement. Here's how I walked through debugging it or building it or designing it, whatever that thought process was, here's the result. And it turns out we can do the same thing with generative AI through in-context learning. Basically, what this is, is where we do the exame thing that we do with an intern, where we teach through examples, but we can do it with generative AI. And in context learning is incredibly powerful. And for many, many agentic AI systems, it is difficult to get it to perform well without examples, without relying on in context learning. It can be really, really tough to get it to perform as well as we want it to. So, I'm going to give you an example of this and the power of this. So, I want to take you back to that alien spaceship escape example I had. So, I had all these tools that had lost contacts because I changed the names and I had removed the descriptions. And now it doesn't know how to problem solve with them because it doesn't know what they do. Now, I could go and explicitly say this is what the tool does, but I'm going to do something different. Instead of telling it what the tool does, I'm going to show it examples of using the tools to solve problems so it can learn what the tools do from seeing how they were used. So here are my tools. Now, I've changed the ordering and the naming slightly, so bear with me. X155, Q63, L199, totally incomprehensible names, no descriptions. Now what I'm doing is I'm going to tap into in context learning, I'm using few shot examples. Basically, I'm giving it examples of solving a problem. So I say, example tool use. Problem: feeling hungry. Thought: I need to prepare food. Tool: q63. Result: alien pizza prepared. Now, you as a human being can look at this, and you can say, okay, the q63 tool has something to do with cooking pizza, or cooking or making pizzas materialize out of thin air. Next example. Problem, need to move to other world. Thought, I need a wormhole. Tool, X155. Result, wormhole open to Vanderbilt University. And then I go on to the next one. Problem, I need to get to the anti gravity room. Thought, I need a method of transportation around the ship. Tool L 199 result, I'm riding on a scooter towards the room, and so I haven't explicitly told it what the tools do, but I've told it if I have this problem, this is my thought process behind solving it. Here is the tool I'm going to use, and then I'm showing it the result of that tool's application. And so I'm teaching it how to solve problems and to think about solving problems, and I'm showing it in that context how the tools are actually doing things, but I'm not telling it. This tool does this very different. This is like what we would do with a human being. So what do we get? Problem, I need to get off the ship to Earth! Thought colon. Now, one of the things that, in context, learning does is it establishes a pattern that the large language model wants to follow. And so if I show it an incomplete part of a pattern, it wants to finish the pattern. So these models were taught to, you know, roughly predict the next word in a sentence at a very high level, we can think about it that way. So, if I show it an incomplete sentence, it wants to complete it. If I show it a partial pattern of solving a problem, it wants to finish solving the problem with one of the tools, and it wants to solve it in a way that is similar to what was done before, because it wants to stick its reasoning into that pattern, its output should fall into that pattern. So when I pick up in the middle of the pattern and I say, problem, I need to get off the ship to earth, and just leave it with thought, it now says, problem, I need to get off the ship to Earthen. Thought, I need a way to locate the nearest escape pod. Tool X155. Result, wormhole open to Vanderbilt University. So now I can escape the ship. And if I then introduce new things, what's interesting, because remember, I didn't tell it, I told it tools that were available to it. And I told it, pattern on how to solve. And now, what happens when I, for example, give it a new tool at its disposal? So. I say, result, warm hole open to Vanderbilt University. New tool added, inner wormhole. Now it says, problem, I need to get back to Earth from Vanderbilt University, which is kind of funny, because now it thinks that Vanderbilt is not on earth. Thought, I need to use the wormhole tool to travel back to Earth. Tool, inner wormhole. And so, it may not be completely accurate in his description of Vanderbilt University not being on Earth, but it is recognizing now that this new tool that was introduced, it can use to escape by going into the wormhole. But the more important thing is, it followed the pattern. It again restated what the current problem is. It explained its thought behind it, and then explained the tool that it was going to use. So it picked up on the pattern. In many cases, this is going to be really important when we think about interfacing with our real computer systems, where the problem solving process, the steps that we use to solve the problem, must follow well defined patterns, because these systems can't deal with arbitrary patterns, they can't deal with arbitrary language. So using in context learning helps us not only to teach it about things like the tools, but it also helps us to teach it patterns of the steps that we follow, patterns behind the reasoning, like the thinking. And that's can, when we say thought or think step by step, all these things like that around, we're showing it thought processes, decomposition of the problem into thoughts, you know, based on the thought, what tool to use, these types of things, we often are also teaching it not only how to problem solve, but how to problem solve in a pattern or sequence that will work with the systems that we need it to work with. Very, very powerful concept, very, very important tool to use. If you're working with an agentic AI system and you're having trouble with the prompts behind it, and you want to figure out, what do I do to get it more constrained, more sort of, like, unified and predictable in its behavior? Start by looking at in context learning and giving it examples of what you want it to do, and often that can help but constrain it. And if you think about it, if you went to an intern and you tried to explain to an intern how to write like Tolstoy, that's going to be a very long list of rules. But you could probably get that intern up to speed much faster by simply showing them a few pages of Tolsoi's work. And so many problems it's going to be easier to describe it in examples than it is going to be to describe it in rules or instructions that, may not cover every edge case and may not be perfectly crisp and understandable. Let's look at another example of this. I go back to the microwave example, and in this case, what I want it to do is I want it to learn the concept of time that I want it to infer that increased time is going to be by 5 seconds. And therefore it needs to use that microwave increase time tool over and over in order to get the time to what it wants. Now, before it told me, hey, just go and increase the time until you get to 60 seconds. So I show it the tools again, similar tools to last time with the microwave. Now I show it example tool usage. And I say, problem, reheat leftover pizza. Thought, I need to heat the pizza for 20 seconds. And then I show it applying. First, I say, open microwave door. Closed door. And then I show it increasing the time four times, which is four times five is 20 seconds. So I'm showing it how to do that and then the result. And then I do another one. I'm going to soften ice cream, and this one is going to be 10 seconds. So it's two applications now. I say, I'm warming up food in the microwave. You will output one step at a time with a problem thought and one or more tool steps. The system will provide the result after you output all your tool steps. I need to melt some shredded cheddar on nachos. So, I've put in that problem thought tool format again, but this time I'm expecting multiple tool outputs. So, I need to melt some shredded cheddar on nachos thought. And it comes back and it says, tool, microwave, open door. Tool, microwave closed door. It's picking up on the pattern that, okay, I need to open the door, put the food in, close the door, and then it says 30 seconds. And it translates that into six. Microwave, increase time tool calls. So now what we've moved is even closer to being able to interface with a real computing system. Does that set of tool calls and then says, please provide the result. I say, heated food or food heated, sitting in microwave. Now it problem solves a little bit. Open door. I tell it, result, food heated sitting in microwave door open. It queries again and it says, okay, now you've, it's gone and gotten the food out, so close the door. So, I've now got the nachos ready to serve. So, in context learning is critically important many times for agentic AI. Particularly, we want it to interact with computer systems that are brittle, fragile, very, they have very limited formats. They are not like a human being. They do not want to accept weird things that they have never seen before. A human will gladly interpret and figure it out. Computer systems do not do this. And the moment that we start needing that rigid pattern, we typically also often need in context learning to start showing it the patterns that are acceptable to those computer systems and start teaching it how to interact with them in structures that are acceptable. How to query a database, how to go and interact with an exchange server to send an email, how to do x. We go and we give it examples of sort of fake examples of using that thing correctly in order to solve the problem. Or we get blog some way we capture real traces of solving that problem in that language and we show it to it and say, this is how you do it. Critically important to use in context learning. When you're going and trying to build these agents up and you're trying to think, well, how do I get it to work? Why is it not working? It keeps failing. It keeps trying to interact with the computer system, and computer keeps like, having absolute fit over what's being set. Think about going and adding a few shot examples or even tons of examples to get it to work."
  },
  {
    "index": 2,
    "title": "2 Overview of Custom GPTs",
    "content": ">> Agentic AI is one of the most exciting things that I've worked with in my career as a computer scientist. I mean, generative AI was transformative and agentic AI really takes it to the next level. And normally at this point you go into a course and what you hear is basically, now go be a programmer and you can take advantage of agentic AI. But the truth is you don't have to do that. Anybody can go and build incredible agentic AI or agents. They can do amazing things for them. And that's what I'm going to show you next. Now, the pathway to get there is going to be through something that you may have heard about that maybe you haven't paid a lot of attention to, and that is a GPT, a custom GPT. Now, what is a custom GPT? Well, that's what you're going to learn about in this section. And what you want to think of is a GPT is an agent. It is basically a version of ChatGPT that is customized. So it has agentic capabilities. And so what we're going to do is we're going to go and build, you can think of it as a version of chat GPT. We're going to essentially program a version of chat GPT, but we're going to program it in natural language to tell it what to do, to describe all the tools that it can use. And then we're going to connect those tools to real systems. We're going to do some amazing things that then you can take and do just awesome automations for your work, for your life, for your business, whatever it is. But the core of it is going to be agentic AI. Now, I struggled in this course not to just jump directly into having you build something, but I think it's really important to understand the concepts as you go, because what you'll see is that sometimes these systems just don't work right. And if you understand the fundamental concepts, you will be so much more powerful. So I'm going to take you through some of the state of the art concepts in agentic AI, but now I'm going to explore the rest of the course in the context of GPTs. Basically, these are OpenAI's ability to go and customize the ChatGPT experience, but also to add agentic behavior to it. Now, I feel a little bit sad as a programmer telling you this, because the programming has always been my domain, right? I've always been able to build myself these types of things. And people who were not programmers did not get to do that. And now I'm going to let you in on the world that programmers have roughly locked you out of. Now, we've had these no code, low code, all these other tools to try to make it easier, but nothing has ever compared to having GPT's knowledge of agentic AI and then putting them together with something amazing that we're going to see in a minute, which is Zapier, and the ability to go and have it actually perform real automations that will help you and support you and do it intuitively with natural language is a game changer for most people, even as a programmer. It is a fundamental game changer for me personally. And now I'm going to walk you through how do we go and build GPTs, and we'll talk about how agentic AI is manifested in GPTs, and then how the principles we're learning apply, and then we'll go and build some amazing things together. So stick with me. I know we've gone through a lot of theory, but this is where we're going to start really getting down to how you build real agents to help yourself.\n\nTo really use generative AI effectively, we have to start thinking about programming it. Now, if you're not a software engineer or a computer scientist, this may seem alien to you. But it's really quite simple and it's important for actually really getting the potential of generative AI. I want to show you what I mean by this. I want you to start thinking about programming generative AI to do something. Now, what does that mean? Well, let's take a look at a really simple example. I've gone into ChatGPT and I've said I'm going to the grocery store. Really simple statement, not a lot of context about what this means, what it's supposed to do, and how does it respond. That's great. Do you need to help with making a shopping list or perhaps some tips to what to buy? You see, I'm just giving it a simple statement. Now, what if when I want it to respond, I want it to respond in a happy, excited over the top way, rather than something really flat? What would I do? Well, I could go into this conversation, I could say, redo your last response and make it really exciting. But there's another way to get there, and it's to start thinking about time, telling it what to do in the future before you've given it the message that you want it to change its response. I could go now and I could say update your response. Make your response really excited and over the top, like that. Now it says, oh, wow, that's absolutely fantastic. Heading to the grocery store is like embarking on a thrilling adventure. But the challenge with this is I had to go after the fact and tell it what I really wanted. I had to go and give it that additional context. Let's say you were trying to build some type of custom GPT for somebody else to use, you don't want them to have to know to go in after the message and say what you want. You want to be able to tell them upfront, go do whatever you want and it will figure out the right way to respond. Let's see how we can do that. Rather than thinking about just saying, here's the message, instead, we can start giving it time and programming it. We can say from now on, and this is really important, we're saying, in the future, message is to come. Respond every message with a super excited and happy tone. Be way over the top. What I'm doing is I'm not giving it a message for right now. I'm giving it a message about what to do in the future. When you start thinking about that, describing what you want done in the future is programming. That is, programming how it's going to respond in the future when you give it a message, or maybe you're creating a custom GPT and somebody else gives it a message, but it's creating that context. From now on, respond to every message with a super and excited, happy tone. Be way over the top. I'm going to the grocery store. Now we get that excited message that we saw before. Wow, that sounds like an epic journey. You see we're programming it to respond differently. Now, what are the types of things we might want to program in? We might want to program in tone like we have right here. What does it sound like when it responds? Is it really formal? We could have gone in and we could say from now on, respond in an extremely formal way. That's one thing we could have done. We could have said, be really over the top and excited. We could say be neutral. We can adjust the tone that it uses to respond to the user. What else can we do? Well, I'm going to go in and I'm going to ask it, give me some unique ideas for how to use generative AI that combine all of my roles and what you know about me. Now, he doesn't know anything about me, doesn't know who I am, doesn't know my name, doesn't know where I live, doesn't know what my job title is, and it gives me a bunch of generic stuff. Some of them actually, I think, randomly hit the nail on the head. Personalized learning and development tools. Well, that would be something that's interest to me. Creative content generation, data-driven wellness, not so much of an interest to me. AI-assisted project management, innovative problem-solving sessions, enhanced environmental initiatives, interactive historical learning experiences. All of these are exciting things, but they're not really about me, they're not personalized to me. What's something else we can do to program it? Well, we can program it with background knowledge. It could be knowledge about who I'm and where I live, or it could be knowledge about my business, about my university. It could be knowledge about how I want things done in terms of here's background knowledge and process that you should know. I can do that, I can give it background knowledge. I can say from now on remember the following things and use them to help you respond. My name is Jules White. I'm a Professor in Computer Science at Vanderbilt University. I'm the Senior Advisor to the chancellor in Generative AI Enterprise and Education Solutions. I live in Nashville, Tennessee, and I teach a large class on Prompt Engineering on Coursera. Now I've given it background information about me. Now if I go and I say, give me some unique ideas for how to use Gen AI that combine all the roles you know about me. Now I get things that are based on where I live, Nashville-themed AI generated educational content, AI-driven enterprise innovation workshops. You see it's pulling information it knows about me. I've given it background information. We can program tone, we can program it with background knowledge that it should base its answers on. Not just the knowledge that it was trained on, but we can actually give it new knowledge that's just baked in. Now, people talk a lot about having to train a new model, but in reality, most of the time you don't even need to train a new model. You just need to tell it. Here's the information that you don't know today, that you need to know in order to respond or solve the problems that I want you to solve. What else can we do with programming them? Well, I've gone through and I've taken Vanderbilt's webpage and I just copy and paste it. I just highlighted everything on Vanderbilt's webpage here, and I have dumped it into ChatGPT. It's got tons and tons of unformatted information. What does it do with this. Well, what it does is it starts summarizing, because that's a common use case that a lot of people use it for. It starts summarizing all of this stuff, which is great if that's what I wanted, but I wanted something slightly different. I wanted to say from now on, turn everything I give you into CSV. This is a comma separated value, or tabular data like you could import into a tool like Excel, or numbers. No matter what I dump into it, it gives me structured information out. That's a very different way of behaving than ChatGPT normally does. I'm totally changing its standard behavior and reply to do something much more complex, which is turned into data. I'm basically giving instructions about what to do and how to respond by default. It says, okay, I understand. Now I go and I dump all that same Vanderbilt information from the webpage into it, and now you see what it's doing. It's turning it into data. It's creating a version of ChatGPT that's customized, that knows no matter what I give you, turn it into structured data that I can do something with. You're beginning to see that we can program the large language model to do unique and new things that we normally don't think it can do. We normally think, it doesn't know about me, it doesn't know my background. Or it won't know the user's background that comes in and starts interacting with it. Because when we start think about customizing it for other people, we want it to know who they are. Or if we start thinking about customizing it for other people and we're going to be representing our organization, we may want to customize the tone that it takes and how it communicates with people. If we're a law firm, we might want it to be very formal and very careful in what it says. If we're a really fun brand, we want it to be over the top and excited about what it's doing. Great job that you're going to the grocery store. But in order to do that, we have to customize that tone. Programming it is all about telling it what's going to happen in the future. We go from being able to just say, I'm going to the grocery store and getting a flat response to this. To going and saying, from now on, which are really powerful words, everything into the future, go and do this differently, and now we get the big over the top answers like we have here with emojis and we can program what it knows about you or the user, the background information it has. We can program really complex things like that. No matter what you give it, it turns it into data and suddenly we can build office applications, work applications, all kinds of complex things. These all come from the simple idea that we can program how it responds in the future in the conversation. That's the key to really building exciting, customized versions of large language models like ChatGPT.\n\nNow, if we have to go in every time we're going to interact with ChatGPT or a large language model, and type all this background programming in, from now on do this, this and this, that's a pain. There's also other issues with it. Like if we have a really long context, like we have a really long conversation, it may forget what we told it at the very beginning so we need some way of giving it these instructions so that every time we start a new conversation, it'll automatically have all of the background programming we want it to have about who you are, or the user is, what the tone is, all this stuff. Now, how do we do that? Well, one way we can do that in ChatGPT is through custom instructions or this menu that says, customize ChatGPT. Now, if there's one thing I've learned from these classes and recording video for them, by the time you see this, the interface for this very well hood of change, because it's moving so quickly, that the fundamental concept is the same. If you're watching this video later and there's not this exact screen, go and look for the place where you can put in custom instructions or something like that, customize ChatGPT, because that's what I'm doing here. Now, I'm going to build some custom instructions for my 10-year-old. I'm going to say, I'm a 10-year-old that lives in Nashville, Tennessee. I'm in the 4th grade, we're learning about geometry in school. I love BMX and mountain biking. I'm putting that in the custom instructions where it says, what would you like ChatGPT to know about you to provide better responses? Now, remember, I did this for myself before, but I'm going to try to build a custom set of instructions for my 10-year-old. How would you like ChatGPT to respond? I'm going to say, always explain concepts in a fun way that I can understand and weave in my interest. I want to customize ChatGPT to what he does and what he likes. I'm going to save this, and now, we're going to have a customized version of ChatGPT. Let's go and start a new conversation and see how this changes things. Now, we've got a new conversation, I'm going to switch it to GPT 4, and I'm going to say, please explain some fun math concepts. Now, let's see if we get something different. We'll notice already it knows who I'm and what I'm interested in. Here's your love for BMX bikes and mountain biking, mixing in, imagine math is like a secret trail map that helps us explore and understand the world in some awesome ways. We've immediately customize ChatGPT in a totally new way. It's going to respond differently. The magic of pi and circles, and imagine your BMX bike wheel is a perfect circle, angles and jumps. When you're doing a jump on your BMX or your mountain bike, this is by the way, something he loves to do and will probably pay attention to, you're creating angles with your bike in the air. It's completely changed how it responds because now we've gone and we've programmed it to have a different default. Now, this is something simple that a lot of people don't take advantage of, but you can go and customize ChatGPT. Now, a lot of people talk about training new models. There are cases where you probably do want to train new models, but the vast majority of us, the vast majority of businesses and organizations do not need to train new models. They need to customize the model, program it to do what they want and respond in the way they want so that when you go into it, it is already primed and ready to go in the direction you want as we see here. I didn't need to train a new model in order to have something that now knows about my son, knows about what he's interested in, explains concepts at the right level and does the right thing, I just went in and started programming the custom GPT, or in this case I customized the ChatGPT experience for that with additional instructions. Now, there's something unique about what's going on here. One of the things that we're doing is we're doing it in this way through these custom instructions that if the conversation gets long, these instructions will always be emphasized and remembered. That's a key part of this. If we just went into the conversation, we would have to type all this in for one, and that is a pain, but it's not the major problem. One of the bigger problems is if we had a really long conversation, it might totally forget these background instructions and rules. Part of what we get with this customization option, these custom instructions for ChatGPT is that behind the scenes, it keeps reminding ChatGPT by inserting them back into the prompt of what we've asked for, how we've wanted to program so it won't forget them. Now, you could do it in a conversation, but it might forget them, but if you do it in this way, they'll always be there and always be present. That's the difference, is we're essentially customizing it forever, not for the length that it can remember those instructions. If we did it just based on typing it into a message, at some point it might start forgetting again and we'd have to go and re-provide it the information on our ground rules so it remembered what we wanted again whereas this is brought back into the conversation over and over again. Now, this is a key concept, and I just want to take a moment to explain this. Behind the scenes, when we're working with these tools, these large language models, we can introduce programming or instructions at the right point in time to change their behavior. We can give it new information, we can give it new rules, and we can do that conversationally if we are power users through the interface so that you type in the right things at the right time. But if you want to create some custom experience like a custom Chat-bot or a special assistant that's going to help people in a particular part of your organization, what you can do is you can create some customized ChatGPT or large language model experience where the right programming is injected to it, either at the beginning of the conversation or you might have something if you're more sophisticated, where at certain points in time it changes the programming. As they're going in and doing one task, you have a certain set of instructions guiding that task. Then at a different point in time, you inject a new set of instructions, you reprogram it a different way. What might this look like? Well, as I'm going through and having my son, he's interacting with it, I might have a mode that's exploration mode, figuring out what he's interested in. Then when he gets there, and he says, I'm interested in that, it goes into teaching mode. It has a different set and style of interacting with him to teach him. Then when he gets into the point where it thinks that he knows enough about the topic to begin being assessed or tested on it, it could go into quiz mode and have a different set of rules for how it responds. Now, we could do this all in custom instructions where we tell it how and when to switch out, or we could have more complex ways of doing it, which a lot of people and organizations are doing these days. But the key thing is these instructions, behind the scenes, that are inserted and can program and change what it knows, how it responds, and in a way that the user cannot change, now one of the things that could have happened before is my son could have gone and he could have changed what I had said. He could have said I'm an 18-year-old and these are the things I'm interested in. I wouldn't want that guardrail, I wouldn't want that customization, I wouldn't want him to be able to go and change it. I want it to be locked in at his age and how I've told it to respond to him. What we're also doing is we're putting guardrails in place. If we do it through the conversation, he can always go and override the guardrails by saying, \"Hey, I've grown up. I'm 18 now. You don't realize it, but I grew up.\" Suddenly it might think he's 18. What we're also doing is by putting these instructions in we can say things like no matter what, you're going to assume that the user is in the 4th grade, even if they tell you otherwise. Then he can't go and change it. Now, there is a whole cottage industry and research of how do we get around these guardrails as users. But in general, the key thing to know is that these guardrails, these custom instructions that are programming it, are putting in place rules that the user cannot easily change. Now, if you have a sophisticated user who wants to go after them and wants to manipulate them, they probably can at this point. It'll always be an arm's race to try to defend against these changes, but you can put in place rules that are much more difficult for the user to change or you can actually make it easy and say, this is the default of how I want you to behave. But if the user wants to tell you to do it in other ways, gladly accept their rules. You get to choose and decide and put those in place in the background, and in some say, cases it may be that you want to put really hard guardrails, and in some places it may be others. But when you start using this customization function, these custom instructions, you're putting in place rules behind the scenes that the user cannot see, they do not have to type in at the beginning of the conversation in order to make these things happen and you can also sort of use them as guardrails to limit what can or cannot be done, which can be a very powerful use case.\n\nOne of the things that really helps to understand is how these models incorporate information. Because a lot of people think that you have to train a model on the information that it's going to use to reason. As we've already seen, we don't have to do that. We can go and give it new information. But I really want to take a moment to really drill into this point a little bit more and talk about an important concept called retrieval augmented generation or RAG. It is a complicated sounding term. Underneath the hood, there is some complexity to it, but at a high level, most people don't need to understand the deep technical details. You just want to understand what it is, how it works, and how it benefits you. Particularly when you're trying to build a custom GPT or some other large language model that's customized to a particular information set without being trained from scratch. Most of the time, most organizations do not need to fine tune or customize a model by retraining it. They need to augment its information that it's used. I'm going to give you an example of this. This is a news article from Vanderbilt from March 25th, of 2024. We have announced a new college dedicated to computing AI and data science. Now, of course, I just instantly dated this video, but that's okay, because I'm also going to show you another important piece. Vanderbilt just announced this. Now, what is the date that the training cut off for GPT 4, which is what I'm using in this conversation. I say, when is your training cutoff? It's April 2023. Now, what happens if I ask you a question about the Vanderbilt new announcement on the connected computing college which came after in 2024? Well, let's find out. What is the new college that Vanderbilt just announced it is creating? Vanderbilt recently announced it has created a new college of connected computing which features programs and various computing disciplines. That's exactly right. How does it know that? Well, look at this. If you highlight, over the end of the conversation, there's these little quotation marks at the very end, right after this. After the 1981, there are these quotation marks. If you highlight that, what does it show? Well, it shows a link to this web page, Vanderbilt to establish a college dedicated to computing AI, data and data science. What's going on here? How does it know that if it wasn't trained on it? Well, what's happening behind the scenes is that when you ask the question, it needs new updated information that it's going and doing a web search or pulling from an updated database of information about the world of what's current. It's searching for current information, and then it's pulling that current information in and it's putting it into the prompt so that it now can respond to it. It's not retraining the model. The model was not retrained with Vanderbilt's latest press release. No. What's happening is, just like we did when we took the Vanderbilt web page and we copied and pasted it in and said turn it into data. What's happening is, behind the scenes, they are finding updated relevant information from the web, either through a web search that's happening dynamically or through some database of up to date information that they're just giving it direct access to pull from. Then they're finding relevant chunks of information and they're putting it into the prompt. You're just not seeing it. Then they're combining that information with your prompt and putting it all into ChatGPT or GPT 4 in this case, and it's responding. Okay. Let's break this down and see what's happening. They're using RAG more than likely to do this. Tell me about the new Generative AI innovation program that Jules White's team is running at Vanderbilt. This is a question that it cannot answer with current information because that announcement is trapped on my linked in page and it does not have access to it. It probably can't find it. It's probably not high enough visibility for it to have access to it and see it or it's not in the top search results. What happens when I ask this question, and I just put this out there like a week or two ago. It says the new Generative AI innovation program at Vanderbilt, led by Jules White, is part of the university's broader future of learning and generative I initiative. Then it goes on and it describes the future of learning initiative which we announced in 2023. It's actually answering the question with old information that is not correct. Because it doesn't have access to that newer information. This is a question that it cannot answer correctly. How do I fix this? If I wanted it to know about my new innovation program, I wanted it to have updated information, I just go and I copy and paste the information in. I went and copied and paste a bunch of the information about the program in, and now I say, tell me about the new Generative AI innovation program, and it does a much better job of getting it right. But one of the things that it's doing is it's still weaving in the other information it had access to. What's happening, and I just want to emphasize this is, in order to make it able to answer this question correctly, tell me about the new generative AI innovation program that Jules White's team is running at Vanderbilt, it does not know and does not have access to that information. I fix that problem by up here, I'm copying and pasting a bunch of information about that program into the prompt. I'm giving it the information it needs in the prompt, and then I'm saying, tell me about the new Generative AI program that Jules White's team is writing at Vanderbilt. I'm basically providing it that information in a prompt, and now, it can answer the question. Now, what's happening over here? Exactly the same thing is happening. When you ask the question, the difference is that you are not having to cut and piece the information into the prompt. Instead, what's happening is behind the scenes retrieval-augmented generation or RAG is being used to pull in the appropriate contents to go into that or a web search or some other mechanism, you can do this in many different ways, is being used to pull in relevant information. It might be you search a database, it might be that you run a web search, it might be that you do RAG, it might be that you do RAG on the results of those things, but something is happening that's pulling in new information and putting it into the prompt to allow it to answer. Most commonly, what's happening is called retrieval-augmented generation. What you can think of this is, let's imagine that you give it a bunch of documents and you want it to be able to go and do a really fancy search of those documents. Somebody just asked me for information about Jules's new innovation program, go and search for information about that innovation program in these documents or in these web pages. Then it would take all the search results that they got. It would concatenate them together, think of it as part of your prompt, and then it would do exactly what I did here, which is copy and paste it into the prompt somewhere, and then add in the user's original question. It's a complex process, but the fundamentals are, take the user's prompt, go and look through a set of documents or a set of web pages or some other set of information, find relevant excerpts, chunks of information from that. You can think of it as find when you go and search of documents. Basically, doing something like that and finding relevant chunks of information and then it's doing what we're doing here, it's pasting them, is what you can think of into the prompt. But it's then adding in the user's original prompt. Now, we have an augmented prompt. We've retrieved information, then we use that retrieved information to augment the prompt and then we send that submit the prompt which is generate using the augmented prompt. The augmentation is basically the cutting and pasting, is the way you can think of it, of the relevant information that was retrieved from the document into the prompt and now, this whole thing is what it's responding to. That is essentially retrieval-augmented generation. Now, there's lots of different ways we can do this, lots of different information sources we can incorporate, but it allows us to not have to retrain the model. We just give it the updated information in the prompt and dynamically, based on what the user asks, we go and try to retrieve relevant information. Now, this is a whole art and science in of itself, but just for now, you can think of it as we take the user's prompt, we look at it, and we figure out what information to retrieve, what chunks of text that can fit into the prompt to retrieve, and then augment the prompt with before we do generation. Now, one thing that we'll have to do sometimes, is tell it how to preference and use the different information sources. For example, we might want it to only reason with the augmented information we give it and not any prior information it found on the Internet or from other sources. I'm going to give you an example of this. I liked its answer and it was fairly good that it gave about tell me about the new generative AI innovation program. But it wanted to weave in all of this discussion of future of learning, which is another thing that we have at Vanderbilt and it is part of that and related to it. But I'd like to just describe it in terms of the new information I'm giving it. One thing we have to also think about is giving it instructions about how to merge these different pieces of information together so it knows what to prioritize and when and how in its answer. I'm going to say, using only the information below, answer the question. Then I give it all that same information I gave it before and then now, I give it the prompt. What we see is that we get answer that is much more driven by what came before, not by other sources of information outside. Instead, now, it's really drilling in on this information that I gave it and preferencing it in its answer. One of the things we have to do is we have to think about what information sources are we going to augment, basically, are we going to retrieve information from and then augment our prompt with before we generate? RAG is we retrieve information, we put chunks of information in the prompt, that's the augmentation, and then we generate the answer. That is, we submit the prompt or hit ''Enter''. That's what's happening here. Then we also have to think about it giving instructions on when you have multiple sources of information, which ones do you preference and how or how do you use different pieces of information. That can often be an important part, is to tell it how to fuse all these things together into reason. But this is where a lot of the magic happens. There's been this misconception for a long time that you have to train a new model if you want to give it new information and that is not the case. You just need RAG, retrieval-augmented generation, you go and retrieve new information that you need. You augment the prompt with it and then you generate. That is where a lot of the magic of thinking of using generative AI is computing that can do new things, comes in because we're feeding it data and we're feeding it instructions and then we're producing, and we're computing an output or an answer. That's exactly what CPUs, GPUs are doing, is they're taking in information and instructions and they're computing an answer. That's what we're doing and that's a really powerful capability.\n\nNow that we've covered the really core concepts, I want to start taking a look at custom GPTs. What is this? You know, what is this new concept of a custom GPT? What does it mean? Let's look at it, and I think it's going to start to make sense to you what they are and why they're powerful. So what we've got here is the current, and I fully aware that this will likely change at some point, but all of the concepts should carry over. So don't worry, fear not, just because the user interface changes doesn't mean the fundamental concepts behind it are going to change. Now, I'm going to go and create a custom GPT. Before I do that, I'm going to read what it says, create custom versions of ChatGPT that combine instructions, extra knowledge, and combination of skills. Now, think about all the concepts that we just learned. Instructions, remember when we talked about programming ChatGPT to do different things from now on, do this. We changed its tone. We changed the way that it would respond to the user's input. So if we cut and paste information in there, we'd get data out. We could do all these complex customizations of the instructions. We looked at how we could put those instructions into custom instructions so that every time you start a new conversation, automatically did those things. And we could also make it automatically know about me and who I am. Then we talked about retrieval augmented generation or extra knowledge that we want to give it. So it stopped training at a certain point, but we saw that through retrieval augmented generation, we could give it extra knowledge that it could pull from, it could retrieve from, augment the problem from, and then generate with. That's the extra knowledge and any combination of skills. We're going to talk about this one later. But what we're doing is we're building up a custom ChatGPT that's based on these concepts that we've learned. So let's go and hit the create button and see what we get. Now, I'm going to start with this configure tab over here because we'll see how all these things fit in. We have a name for the custom GPT, which if we go and type something in here like quiz or let's say fourth grade quizzer, now we can type in a description. I am a custom GPT that can create quizzes for fourth graders. Now we have a description of this thing. Now we have the instructions. Well, what do we want it to do? Whatever the user types in, figure out a creative way to turn it into a quiz for a fourth grader. Make sure that the content is appropriate for a fourth grader, if it grader. If it isn't appropriate, refuse to answer. Make it fun and exciting. Okay, what are we doing here? We're doing something that looks very similar to what we did before where we were programming the GPT. We're creating custom instructions that guide it. But rather than custom instructions that sit in the user's account, every time you trigger this GPT, it's going to be loading this custom instructions. And just like custom instructions, they're going to go with the conversation and they're always going to be relevant and in the context. Now, conversation starters, create a quiz on geometry. Now that's one of our conversation starters. And then knowledge. We could go and upload files. I'm going to skip that for the moment, but we'll come back to it in a future one. But know that we could just go and give it a file that we want to do. Now we've got a basis of a custom GPT that does something very similar to what we saw before. Now let's go and type something in, what happens when a plane takes off? Now we've got the flight takeoff quiz. Now notice something, this is very different behavior than you would get with your normal GPT or, sorry, your normal ChatGPT. We've customized the instructions by default. It knows what to do. It has a name, it has a description describing what it does. It turns things into quizzes for a fourth grader. If I go and create that GPT, now I have this interesting new thing. See, I've got all of this built in and now I've created a customized ChatGPT experience. I could go and save it for just myself. And I'm going to go and create this thing, now I've got this custom ChatGPT. Grader or quizzer, I'm going to put that in and open it up. Now I can go and open my fourth grade quizzer and start immediately. Now, I also created this conversation starter down here. Create a quiz on geometry so I can go and I can hit that button. So if the user doesn't know what to do to get started because they're not sure how to use it, then you've got a little conversation starter. So all this is, is taking all the concepts that we've learned and putting them together into a customized experience. So when I start a new conversation, it's already programmed with the right instructions, it already has the right knowledge to pull from for rag or some other way of doing this. And it also has things like a name, and a description, and a set of conversation starters to get the user going. So this is basically doing all of these things we've seen before, giving it retrieval augmented generation knowledge, but it's going and turning it into something that's simple to get started with. And now somebody can go and discover that basically that custom GPT, and they can get started immediately with everything that I want them to be able to do and know and all of those things. Now you can think about this in an organization. You can go and create a custom GPT for your accounting department, and it automatically knows your rules of accounting. It already has some prompt engineering built into it to do useful things for that group. It already has knowledge built into it. Maybe it's a connection to your systems, or maybe it's documents describing your processes and procedures. Or if you're going to be a university and you want to create a policy GPT, you can go and upload all your policies into the knowledge base. Then when a user asks a question, you can use retrieval augmented generation to go and pull information about your policies and answer based on your information. And you can tell it, I want to preference this information. I only want to answer if you can find the answer in my policy documents, and then you can have it answer. And that's what a custom GPT is, is we're putting all these things together, but in this nice experience where the user can just go and launch it under some named thing, like, I want to launch my policy GPT, and it already has all the right information, all the right programming and instructions. It's like you're building a mini app. We're used to going to our phone and hitting those icons on the phone in order to launch an app. We're doing the same thing, but with a large language model. Now we've got a name for it, we've got a description for it, we're going to see an icon for it later. We hit that icon, it launches that app. Or you can think of it as that custom GPT, which is basically an application. It's pre-programmed with our instructions, it's pre-programmed with our knowledge. It automatically has some useful little conversation starters that the user can hit to get going. It's basically a mini app, but it's the app in the large language model or ChatGPT world. It's not the app on just our phone anymore, and it's something that anybody can build. You can build one, I can build one. Everybody can build one. My fourth grader can build one. But it's just about knowing these basic concepts is the key thing. And then get really creative about what are the instructions we give it? What are the knowledge sources we give it access to? And then later we'll see what are the tools that we give it access to.\n\nNow we're going to talk about tools with GPTs. We've learned a lot about Agentic AI, but we haven't really gotten down to the point where our Agentic AI is manifesting in a GPT that can actually go and access tools. That's what we're going to do now, is we're going to do our first take at looking at how do we give a GPT tools or actions that it can use or, in this case, we're going to call it capabilities. We're going to take a look at this with this fourth grade quizzer GPT. Down here, we see a set of capabilities. In those capabilities, you can think of it as sets of tools that it can go and use. In this case, we've got two that are checked, web browsing and DALL-E image generation. Now, if you think about it, all the GPTs that we've done so far have just been speaking in text. That's it. They haven't had an additional capability because a large language model can't go and just reach out to the Internet despite what many people think, but we change that when we check this box because what happens is it gets access to a tool to be able to search the Internet. Let's go and try this out. Here's what we're going to do. I'm going to say please go and search the Internet for recent information about BMX racing, then use that information to create a fun quiz for me based on math. What we're going to do is we're going to see that right now it's going and it says it searched five sites, and now it's giving us a quiz based on what it found and math. Alise Willoughby won her third UCI World Title recently. Go, Alise. If she had already won two times before, how many titles does she have now in total? Now what we see is that it's going and basically asking us questions. It's not the greatest question on the Earth. I'm not sure this is a fourth grade appropriate, but it is actually going and using the tool in searching. The 2024 BMX Racing World Championship saw writers from over 45 nations compete. If each nation sent three writers, how many writers participated? It's got some information. Then we can go and look at all of this and have a quiz. Now, how did it do that? Well, you see right here where it says searched five sites? It was given access to a tool, a capability. That's what we clicked on over here. When it says searched five sites, we can go and look at the tool usage. The tool it used was bing.com. It was given access to Bing, and then told it could go and run a search for it. If I went and looked there, what I would find is the results of running that search. Now, how did it do this? Well, it went and said, I want to select the search tool and here's what I want to search for. You think back to when we were getting it to output, different tools that it wanted to use, that's what's happening here, except then rather than me as a human being have to go and copy and paste that search string into bing.com, get the result and bring it back, it's happening automatically behind the scene. We don't have to figure that out. OpenAI has done that for us. They've automatically created that part, that automation. They'll go and say, hey, it says it wants to use the search tool. I'm going to go and use the search tool, I'm going to get the results, and I'm going to bring it back in. That's part of the beauty of all of this, is it's just happening automatically and we get to tap into it. We get to focus at the level of the agent, thinking about what do we want it to do, what are the rules that we want to give it, what are the tools and actions we want to give it or capabilities we want to give it access to. We get to focus at that creative domain, this lower level domain that probably requires a bunch of programming and all kinds of stuff to get it to work well. We don't have to mess with it. We just have to focus on being creative and thinking about how it's happened to this amazing capability, and what tools we want it to have, what instructions we wanted to give it with those tools, all of those things are where we're going to focus. Then what we see down here, of course, is all the results it got back. Basically, it was given probably either the full web page or a chunk of the text from this web page, and then that was given back as basically a message just like we replied. This would be actually an automated reply being sent in, and then it's responding to that reply or that response with the raw information that it requested, and then it's going and translating that into our math quiz. Now, let me show you one more tool that's in its toolbox. Please create an image based on all of this. The model itself now is going to reach out and it's going to say, okay, I need to use a tool to create an image. What's happening right now is that image is being created. That tool is running. It's now gone and created the image. It's fun. Now we've got this BMX racing image that it's created. The way it did that was it said, okay, go and run the image generation tool, which if we see over here it's DALL-E. DALL-E is the image generation tool. Basically what it said is, I want to run the DALL-E image generation tool, here's what I'm going to tell it to do, and it gives it basically instructions on the image it once generated. DALL-E goes and runs, creates the image, and then hands it back to it. Basically, it hands it back probably a link to the image and says here's the link to the image. You can display this for the user. That's tool use in the real world with GPTs, where we've moved out of theory now and we're working with the actual GPTs and seeing them go and use tools. Now, what we're going to go and move to next is how do we plug in our own tools, and this is an exciting thing that we're going to do.\n\nThank you so much on embarking on this fascinating journey with me through how do we use agentic AI, but not only how do we understand what it does, but how do we use it to benefit our everyday lives to help us with things like this travel expense agent. I hope you'll feel inspired to go and build some amazing agents and I hope if you do build some that you want to talk about, that you'll post them on LinkedIn or other things and link back to this course. I'd really appreciate it. And my ability to go and create these courses is directly supported by your work and going and talking about the course and how it's helped you. So if you do something great, please talk about it on LinkedIn. Please talk about it on Facebook or X or wherever you go and post social media. And if you create something amazing, record a video of it and post it. Tag me in it. I would love to see it. I would love to hear about the amazing things that you're doing. And I'd also say just stay tuned. Some more great courses may already be available on these topics or may coming very soon. So please check out my instructor profile on Coursera to see more courses that I have. I've got close to 20 courses on coursera on different topics. There's lots of other amazing stuff there. If you've taken this course and you want to learn how to build really great GPTs and really focus on just the prompt aspect and all the ways the GPT can interact outside of the actions, go and check out my course on OpenAI GPTs building custom assistance. Go check that out. Great course. If you want to know more about a lot of the prompt engineering I was doing, which is really important for making a lot of these assistants have great outputs, go and check my course on prompt engineering or my course on advanced prompt engineering. Both of those are great courses to go and look at. If you want to understand how do we make these systems more trustworthy, check out my course on trustworthy generative AI. And if you've got a kid and you think this is really fun, you want to start teaching them these things, go and check out my class on teaching generative AI to kids. It's called generative AI for kids, and it's for parents and teachers to go and teach kids about the fundamental things they want to know about when they start working with generative AI and how do we set them up for success in this world. Thank you so much for being part of this course. I really do hope that you'll post some of the amazing things that you build with this and that you'll tag me so that I can see them."
  }
]