## 1. RAG Overview

## Questions

#### 1. What is the primary purpose of Retrieval Augmented Generation (RAG)?  
A) To improve LLM responses by grounding them in relevant external information  
B) To replace LLMs with traditional search engines  
C) To enable LLMs to generate text without any training data  
D) To reduce hallucinations by injecting up-to-date knowledge  

#### 2. Which of the following are limitations of large language models (LLMs) that RAG aims to address?  
A) Inability to access real-time or recent information  
B) Lack of access to private or confidential databases  
C) Perfect accuracy in all generated responses  
D) Knowledge cutoff due to static training data  

#### 3. In the RAG architecture, what is the role of the retriever?  
A) To generate text based on user queries  
B) To search a knowledge base and find relevant documents  
C) To update the LLM’s training parameters in real time  
D) To rank and filter documents for relevance  

#### 4. Why can LLMs sometimes produce “hallucinated” information?  
A) Because they generate text based on statistical likelihood, not factual truth  
B) Because they have access to too much real-time data  
C) Because they are designed to always produce truthful text  
D) Because they rely solely on the retriever for information  

#### 5. Which of the following best describes the “context window” limitation in LLMs?  
A) The maximum number of tokens the model can process at once  
B) The number of documents the retriever can return  
C) The total size of the knowledge base  
D) The number of parameters in the LLM  

#### 6. How does RAG help reduce hallucinations in LLM-generated responses?  
A) By grounding answers in retrieved, relevant documents  
B) By limiting the LLM to only generate short responses  
C) By updating the LLM’s training data continuously  
D) By providing the LLM with external factual context  

#### 7. Which of the following are typical sources for the knowledge base in a RAG system?  
A) Company internal manuals and FAQs  
B) Real-time social media posts without verification  
C) Specialized legal or medical case files  
D) The LLM’s original training dataset  

#### 8. What is a key trade-off when deciding how many documents the retriever should return?  
A) Returning too many documents wastes computational resources and context space  
B) Returning too few documents may omit important information  
C) Returning only one document guarantees the best answer  
D) Returning irrelevant documents improves answer diversity  

#### 9. Which of the following statements about LLM training is true?  
A) LLMs learn by updating billions of parameters based on large text corpora  
B) LLMs are trained to generate only truthful and verified information  
C) Before training, LLMs generate random or nonsensical text  
D) LLMs memorize every fact from their training data exactly  

#### 10. What does “autoregressive” mean in the context of LLMs?  
A) The model generates each new token based on all previously generated tokens  
B) The model generates all tokens simultaneously  
C) The model updates its parameters after each token generation  
D) The model ignores previous tokens when generating new ones  

#### 11. Why can’t LLMs access private or confidential information on their own?  
A) Because such data is not included in their training datasets  
B) Because LLMs are designed to avoid privacy violations  
C) Because private data is encrypted and inaccessible to the model  
D) Because LLMs automatically update with all internet data  

#### 12. How does the retriever understand which documents are relevant to a query?  
A) By using semantic similarity and vector search techniques  
B) By randomly selecting documents from the knowledge base  
C) By keyword matching only, ignoring context  
D) By scanning every token in the knowledge base for exact matches  

#### 13. Which of the following are advantages of using RAG over a traditional LLM alone?  
A) Ability to provide source citations for answers  
B) Reduced latency in generating responses  
C) Ability to incorporate recent or domain-specific knowledge  
D) Complete elimination of all hallucinations  

#### 14. What is a vector database and why is it important in RAG systems?  
A) A database optimized for semantic similarity search using vector representations  
B) A traditional relational database storing structured tables  
C) A database that stores only numerical data, not text  
D) A database that automatically trains LLMs  

#### 15. In what way does RAG improve code generation tasks?  
A) By retrieving project-specific code snippets and documentation  
B) By replacing the need for human programmers entirely  
C) By generating code without any context or prior knowledge  
D) By tailoring answers to the actual codebase and coding style  

#### 16. Which of the following best explains why simply adding all possible information to an LLM’s prompt is not practical?  
A) It increases computational cost and exceeds the model’s context window  
B) It guarantees perfect answers but is too slow  
C) It confuses the LLM and causes it to hallucinate more  
D) It is impossible because LLMs cannot process text  

#### 17. How does RAG enable LLMs to stay up-to-date with new information?  
A) By updating the knowledge base independently of the LLM’s training  
B) By retraining the entire LLM every time new data appears  
C) By ignoring outdated information in the training data  
D) By using only the original training data without retrieval  

#### 18. Which of the following are challenges faced by the retriever component in RAG?  
A) Balancing relevance and completeness of retrieved documents  
B) Perfectly ranking all documents every time  
C) Avoiding returning irrelevant or redundant documents  
D) Automatically generating new knowledge  

#### 19. What is meant by “grounding” the LLM’s responses in RAG?  
A) Providing the LLM with relevant retrieved documents to base its answers on  
B) Restricting the LLM to only generate answers from its training data  
C) Forcing the LLM to cite sources it invents  
D) Preventing the LLM from generating any new text  

#### 20. Which of the following statements about the relationship between information retrieval and LLMs is correct?  
A) Information retrieval systems existed before LLMs and provide precise data search  
B) LLMs replace the need for any information retrieval techniques  
C) Combining IR with LLMs leverages strengths of both for better results  
D) IR systems generate text, while LLMs only search databases



<br>

## Answers

#### 1. What is the primary purpose of Retrieval Augmented Generation (RAG)?  
A) ✓ To improve LLM responses by grounding them in relevant external information  
B) ✗ RAG does not replace LLMs with search engines; it complements them  
C) ✗ LLMs still require training data; RAG does not eliminate this need  
D) ✓ To reduce hallucinations by injecting up-to-date knowledge  

**Correct:** A, D


#### 2. Which of the following are limitations of large language models (LLMs) that RAG aims to address?  
A) ✓ LLMs cannot access real-time or recent information after training  
B) ✓ LLMs cannot access private or confidential databases on their own  
C) ✗ LLMs do not have perfect accuracy; hallucinations occur  
D) ✓ LLMs have a knowledge cutoff due to static training data  

**Correct:** A, B, D


#### 3. In the RAG architecture, what is the role of the retriever?  
A) ✗ The retriever does not generate text; that is the LLM’s job  
B) ✓ The retriever searches the knowledge base for relevant documents  
C) ✗ The retriever does not update LLM parameters  
D) ✓ The retriever ranks and filters documents for relevance  

**Correct:** B, D


#### 4. Why can LLMs sometimes produce “hallucinated” information?  
A) ✓ Because they generate text based on statistical likelihood, not guaranteed truth  
B) ✗ LLMs do not have access to too much real-time data; they lack it  
C) ✗ LLMs are designed to generate probable, not always truthful, text  
D) ✗ LLMs do not rely solely on the retriever; hallucinations occur without retrieval  

**Correct:** A


#### 5. Which of the following best describes the “context window” limitation in LLMs?  
A) ✓ It is the maximum number of tokens the model can process at once  
B) ✗ Number of documents returned is a retriever setting, not context window  
C) ✗ Knowledge base size is unrelated to context window  
D) ✗ Number of parameters is model size, not context window  

**Correct:** A


#### 6. How does RAG help reduce hallucinations in LLM-generated responses?  
A) ✓ By grounding answers in retrieved, relevant documents  
B) ✗ Limiting response length alone does not reduce hallucinations  
C) ✗ RAG does not retrain the LLM continuously  
D) ✓ By providing external factual context to the LLM  

**Correct:** A, D


#### 7. Which of the following are typical sources for the knowledge base in a RAG system?  
A) ✓ Company internal manuals and FAQs are common knowledge bases  
B) ✗ Unverified social media posts are unreliable and usually excluded  
C) ✓ Specialized legal or medical case files are used in high-impact domains  
D) ✗ The LLM’s original training data is static and not part of the retriever’s knowledge base  

**Correct:** A, C


#### 8. What is a key trade-off when deciding how many documents the retriever should return?  
A) ✓ Returning too many wastes computational resources and context space  
B) ✓ Returning too few may miss important information  
C) ✗ Returning only one document does not guarantee the best answer  
D) ✗ Returning irrelevant documents does not improve answer quality  

**Correct:** A, B


#### 9. Which of the following statements about LLM training is true?  
A) ✓ LLMs learn by updating billions of parameters on large text corpora  
B) ✗ LLMs are not trained to guarantee truthful information, only probable text  
C) ✓ Before training, LLMs generate nonsensical or random text  
D) ✗ LLMs do not memorize every fact exactly; they learn statistical patterns  

**Correct:** A, C


#### 10. What does “autoregressive” mean in the context of LLMs?  
A) ✓ The model generates each new token based on all previously generated tokens  
B) ✗ Tokens are generated sequentially, not simultaneously  
C) ✗ Parameters are fixed during generation, not updated token-by-token  
D) ✗ The model depends on previous tokens, not ignores them  

**Correct:** A


#### 11. Why can’t LLMs access private or confidential information on their own?  
A) ✓ Such data is not included in their training datasets  
B) ✓ LLMs are designed to respect privacy and avoid unauthorized data  
C) ✓ Private data is often encrypted or inaccessible to the model  
D) ✗ LLMs do not automatically update with all internet data  

**Correct:** A, B, C


#### 12. How does the retriever understand which documents are relevant to a query?  
A) ✓ By using semantic similarity and vector search techniques  
B) ✗ Random selection is not used in retrieval  
C) ✗ Keyword matching alone is insufficient for semantic relevance  
D) ✗ Exact token scanning is inefficient and not typical in RAG  

**Correct:** A


#### 13. Which of the following are advantages of using RAG over a traditional LLM alone?  
A) ✓ Ability to provide source citations for answers  
B) ✗ RAG typically adds latency, not reduces it  
C) ✓ Ability to incorporate recent or domain-specific knowledge  
D) ✗ RAG reduces hallucinations but does not eliminate them completely  

**Correct:** A, C


#### 14. What is a vector database and why is it important in RAG systems?  
A) ✓ It is optimized for semantic similarity search using vector embeddings  
B) ✗ Relational databases store structured data but are less suited for semantic search  
C) ✗ Vector databases store text as vectors, not only numerical data  
D) ✗ Vector databases do not train LLMs  

**Correct:** A


#### 15. In what way does RAG improve code generation tasks?  
A) ✓ By retrieving project-specific code snippets and documentation  
B) ✗ RAG does not replace human programmers entirely  
C) ✗ Generating code without context leads to poor results  
D) ✓ Tailors answers to the actual codebase and coding style  

**Correct:** A, D


#### 16. Which of the following best explains why simply adding all possible information to an LLM’s prompt is not practical?  
A) ✓ It increases computational cost and exceeds the model’s context window  
B) ✗ Adding all info does not guarantee perfect answers and is inefficient  
C) ✗ Confusion is not the main reason; computational limits are  
D) ✗ LLMs can process text but have token limits  

**Correct:** A


#### 17. How does RAG enable LLMs to stay up-to-date with new information?  
A) ✓ By updating the knowledge base independently of the LLM’s training  
B) ✗ Retraining the entire LLM frequently is impractical  
C) ✗ RAG does not ignore outdated info but supplements with new data  
D) ✗ RAG relies on retrieval, not just original training data  

**Correct:** A


#### 18. Which of the following are challenges faced by the retriever component in RAG?  
A) ✓ Balancing relevance and completeness of retrieved documents  
B) ✗ Perfect ranking is not achievable every time  
C) ✓ Avoiding irrelevant or redundant documents is important  
D) ✗ Retriever does not generate new knowledge, only finds existing data  

**Correct:** A, C


#### 19. What is meant by “grounding” the LLM’s responses in RAG?  
A) ✓ Providing the LLM with relevant retrieved documents to base its answers on  
B) ✗ Grounding means supplementing, not restricting to training data only  
C) ✗ LLMs do not invent sources; grounding uses real retrieved info  
D) ✗ Grounding does not prevent new text generation, it informs it  

**Correct:** A


#### 20. Which of the following statements about the relationship between information retrieval and LLMs is correct?  
A) ✓ Information retrieval systems existed before LLMs and provide precise data search  
B) ✗ LLMs do not replace IR; they complement it in RAG  
C) ✓ Combining IR with LLMs leverages strengths of both for better results  
D) ✗ IR systems search data; LLMs generate text, not the other way around  

**Correct:** A, C, D