## 3. Information Retrieval with Vector Databases

[Study Notes](#study-notes)

[Questions](#questions)



### Key Points

#### 1. ‚öôÔ∏è Vector Databases and ANN  
- Vector databases are optimized to store high-dimensional vectors and perform Approximate Nearest Neighbor (ANN) searches.  
- ANN algorithms are significantly faster than brute-force K-Nearest Neighbors (KNN) but do not guarantee finding the absolute closest vectors.  
- KNN requires calculating distances to all document vectors, resulting in linear time complexity.  
- ANN uses additional data structures like proximity graphs to speed up search.

#### 2. üåê Navigable Small World (NSW) and HNSW  
- NSW graphs connect each document vector node to its nearest neighbors, enabling graph traversal for search.  
- HNSW (Hierarchical Navigable Small World) improves NSW by adding multiple graph layers with exponentially fewer nodes at higher layers.  
- HNSW search starts at the top layer with few nodes and moves down to more detailed layers, making search approximately logarithmic in time complexity.  
- HNSW scales efficiently to billions of vectors and is much faster than KNN.

#### 3. üóÑÔ∏è Vector Database Operations  
- Typical operations include creating collections, specifying vectorizers, adding documents, building HNSW indexes, and running vector or hybrid searches.  
- Hybrid search combines vector similarity and keyword search, often with weighted contributions (e.g., 25% vector, 75% keyword).  
- Metadata filters can be applied to narrow search results.

#### 4. üìÑ Chunking Documents  
- Chunking splits large documents into smaller pieces to improve search relevance and fit within LLM token limits.  
- Fixed-size chunking uses character limits with overlaps (e.g., 250 characters with 10% overlap) to preserve context.  
- Recursive character splitting respects document structure (e.g., paragraphs, headers).  
- Semantic chunking groups sentences based on vector similarity rather than arbitrary size.  
- Chunking improves recall and precision but increases computational cost and storage needs.

#### 5. üîç Query Parsing and Rewriting  
- Query rewriting uses LLMs to clarify ambiguous queries, add synonyms, and remove irrelevant information before retrieval.  
- Named Entity Recognition (NER) identifies entities like people, places, and dates to improve search filtering.  
- Hypothetical Document Embeddings (HyDE) generate an ideal document from the query to improve retrieval by matching documents to this hypothetical vector.

#### 6. ü§ñ Bi-Encoders, Cross-Encoders, and ColBERT  
- Bi-encoders embed queries and documents separately and use ANN for fast, scalable semantic search.  
- Cross-encoders concatenate query and document and output a relevance score, providing higher accuracy but much slower performance.  
- ColBERT vectorizes each token separately, balancing speed and interaction richness, but requires more storage.

#### 7. üîÑ Reranking  
- Reranking reorders initial retrieval results using cross-encoders or LLM scoring to improve relevance.  
- It is applied after fast initial retrieval to balance speed and accuracy.  
- Typically reranks the top 5-10 documents to return the most relevant results.

#### 8. üèÅ Summary of Key Techniques  
- ANN algorithms enable scalable, fast vector search compared to brute-force KNN.  
- Vector databases are designed for efficient ANN search and outperform relational databases for semantic search.  
- Chunking, query parsing, and reranking are essential production techniques in retrieval-augmented generation (RAG) systems.



<br>

## Study Notes

### 1. üìö Introduction to Information Retrieval in Production

Information retrieval (IR) is the process of finding relevant information from a large collection of data. In modern applications, especially those involving natural language and AI, IR often involves searching through vast amounts of unstructured text or documents to find the most relevant pieces of information.

In production environments, IR systems must be efficient, scalable, and accurate. This lecture focuses on **vector databases**, which are specialized databases designed to handle large-scale vector data and enable fast, semantic search. These vector databases are closely tied to **Retrieval-Augmented Generation (RAG)** systems, where retrieved documents help language models generate better responses.


### 2. ‚öôÔ∏è Vector Databases and Approximate Nearest Neighbors (ANN)

#### What Are Vector Databases?

Vector databases are specialized systems optimized to store and search through **high-dimensional vectors**. These vectors represent documents, sentences, or other data points in a numerical form that captures semantic meaning. Unlike traditional relational databases, vector databases are built to efficiently perform **vector similarity searches** using Approximate Nearest Neighbors (ANN) algorithms.

#### Why Use Vector Databases?

- **Scale:** They handle billions of vectors efficiently.
- **Speed:** They perform searches much faster than brute-force methods.
- **Semantic Search:** They enable searching based on meaning, not just keywords.

#### Basic Vector Retrieval: K-Nearest Neighbors (KNN)

The simplest vector search method is KNN, which finds the *K* closest vectors to a query vector by calculating distances (e.g., cosine similarity or Euclidean distance) between the query and every document vector.

**Challenges with KNN:**

- **Linear time complexity:** For *N* documents, each search requires *N* distance calculations.
- **Scalability issues:** With 1 billion documents, this becomes computationally infeasible.

#### Approximate Nearest Neighbors (ANN)

ANN algorithms speed up vector search by trading off some accuracy for much faster retrieval. They do not guarantee finding the absolute closest vectors but find very close neighbors quickly.


### 3. üåê Navigable Small World Graphs and HNSW

#### Navigable Small World (NSW) Graphs

ANN algorithms often use graph structures to speed up search:

- Each document vector is a **node** in a graph.
- Nodes connect to their nearest neighbors.
- Searching involves traversing the graph from a starting node, moving to neighbors closer to the query vector.

This approach avoids calculating distances to all nodes, drastically reducing search time.

#### Hierarchical Navigable Small World (HNSW)

HNSW improves NSW by adding multiple layers of graphs:

- **Layer 1:** Contains all vectors with a full proximity graph for precise search.
- **Layer 2:** A smaller subset (e.g., 100 vectors) for intermediate navigation.
- **Layer 3:** An even smaller subset (e.g., 10 vectors) for fast, high-level navigation.

**Search process:**

- Start at the top layer with a random node.
- Traverse down layers, making large jumps early and fine-tuning search in the bottom layer.
- This hierarchical approach reduces search time from linear (KNN) to approximately logarithmic.

#### Benefits of HNSW

- Scales to billions of vectors.
- Much faster than KNN.
- Good balance between speed and accuracy.


### 4. üóÑÔ∏è Vector Database Operations in Practice

#### Setting Up a Vector Database

- **Create collections:** Logical groupings of documents.
- **Specify vectorizers:** Models that convert text into vectors.
- **Add documents:** Batch upload documents, converting them into vectors.
- **Build HNSW indexes:** Precompute graph structures for fast ANN search.

#### Searching

- **Vector search:** Query with a vector (e.g., from a user prompt) to find semantically similar documents.
- **Keyword search:** Traditional search using sparse vectors or BM25.
- **Hybrid search:** Combines vector and keyword search, weighting each to improve results.
- **Filtered search:** Adds metadata filters to narrow down results.

#### Popular Vector Databases

- **Weaviate:** Open-source, supports ANN, hybrid search, and metadata filtering.


### 5. üìÑ Chunking: Breaking Documents into Manageable Pieces

#### Why Chunk Documents?

Large documents (e.g., books) compressed into a single vector lose detail and specificity. This leads to poor search relevance because the vector represents an "average" of the entire content.

#### What is Chunking?

Chunking splits documents into smaller, meaningful pieces (chunks) before vectorizing. This allows:

- More precise search results.
- Better use of the language model‚Äôs context window.
- Improved relevancy by focusing on specific topics or sections.

#### Chunk Sizes

- **Too large:** Chapters or entire books lose specificity.
- **Too small:** Words or sentences lose context.
- **Optimal:** Paragraphs or sentences with some overlap.

#### Chunking Techniques

- **Fixed-size chunking:** Split text into fixed character lengths (e.g., 250 characters) with overlaps to preserve context.
- **Recursive character splitting:** Split based on document structure (e.g., paragraphs, headers).
- **Semantic chunking:** Group sentences based on meaning similarity using vector distances.
- **LLM-based chunking:** Use language models to create conceptually coherent chunks.

#### Overlapping Chunks

Overlap chunks by a small percentage (e.g., 10%) to avoid cutting off important context at chunk boundaries.

#### Pros and Cons of Chunking

- **Pros:** Improves search relevance, preserves context.
- **Cons:** Computationally expensive, requires more storage.


### 6. üîç Query Parsing and Optimization

#### Why Parse Queries?

User queries are often ambiguous, verbose, or informal. Parsing improves search accuracy by:

- Clarifying ambiguous terms.
- Adding synonyms or related terms.
- Removing irrelevant information.

#### Query Rewriting with LLMs

Use a language model to rewrite user queries into optimized search queries. For example:

- Original: "I was out walking my dog, a beautiful black lab named Poppy, when she yanked on the leash..."
- Optimized: "Experienced sudden shoulder pull causing numbness and pins and needles in fingers. Possible neuropathy or nerve impingement?"

#### Named Entity Recognition (NER)

NER identifies key entities (people, places, dates) in queries to improve filtering and search precision.

#### Hypothetical Document Embeddings (HyDE)

HyDE generates a hypothetical "ideal" document from the query using an LLM, then searches for documents similar to this hypothetical one. This can improve retrieval quality but adds latency.


### 7. ü§ñ Bi-Encoders, Cross-Encoders, and ColBERT: Different Search Architectures

#### Bi-Encoders

- Embed query and documents separately.
- Use ANN to find nearest document vectors.
- Fast and scalable.
- Default choice for semantic search.

#### Cross-Encoders

- Concatenate query and document.
- Use a model to score relevance directly.
- Much more accurate but very slow.
- Not feasible for large-scale search but great for reranking.

#### ColBERT (Contextualized Late Interaction Over BERT)

- Each token in query and document gets its own vector.
- Combines speed of bi-encoders with interaction richness of cross-encoders.
- Requires more storage.
- Increasingly supported by vector databases.


### 8. üîÑ Reranking: Improving Search Results

#### What is Reranking?

After an initial fast retrieval (e.g., with bi-encoders), reranking reorders the top results to improve relevance using more expensive models like cross-encoders or LLM-based scoring.

#### Why Rerank?

- Initial retrieval is fast but imprecise.
- Reranking ensures the final results are highly relevant.
- Typically rerank top 5-10 documents.

#### How Reranking Works

- Concatenate query and each candidate document.
- Use cross-encoder or LLM to assign a relevance score.
- Sort documents by score before returning results.


### 9. üèÅ Conclusion: Key Takeaways for Information Retrieval in Production

- **ANN algorithms** like HNSW enable fast, scalable vector search, essential for large datasets.
- **Vector databases** are optimized for storing and searching high-dimensional vectors, outperforming traditional databases for semantic search.
- **Chunking** documents improves search relevance by breaking content into meaningful pieces.
- **Query parsing and rewriting** enhance retrieval by clarifying user intent.
- **Bi-encoders** provide a good balance of speed and quality; **cross-encoders** offer the best quality but are slower.
- **Reranking** combines speed and accuracy by refining initial search results.
- These techniques together form the backbone of modern **Retrieval-Augmented Generation (RAG)** systems used in production.



<br>

## Questions

#### 1. What is the primary advantage of Approximate Nearest Neighbors (ANN) over K-Nearest Neighbors (KNN) in vector search?  
A) Guarantees finding the absolute closest vectors  
B) Significantly faster search time at scale  
C) Uses graph-based data structures for navigation  
D) Requires no pre-computation or indexing  


#### 2. Which of the following statements about Hierarchical Navigable Small World (HNSW) graphs are true?  
A) HNSW uses multiple layers with exponentially fewer vectors in higher layers  
B) The search starts at the bottom layer and moves upward  
C) HNSW reduces search complexity from linear to approximately logarithmic  
D) Each layer contains a complete proximity graph of all vectors  


#### 3. Why is chunking important when vectorizing large documents for retrieval?  
A) It compresses the entire document into a single vector for faster search  
B) It improves search relevance by preserving local context  
C) It reduces the number of vectors stored in the database  
D) It helps balance between too much and too little context in each chunk  


#### 4. Which chunking strategies help minimize cutting off important context at chunk boundaries?  
A) Fixed-size chunking without overlap  
B) Overlapping chunking with 10% overlap  
C) Recursive character splitting based on document structure  
D) Semantic chunking based on vector similarity  


#### 5. What are the main challenges of using KNN for vector search in large datasets?  
A) Linear growth in distance calculations with dataset size  
B) Inability to find any relevant documents  
C) High computational cost for billions of documents  
D) Requires complex graph structures for navigation  


#### 6. In query rewriting for information retrieval, which of the following techniques improve search effectiveness?  
A) Clarifying ambiguous phrases  
B) Adding synonyms to increase match chances  
C) Removing unnecessary information  
D) Randomly expanding the query with unrelated terms  


#### 7. Which of the following are true about vector databases compared to relational databases?  
A) Vector databases are optimized for high-dimensional vector storage  
B) Relational databases perform vector search efficiently using ANN  
C) Vector databases build HNSW indexes for fast approximate search  
D) Relational databases typically perform vector search as inefficient KNN  


#### 8. What is a key limitation of cross-encoders in large-scale information retrieval?  
A) They provide poor search quality compared to bi-encoders  
B) They cannot pre-compute document embeddings  
C) They scale well to billions of documents  
D) They are extremely slow for real-time search  


#### 9. How does ColBERT differ from bi-encoders and cross-encoders?  
A) It generates one vector per document like bi-encoders  
B) It generates vectors for each token in the query and document  
C) It balances speed and interaction richness between bi- and cross-encoders  
D) It requires less vector storage than bi-encoders  


#### 10. Which of the following are benefits of reranking in information retrieval?  
A) Improves the relevance of top search results  
B) Eliminates the need for initial fast retrieval  
C) Uses more computationally expensive models after filtering  
D) Always returns more documents than the initial retrieval  


#### 11. What is the main reason semantic chunking can be more effective than fixed-size chunking?  
A) It groups sentences based on similar meanings rather than arbitrary lengths  
B) It always produces smaller chunks than fixed-size chunking  
C) It requires no vector calculations  
D) It follows the author‚Äôs train of thought more closely  


#### 12. Which of the following statements about Hypothetical Document Embeddings (HyDE) are correct?  
A) HyDE generates a hypothetical document from the query to improve search  
B) It matches the query vector directly to document vectors  
C) It can improve retrieval quality but adds latency and cost  
D) It replaces the need for any other retrieval method  


#### 13. Why is overlapping chunking often preferred over non-overlapping chunking?  
A) It reduces the total number of chunks needed  
B) It ensures words at chunk edges have context in multiple chunks  
C) It eliminates the need for chunk metadata  
D) It increases search relevance by preserving context  


#### 14. Which of the following are true about the search process in Navigable Small World graphs?  
A) The algorithm always finds the absolute closest neighbors  
B) It traverses edges between neighboring nodes to approach the query vector  
C) It picks the best path globally at each step  
D) It may not find the optimal overall path but performs well in practice  


#### 15. What are the trade-offs when choosing between bi-encoders and cross-encoders?  
A) Bi-encoders are faster but less accurate  
B) Cross-encoders are slower but provide better relevance scores  
C) Bi-encoders require concatenating query and document for scoring  
D) Cross-encoders can pre-compute document embeddings  


#### 16. In the context of vector search, what does the term "proximity graph" refer to?  
A) A graph connecting documents based on keyword similarity  
B) A graph where nodes represent vectors connected to their nearest neighbors  
C) A hierarchical structure used only in relational databases  
D) A graph used to store metadata filters  


#### 17. Which of the following are typical operations when working with a vector database?  
A) Creating collections and specifying vectorizers  
B) Batch adding documents and creating HNSW indexes  
C) Running keyword-only searches without vectors  
D) Performing vector searches with ‚Äúnear_text‚Äù queries  


#### 18. What is a key disadvantage of semantic chunking compared to fixed-size chunking?  
A) It is computationally expensive due to repeated vector calculations  
B) It always produces less relevant chunks  
C) It ignores document structure and meaning  
D) It cannot be combined with LLM-based chunking  


#### 19. How does context-aware chunking improve retrieval quality?  
A) By adding additional context metadata to chunks  
B) By ignoring the original document structure  
C) By reducing the number of chunks to speed up search  
D) By precomputing all possible queries  


#### 20. Which of the following statements about hybrid search in vector databases are true?  
A) It combines vector and keyword search results with weighted importance  
B) It only uses keyword search and ignores vector similarity  
C) It can improve search relevance by leveraging both semantic and lexical signals  
D) It requires separate databases for vector and keyword data



<br>

## Answers

#### 1. What is the primary advantage of Approximate Nearest Neighbors (ANN) over K-Nearest Neighbors (KNN) in vector search?  
A) ‚úó Guarantees finding the absolute closest vectors ‚Äî ANN trades accuracy for speed, so no guarantee.  
B) ‚úì Significantly faster search time at scale ‚Äî ANN is designed to speed up search dramatically.  
C) ‚úì Uses graph-based data structures for navigation ‚Äî Many ANN methods use graphs like HNSW.  
D) ‚úó Requires no pre-computation or indexing ‚Äî ANN requires building indexes like proximity graphs.  

**Correct:** B, C


#### 2. Which of the following statements about Hierarchical Navigable Small World (HNSW) graphs are true?  
A) ‚úì HNSW uses multiple layers with exponentially fewer vectors in higher layers ‚Äî This is core to HNSW‚Äôs speed.  
B) ‚úó The search starts at the bottom layer and moves upward ‚Äî Search starts at the top (smallest) layer and moves down.  
C) ‚úì HNSW reduces search complexity from linear to approximately logarithmic ‚Äî Hierarchy enables logarithmic search time.  
D) ‚úó Each layer contains a complete proximity graph of all vectors ‚Äî Only the bottom layer has all vectors; upper layers have subsets.  

**Correct:** A, C


#### 3. Why is chunking important when vectorizing large documents for retrieval?  
A) ‚úó It compresses the entire document into a single vector for faster search ‚Äî This is what chunking tries to avoid.  
B) ‚úì It improves search relevance by preserving local context ‚Äî Smaller chunks keep specific topics distinct.  
C) ‚úó It reduces the number of vectors stored in the database ‚Äî Chunking increases the number of vectors.  
D) ‚úì It helps balance between too much and too little context in each chunk ‚Äî Optimal chunk size balances detail and context.  

**Correct:** B, D


#### 4. Which chunking strategies help minimize cutting off important context at chunk boundaries?  
A) ‚úó Fixed-size chunking without overlap ‚Äî No overlap causes loss of context at edges.  
B) ‚úì Overlapping chunking with 10% overlap ‚Äî Overlap preserves context across chunk boundaries.  
C) ‚úì Recursive character splitting based on document structure ‚Äî Respects natural breaks, reducing context loss.  
D) ‚úì Semantic chunking based on vector similarity ‚Äî Groups related sentences, preserving meaning.  

**Correct:** B, C, D


#### 5. What are the main challenges of using KNN for vector search in large datasets?  
A) ‚úì Linear growth in distance calculations with dataset size ‚Äî KNN requires comparing to all vectors.  
B) ‚úó Inability to find any relevant documents ‚Äî KNN finds closest neighbors but is slow.  
C) ‚úì High computational cost for billions of documents ‚Äî Linear scaling makes it impractical at scale.  
D) ‚úó Requires complex graph structures for navigation ‚Äî KNN is brute force, no graphs needed.  

**Correct:** A, C


#### 6. In query rewriting for information retrieval, which of the following techniques improve search effectiveness?  
A) ‚úì Clarifying ambiguous phrases ‚Äî Makes queries more precise.  
B) ‚úì Adding synonyms to increase match chances ‚Äî Broadens search scope.  
C) ‚úì Removing unnecessary information ‚Äî Focuses on relevant terms.  
D) ‚úó Randomly expanding the query with unrelated terms ‚Äî Adds noise, reduces precision.  

**Correct:** A, B, C


#### 7. Which of the following are true about vector databases compared to relational databases?  
A) ‚úì Vector databases are optimized for high-dimensional vector storage ‚Äî Designed specifically for vectors.  
B) ‚úó Relational databases perform vector search efficiently using ANN ‚Äî They usually do brute-force KNN, inefficient.  
C) ‚úì Vector databases build HNSW indexes for fast approximate search ‚Äî Common ANN indexing method.  
D) ‚úì Relational databases typically perform vector search as inefficient KNN ‚Äî They lack specialized indexing.  

**Correct:** A, C, D


#### 8. What is a key limitation of cross-encoders in large-scale information retrieval?  
A) ‚úó They provide poor search quality compared to bi-encoders ‚Äî Cross-encoders provide better quality.  
B) ‚úì They cannot pre-compute document embeddings ‚Äî Must process query-document pairs together.  
C) ‚úó They scale well to billions of documents ‚Äî They scale poorly due to computation cost.  
D) ‚úì They are extremely slow for real-time search ‚Äî Computationally expensive for large corpora.  

**Correct:** B, D


#### 9. How does ColBERT differ from bi-encoders and cross-encoders?  
A) ‚úó It generates one vector per document like bi-encoders ‚Äî ColBERT generates vectors per token.  
B) ‚úì It generates vectors for each token in the query and document ‚Äî Token-level embeddings are core to ColBERT.  
C) ‚úì It balances speed and interaction richness between bi- and cross-encoders ‚Äî Combines benefits of both.  
D) ‚úó It requires less vector storage than bi-encoders ‚Äî Requires more storage due to token vectors.  

**Correct:** B, C


#### 10. Which of the following are benefits of reranking in information retrieval?  
A) ‚úì Improves the relevance of top search results ‚Äî Reranking refines initial results.  
B) ‚úó Eliminates the need for initial fast retrieval ‚Äî Initial retrieval is still needed for efficiency.  
C) ‚úì Uses more computationally expensive models after filtering ‚Äî Applies slower models on fewer candidates.  
D) ‚úó Always returns more documents than the initial retrieval ‚Äî Usually returns fewer, more relevant documents.  

**Correct:** A, C


#### 11. What is the main reason semantic chunking can be more effective than fixed-size chunking?  
A) ‚úì It groups sentences based on similar meanings rather than arbitrary lengths ‚Äî Preserves semantic coherence.  
B) ‚úó It always produces smaller chunks than fixed-size chunking ‚Äî Chunk size varies, not always smaller.  
C) ‚úó It requires no vector calculations ‚Äî Semantic chunking depends on vector similarity.  
D) ‚úì It follows the author‚Äôs train of thought more closely ‚Äî Groups related concepts together.  

**Correct:** A, D


#### 12. Which of the following statements about Hypothetical Document Embeddings (HyDE) are correct?  
A) ‚úì HyDE generates a hypothetical document from the query to improve search ‚Äî Creates an idealized target.  
B) ‚úó It matches the query vector directly to document vectors ‚Äî Matches hypothetical doc embeddings to docs.  
C) ‚úì It can improve retrieval quality but adds latency and cost ‚Äî Extra generation step adds overhead.  
D) ‚úó It replaces the need for any other retrieval method ‚Äî Used as an enhancement, not a replacement.  

**Correct:** A, C


#### 13. Why is overlapping chunking often preferred over non-overlapping chunking?  
A) ‚úó It reduces the total number of chunks needed ‚Äî Overlapping increases chunk count.  
B) ‚úì It ensures words at chunk edges have context in multiple chunks ‚Äî Preserves context at boundaries.  
C) ‚úó It eliminates the need for chunk metadata ‚Äî Metadata is still needed for tracking.  
D) ‚úì It increases search relevance by preserving context ‚Äî Better context improves retrieval.  

**Correct:** B, D


#### 14. Which of the following are true about the search process in Navigable Small World graphs?  
A) ‚úó The algorithm always finds the absolute closest neighbors ‚Äî It finds close but not guaranteed closest.  
B) ‚úì It traverses edges between neighboring nodes to approach the query vector ‚Äî Core graph traversal method.  
C) ‚úó It picks the best path globally at each step ‚Äî Only chooses best local step, not global path.  
D) ‚úì It may not find the optimal overall path but performs well in practice ‚Äî Approximate but effective.  

**Correct:** B, D


#### 15. What are the trade-offs when choosing between bi-encoders and cross-encoders?  
A) ‚úì Bi-encoders are faster but less accurate ‚Äî Speed vs. quality trade-off.  
B) ‚úì Cross-encoders are slower but provide better relevance scores ‚Äî More computation, better results.  
C) ‚úó Bi-encoders require concatenating query and document for scoring ‚Äî They embed separately.  
D) ‚úó Cross-encoders can pre-compute document embeddings ‚Äî They must process pairs together.  

**Correct:** A, B


#### 16. In the context of vector search, what does the term "proximity graph" refer to?  
A) ‚úó A graph connecting documents based on keyword similarity ‚Äî Proximity graphs connect vectors, not keywords.  
B) ‚úì A graph where nodes represent vectors connected to their nearest neighbors ‚Äî Defines neighborhood for ANN search.  
C) ‚úó A hierarchical structure used only in relational databases ‚Äî Used in vector databases, not relational.  
D) ‚úó A graph used to store metadata filters ‚Äî Metadata filters are separate from proximity graphs.  

**Correct:** B


#### 17. Which of the following are typical operations when working with a vector database?  
A) ‚úì Creating collections and specifying vectorizers ‚Äî Setup steps for organizing data.  
B) ‚úì Batch adding documents and creating HNSW indexes ‚Äî Common ingestion and indexing tasks.  
C) ‚úó Running keyword-only searches without vectors ‚Äî Vector DBs focus on vector search, though some support hybrid.  
D) ‚úì Performing vector searches with ‚Äúnear_text‚Äù queries ‚Äî Typical vector similarity search method.  

**Correct:** A, B, D


#### 18. What is a key disadvantage of semantic chunking compared to fixed-size chunking?  
A) ‚úì It is computationally expensive due to repeated vector calculations ‚Äî Requires many similarity checks.  
B) ‚úó It always produces less relevant chunks ‚Äî Usually more relevant due to semantic grouping.  
C) ‚úó It ignores document structure and meaning ‚Äî It explicitly uses meaning for chunking.  
D) ‚úó It cannot be combined with LLM-based chunking ‚Äî Can be combined for better results.  

**Correct:** A


#### 19. How does context-aware chunking improve retrieval quality?  
A) ‚úì By adding additional context metadata to chunks ‚Äî Helps LLMs generate better responses.  
B) ‚úó By ignoring the original document structure ‚Äî It respects and enhances structure.  
C) ‚úó By reducing the number of chunks to speed up search ‚Äî Usually adds processing, not reduces chunks.  
D) ‚úó By precomputing all possible queries ‚Äî Does not precompute queries, adds context to chunks.  

**Correct:** A


#### 20. Which of the following statements about hybrid search in vector databases are true?  
A) ‚úì It combines vector and keyword search results with weighted importance ‚Äî Hybrid search blends both.  
B) ‚úó It only uses keyword search and ignores vector similarity ‚Äî Vector similarity is integral.  
C) ‚úì It can improve search relevance by leveraging both semantic and lexical signals ‚Äî Combines strengths of both.  
D) ‚úó It requires separate databases for vector and keyword data ‚Äî Usually integrated in one system.  

**Correct:** A, C