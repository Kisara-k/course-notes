## 4. LLMs and Text Generation

[Study Notes](#study-notes)

[Questions](#questions)



### Key Points

#### 1. ü§ñ Transformer Architecture  
- The Transformer architecture was introduced in the paper *‚ÄúAttention is All You Need‚Äù* by Vaswani et al. (Google Brain).  
- Transformers use an **attention mechanism** that allows each token to consider the meaning and position of every other token in the input.  
- Most LLMs use only the **decoder** component of the transformer for text generation.  
- Transformers process input tokens by combining **semantic embeddings** with **position vectors**.  
- Transformers have multiple **attention heads**, each learning different abstract patterns (e.g., object relations, spatial relations).  
- Modern models typically have between 8 and 64 transformer layers.  
- Text generation is iterative: the model predicts one token at a time, reprocessing the entire sequence for each new token.

#### 2. üé≤ Sampling Strategies  
- **Greedy decoding** always selects the highest probability token, leading to deterministic but sometimes repetitive text.  
- The **temperature** parameter controls randomness: lower values (<1) make output more conservative; higher values (>1) increase creativity.  
- **Top-K sampling** restricts token selection to the top *K* most probable tokens.  
- **Top-P (nucleus) sampling** selects tokens from the smallest set whose cumulative probability exceeds *P* (e.g., 85%).  
- **Repetition penalties** reduce the probability of tokens already used to prevent loops and redundancy.  
- **Logit biases** allow direct manipulation of token probabilities to filter or boost specific tokens.

#### 3. üß† Prompt Engineering  
- Prompts are structured as **messages** with roles: *system*, *user*, and *assistant*.  
- The **system prompt** provides high-level instructions influencing the LLM‚Äôs behavior and tone.  
- **In-context learning** involves including example question-response pairs in the prompt to teach the model desired patterns.  
- **Chain of Thought prompting** encourages the model to reason step-by-step before answering.  
- Managing the **context window** is critical because LLMs have a maximum token limit for prompt plus completion.

#### 4. üìö Retrieval-Augmented Generation (RAG)  
- RAG systems combine a **retriever** that finds relevant documents with an LLM that generates responses grounded in those documents.  
- RAG reduces hallucinations by injecting factual information into the prompt.  
- Hallucinations occur because LLMs generate probable text sequences, not guaranteed facts.  
- Citation generation instructs LLMs to cite sources after facts, improving verifiability but not eliminating hallucinations.  
- Self-consistency methods generate multiple answers to check for factual consistency but are costly and unreliable.

#### 5. üìä LLM Evaluation Metrics  
- **Response relevancy** measures if the response relates to the user prompt, regardless of factual accuracy.  
- **Faithfulness** measures if the response is factually consistent with retrieved information.  
- **Citation quality** evaluates how well citations align with correct sources.  
- Benchmarks include automated tests (e.g., MMLU), human evaluations (e.g., LLM Arena), and LLM-as-a-judge methods.  
- Benchmark scores improve over time but can saturate, requiring new benchmarks.

#### 6. üõ†Ô∏è Agentic Systems and Fine-Tuning  
- Agentic systems break down tasks into multiple steps, with different LLMs handling routing, evaluation, writing, and citation.  
- Fine-tuning retrains an LLM on specific data to improve task or domain performance.  
- Fine-tuning changes how a model talks more than what it knows and does not teach new facts well.  
- RAG is best for injecting new knowledge dynamically; fine-tuning is best for domain adaptation and task specialization.  
- Combining RAG and fine-tuning can optimize both knowledge injection and task performance.

#### 7. ‚öôÔ∏è LLM Characteristics and Costs  
- Small LLMs have 1‚Äì10 billion parameters; large models have 100‚Äì500+ billion parameters.  
- Larger models are generally more capable but more expensive to run.  
- The **context window** limits the total tokens (prompt + completion) the model can process at once.  
- Generating each token requires extensive computation because the model attends to all previous tokens.  
- Latency and speed vary by model size and infrastructure.



<br>

## Study Notes

### 1. ü§ñ Understanding Transformer Architecture: The Foundation of LLMs

Large Language Models (LLMs) like GPT and others are built on a revolutionary architecture called the **Transformer**, introduced by Vaswani et al. in the paper *‚ÄúAttention is All You Need‚Äù*. Understanding how transformers work is essential to grasp how LLMs generate text.

#### What is a Transformer?

A transformer is a neural network architecture designed to process sequences of data, such as sentences, by focusing on the relationships between all parts of the input simultaneously. Unlike older models that processed text sequentially, transformers use a mechanism called **attention** to weigh the importance of every word relative to every other word in the input.

#### Key Components:

- **Encoder and Decoder:**  
  - The **encoder** reads and understands the input text, building a deep contextual representation. For example, if the input is a German sentence, the encoder captures its meaning.  
  - The **decoder** uses this understanding to generate new text, such as translating the German sentence into English.  
  - Many LLMs only use the **decoder** part because their main goal is to generate text, not translate or encode.

- **Attention Mechanism:**  
  Attention is like a smart spotlight that tells the model which words in the sentence should influence the understanding of each word. For example, in the sentence ‚Äúthe brown dog sat next to the red fox,‚Äù the word ‚Äúdog‚Äù pays attention to ‚Äúbrown‚Äù and ‚Äúsat‚Äù to understand its role better.

- **Input Embeddings:**  
  Each word (or token) is converted into a dense vector representing its meaning. Additionally, a **position vector** is added to indicate the token‚Äôs position in the sentence, so the model knows the order of words.

- **Attention Heads:**  
  Transformers have multiple attention heads, each learning different abstract patterns. For example, one head might focus on object relationships (‚Äúdog‚Äù and ‚Äúfox‚Äù), while another focuses on spatial relations (‚Äúnext to‚Äù).

- **Feed Forward Layers:**  
  After attention, the model refines its understanding through feed-forward neural networks, iteratively improving the representation of each token.

- **Iterative Refinement:**  
  The model repeats attention and feed-forward steps multiple times (layers) to refine its understanding, often using 8 to 64 layers in modern models.

#### How Text Generation Works:

To generate text, the model predicts the next token based on all previous tokens, repeating this process token by token until it reaches a stopping point or token limit. Each token generation involves reprocessing the entire sequence with attention, which is computationally expensive.


### 2. üé≤ Sampling Strategies: Controlling Randomness in Text Generation

When LLMs generate text, they don‚Äôt always pick the single most likely next word. Instead, they use **sampling strategies** to balance between predictable and creative outputs.

#### Why Sampling Matters:

- If the model always picks the highest probability token (**greedy decoding**), the output can be repetitive or generic, like ‚Äúwhich the data confirms, which the data confirms...‚Äù
- Introducing randomness allows for more natural, varied, and sometimes creative text.

#### Common Sampling Techniques:

- **Greedy Decoding:**  
  Always picks the most probable next token. Deterministic but can get stuck in loops or produce dull text.

- **Temperature:**  
  A parameter that adjusts the ‚Äúsharpness‚Äù of the probability distribution.  
  - Low temperature (<1) makes the model more confident and conservative (spiky distribution).  
  - High temperature (>1) makes the model more creative and random (flatter distribution).

- **Top-K Sampling:**  
  The model only picks from the top *K* most likely tokens, ignoring the rest. For example, Top-K=5 means only the 5 most probable tokens are considered.

- **Top-P (Nucleus) Sampling:**  
  Instead of a fixed number, Top-P picks tokens from the smallest set whose cumulative probability exceeds *P* (e.g., 85%). This adapts dynamically based on the model‚Äôs confidence.

#### Token-Specific Strategies:

- **Repetition Penalties:**  
  Reduce the probability of tokens already used to avoid loops and redundant phrases.

- **Logit Biases:**  
  Directly increase or decrease the probability of specific tokens to filter profanity, boost certain categories, or avoid unwanted words.

#### Choosing Sampling Parameters:

- For factual or code generation, use low temperature and top-p to keep outputs precise.  
- For creative writing, increase temperature and top-p to encourage diversity.  
- Add repetition penalties and logit biases as needed to improve output quality.


### 3. üß† Prompt Engineering: Building Effective Inputs for LLMs

Prompt engineering is the art of crafting inputs to guide LLMs to produce the best possible responses.

#### Message Format:

LLMs often use a **messages** format with roles:  
- **System:** Provides high-level instructions about the model‚Äôs behavior (e.g., ‚ÄúYou are a helpful assistant‚Äù).  
- **User:** Contains the user‚Äôs question or prompt.  
- **Assistant:** Contains the model‚Äôs previous responses.

#### System Prompts:

These set the tone, style, and rules for the LLM. For example, a system prompt might instruct the model to always provide factual answers and cite sources.

#### Prompt Templates:

Templates structure prompts by combining system instructions, conversation history, retrieved documents, and user queries. This helps maintain context and consistency.

#### Advanced Techniques:

- **In-Context Learning:**  
  Provide examples of question-answer pairs within the prompt to teach the model the desired response style (few-shot or one-shot learning).

- **Chain of Thought / Reasoning:**  
  Encourage the model to ‚Äúthink aloud‚Äù step-by-step before answering, improving accuracy for complex questions.

- **Context Window Management:**  
  Since LLMs have a limit on how many tokens they can process at once, it‚Äôs important to prune irrelevant context and keep prompts efficient.


### 4. üìö Retrieval-Augmented Generation (RAG): Grounding LLMs in Real Information

LLMs sometimes **hallucinate**, meaning they generate plausible but incorrect or fabricated information. RAG helps reduce hallucinations by grounding responses in retrieved documents.

#### How RAG Works:

- A **retriever** finds relevant documents from a knowledge base.  
- The LLM incorporates this retrieved information into its prompt, using attention to deeply understand and generate grounded answers.

#### Why RAG is Effective:

- LLMs can better use factual information injected into prompts.  
- It reduces hallucinations by forcing the model to base answers on real data.  
- However, some randomness remains, so controlling sampling and confirming grounding is important.

#### Hallucination Challenges:

- LLMs generate probable text, not guaranteed facts.  
- Hallucinations can be subtle and hard to detect but erode trust.  
- Self-consistency methods (generating multiple answers and comparing) help but are costly.

#### Citation Generation:

- LLMs can be instructed to cite sources after each fact, improving transparency and verifiability.  
- Still, LLMs can hallucinate citations, so human verification is needed.


### 5. üìä Evaluating LLM Performance: Measuring Quality and Reliability

Evaluating LLMs is complex because quality depends on many factors.

#### Types of Benchmarks:

- **Automated Benchmarks:**  
  Use multiple-choice tests or code challenges (e.g., MMLU) to measure knowledge across subjects.

- **Human-Evaluated Benchmarks:**  
  Humans compare responses and rank models, capturing nuances automated tests miss.

- **LLM-as-a-Judge:**  
  One LLM rates another‚Äôs responses, which is cheap but can be biased.

#### Key Metrics:

- **Response Relevancy:**  
  Does the response relate to the user‚Äôs prompt?

- **Faithfulness:**  
  Is the response factually consistent with retrieved information?

- **Citation Quality:**  
  Are citations accurate and aligned with sources?

#### Challenges:

- Benchmarks can saturate as models improve, requiring new tests.  
- Data contamination (test data leaking into training) can skew results.


### 6. üõ†Ô∏è Advanced Capabilities: Agentic Systems and Fine-Tuning

#### Agentic Systems:

These systems break down complex tasks into multiple steps, with different LLMs or components handling routing, evaluation, writing, and citation. They can operate sequentially, conditionally, or in parallel to improve performance and reliability.

#### Fine-Tuning:

Fine-tuning retrains an LLM on specific data to improve performance in a target domain or task.

- **When Fine-Tuning Works Well:**  
  For domain adaptation (e.g., medical diagnosis, legal summaries) where task-specific language and style matter.

- **Limitations:**  
  Fine-tuning changes how a model talks more than what it knows. It doesn‚Äôt teach new facts well and can reduce performance outside the target domain.

#### RAG vs. Fine-Tuning:

- **RAG** is best for injecting up-to-date or external knowledge dynamically.  
- **Fine-tuning** is best for specializing the model‚Äôs behavior or style.  
- Often, combining both yields the best results.


### 7. üìù Summary and Key Takeaways

- **Transformers** use attention to understand and generate text by considering all tokens simultaneously.  
- **Sampling strategies** control randomness, balancing creativity and accuracy.  
- **Prompt engineering** shapes how LLMs respond, using system instructions, examples, and reasoning prompts.  
- **RAG** grounds LLMs in real data, reducing hallucinations and improving factuality.  
- **Evaluation** requires multiple metrics and methods to measure relevancy, faithfulness, and fluency.  
- **Agentic systems** and **fine-tuning** enhance LLM capabilities for complex tasks and domain-specific needs.



<br>

## Questions

#### 1. What is the primary role of the attention mechanism in transformer models?  
A) To sequentially process tokens one at a time  
B) To determine which tokens in the input should most influence the representation of each token  
C) To encode the position of tokens in the input sequence  
D) To allow each token to consider the meaning and position of every other token simultaneously  

#### 2. Which of the following statements about transformer encoder and decoder components are true?  
A) The encoder builds a deep contextual understanding of the input text  
B) Most LLMs use both encoder and decoder components equally  
C) The decoder generates new text based on the encoder‚Äôs output  
D) Many LLMs only use the decoder component because they focus on text generation  

#### 3. How do input embeddings in transformers represent tokens?  
A) As sparse one-hot vectors indicating token identity only  
B) As dense semantic vectors combined with position vectors  
C) As vectors encoding only the token‚Äôs position in the sequence  
D) As vectors that represent both the token‚Äôs meaning and its position  

#### 4. What is the function of multiple attention heads in a transformer layer?  
A) Each head learns different abstract patterns from the input data  
B) They split the input sequence into smaller chunks for parallel processing  
C) They apply human-defined linguistic rules to the tokens  
D) Larger models may use over 100 attention heads to capture complex relationships  

#### 5. Why is text generation by LLMs computationally expensive?  
A) Because each token generation requires reprocessing the entire input sequence with attention  
B) Because the model must generate all tokens simultaneously  
C) Because the model must examine all tokens for context at every step  
D) Because transformers use recurrent neural networks internally  

#### 6. Which of the following best describes greedy decoding?  
A) Selecting the token with the highest probability at each step  
B) Sampling tokens randomly from the entire vocabulary  
C) Producing highly creative and diverse text outputs  
D) Often resulting in repetitive or generic text loops  

#### 7. How does the temperature parameter affect the token probability distribution during sampling?  
A) Higher temperature sharpens the distribution, making the model more confident  
B) Lower temperature flattens the distribution, increasing randomness  
C) Lower temperature makes the distribution spikier, favoring high-probability tokens  
D) Higher temperature flattens the distribution, encouraging more creative outputs  

#### 8. What is the key difference between Top-K and Top-P (nucleus) sampling?  
A) Top-K always selects from a fixed number of tokens regardless of their cumulative probability  
B) Top-P selects tokens until their cumulative probability exceeds a threshold  
C) Top-K dynamically adjusts the number of tokens based on model confidence  
D) Top-P always selects exactly K tokens  

#### 9. Which strategies help reduce repetition and loops in generated text?  
A) Increasing temperature to encourage randomness  
B) Applying repetition penalties to reduce probabilities of already used tokens  
C) Using logit biases to adjust token probabilities directly  
D) Using greedy decoding exclusively  

#### 10. In prompt engineering, what is the role of the system message?  
A) To provide high-level instructions that influence the LLM‚Äôs overall behavior  
B) To record the user‚Äôs questions and inputs  
C) To store the assistant‚Äôs previous responses  
D) To define the tone, style, and constraints for the model‚Äôs replies  

#### 11. Why is Retrieval-Augmented Generation (RAG) effective in reducing hallucinations?  
A) It retrains the model‚Äôs internal parameters with new data  
B) It injects relevant external documents into the prompt for grounding  
C) It forces the model to generate text only from retrieved factual information  
D) It eliminates all randomness in token sampling  

#### 12. What are common causes and consequences of hallucinations in LLM outputs?  
A) LLMs generate probable text sequences, which may not always be factually accurate  
B) Hallucinations are easy to detect because they produce nonsensical text  
C) Hallucinations can erode user trust even if they occur occasionally  
D) Hallucinations never occur when using RAG systems  

#### 13. Which of the following are true about citation generation in LLM responses?  
A) LLMs can be instructed to cite sources after each factual claim  
B) Citations generated by LLMs are always accurate and reliable  
C) Citation generation improves human verification of facts  
D) LLMs may hallucinate citations, so human oversight is necessary  

#### 14. When evaluating LLM performance, what does the faithfulness metric measure?  
A) Whether the response is relevant to the user‚Äôs prompt  
B) Whether the response is consistent with retrieved factual information  
C) The fluency and clarity of the generated text  
D) The number of tokens generated per second  

#### 15. What are the advantages and limitations of fine-tuning LLMs?  
A) Fine-tuning updates the model‚Äôs internal parameters to improve domain-specific performance  
B) Fine-tuning teaches the model new factual knowledge effectively  
C) Fine-tuning can degrade performance outside the target domain  
D) Fine-tuning is best suited for injecting up-to-date external knowledge dynamically  

#### 16. How do agentic LLM systems typically operate?  
A) A single LLM handles all tasks from retrieval to response generation  
B) Different specialized LLMs perform distinct tasks such as routing, evaluation, and citation  
C) They can operate sequentially, conditionally, or in parallel workflows  
D) They eliminate the need for retrievers by generating all information internally  

#### 17. Which of the following are true about context window management in LLMs?  
A) The context window limits the total number of tokens processed in prompt and completion  
B) Including irrelevant conversation history improves model accuracy  
C) Pruning old or irrelevant messages helps maintain prompt efficiency  
D) Reasoning tokens always reduce the context window size without benefits  

#### 18. What is the main reason why many LLMs only use the decoder part of the transformer architecture?  
A) Because decoders are simpler to train than encoders  
B) Because text generation requires predicting the next token based on previous tokens, which decoders specialize in  
C) Because encoders are only useful for translation tasks  
D) Because decoders can generate embeddings for semantic understanding  

#### 19. Which of the following statements about sampling parameters are correct?  
A) Setting temperature to zero is equivalent to greedy decoding  
B) Top-P sampling is more dynamic than Top-K because it adapts to the model‚Äôs confidence  
C) Increasing temperature always improves factual accuracy  
D) Repetition penalties discourage the model from repeating the same tokens excessively  

#### 20. Why might reasoning models struggle with in-context learning and example mixing?  
A) They perform best with clear goals and strict prompt formats  
B) They rely heavily on large context windows and high-level guidance  
C) They are designed to ignore example prompts to avoid bias  
D) They add computational cost but improve accuracy by encouraging step-by-step reasoning



<br>

## Answers

#### 1. What is the primary role of the attention mechanism in transformer models?  
A) ‚úó Attention does not process tokens sequentially; it considers all tokens simultaneously.  
B) ‚úì Correct. Attention weighs which tokens most influence each token‚Äôs representation.  
C) ‚úó Position encoding is separate from attention.  
D) ‚úì Correct. Each token considers meaning and position of every other token via attention.  

**Correct:** B, D


#### 2. Which of the following statements about transformer encoder and decoder components are true?  
A) ‚úì Encoder builds deep contextual understanding of input text.  
B) ‚úó Most LLMs use only the decoder, not both equally.  
C) ‚úì Decoder generates new text based on encoder output in full transformer models.  
D) ‚úì Many LLMs only use decoder because they focus on text generation.  

**Correct:** A, C, D


#### 3. How do input embeddings in transformers represent tokens?  
A) ‚úó Transformers use dense vectors, not sparse one-hot vectors.  
B) ‚úì Correct. Embeddings combine semantic meaning and position vectors.  
C) ‚úó Position vectors alone don‚Äôt represent token meaning.  
D) ‚úì Correct. Both meaning and position are encoded together.  

**Correct:** B, D


#### 4. What is the function of multiple attention heads in a transformer layer?  
A) ‚úì Each head learns different abstract patterns from data.  
B) ‚úó Attention heads do not split input into chunks; they attend to all tokens.  
C) ‚úó Heads learn patterns automatically, not human-defined rules.  
D) ‚úì Larger models use many heads to capture complex relationships.  

**Correct:** A, D


#### 5. Why is text generation by LLMs computationally expensive?  
A) ‚úì Each token generation requires reprocessing the entire sequence with attention.  
B) ‚úó Tokens are generated sequentially, not simultaneously.  
C) ‚úì The model must examine all tokens for context at every step.  
D) ‚úó Transformers do not use recurrent neural networks internally.  

**Correct:** A, C


#### 6. Which of the following best describes greedy decoding?  
A) ‚úì Always picks the highest probability token at each step.  
B) ‚úó Greedy decoding is deterministic, not random sampling.  
C) ‚úó Greedy decoding tends to produce generic, not creative, text.  
D) ‚úì Can result in repetitive loops or generic text.  

**Correct:** A, D


#### 7. How does the temperature parameter affect the token probability distribution during sampling?  
A) ‚úó Higher temperature flattens, not sharpens, the distribution.  
B) ‚úó Lower temperature sharpens (spikier), not flattens, the distribution.  
C) ‚úì Lower temperature makes distribution spikier, favoring high-probability tokens.  
D) ‚úì Higher temperature flattens distribution, encouraging creativity.  

**Correct:** C, D


#### 8. What is the key difference between Top-K and Top-P (nucleus) sampling?  
A) ‚úì Top-K selects from a fixed number of tokens regardless of cumulative probability.  
B) ‚úì Top-P selects tokens until cumulative probability exceeds a threshold.  
C) ‚úó Top-K does not dynamically adjust number of tokens based on confidence.  
D) ‚úó Top-P does not select exactly K tokens; it varies.  

**Correct:** A, B


#### 9. Which strategies help reduce repetition and loops in generated text?  
A) ‚úó Increasing temperature increases randomness but may not reduce repetition.  
B) ‚úì Repetition penalties reduce probabilities of already used tokens.  
C) ‚úì Logit biases can adjust token probabilities to discourage repetition.  
D) ‚úó Greedy decoding often causes repetition rather than prevents it.  

**Correct:** B, C


#### 10. In prompt engineering, what is the role of the system message?  
A) ‚úì Provides high-level instructions influencing model behavior.  
B) ‚úó User messages record user inputs, not system messages.  
C) ‚úó Assistant messages record model responses, not system messages.  
D) ‚úì Defines tone, style, and constraints for replies.  

**Correct:** A, D


#### 11. Why is Retrieval-Augmented Generation (RAG) effective in reducing hallucinations?  
A) ‚úó RAG does not retrain model parameters; it augments prompts.  
B) ‚úì Injects relevant external documents into prompts for grounding.  
C) ‚úì Forces model to base answers on retrieved factual information.  
D) ‚úó RAG does not eliminate all randomness in sampling.  

**Correct:** B, C


#### 12. What are common causes and consequences of hallucinations in LLM outputs?  
A) ‚úì LLMs generate probable text, which may not be factually accurate.  
B) ‚úó Hallucinations can be subtle and plausible, not always nonsensical.  
C) ‚úì Hallucinations erode user trust even if occasional.  
D) ‚úó Hallucinations can still occur with RAG, though less frequently.  

**Correct:** A, C


#### 13. Which of the following are true about citation generation in LLM responses?  
A) ‚úì LLMs can be instructed to cite sources after factual claims.  
B) ‚úó Citations generated by LLMs are not always accurate or reliable.  
C) ‚úì Citation generation aids human verification of facts.  
D) ‚úì LLMs may hallucinate citations, so human oversight is needed.  

**Correct:** A, C, D


#### 14. When evaluating LLM performance, what does the faithfulness metric measure?  
A) ‚úó Relevancy measures relation to prompt, not faithfulness.  
B) ‚úì Faithfulness measures consistency with retrieved factual information.  
C) ‚úó Fluency measures clarity, not faithfulness.  
D) ‚úó Tokens per second measure speed, not faithfulness.  

**Correct:** B


#### 15. What are the advantages and limitations of fine-tuning LLMs?  
A) ‚úì Fine-tuning improves domain-specific performance by updating parameters.  
B) ‚úó Fine-tuning does not effectively teach new factual knowledge.  
C) ‚úì Fine-tuning can degrade performance outside the target domain.  
D) ‚úó Fine-tuning is not ideal for injecting up-to-date external knowledge dynamically.  

**Correct:** A, C


#### 16. How do agentic LLM systems typically operate?  
A) ‚úó Agentic systems use multiple specialized LLMs, not a single one.  
B) ‚úì Different LLMs perform distinct tasks like routing, evaluation, citation.  
C) ‚úì They can operate sequentially, conditionally, or in parallel workflows.  
D) ‚úó They rely on retrievers; do not eliminate them.  

**Correct:** B, C


#### 17. Which of the following are true about context window management in LLMs?  
A) ‚úì Context window limits total tokens processed in prompt and completion.  
B) ‚úó Including irrelevant history usually reduces accuracy and efficiency.  
C) ‚úì Pruning old or irrelevant messages maintains prompt efficiency.  
D) ‚úó Reasoning tokens add cost but improve accuracy; they are not always dropped.  

**Correct:** A, C


#### 18. What is the main reason why many LLMs only use the decoder part of the transformer architecture?  
A) ‚úó Decoders are not necessarily simpler to train.  
B) ‚úì Decoders specialize in predicting next tokens based on previous tokens, ideal for generation.  
C) ‚úó Encoders are useful beyond translation, but decoders suffice for generation.  
D) ‚úó Decoders do not generate embeddings; encoders do that.  

**Correct:** B


#### 19. Which of the following statements about sampling parameters are correct?  
A) ‚úì Temperature zero equals greedy decoding (always pick highest probability).  
B) ‚úì Top-P is more dynamic than Top-K, adapting to model confidence.  
C) ‚úó Increasing temperature does not guarantee better factual accuracy; often reduces it.  
D) ‚úì Repetition penalties discourage excessive token repetition.  

**Correct:** A, B, D


#### 20. Why might reasoning models struggle with in-context learning and example mixing?  
A) ‚úì They perform best with clear goals and strict prompt formats.  
B) ‚úì They rely on large context windows and high-level guidance.  
C) ‚úó They do not ignore example prompts; they use them carefully.  
D) ‚úì They add computational cost but improve accuracy by encouraging step-by-step reasoning.  

**Correct:** A, B, D