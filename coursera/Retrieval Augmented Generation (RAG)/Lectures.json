[
  {
    "index": 1,
    "title": "1. RAG Overview",
    "content": "Introduction to RAG Module 1 Introduction Zain Hasan Module Overview Introduction to RAG RAG. Architecture Deep Dive Real-World Applications Zain Hasan Hands-on Projects Module 5 Monitoring & Evaluation Module 4 LLM Module 3 Vector Database Module 2 Retriever Module 1 Basic RAG Introduction to RAG Introduction to RAG Zain Hasan Retrieval Augmented Generation (RAG) Summarize Text LLMS are already powerful Generate Code Rewrite Content RAG further improves them LLM New information Zain Hasan Why are hotels expensive on the weekend? \"More people travel on weekends, so there's more competition for rooms.” Zain Hasan Why are hotels in Vancouver super expensive this coming weekend? Zain Hasan Why are hotels in Vancouver super expensive this coming weekend? Taylor Swift is in town this weekend Zain Hasan Why doesn't Vancouver have more hotel capacity close to downtown? Zain Hasan Two Steps for Answering Questions Collect Information “Generation” Reason & Respond “Retrieval” Why are hotels expensive on the weekend? Why doesn't Vancouver have more hotel capacity close to downtown? No need to collect information Extensive Research Respond based on your knowledge Synthesize research Zain Hasan Traditional Language Model Usage Great answer Not so great Very recent information Specialized knowledge LLM Prompt Internet Data Italian Dishes Movies US History Zain Hasan Training Data Books Web Pages Forums Code LLM Mathematical Models Why doesn't Vancouver have more hotel capacity close to downtown? User Query Forum Comment Probably because land costs are super high near downtown. Urban Development in Vancouver Zoning laws and city planning since the early 1900s limited hotel growth downtown. Massive dataset from the open internet Zain Hasan What LLMs don't know Unfortunately, many types of information are missing Private Databases LLMS can't access confidential information Real time data LLMs are trained on past data and don't update automatically Hard to access information Some information isn't widely available online, making it inaccessible to LLMs. How do you make sure the LLM knows this useful information? Zain Hasan Just put it in the prompt! Why are hotels in Vancouver super expensive this coming weekend? User Prompt News reports Forum posts LLM Zain Hasan LLM Augmented Prompt Response Why are hotels in Vancouver super expensive this weekend? User Prompt RAG System Taylor Swift is performing her Eras Tour in Vancouver. This weekend at BC Place Stadium on December 6-8, 2024 Zain Hasan Retriever Manages knowledge base of trusted information Finds the most relevant Information and shares information with the LLM Improves generation Retriever Knowledge Base Zain Hasan Retrieval Augmented Generation Applications of RAG Applications of RAG Zain Hasan Applications of RAG - Code Generation LLM needs your project's context Classes, functions, definitions, and coding style Use your codebase as a knowledge base RAG retrieves project-specific content for the LLM Improves code generation and Q&A Answers are tailored to your actual repository Zain Hasan Applications of RAG - Company Chatbots Tailored to your company Every business has its own products, policies, and communication style Uses your internal documents Manuals, support guides, FAQs Grounds answers in real context Reduces generic or incorrect answers Zain Hasan Applications of RAG - Specialized Knowledge High-impact domains Legal and medical use cases Uses specialized documents Case files, journals, private data Enables accurate, secure use Supports precision and privacy needs Zain Hasan Applications of RAG Search engines as retrievers Return websites for a given query AI summarizes search results Presents key info in a skimmable format RAG with the internet as a knowledge base Summaries powered by real-time retrieval Zain Hasan Your Data Personalized RAG Emails Text Messages Contacts Calendar Events More software includes personal assistants Messaging app, email client, etc. Your data is the knowledge base Texts, contacts, etc. These tools need context More context leads to better results Zain Hasan LLM + your data RAG Architecture Overview Introduction to RAG Zain Hasan RAG System Retriever Knowledge Base Relevant Documents Normal LLM Use Prompt Response LLM Zain Hasan RAG System Knowledge Base Normal LLM Use Augmented Prompt Prompt “Why are hotels in Vancouver so expensive this coming weekend? Here are the five relevant articles that may help you respond. <retrieved articles>” Retriever Relevant Documents Zain Hasan RAG System Retriever Knowledge Base Relevant Documents Normal LLM Use Augmented Prompt Prompt Response LLM “Why are hotels in Vancouver so expensive this coming weekend? Here are the five relevant articles that may help you respond. <retrieved articles>” Added latency Taylor Swift is performing her Eras Tour in Vancouver. This weekend at BC Place Stadium on December 6-8, 2024 Better responses Zain Hasan Injects missing knowledge Adds info not in the training data (e.g. policies, updates) Reduces hallucinations Grounds answers with relevant context Keeps models up to date Reflects new info by updating the knowledge base Enables source citation Includes sources for verifiable answers Focuses model on generation Retriever finds facts, LLM writes responses Advantages of RAG “Why are hotels in Vancouver so expensive this coming weekend? Here are the five relevant articles that may help you respond. <retrieved articles>” Retriever Knowledge Base Introduction to LLMs Introduction to RAG Zain Hasan LLMs are just fancy autocomplete Zain Hasan shining rising out Prompt Completions What a beautiful day the sun is shining What a beautiful day the sun is rising What a beautiful day the sun is out What a beautiful day, the sun is Zain Hasan exploding Neural Network a complex mathematical model of language sun shining stores which words frequently appear together, in which order, and contextual meaning LLMs use this model to generate text What a beautiful day, the sun is Zain Hasan What a beautiful day, the sun is shining in the sky Token a piece of a word some words get single tokens compound words use multiple tokens punctuation marks ~10,000 - 100,000 tokens in LLM's vocabulary, allowing models to represent any possible word with fewer tokens unhappy programmatically London door Completely, I agree! Zain Hasan Calculate Probabilities What a beautiful day, the sun is Process Current State Vocabulary ~10,000 - 100,000 tokens shining rising out bright snoring exploding Select Next Token shining Zain Hasan Calculate Probabilities Process Current State What a beautiful day, the sun is through new tokens make sense in context of old ones Select Next Token shining Zain Hasan What a beautiful day, the sun is shining in the sky warming our faces Autoregressive “self-influencing” new tokens make sense in context of old ones running the same prompt leads to different completion Zain Hasan How LLMs Learn Billions of parameters Before training, LLMs generate gibberish \"Forward to Saturn's dance floor!\" she yowled, tail transmitting disco beats. LLM Large text collections Trains model usually billions of parameters! Zain Hasan How LLMs Learn LLM Training Billions of parameters What a beautiful day is, the sun _____ Roses are red, violets are Predictions Accurate? Update parameters Zain Hasan Why LLMs Hallucinate LLMs generate probable word sequences LLMs just reproduce statistical patterns from their training data Knowledge gaps cause inaccurate responses Responses can “sound right” but aren't true. Truthful ≠ probable LLMs are designed to generate \"probable\" text, not truthful text Zain Hasan How RAG solves the problem Retriever Knowledge Base LLM relevant context “grounds” the LLM's responses Zain Hasan Why not add everything? Higher Computational Cost Longer prompts take more computation to run Model performs computationally complex scan of every token Scan happens before generating each new token Context Window Limit Eventually you hit the limit of LLM's context window Smaller models: only a few thousand tokens Largest models: millions Zain Hasan Introduction to information retrieval Introduction to RAG Zain Hasan How can I make New York style pizza at home? Your Question Books on many topics Collection Retriever Documents in a database Different shelves and sections Organization “index” for search Librarian helps you find best sections or books Search Retriever searches the index Library Zain Hasan Search with a librarian Librarian Understands the meaning of your question Identifies the right shelves to search Eventually finds relevant books Zain Hasan Retriever Knowledge Base How can I make New York style pizza at home? Your Question “What does the prompt mean?” “What documents in the knowledge base are similar?” A History of NYC Sauce Secrets Cooking at Home Pizza Basics Zain Hasan A History of NYC Sauce Secrets Cooking at Home Pizza Basics Retriever Knowledge Base How can I make New York style pizza at home? Your Question “What does the prompt mean?” “What documents in the knowledge base are similar?” Zain Hasan Retriever Tradeoffs Relevance vs irrelevance Need to return relevant documents and withhold irrelevant ones Return every document? Mountains of irrelevant docs. Wastes context window Return the single highest ranked document? Miss valuable information No perfect solution Retriever usually doesn't perfectly rank documents Monitor and experiment Change settings to find what works Zain Hasan Historical Context Information Retrieval was already mature when LLMs were first developed Search Engine Retrieves relevant webpages Database Retrieves relevant tables and rows Zain Hasan Practical Implementation Relational Database Already widely adopted Vector Database Specialized for retrieval in a RAG system Module 1 Conclusion Introduction to RAG Zain Hasan Key Concepts RAG pairs an LLM with a knowledge base Data is private, recent, or highly specific and so missing from the LLM's training data Retriever finds relevant documents and adds them to an augmented prompt LLMs ground their responses in the retrieved information"
  },
  {
    "index": 2,
    "title": "2. Information Retrieval and Search Foundations",
    "content": "Information Retrieval Foundations Module 2 introduction Zain Hasan Prompts Unstructured Conversational Retriever Needs to rapidly find relevant documents despite this mess! Documents Wide range of formats Designed for humans to read Zain Hasan Module Topics Widely used retrieval techniques Theoretical foundations and relative strengths of each technique, plus how they're used in combination Evaluation strategies Hands-on examples and a programming assignment Retriever architecture overview Information Retrieval Foundations Zahin Hasan Retriever Knowledge Base Relevant Documents Augmented Prompt Prompt Response LLM Zain Hasan Two Search Approaches Keyword Search Looks for documents containing the exact words found in the prompt. Semantic Search Looks for documents with similar meaning to the prompt. Zain Hasan Search Techniques 1. Document A 2. Document B 3. Document C 4. Document D 1. Document C 2. Document B 3. Document X 4. Document Y 1. Document A 2. Document B 3. Document C 4. Document D 1. Document C 2. Document B 3. Document X 4. Document Y Keyword Search Semantic Search Each search returns 20 - 50 documents Each list filtered on document metadata Metadata Filter Metadata Filter 1. Document A 2. Document B 3. Document X 4. Document Y Knowledge Base Zain Hasan Keyword Search Ensures sensitivity to exact words the user included in the prompt Semantic Search Finds documents with similar meaning, even without matching words High-performing retrievers balance all three techniques based on project needs. Hybrid Search Metadata Filtering Excludes documents based on rigid criteria Metadata filtering Information Retrieval Foundations Zain Hasan Return all articles published on specific date SELECT * FROM. articles WHERE. publication_date = '2023-10-01'; Metadata Filtering Uses rigid criteria to narrow down documents based on metadata like title, author, creation date, access privileges, and more. Title Publication Date Author Section Tags The Daily Maple section = \"Opinion\" author = \"Michael Chen\" date = June to July 2024 Zain Hasan Metadata Filtering In RAG Subscription Filtering Filter: Exclude all articles with metadata \"subscription = paid\" Geographic Filtering Filter: Include only articles with metadata \"region = North America\" Metadata filtering doesn't perform retrieval, it narrows down results from other techniques based on user attributes, not query content. Zain Hasan Simple to understand and debug Fast, optimized, mature, and reliable Enforces strict retrieval rules, matching exact filter criteria Advantages and Limitations Not true search Rigid, ignores content, and provides no way for ranking Useless alone Cons Pros Keyword search - TF-IDF. Keyword Search TF-IDF. Zain Hasan Introduction to Keyword Search Decades of proven simplicity The simplicity and effectiveness that has powered retrieval for decades makes keyword search a key component of modern RAG systems. Keyword Search Zain Hasan How can I make New York style pizza at home? Keyword Search Doc 1 Doc 2 Doc 3 Use bread flour for New York pizza dough. New York pizzerias stay open late for home delivery. At New York Pizza Mexico, they serve pizza with jalapeño Zain Hasan Bag of words Bag of Words “Making pizza without a pizza oven” Word order is ignored, only word presence and frequency matter pizza making without oven Keywords Prompt Zain Hasan Sparse Vectors Most words aren't used. The bag of words is sparse, with few non-zero entries. “Making pizza without a pizza oven” making pizza without cake drink pan pasta oven pan pasta with tea burger salad taco blend the Zain Hasan Document 1 Document 2 Document 3 Zain Hasan making without oven Doc 2 Doc 3 Doc 4 Doc 5 Doc 1 Zain Hasan Doc 2 Doc 3 Doc 4 Doc 5 Doc 1 making without oven Prompt sparse vector Making pizza without a pizza oven Retriever Zain Hasan making pizza without oven Doc 2 Doc 3 Doc 4 Doc 5 Doc 1 “Making pizza without a pizza oven” “Making pizza without a pizza oven” Zain Hasan making pizza without oven Doc 2 Doc 3 Doc 4 Doc 5 Doc 1 “Making pizza without a pizza oven” Zain Hasan “Making pizza without a pizza oven” “Making pizza without a pizza oven” making pizza without oven Doc 2 Doc 3 Doc 4 Doc 5 Doc 1 pizza without oven Zain Hasan Frequency Based Scoring Example Query: pizza oven Homemade pizza in oven is better than frozen pizza Contains: Pizza (2x) Oven (1x) Document 1 Simple Scoring = 2 points TF Scoring = 3 points Wood-fired oven is a better oven than a stone oven for cooking pizza Document 2 Contains: Pizza (1x) Oven ( 3x) Simple Scoring = 2 points TF Scoring = 4 point Zain Hasan Normalized TF Scoring Longer documents may contain keywords many times simply because they are longer. Solution: Normalize by document length Score = (Number of keyword occurrences) / (Total words in document) Zain Hasan Basic TF scoring treats all words equally, whether they're common filler words or rare, meaningful terms. Solution: Weight terms using “inverse document frequency” (IDF). Score = TF(word, doc) × log(Total docs / Docs containing word) TF-IDF. Zain Hasan total docs docs word appears in Count Documents making pizza For each word Zain Hasan Count Documents Appears in 5 out of 100 documents Pizza Appears in all 100 documents The Zain Hasan Flip to reward rare words The High Score IDF = 1/0.05 Pizza Low Score Count Documents Appears in 5 out of 100 documents Pizza Appears in all 100 documents The Zain Hasan Pizza The Flip to reward rare words High Score IDF = 1/0.05 Low Score Pizza Too common, no weight log( 1 ) The Still higher log( 1/20 ) Apply log Count Documents Appears in 5 out of 100 documents Pizza Appears in all 100 documents The Zain Hasan making pizza without oven Doc 2 Doc 3 Doc 4 Doc 5 Doc 1 pizza without oven TF-IDF. making pizza without oven Doc 2 Doc 3 Doc 4 Doc 5 Doc 1 pizza without oven making: 0.4 (appears in 3/5 docs) moderately common pizza: 0.7 (appears in 2/5 docs) less common without: 0.2 (appears in 4/5 docs) very common a: 0.1 (appears in 4/5 docs) very common oven: 0.2 (appears in 4/5 docs) very common Zain Hasan Documents with rare keywords score higher than documents with common words “Making pizza without a pizza oven” High Scoring Document Pizza oven Low Scoring Document without Zain Hasan TF-IDF. Refined into Modern systems use a slightly refined version called Keyword search - Information Retrieval Foundations Zain Hasan BM25 Scoring BM25 (Best Matching 25) was named as the 25th variant in a series of scoring functions proposed by its creators. This gives the score for a single keyword Sum scores across all keywords for total relevance score for a document Zain Hasan Term Frequency Saturation TF-IDF. \" pizza\" 10 times = Score X \"pizza\" 20 times = Score 2X \"pizza\" 10 times = Score X \"pizza\" 20 times = Score 1.3X Term Frequency Saturation Document Length Normalization Short Doc = Good Score Long Doc = Heavy Penalty TF-IDF. Too aggressive Short Doc = Good Score Long Doc = Smaller Penalty Document length normalization Zain Hasan BM25 Tunable Parameters k₁ - Term Frequency Saturation Controls: How much term frequency influences the score. Range: Typically between 1.2 and 2.0. Effect: Higher values increase the impact of term frequency; lower values reduce it. b - Length Normalization Controls: The degree of normalization for document length. Range: Between 0 (no normalization) and 1 (full normalization). Effect: Balances favoring shorter vs. longer documents. Zain Hasan TF-IDF. vs BM25 BM25 = Standard keyword search algorithm in production retrievers better performance + same cost + more flexibility than TF-IDF. Zain Hasan Keyword Search Overview Match documents by keyword frequency Sparse Vectors Scored Ranked Document length normalization Term Frequency Saturation TF-IDF. Keyword rarity Term frequency Document length Most commonly used Zain Hasan Keyword Search Strengths Simplicity Guaranteed keyword matching Semantic search introduction Information Retrieval Foundations Zain Hasan In order to improve on lexical search it's necessary to capture not only the presence of words, but their meaning. “happy” “glad” Cannot match synonyms Incorrectly matches different meanings Zain Hasan Semantic Search vs. Keyword Search Prompt and documents each get a vector Vectors compared to generate scores The main difference is how vectors are assigned Keyword Search: count words Semantic Search: use embedding model Zain Hasan Understanding Embedding Models Vector Space Embedding models map tokens, to a location in space. This location is represented by a vector. “Pizza” vector [3, 1] “Bear” vector [5, 2] In two dimensions, these can be represented as points food cuisine cat trombone Zain Hasan Understanding Embedding Models No simple interpretation of X and Y axis ...instead... points “float around” in space and similar words cluster together food cuisine cat trombone Vector Space Zain Hasan Understanding Embedding Models More dimensions means more room to form clusters and capture nuanced relationships 2 dimensions 3 dimensions 100 - 1,000+ dimensions Same principles hold Close vectors, similar meanings Zain Hasan Individual Words cat happy Word Embedding Model Sentences The weather is nice What a lovely day Sentence Embedding Model Documents Document Embedding Model Zain Hasan Sentence Embedding Example 1 He spoke softly in class 2 He whispered quietly during class 3 Her daughter brightened the gloomy day Zain Hasan Measuring Vector Distance Euclidean Distance Measures how far apart two vectors are by drawing a straight line from one vector to the other - the shortest possible distance between them. Zain Hasan Measuring Vector Distance Cosine Similarity Looks at the similarity in the direction of two vectors, regardless of whether they're close to one another in space Opposite direction Perpendicular Same direction Zain Hasan Measuring Vector Distance Measures the length of the projection of one vector onto another. Dot product Opposite direction Negative Positive Perpendicular Same direction Zain Hasan Measuring Vector Distance Dot product Cosine Similarity Higher values, closer vectors Opposite direction Perpendicular Same direction Opposite direction Negative Positive Perpendicular Same direction Zain Hasan Semantic Search Embedding Model Vector Space Documents Prompt Rank by distance and return the closest documents Semantic search embedding model deepdive Information Retrieval Foundations Zain Hasan Embedding Models How can an embedding model know to place similar text together, and dissimilar text farther apart? Hello Good morning! Noisy trombone Zain Hasan Positive and Negative Examples in Training Positive Pair Hello Good Morning Negative Pair Good Morning That's a noisy trombone Zain Hasan Positive and Negative Examples in Training Compile massive training dataset of positive and negative pairs Zain Hasan Initial Random Vectors in Embedding Models “Good morning” Text Input Untrained Model Vector Space Random Initialization Zain Hasan Contrastive Training Process Look at where each pair was placed in vector space Scores better when closer together Scores better when farther apart Score Results Zain Hasan Embed Score Update Evaluate Repeat many times Contrastive Training Process Trained Vector Space Trained Vector Space Zain Hasan Contrastive Training Process Update internal parameters based on scoring the positive and negative pairs Repeat the process: Embed → Score with Pairs → Update parameters Iteratively repeat the process, improving the model Trained model Mid-training Early in training Zain Hasan Contrastive Training Process “He could smell the roses” “A field of fragrant flowers” “The lion roared majestically” Training Pushing and pulling After Training Meaningful Embeddings Beginning of Training Random Positions Zain Hasan Scaling up Contrastive Learning In reality every vector is simultaneously pushed and pulled in many directions Millions of positive and negative pairs with complex relationships! Using 100s or 1,000s of dimensions creates more space in which to push and pull vectors Eventually vectors pulled near similar words or text Zain Hasan Key Takeaways Semantic vectors are abstract and somewhat random Before training: locations in space have no meaning After training: locations have meaning because clusters of similar text have formed Only compare vectors from same embedding model Vector embeddings in RAG Hybrid Search in Information Retrieval Zain Hasan Key Strategies Keyword Search Scores documents based on having the same keywords found in the prompt Fast, performs especially well when keywords matter, but relies on exact matches Semantic Search Scores and ranks documents based on having similar meaning to the prompt Slower, computationally expensive, but more flexible Metadata Filtering Uses rigid criteria stored in document metadata to narrow down search results Fast, easy, yes-no filter, but can't be used alone Zain Hasan Hybrid Search 1. Document A 2. Document B 3. Document C 4. Document D 1. Document C 2. Document B 3. Document X 4. Document Y 1. Document A 2. Document B 3. Document C 4. Document D 1. Document C 2. Document B 3. Document X 4. Document Y Lists combined to form single ranking Keyword Search Semantic Search Each search returns 50 documents Retriever 35 remaining Metadata Filter Metadata Filter 30 remaining Zain Hasan Reciprocal Rank Fusion Rewards documents for being highly ranked on each list Control weight of keyword vs. semantic ranking Score points equal to reciprocal of ranking 1st = 1 point, 2nd = 0.5 points, etc. Total points from all ranked list used to perform final ranking Zain Hasan RRF Calculation Keyword Rank Semantic Rank Total Score Zain Hasan Reciprocal Rank Fusion Top ranked document shoots to top of overall ranking 1st vs 10th: 10x difference When k = 0 Single high rank doesn't dominate overall ranking 1st vs 10th: 1.2x difference When k = 50 RRF only cares about ranks, not scores Zain Hasan Semantic Search Keyword Search Beta: Weighting Semantic vs. Keyword Semantic Search Keyword Search If exact keyword matching is important, set a lower beta Zain Hasan Hybrid Search 1. Document A 2. Document B 3. Document C 4. Document D 1. Document C 2. Document B 3. Document X 4. Document Y 1. Document A 2. Document B 3. Document C 4. Document D 1. Document C 2. Document B 3. Document X 4. Document Y Keyword Search Semantic Search Each search returns 50 documents Retriever 35 remaining Metadata Filter Metadata Filter 30 remaining 1. Document A 2. Document B 3. Document X 4. Document Y Return the “top_k” most similar documents Hybrid search Information Retrieval Foundations Zain Hasan Retrieval Quality Metrics Common ingredients to most retriever quality metrics: The Prompt The specific prompt being evaluated Ranked Results Documents returned in ranked order Ground Truth All documents labeled as relevant or irrelevant If you want to evaluate your retriever you need to know the correct answers Zain Hasan Precision and Recall Precision Measures how many of the returned documents are relevant Relevant Retrieved / Total Retrieved Recall Measures how many of the relevant documents are returned Relevant Retrieved / Total Relevant Zain Hasan Example First Run Retrieved: 12 Documents Relevant: 8 Documents Precision (8/12) Recall (8/10) 10 Relevant Documents in Knowledge Base Precision penalizes for returning irrelevant documents Recall penalizes for leaving out relevant documents 100% Precision & Recall: Rank relevant documents most highly and only return those Second Run Retrieved: 15 Documents Relevant: 9 Documents Precision (9/15) Recall (9/10) Zain Hasan Top k Top K Retrieval metrics are influenced by how many documents the retriever returns Metrics are discussed in terms of top-k documents Zain Hasan Precision @5 2 out of 5 Precision @10 6 out of 10 Recall@10 6 out of 8 total relevant Rank Relevant relevant relevant relevant relevant relevant relevant Example Top-5 or Top-1 is stricter Top-5 to Top-15 often used Zain Hasan Mean Average Precision MAP@K evaluates average precision for relevant documents in first K documents. It is built off a related metric called “average precision”. Rank Item Precision@K relevant relevant relevant Divide by number of relevant documents Sum precisions for relevant docs only. Rewards ranking relevant documents highly This calculation gives Average Precision or AP, for Mean Average Precision you find the average AP value across many prompts Zain Hasan Reciprocal rank Measures the rank of the first relevant document in the returned list Reciprocal Rank = 1 / Rank First relevant at rank 1 First relevant at rank 2 First relevant at rank 4 The later the first relevant document appears, the worse the reciprocal rank Mean Reciprocal Rank (MRR) averages over many prompt Zain Hasan Mean Reciprocal Rank First relevant at rank 1 First relevant at rank 3 First relevant at rank 6 First relevant at rank 2 Search Search Search Search MRR = 0.5 Divide by number of searches Sum all ranks Zain Hasan How to use retriever metrics Recall or recall@K Most cited metric, captures fundamental goal of finding relevant documents Mean Reciprocal Rank How well model performs at the very top of ranking Precision & MAP Asses irrelevant documents and ranking effectiveness All metrics depend on having ground truth relevant documents Metrics help: Evaluate retriever performance Check if adjustments improve results Module 2 Conclusion Information Retrieval Foundations Zain Hasan Conclusion Keyword Search Ranks by keyword frequency exact matches Semantic Search Ranks by meaning, flexible Metadata Filtering Excludes by criteria Hybrid Search Combines all three techniques Evaluation Metrics Precision & Recall MAP Mean Reciprocal Rank Measure improvement from adjusting tunable parameters in hybrid search"
  },
  {
    "index": 3,
    "title": "3. Information Retrieval with Vector Databases",
    "content": "Module 3 introduction Information Retrieval in Production Zain Hasan Introduction Vector Databases Vector databases: optimized for huge quantities of vector data Almost synonymous with RAG systems What You'll Learn Hands-on vector database usage Production techniques: chunking, query parsing, reranking Programming assignment applying all concepts Information Retrieval in Production Approximate nearest neighbors algorithms (ANN) Zain Hasan Basic Vector Retrieval - KNN Vectorize all documents and prompt KNN: K Nearest Neighbors Prompt vector Compute distances to all document vectors Sort by distance Zain Hasan Dataset Size Calculation Time Linear Growth 1,000 docs = 1,000 distance calculations per search Scaling Challenges If you want to build high-performing retrievers, you'll need a better approach! 1B docs = 1B distance calculations every time Zain Hasan Approximate Nearest Neighbors ANN is significantly faster than KNN Rely on additional data structures Not guaranteed to find the absolute closest documents Zain Hasan Navigable Small World Compute distances between all document vectors Add one node to the graph for each document Connect each node to its nearest neighbors Can traverse the graph moving along edges between neighboring documents Proximity Graph Zain Hasan Query Entry Point Candidate Query Vector Zain Hasan Search Algorithm Query Vector Zain Hasan Search Algorithm Query Vector No closer neighbors Zain Hasan Search Algorithm Query Vector Can find multiple nearest neighbors May not find closest possible vectors, algorithm doesn't pick optimal overall path, just best path in each moment In practice performs well and much faster than KNN Zain Hasan Hierarchical Improvement Hierarchical Navigable Small World (HNSW) enhances Navigable Small World by speeding up early parts of the search Relies on a hierarchical proximity graph Zain Hasan Layer 1 Contains all 1,000 vectors with complete proximity graph for precise final search Layer 2 Randomly drop to 100 vectors and build a new proximity graph for intermediate navigation Layer 3 Randomly drop to just 10 vectors and create a proximity graph for fast navigation at the highest level Zain Hasan Layer 1 Start at best candidate in layer 2, and complete normal search through the layer 1 proximity graph Layer 2 Start at best candidate from layer 3, and complete normal search through the layer 2 proximity graph Layer 3 Choose random candidate vector and search top layer to get as close as you can in this layer Make big jumps early Close to prompt vector once in Layer 1 Zain Hasan Hierarchical Navigable Small World Significantly faster than KNN Dataset Size Calculation Time KNN Linear HNSW Logarithmic Allows scaling up to billions of vectors Exponentially fewer vectors in each layer (for example 1000 → 100 → 10) makes it approximately logarithmic Zain Hasan Approximate Nearest Neighbors - Takeaways ANN is significantly faster than KNN at scale Find close documents, but can't guarantee best matches Depends on proximity graph, computationally expensive to build but can be pre-computed Information Retrieval in Production Vector databases Zain Hasan Vector Database Vector Database Designed for Vector Search Designed to store high-dimensional vectors and perform vector search using ANN algorithms Outperform Relational Databases Relational databases perform vector search similarly to an inefficient KNN search Optimized for ANN Search Designed to build HNSW indexes and compute vector distances. They scale well and operate quickly Weaviate Popular open-source vector database you'll use in this course Zain Hasan Typical Vector Database Operations Database setup Load documents Create sparse vectors for keyword search Create dense vectors for semantic search Create HNSW index to power ANN algorithm Run searches! Zain Hasan Connect to Database and Create a Collection Creates collection Specifies vectorizer Specifies Properties Zain Hasan Adding objects to a collection Batch adding data Detecting errors Handling failures Zain Hasan Vector Search in Action Specifies Collection Performs vector search with “near_text” Asks for vector distances Zain Hasan Keyword Search in Action BM25 keyword search Zain Hasan Hybrid Search in Action Hybrid search Weighs the hybrid search to 25% vector, 75% keyword Zain Hasan Filtered Search in Action Adding a metadata filter Zain Hasan Complete Workflow Configure Database Load and Index Data Perform Searches Chunking Information Retrieval in Production Zain Hasan Why Chunk Documents? Token Limits Improved Relevancy LLM only sent relevant context Zain Hasan Indexing without chunking Knowledge base contains 1,000 books Each book is vectorized by an embedding model Result: 1,000 vectors Entire Book Entire book compressed into a single vector Zain Hasan The problems with this approach Compresses entire book meaning into single vector Can't sharply represent specific topics, chapters or pages Creates \"averaged\" representation across all content Results in poor search relevance Retrieves entire books, quickly filling LLM context window Zain Hasan Chunking your content Entire Book Page Paragraph Sentence Zain Hasan Too Small Word level chunks Just Right Optimal Chunks Too Large Chapter level chunks Balancing Chunk Size Too many topics in one vector Fill LLM context window Loses surrounding context Reduces search relevance Balance between capturing too much and too little Zain Hasan Fixed Size Chunking Document Chunking Chunk 1 Fixed Size: 250 Characters per chunk Chunk 2 Chunk 3 Zain Hasan Overlapping Chunking Chunk 2 Characters 226-475 Chunk 3 Characters 451-700 Chunk 1 Characters 1-250 Chunks overlap by 25 characters 10% overlap Minimizes words cut off from context Words in the middle of a chunk have context on each side, words on the ends are in two chunks Increases relevancy, uses more space Zain Hasan Recursive Character Splitting Chunk 1 s Taylor Swift is performing three sold-out shows in Vancouver this weekend as part of her Eras Tour. Chunk 2 s Thousands of fans are traveling to the city, causing hotel demand to spike. Chunk 3 s Local hotels are fully booked, and prices have more than doubled since last month. Taylor Swift is performing three sold-out shows in Vancouver this weekend as part of her Eras Tour. Thousands of fans are traveling to the city, causing hotel demand to spike. Local hotels are fully booked, and prices have more than doubled since last month. Taylor Swift is performing three sold-out shows in Vancouver this weekend as part of her Eras Tour. Thousands of fans are traveling to the city, causing hotel demand to spike. Local hotels are fully booked, and prices have more than doubled since last month. Splitting text into chunks at a specified character, for example newlines Variable chunk size, but better accounts for document structure Zain Hasan Splitting on Different Characters HTML Documents Split paragraph or header characters Python Chunk by function definitions <h1>Document Title</h1> <p>First paragraph</p> <p>Second paragraph</p> def function_one(): \"\"\"Docstring\"\"\" return result class MyClass: def __init__(self): self value = Zain Hasan Implementing Chunking Strategies Implement Yourself vs. Libraries Fixed size splitting with overlaps is straightforward to implement yourself or with external libraries Metadata Preservation Chunks inherit source document metadata plus location information Advanced chunking techniques Information Retrieval in Production Zain Hasan “That night she dreamed, as she did often, that she was finally an Olympic champion” That night she dreamed, as she did often, that she was finally an Olympic champion Zain Hasan Semantic Chunking Groups sentences together based on similar meanings rather than arbitrary character limits Fixed Size Chunking Canada is known for its Maple syrup. The country has beautiful mountains. Canadian landscapes are stunning Semantic Chunking Canada is known for its Maple syrup. The country has beautiful mountains. Canadian landscapes are stunning Zain Hasan Move Through Document Process the document sentence by sentence Vectorize & Compare Convert chunk and next sentence to vectors, calculate cosine distance Check Threshold If distance is below threshold, add sentence to chunk Split when Different When distance crosses threshold, start new chunk Semantic Chunking Canada is known for its Maple syrup. The country has beautiful mountains. Canadian landscapes are stunning Zain Hasan Semantic Chunking I was puzzled by the 1401. I couldn't figure out what to do with it. And in retrospect there's not much I could have done with it. The only form of input to programs was data stored on punched cards... Kamradt, G. (2023). 5 Levels of Text Splitting. GitHub. Zain Hasan Pros and Cons Chunking can be Computationally expensive Requires repeated vector calculations Cons Follows author's train of thought Smarter chunk boundaries Higher recall and precision Pros Zain Hasan Language Based Chunking Document LLM Chunk 1 Concept A Chunk 2 Concept B Chunk 3 Concept C Prompt LLM to create chunks from a document Include instructions on types of chunks, like keeping concepts together, adding breaks when new topic starts Performs well, increasingly more economically viable Zain Hasan Thanks to Daniel Townsend, Sarah Johnson, Mike Chen,... context: list of supporters and contributors. Context-Aware Chunking Chunks Additional Context \"Reflections on Launching Our First App\" After six months of development, we finally launched our first app... The biggest lessons were around user feedback... Document LLM context: app launch title. context: launch announcement. context: summary of key development lessons. Zain Hasan Context-Aware Chunking Retriever Knowledge Base Added context helps both search relevance and as the LLM generates responses Costly pre-processing: LLM adds context chunk by chunk. Benefit: better search, no impact on speed. Additional Context \"Reflections on Launching Our First App\" After six months of development, we finally launched our first app... The biggest lessons were around user feedback... Thanks to Daniel Townsend, Sarah Johnson, Mike Chen,... context: list of supporters and contributors. context: app launch title. context: launch announcement. context: summary of key development lessons. LLM Zain Hasan Choosing a Chunking Approach Fixed Width and Recursive Character Splitting: good defaults Semantic and LLM Chunking: can yield higher performance, but more complex. Experiment to see if it's worth it Context Aware Chunking: improves any chunking technique at some cost. A good “first improvement” to explore Query parsing Information Retrieval in Production Zain Hasan Hey, can you tell me about that climate thing we talked about?\" Prompt LLM Zain Hasan The following prompt was submitted by a user in order to query a database of medical documents linking symptoms to diagnoses. Rewrite the prompt to optimize it for searching the database by doing the following: - Clarify ambiguous phrases - Use medical terminology where applicable - Add synonyms that increase odds of finding matching documents - Remove unnecessary or distracting information. {user prompt} Query Rewriting Use an LLM to rewrite the query before it's submitted to the retriever. Retriever Messy Prompt Query Rewriter LLM Optimized Prompt Zain Hasan I was out walking my dog, a beautiful black lab named Poppy, when she raced away from me and yanked on her leash hard while I was holding it. Three days later my shoulder is still numb and my fingers are all pins and needles. What's going on? Patient's Original Prompt Optimized Prompt Experienced a sudden, forceful pull on the shoulder, resulting in persistent shoulder numbness and finger numbness for three days. What are the potential causes or diagnoses, such as neuropathy or nerve impingement? LLM Query Rewriter Zain Hasan Query Rewriting Iterate on the prompt for your query rewriter Benefits are substantial, easily justify costs Retriever Messy Prompt Query Rewriter LLM Optimized Prompt Zain Hasan Named Entity Recognition Identifies and categorizes specific types of information within queries, enabling more targeted search and filtering strategies. Places Dates People Characters Organizations Zain Hasan GLINER. Named Entity Recognition Labeled Prompt \"I read The Great Gatsby by F. Scott Fitzgerald last summer while visiting New York. Are there any similar books set in the 1920s that I might enjoy?\" BOOK LOCATION PERSON DATE \"I. read The Great Gatsby by F. Scott Fitzgerald last summer while visiting New York. Are there any similar books set in the 1920s that I might enjoy?\" Original Prompt Person, books, location, dates, actors, characters Entities Zain Hasan HyDE LLM writes a hypothetical document that answers the prompt Hypothetical Document Embeddings (HyDE) \"my left shoulder hurts and I have numbness in my thumb and index finger, what should I do?” Original Prompt Uses generated “hypothetical documents” that would be ideal search results to help with the search process Embedding Model Embedding vector of hypothetical document is used to complete search Zain Hasan Hypothetical Document Embeddings (HyDE) Normally a retriever is matching prompts to documents HyDE means the retriever is matching documents to documents, one is the “perfect” hypothetical one generated from the prompt Can provide performance improvements but adds latency and some cost Cross-encoders and ColBERT Information Retrieval in Production Zain Hasan Bi-Encoder If you're willing to give up some speed, you can get even higher quality search results Separate Semantic Vectors Documents and prompts are embedded separately by an embedding mode ANN Search ANN is used by a vector database to rapidly identify documents whose vectors are close to the prompt vector. Document Vectors are Pre-Computed Documents can be embedded ahead of time and only the prompt itself needs to be embedded after it's received Prompt Bi-encoder Vector DB uses ANN search to compare prompt and document vectors Documents Embedding Model Zain Hasan Cross-Encoder Feed to Cross Encoder Cross encoder develops deep contextual understanding of interactions between prompt and document Cross-Encoder Concatenate Document and Prompt One by one documents are concatenated with the prompt Document Prompt Concatenated Generate Relevance Score Cross encoder directly outputs relevance score between 0 and 1 Relevance Score Zain Hasan Prompt: Great places to eat in New York Prompt: Great places to eat in New York Prompt: Great places to eat in New York Top 10 restaurants in Manhattan: From upscale dining in Times Square to hidden gems in the East Village... New York's subway system includes 472 stations and is one of the oldest public transit systems in the world... Brooklyn's food scene features diverse cuisines. From pizza in DUMBO. to dim sum in Sunset Park... Cross Encoder Match Score Cross Encoder Match Score Cross Encoder Match Score Zain Hasan Cross-Encoder Pros and Cons Cons Pros Great for improving the results of other search techniques Almost always provide better search results than a bi-encoder Too inefficient to use as a default search technique Can't pre-process since they run on prompt-document pairs Scale terribly with millions or billions of documents Zain Hasan ColBERT (Contextualized Late Interaction Over BERT) Each Token Gets a Vector Rather than generating one vector for each prompt and document, each token is vectorized Split the Difference between Bi and Cross Encoders Generate document vectors ahead of time like bi-encoders but also capture deep text interactions like in cross-encoders Prompt One vector per token Document Embedding Model One vector per token ColBERT Scoring Each prompt vector tries to find its most similar document vector Scoring Each prompt vector finds its most similar document vector Zain Hasan The cuisine New York City comprises many Great places eat New York Document Tokens Prompt Tokens Similarity scores between document and prompt tokens Zain Hasan The cuisine New York City comprises many Great places eat New York Document Tokens Prompt Tokens MaxSim score for this document Zain Hasan ColBERT Pros and Cons Cons Pros Reasonably fast, can still be used in real-time or close-to-real-time scenarios Scalability of bi-encoder, much of the rich interactions of a cross-encoder Requires significant vector storage as each token, rather than each document, needs a dense vector Zain Hasan Key Takeaways Bi-encoders: reasonably good quality, great speed, minimal storage, default semantic search Cross-encoders: best quality, extremely slow, minimal storage ColBERT: nearly the quality of a cross-encoder, decent speed, significant vector storage. ColBERT and similar approaches increasingly supported by vector DBs Reranking Information Retrieval in Production Zain Hasan User Query Initial Retrieval Reranking LLM Generation Purpose of Reranking Reranking Engine Cross Encoder LLM Scoring Top Ranked Documents from Hybrid Search Optimal Document Order Zain Hasan What is the capital of Canada? Original Query Initial Results Toronto is in Canada The capital of France is in Paris Canada is the maple syrup capital of the world Ottawa is the capital of Canada Semantically similar but not all directly relevant Zain Hasan Knowledge Base Reranking Overview Final Results Return the best 3-5 documents to the LLM Initial Retrieval Fast but Imprecise Zain Hasan Reranked Results Toronto is in Canada The Capital of France is in Paris Canada is the maple syrup capital of the world Ottawa is the capital of Canada What is the capital of Canada? Original Query You'll still return just 5-10 documents but reranking ensures they're far more relevant. Ottawa Parliament Zain Hasan Cross-Encoder re-rankers Cross-encoders give better results than bi-encoders but are slower Using cross-encoders only after initial bi-encoder filtering makes the quality-time tradeoff feasible. Adds minor latency, but typically yields significantly better results. Cross-Encoder Document Prompt Concatenated Relevance Score Zain Hasan LLM Based Scoring Document “What is the capital of Canada” Query Relevance Score LLM based scoring is powerful but costly. Like cross encoders, it is too slow for large scale retrieval and is best used for reranking after initial filtering. Fine Tuned LLM Module 3 Conclusion Information Retrieval in Production Zain Hasan Conclusion Approximate Nearest Neighbors ANN algorithms perform vector search significantly faster than brute force k-nearest neighbors Vector Databases Optimized to store high-dimensional vector data and perform approximate nearest neighbor searches RAG Techniques Chunking Query Parsing Re-Ranking"
  },
  {
    "index": 4,
    "title": "4. LLMs and Text Generation",
    "content": "Module 4 introduction LLMs and Text Generation. Zain Hasan Module Overview LLM Foundations Learn how transformers work and how to structure effective LLM calls. Transformer Workflows and Grounding Dive into the transformer architecture and build iterative LLM workflows grounded in retrieved information. Advanced Techniques Explore what works best in real-world RAG setups. Hands-on project Apply what you've learned to build a full RAG pipeline. Transformer architecture LLMs and Text Generation Zain Hasan Origins of the Transformer Transformer Architecture \"Attention is All You Need\" (Vaswani et al., Google Brain) Attention Mechanism Pre-trained Language Models LLMs become accessible to general public Zain Hasan Encoder Processes the original text (e.g., German paragraph) Develops deep contextual understanding of the text's meaning Decoder Uses the deep understanding from the encoder Generates new text in target language (e.g., English translation) Used in embedding models for rich semantic representations Most LLMs only include the decoder component, as they just care about text generation. Zain Hasan the brown dog sat next to the red fox the brown dog sat next to the red fox Zain Hasan the brown dog sat next to the red fox Zain Hasan the brown dog sat next the red fox Dense Semantic Vector “First Guess” of meaning Position Vector Position in prompt Semantic and Position vectors sent along together for processing Zain Hasan the brown dog sat next to the red fox Each token sees the meaning and the position of every other token Attention is a fancy way of saying “which other tokens should have the biggest impact on my meaning” Zain Hasan the brown dog sat next to the red fox the brown dog sat next to the red fox The other 10% distributed across the other tokens. Zain Hasan Input Embeddings The + pos_0 [0.12, 0.87, ..., 0.45] Brown + pos_1 [0.33, 0.22, ..., 0.67] Dog + pos_3 [0.50, 0.74, ..., 0.32] Transformer Layers Attention heads Each head learns abstract patterns, not human defined rules Head 1: Object Relations fox brown sat next Head 2: Spatial Relations fox brown sat next Smaller models may use 8 or 16 attention heads, but larger ones might use over 100. Zain Hasan The Feed Forward Phase Attention Input Embeddings Feed Forward dog First Guess: “dog” is an animal Second Guess: “dog” is a brown animal that sat near a red fox the brown sat next the red fox Zain Hasan Attention Updated Token Embeddings Input Embeddings First Guess Second Guess Third Guess Nth Guess first attention + feed forward Understanding 'dog' as subject of sentence Connecting to 'brown' and 'sat' Final refined representation models typically need 8 to 64 layers Feed Forward Iterative Refinement repeated multiple times Zain Hasan and that with who What tokens are likely to come next? Refined Embeddings Model's Vocabulary Thousands more tokens forest brazil banana the brown dog sat next to the red fox and Zain Hasan and the brown dog sat next to the red fox To generate the next token, the model repeats the entire process from scratch This loop continues until it hits either token limit or end of completion token ...[EOS] The LLM's output tokens are de-tokenized and returned to the user as the final response. the brown dog sat next to the red fox and waggled its tail Zain Hasan Conclusion Why RAG works: LLMs can deeply understand information added to prompts through Attention mechanism processing World knowledge in feed forward layers Inherent randomness remains LLMs may randomly ignore injected information Need to control randomness Must confirm LLM grounds answers in retrieved information Computational expense Generating single tokens requires extensive processing Costs grow with prompt/completion length Each token must examine all others for context Most RAG system costs come from running transformers LLM sampling strategies LLMs and Text Generation Zain Hasan The sky is blue bright falling clear overcast dark Peaked distribution Model is “confident” turbid red crepuscular crimson green cheese Flat distribution Model is “uncertain” Zain Hasan Greedy Decoding “which the data confirms, which the data confirms, which the data confirms...” Sampling strategy that always selects the token with the highest probability at each step of text generation. Deterministic Generic-sounding text Can get stuck in a loop Useful in code completion or debugging Zain Hasan Temperature Parameter that changes the shape of the distribution generated by the LLM Temp: 1 Origin distribution Temp: 0.5 More spiky distribution Temp: 0 Greedy decoding Temp: 1.2 Flatter “Creative” Temp: 5 Very flat Nonsense Zain Hasan Advanced Token Sampling jumped Picks only from the k most likely tokens, ignoring the rest. Top-K fence gate wall river house pizza brazil banana Zain Hasan Advanced Token Sampling jumped Picks only from the k most likely tokens, ignoring the rest. Top-K fence gate wall river house pizza brazil banana Top-k: 5 Top-K of 5 means only the 5 most likely tokens can be chosen Zain Hasan Advanced Token Sampling Picks from tokens whose cumulative probability is below some threshold Top-P jumped fence gate wall river house pizza brazil banana Zain Hasan Advanced Token Sampling Picks from tokens whose cumulative probability is below some threshold Top-P Top-p of 85% includes tokens until their probability is above 85% jumped fence gate wall river house pizza brazil banana Cumulative sum Top-p: 85% Zain Hasan Top-k vs. Top-p Top-p is more dynamic than top-k Top-k always includes the same number of tokens regardless of the shape of the distribution Top-p includes more or fewer tokens depending on how “certain” the model is Zain Hasan Token Specific Strategies The brown dog jumped over the... Reduce the probability of already used tokens, discouraging repetition. Helps prevent: Loops (e.g., \"I think, I think...\") Redundant sentence patterns Overuse of specific words Repetition Penalties fence house gate wall dog poop Zain Hasan The brown dog jumped over the... Token Specific Strategies Reduce the probability of already used tokens, discouraging repetition. Helps prevent: Loops (e.g., \"I think, I think...\") Redundant sentence patterns Overuse of specific words Repetition Penalties New probability after penalty fence house gate wall dog poop Zain Hasan Token Specific Strategies Allow direct manipulation of token probabilities by adding or subtracting values from the model's raw calculated probabilities. Logit Biases fence house gate wall dog poop The brown dog jumped over the... “poop” permanently biased down Biases can: Filter profanity Boost categories in a classifier LLM Zain Hasan payload = { \"model\": \"meta-llama/Llama-2-70b-hf\", \"temperature\": 0.8, \"top_p\": 0.9, \"repetition_penalty\": 1.2 “Slightly conservative in token choice” “Avoid choosing from far tail of distribution” “Lightly penalize repeated tokens” Zain Hasan Using Sampling Strategies Set temperature at top-p that fits your application's needs Code or factual domain → low temperature and top-p Creative domain → explore higher temperature and top-p Then add repetition penalties, logit biases, etc. as needs arise Choosing your LLM LLMs and Text Generation Zain Hasan Important LLM Characteristics Model Size Small models: 1 - 10 billion parameters Large Models: 100 - 500 billion and beyond Large models can be more capable, always more expensive Cost Fixed cost per million tokens, sometimes different for input vs output New and larger models usually cost more Context Window Maximum number of tokens an LLM can process, both prompt and completion You still pay per token Latency and Speed Time to first token, tokens per second Training Cutoff Date Last point in time in the model's training data Later is usually preferable Zain Hasan LLM Quality Metrics There are many types of quality There are many benchmarks that try to measure quality There's no single authoritative list Three categories: Automated, Human-scoring, LLM-as-a-judge Zain Hasan Automated Benchmarks Evaluated with code Common format is multiple choice test(s) on various subjects Example: MMLU covers 57 subjects from STEM to humanities Many automated benchmarks exist for wide variety of domains Zain Hasan Human Evaluated Benchmarks Anonymous LLMs respond to a prompt, humans choose preferred response Uses ELO algorithm to create comparative leaderboards Example: LLM Arena is a popular host of human-graded rankings Captures nuanced quality factors automated benchmarks miss Zain Hasan LLM-as-a-judge Benchmarks One LLM rates another's responses against reference answers Produces \"win rate\" for comparing LLM performance Upside: Cheap and flexible evaluation method Downside: Judges prefer their own model family (bias) Zain Hasan Good Benchmark Qualities Relevant to your project Difficult to help distinguish between high and low performing models Reproducible with stable outcomes between test runs and verifiable results Align with real-world performance Look out for data contamination Zain Hasan Benchmarks Over Time Benchmark scores start low Scores rapidly rise to be on par with human experts “Saturated” metrics no longer differentiate models New benchmarks need to be repeatedly introduced Main takeaway - Newer models usually outperform older ones Source: International AI Safety Report - January 2025 Contains public sector information licensed under the Open Government Licence v3.0. Prompt engineering: building your augmented prompt LLMs and Text Generation Zain Hasan Messages Format \"messages\": [ \"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions. Provide factual, educational responses.\" \"role\": \"user\", \"content\": \"What is the capital of Canada?\" Content: The text of the message Role: either \"system\", \"user\", or \"assistant\" System: provides high-level instructions to influence LLM behavior User: records prompts sent by users Assistant: records responses previously generated by the LLM Zain Hasan \"messages\": [ \"role\": \"user\", \"content\": \"What's the capital of Canada?\" \"role\": \"assistant\", \"content\": \"Ottawa - known for its government and heritage.\" My Chatbot \"messages\": \"role\": \"user\", \"content\": \"What's the capital of Canada?\" \"role\": \"assistant\", \"content\": \"Ottawa - known for its government and heritage.\" \"role\": \"user\", \"content\": \"Why was it chosen?\" Zain Hasan You are a helpful AI assistant that provides accurate facts about countries <|begin_of_text|><|start_header_id|> You are a helpful AI assistant that provides accurate facts about countries<|eot_id|> <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a helpful AI assistant that provides accurate facts about countries<|eot_id|> <|start_header_id|>user<|end_header_id|> What's the capital of Canada?<|eot_id|> <|start_header_id|>assistant<|end_header_id|> Ottawa - known for Parliament Hill and its bilingual culture.<|eot_id|> Zain Hasan System Prompt System prompt provides high-level instructions on how the LLM should behave Include desired tone and procedures the LLM should follow Example system prompt: approximately 2,100 words. About 4-5 pages Zain Hasan High-Level Instructions Fundamental behavior and knowledge cut-off Claude's knowledge base was last updated at the end of October 2024. It answers questions about events prior to and after October 2024 the way a highly informed individual in October 2024 would.... Tone & Personality Define how the LLM should communicate Claude reasons through answers step by step. Claude does not help with potentially harmful requests. Claude responds in markdown. Claude is intellectually curious and enjoys hearing what humans think. Zain Hasan Your own system prompt Use these principles to construct your own system prompt Instruct your LLM to respond in great detail OR answer questions succinctly For RAG applications, tell the language model to: Use only retrieved documents to answer prompts Judge whether a document is relevant Cite sources in its response System prompts are added to every prompt your LLM processes Zain Hasan Prompt Templates # System Instructions {system_prompt} # behavioral guidance # Conversation History # if conversation_history exists User: {message_1} Assistant: {response_1} User: {message_2} Assistant: {response_2} # Retrieved Information [DOCUMENT. 1] {chunk_1_text} Source: {chunk_1_source} # User Prompt User: {user_query} You're ready to build your augmented prompt Use a well-considered template since prompts include many pieces of information Templates provide structure and decide where content gets injected Zain Hasan Prompt Template # System Instructions You are an useful assistant for geographic information . Only use retrieved documents to answer. - Use only retrieved documents - Cite sources as [DOC X] - Admit if information is missing # Conversation History User: What is the largest city in Canada? Assistant: Toronto is Canada's largest city with a population of 2.9 million [DOC # Retrieved Documents [DOC 1] Ottawa is the capital city of Canada, located in the province of Ontario [DOC 2] Ottawa became Canada's capital when Queen Victoria chose it in 1857 [DOC 3] The Parliament of Canada is located in Ottawa on Parliament Hill # Current Query What is the capital of Canada? Prompt engineering: advanced techniques LLMs and Text Generation Zain Hasan In Context Learning Including example question-response pairs within your prompt to help the LLM learn a structure and tone to respond in. How do I return an item? Pattern learned from examples Request: How do I return an item? Response: Hi [Customer Name], for order #[Order ID]: Print return label from your email and ship Questions? Contact help@mapleshop.ca Hi Zain, for order #78234: Print return label from your email and ship to us. Questions? Contact help@mapleshop.ca Customer service bot example: Include previous customer requests Include high-quality responses to those request Zain Hasan In Context Learning You're adding extra information to the prompt Many examples it's called few-shot learning One example it's called one-shot learning A common way to implement this is by hard-coding examples # System Instructions You are a helpful customer support assistant. # Example 1 Customer: How do I reset my password? Agent: Click \"Forgot Password\" on the login page. # Example 2 Customer: Can I cancel my subscription? Agent: Yes, from your account settings. Zain Hasan Index successful customer chats Q: \"How do I track my order?\" A: \"To track your order, click the 'Order Status' button in your account dashboard. Retrieve relevant information Refund Policy Customer Chats Inject examples into prompt <refund policy> <chats> How to track a refund from a cancelled order? Zain Hasan In context learning prompt template \"messages\": [ {\"role\": \"system\", \"content\": \"You are a helpful customer service assistant. \"}, {\"role\": \"system\", \"name\": \"example_user \", \"content\": \"How do I track my order? \"}, {\"role\": \"system\", \"name\": \"example_assistant \", \"content\": \"To track your order, click 'Order Status' in your dashboard. \"}, {\"role\": \"system\", \"content\": \"REFUND POLICY:. Refunds take 5-7 days. Check status in 'Refunds' tab. \"}, {\"role\": \"user\", \"content\": \"How to track a refund from a cancelled order? \"} Zain Hasan Another powerful technique encourages the LLM to reason through prompts step by step. Encouraging Reasoning Tell the LLM to 'think aloud' about how to approach the problem before providing a final answer. <scratchpad> Option 1: Could be X because... Option 2: Might be Y if... Actually, Z makes most sense because... </scratchpad> Zain Hasan Chain Of Thought What's the capital of Canada? Think step by step User Query I need to identify Canada's capital city Canada is a North American country The capital is located in Ontario province Ottawa is the capital city of Canada Therefore, the capital of Canada is Ottawa. LLM response Follow Steps Input Generate Steps Output Zain Hasan Reasoning Models Chain of Thought Reasoning Models Encourage model to think before answering Zain Hasan Why do we wear sunglasses in the snow? REASONING TOKENS:. Snow = white → reflects sunlight Sunlight = bright → includes UV rays Reflected light = intense on the eyes Sunglasses reduce glare & UV → protect vision We wear sunglasses in the snow to protect our eyes from the intense reflected sunlight. Reasoning tokens boost accuracy but add cost These models run slower and are more expensive Worth it in RAG for better relevance and integration Reasoning Models Zain Hasan Many prompting techniques don't work well on reasoning models. Struggle with in-context learning and example mixing Perform best with clear goals and strict formats Work well with full context and high-level guidance LLM providers will suggest the best ways to prompt new models Reasoning Models Warnings Zain Hasan Context Window Management Regular LLM use Reasoning Model + RAG Initial Prompt Reasoning Tokens Response Tokens RAG Documents Advanced techniques consume more context Context Window Initial Prompt Response Tokens Zain Hasan Management Strategies - Context Pruning With single-turn conversation skip prompt techniques if they add no value Use context pruning to manage long multi-turn prompts User LLM User LLM User LLM User LLM User Keep last 5 Messages Drop old messages User LLM User LLM User With reasoning models, drop reasoning tokens from chat history Zain Hasan Management Strategies Include only chunks relevant to the latest question. Use long-context models for deeper, multi-turn conversations. Even with big context, keep prompts efficient. Handling hallucinations LLMs and Text Generation Zain Hasan User Prompt “Do you offer student discounts?” Retriever Knowledge Base Relevant Documents Augmented Prompt LLM Retrieved Information “Senior Discount: 10%” “New Customer Discount: 10%” System Prompt “Be helpful with customers” Response “Absolutely, you can get 10% off with a valid student ID, the same great discount we offer seniors and new customers!” The LLM just made it up! Zain Hasan Why LLMs Hallucinate LLMs produce probable text sequences Probable text is not always accurate, LLMs can't tell the difference jumped fence gate wall river house Zain Hasan Why Hallucinations Matter Inaccurate Information Bad on it's own Hard to detect Sounds more plausible than nonsense Erode Trust Occasional hallucinations detract even from mostly accurate systems RAG can help Helps ground responses in retrieved information, but hallucinations still possible Zain Hasan Types of Hallucinations Sometimes mess up small details Other times entirely invent facts Need to evaluate LLM output on many levels to ensure accuracy “You'll love our student discount!” “We're happy to offer a 5 % discount to seniors!” “There isn't a senior discount” Zain Hasan There's no perfect solution for hallucinations Zain Hasan Self-Consistency Methods Repeatedly generate responses to the same prompt and confirm consistency “Our senior discount is 10%” “We're happy to offer a 10% discount to seniors!” “Seniors enjoy a 5% discount” In practice, costly and unreliable Factual inconsistencies can indicate hallucinations Zain Hasan Reducing Hallucinations with RAG RAG. Grounds Responses Retrieve relevant documents Inject factual information 5% Senior Discount There is no senior discount System Prompt “...only make factual claims based on retrieved information...” Reduced Hallucinations “Yes, we offer a 10% discount for senior citizens age 65 and over with valid ID” Without Grounding Zain Hasan Citation Generation “Tell me about Lionel Messi's career achievements.” User Prompt Messi has won a record 8 Ballon d'Or awards [1]. He scored 672 goals for Barcelona [2]. Messi won the World Cup with Argentina in 2022 [3]. [1] Source: FIFA.com, \"Ballon d'Or History\" [2] Source: FCBarcelona.com, \"Club Records\" [3] Source: FIFA.com, \"World Cup 2022\" LLM response “Cite your sources at the end of each sentence using [1], [2], etc.\" System Prompt Instruct the LLM to cite sources after each sentence or paragraph LLMs can just hallucinate citations! Increases likelihood of grounding and makes human verification easier Zain Hasan ContextCite Attributes sentences in response to retrieved documents Can be used to generate citations or as part of evaluation Tags sentences with supporting document Source: https://gradientscience.org/contextcite/ Zain Hasan Citation Quality How well do citations align with correct sources? Correctness How factually accurate is the generated content? Fluency How clear and well-written is the generated text? Evaluating Citation Quality in LLMs Pre-assembled knowledge bases + sample questions Tests RAG system responses on prepared prompts Evaluates responses across three metrics ALCE Benchmark Key Evaluation Metrics Evaluating your LLM's performance LLMs and Text Generation Zain Hasan Retriever Finds relevant information LLM Constructs response LLM metrics should focus on the LLM's role Zain Hasan LLM LLM. Responsibilities Assume the retriever finds relevant information, possibly with a few irrelevant documents LLM should respond clearly, incorporate relevant information, cite sources, ignore irrelevant information These responsibilities are somewhat subjective LLM evals usually use other LLMs, e.g. those found in the RAGAS. library Zain Hasan Response Relevancy Measures where response is relevant to user prompt, regardless of accuracy Evaluator LLM generates several new “sample prompts” that could have lead to the response Embed original and sample prompts to vectors and calculate cosine similarity Average similarity scores for final relevancy measure Doesn't check if information is factual, just if you can “work backwards” from generated response to original user prompt ResponseRelevancy() Zain Hasan Faithfulness Measures whether response in consistent with retrieved information LLM identifies all factual claims in response More LLM calls to determine if claims are factually supported by retrieved information Percentage of supported claims is the “faithfulness” Faithfulness() Zain Hasan Other RAGAS. metrics Other RAGAS. metrics capture noise sensitivity, citations, etc. All metrics rely on LLM-as-a-judge at some point since the LLM's role is complex Zain Hasan Measuring LLM Performance Indirectly Measure LLM performance by measuring overall system performance. Collect system-wide feedback (e.g. thumbs-up/down) and A/B test changes to LLM Important to isolate changes to LLM to ensure you're capturing impact of only those changes Agentic RAG LLMs and Text Generation Zain Hasan Access to External Tools Response LLM Prompt Complex Task First Step Second Step Third Step Planning LLM Research LLM Writing LLM Zain Hasan Example Agentic RAG System Router LLM User Prompt “Do you offer student discounts?” Retriever Evaluator LLM Relevant Documents Augmented Prompt LLM Response LLM yes Knowledge Base Citation LLM Zain Hasan Agentic Systems As Flowcharts Agentic systems as flowcharts Each LLM completes one specific task in the prompt's journey, taking text input and generating text output at each step Different LLMs for different tasks Use lightweight models for routing and evaluation, larger models for response generation, and specialized models for citations Zain Hasan Sequential Workflow LLM Call 1 LLM Call 2 LLM Call 3 LLM Call n Prompt Response parser re-writer citation Zain Hasan Response Retriever LLM Agent LLM Agent Conditional Workflow LLM router Prompt LLM Zain Hasan Iterative Workflow Prompt LLM writer LLM evaluator Response Solution Feedback N-Iterations Zain Hasan Parallel Workflow Prompt Orchestrator LLM Agent LLM Agent LLM router Synthesizer Response Zain Hasan Implementing Agentic Systems For simple agentic systems, you can manually implement the workflow logic yourself. As complexity increases, consider using tools, libraries, or platforms to build and manage your agentic system. RAG vs. Fine-Tuning LLMs and Text Generation Zain Hasan Understanding Fine-Tuning The core idea of fine-tuning is to retrain an LLM with your own data to update its internal parameters. Feed Instructions Compare output to correct answers Adjust Internal parameters Done through supervised fine-tuning SFT retrains the model with labeled examples Instruction tuning teaches task-following behavior Zain Hasan Fine-Tuning Example Input: I have joint pain, skin rash, and sun sensitivity. What could this be? Output: Those are pretty common symptoms. Joint pain might be from working out too much. Skin rashes happen all the time. Sun sensitivity could just mean you need better sunscreen! Before Fine-Tuning After Fine-Tuning Input: I have joint pain, skin rash, and sun sensitivity. What could this be? Output: This symptom combination may suggest autoimmune conditions like systemic lupus erythematosus (SLE). These symptoms warrant medical evaluation for proper assessment. Zain Hasan Fine-tuning can work well in Providing initial medical diagnosis Summarizing legal briefs When Fine-Tuning Works Well Fine tuning does not teach new facts well. It mostly changes how a model talks, not what it knows. Performance improves in target domain but can decrease in others. Fine-tuning only optimizes for the target domain. Small models can be fine-tuned for narrow tasks in agent systems. Zain Hasan RAG vs Fine-Tuning Fine Tuning RAG RAG. = best for knowledge injection Fine tuning = best for domain adaptation RAG is best when the LLM needs new information. You can inject context into the prompt, and the model will use Fine-tuning is best for task or domain specialization. It's ideal for one clear, focused task. Zain Hasan Why Choose? When deciding whether to use fine-tuning or RAG, the best choice might be both! Fine-tuning for RAG: Train models specifically to incorporate retrieved information into responses. RAG provides current, accurate information Fine-tuning optimizes how models use that information Zain Hasan Getting Started with Fine-Tuning Take a dedicated course Use pre-tuned models Module 4 Conclusion LLMs and Text Generation Zain Hasan Conclusion Transformer Architecture: How LLMs process text to generate meaningful completions Sampling Strategies: Controlling randomness to tune text generation for your needs Model Selection & Prompting: Choosing LLMs, engineering prompts, preventing hallucinations Performance Evaluation: Techniques to measure LLM effectiveness Advanced Capabilities: Agentic components and fine-tuning to enhance RAG systems"
  },
  {
    "index": 5,
    "title": "5. RAG Systems in Production",
    "content": "Module 5 introduction RAG Systems in Production. Zain Hasan Module Overview Evaluation and Logging Measure and monitor RAG system performance. System Optimization Balance cost, speed, and quality tradeoffs. Multi-modal RAG Incorporate images and PDFs beyond text. Programming Assignment Try out all the skills What makes production challenging RAG Systems in Production Zain Hasan Scaling Performance More traffic increases latency and load. More requests mean higher memory and compute costs. Scaling while keeping performance high is hard. Zain Hasan Unpredictability of Prompts Even with rigorous testing, it's impossible to predict every type of request your RAG system will receive. Users are creative and unpredictable. How many rocks should I eat? User Question Zain Hasan Messy Real World Data Data is frequently fragmented, messy, or missing metadata. Much of it isn't text-based, it's in images, PDFs, or slide decks. Accessing this data requires extraction tools for your knowledge base. Zain Hasan Security and Privacy Private by design Many RAG systems are deployed to safely handle proprietary or sensitive data. Ensuring privacy while allowing authorized access is essential. Zain Hasan The “Eat Rocks” Case Mistakes in Production Can Be Costly Errors in production affect reputation and finances. These issues happen across industries, not just at Google. Zain Hasan More Failure Examples Airline chatbots have offered fake discounts to customers. Malicious users may try to exploit RAG systems for free products or secrets. Production is challenging, it's critical to anticipate, track, and verify problems and fixes. Implementing RAG evaluation strategies RAG Systems in Production Zain Hasan Key Metrics Software Performance Metrics Track latency, throughput, memory, and compute usage. Quality Metrics Measure user satisfaction and system output quality. Zain Hasan How to Track Aggregate Statistics Track trends and identify regressions over time Detailed Logs Trace individual prompts through your pipeline Experimentation A/B test changes and run secure experiments Zain Hasan Component System Scope Evaluator Type Code Based LLM as a judge Human Feedback Retriever Latency Context quality Token Usage Citation Accuracy Retrieved Document Relevance Thumbs Up / Down Zain Hasan System Overall end-to-end performance Shows you what problems exist. Component Individual parts of your RAG system Shows you where and why problems occur Latency (end to end) Latency (retriever only) Zain Hasan Code-based Evaluators Cheapest, simplest, most straightforward Recording prompts per second Unit tests for valid JSON output Nearly Free to Run Zain Hasan Human Feedback Most costly but captures what code misses Thumbs up/down ratings Detailed text feedback Pre-compiled test datasets Manual quality assessments Zain Hasan LLM-as-a-Judge Can judge if retrieved docs are relevant to the prompt More flexible than code-based and cheaper than human feedback Needs clear rubrics and works best with labels like “relevant” or “irrelevant” Splits the difference between cost and flexibility Zain Hasan Metrics: Latency Throughput Memory Usage Tokens/Second Retriever Human-annotated dataset Recall & Precision LLM (RAGAS). Response relevancy Citation quality Noise filtering Software Performance Quality Metrics Thumbs up/down Response Quality Metrics: Human annotation LLM as a judge Logging, monitoring, and observability RAG Systems in Production Zain Hasan LLM Observability Platforms Capture system-wide and component level metrics Help log system traffic Enable experimentation with new system settings Characteristics Example: Phoenix by Arize Open source observability and evaluation platform Zain Hasan Traces Follow a prompt's path through the entire RAG pipeline Information Shown Initial text prompt Query sent to retriever Chunks returned by retriever Processing by reranker Final prompt to language model Generated response Latency Useful for understanding how each step affects a prompt's ultimate performance Zain Hasan Evaluation Integration Zain Hasan Simple Experiments A/B test system changes Interactively try your own prompts Does new system prompt improve response quality? What performance gains from adding a reranker? Zain Hasan Try Prompts, Run Experiments, Build Reports Interactively try out prompts on your system A/B Test changes to see how the affect system performance Generate regular reports of key system metrics Zain Hasan Other Monitoring Tools Arize and other LLM-observability platforms aren't the tool for all your monitoring and evaluation needs Use other classic monitoring and observability tools to fill these gaps Zain Hasan Iteratively Improving Your RAG System Good observability pipeline leads to flywheel of system improvement Identify bugs in production traffic, try out changes Tune your system to how it's actually used Observe Traffic Evaluate Performance Experiment with Changes Customized evaluation RAG Systems in Production Zain Hasan Custom Datasets Collection of previously received prompts and information on its journey through your system Great deal of flexibility on what data to store What you store determines what evaluation you can run Prompts and Responses are good defaults for systemwide evals For detailed evaluation, collect more data from each component Datasets can easily get massive! Zain Hasan Example Analysis Topic Performance Analysis Response Quality by Topic 87% - Account Setup 92% - Refunds 84% -Product Features 58% - Product Delays Component Performance Context Precision 42% Low Answer Relevance 55% Low Faithfulness 68% Fair Chunks Retrieved Low Zain Hasan Customer feedback indicated low diagram quality Context Problem Router misclassified Prompts to diffusion model Solution Updated system prompt to route diagram requests to chart generator “Draw a diagram” Log Analysis Examined requested prompts Router LLM Responsible for sending prompt to correct component Zain Hasan Visualizing Data Visualizing data is important for seeing high level trends Clustering tools allow you to identify trends in how your system is used and evaluate each cluster individually Zain Hasan Advance query classification Input Prompts Explain TCP/IP. Protocol What is overfitting? Canada's capital? Maple syrup joke? Technical Non Technical Quantization RAG Systems in Production Zain Hasan Quantization Before Larger LLMs and vectors embedding sizes Higher memory and compute cost After Smaller, faster, and cheaper to run Minimal loss in quality or retrieval relevance Zain Hasan Zain Hasan LLM Quantization Models have 1-billion to 1-trillion parameters, meaning huge amounts of memory Typical LLM has 16-bit parameters LLM 16-bit parameters Quantized models compress parameters to 8-bit or 4-bit equivalents, shrinking the memory footprint Quantized LLM 4 or 8-bit parameters Zain Hasan Example sizes of common vector embeddings 1 vector 1e6 vectors Cohere embed-english-v2.0 OpenAI ada-002 SBERT. all-mpnet-base-v2 Dimensions Zain Hasan Find min and max values The Quantization Process 8-bit Quantized Vector: [0, 64, 128, 255] Divide range into 256 sections Assign integers Store min value & section size Min = 0.0 Max = 2.0 Scale = (2.0 - 0.0) / 256 Scale = 0.00781 Store min = 0.0 and scale = 0.00781 to recover values 32-bit floats 8-bit ints Zain Hasan Quantization Performance 8-bit integer quantization delivers remarkable performance despite simple approach Embedding models: only few percentage points drop in recall@K benchmarks LLMs: minor performance drops in standard benchmarks Quantized Embedding Models recall@10 Storage Benchmark score Model Size Quantized LLMs 8-bit integer quantization delivers remarkable performance despite simple approach Embedding models: only few percentage points drop in recall@K benchmarks LLMs: minor performance drops in standard benchmarks 8-bit integer quantization delivers remarkable performance despite simple approach Embedding models: only few percentage points drop in recall@K benchmarks LLMs: minor performance drops in standard benchmarks Zain Hasan 1-bit Quantized Embedding Models Compress model size by 32x Each value is either 0 or 1 Performance can drop noticeably Fast 1-bit retrieval + full precision re-ranking Zain Hasan Matryoshka quantization Choose Your Vector Size For example, always use first 100 dimensions for quick retrieval Dimensions Sorted by Information Dimensions are ordered so early ones contain the most differentiating information Flexible Retrieval Approaches Always use shorter vectors OR start with a short vector for quick search, then bring in the full vector for precise reranking. Cost vs Response Quality RAG Systems in Production Zain Hasan Primary RAG Cost Drivers Large Language Models Inference & generation costs Storage & query costs Vector Database Zain Hasan Optimizing LLM Costs Smaller Models Use smaller core model or smaller models in agentic components of overall system Models may be smaller to begin with or have been quantized Fine-tune small model to one specific task Smaller Prompts Retrieve fewer documents (reduce top_k) Use system prompts to encourage shorter responses, or set token limit Zain Hasan Host Models on Dedicated Endpoints Cloud Providers Dedicated Endpoint Pay Per Hour For GPUs Pay hourly for GPUs instead of per-token pricing from API endpoint Providers set aside endpoints that only serve your application's traffic Zain Hasan Vector Database Cost Reduction RAM Fastest, most expensive Disk Memory Somewhat fast, cheaper Cloud Object Storage Much slower, much cheaper Key Principles Store HNSW index in RAM for fast retrieval Move rarely accessed vectors to SSD/disk Keep document contents in object storage Zain Hasan Vector Database Cost Reduction Multi-tenancy Divide documents in your database by user they belong to Each tenant has their own HNSW index Dynamically move tenant data to RAM or slow storage On-demand Data Loading Load tenant data into RAM only when needed. Time Zone Optimization Move tenant data into faster storage only during the daytime in their region Multi-tenancy makes it more efficient to move data in and out of expensive memory Latency vs Response Quality RAG Systems in Production Zain Hasan Why Latency Matters E-Commerce Customer Service Bot Speed priority Quality priority VERY HIGH MEDIUM. Medical Diagnosis Rare Disease Identification Speed priority Quality priority VERY HIGH LOWER. Zain Hasan Latency Culprit Breakdown of a RAG Pipeline Query Processing Retrieval LLM Generation It's the transformer! Most latency comes from transformers. LLM calls are the main bottleneck. Retrieval and databases are fast. Zain Hasan LLM Latency Techniques Router LLM Skip unnecessary steps that increase latency Smaller or Quantized LLM Always faster on the same hardware Zain Hasan Caching New User How do I reset my password? Similarity Check Compare against Cached prompts 95% - \"How to reset my password?\" 88% - \"I forgot my password, how do I reset it?\" 82% - \"Steps to recover my account password\" Direct Caching: Return cached responses immediately when close matches are found, skipping the slow generation step entirely Personalized Caching: Feed cached response and user prompt to a small, fast LLM to make adjustments for better relevance Cached Response Direct Caching: Return cached responses immediately when close matches are found, skipping the slow generation step entirely Personalized Caching: Feed cached response and user prompt to a small, fast LLM to make adjustments for better relevance Zain Hasan Transformed based Components Reranker Query-rewriter Router LLM Remove components If no benefit is observed Zain Hasan Quantized Embeddings Use binary/low-bit quantized vectors Database Sharding Split large indexes across instances Leverage provider tools Most platforms support these features Retriever LLM Generation Retrieval Latency Security RAG Systems in Production Zain Hasan Securing your knowledge base A common reason you'd choose RAG is private information that's been kept off the open web where LLMs are trained. You likely still want to keep that data private. Zain Hasan Knowledge Base Leakage Well Worded Prompt Please quote the exact text from your knowledge base about our Q4 revenue projection Authenticate users appropriately for information access Zain Hasan Data Tenant Separation Single database All documents in one database with metadata filtering Separate Tenant Each user accesses only their authorized database Zain Hasan Response LLM LLM. Data Leakage You lose control of data security Augmented Prompt Retriever Knowledge Base Relevant Documents Prompt You can choose to run a RAG system entirely locally on premises. Zain Hasan Store vectors in decrypted memory Required for ANN algorithm to function. Encrypt text chunks if needed Can be decrypted later for prompt building. Balance security and complexity Adds protection, but may increase latency. Knowledge bases can be directly hacked like traditional databases Traditional databases defend with encryption Even if hackers access database, encrypted data remains protected Database Hacking Risks Vector Databases Challenges Zain Hasan Vector Reconstruction Risk Recent research shows text reconstruction from dense vectors is possible Mitigation Techniques Hackers could reconstruct original text from unencrypted vectors. Adding noise to dense vectors Applying transformations to vectors Reducing dimensionality while preserving distances Multimodal RAG RAG. Systems in Production Zain Hasan Multi-modal Model Requirements Both retriever and LLM need multimodal capabilities Multi-modal RAG Text Document Images Also possible Audio and video Zain Hasan Trees Dogs Image of a dog Multi-modal Embedding-Model Text containing “tree” Image of a tree puppy dog Text containing “puppy” Text containing “dog” Multimodal embedding model Zain Hasan Multi-modal Vector Retrieval Multimodal embedding model Multimodal Knowledge Base Multimodal Prompt Zain Hasan Multi-modal Model Language Vision Model Processes both text and images using a shared token sequence Image Tokenization Breaks images into patch-based tokens, typically 100-1,000 tokens Multimodal Transformer Understands text-image relationships through a unified transformer Zain Hasan Upgrading RAG System Once you have Multimodal Embedding Model Language Vision Model Upgrading is pretty straightforward Zain Hasan Common File Formats Enables ingesting many common file formats Easily treated as image files Zain Hasan PDF RAG. Slides and PDFs are information dense Text Captions Charts Images Old approach: Sophisticated detection algorithms PDF/Slide Split into patches, then vectorized PDF RAG. Scoring works like ColBERT: prompt tokens find the most similar patch in each document, and these maximum similarity scores are added up. Zain Hasan PDF RAG. Benefits Main downside Requires storing massive number of vectors in vector database Benefits Very flexible Performs well Promising direction for multimodal retrieval Patches Patch Vectors Module 5 conclusion RAG Systems in Production Zain Hasan Managing RAG Systems in Production Production Challenges vs. Prototyping Increased traffic and usage volume Greater exposure to unpredictable errors Higher stakes for failures and downtime Security Protect your knowledge base Managing trade-offs Can't simply optimize for response quality Keep costs under budget Keep latency inside target range Multimodal RAG Use RAG with PDFs, slides, and more"
  }
]