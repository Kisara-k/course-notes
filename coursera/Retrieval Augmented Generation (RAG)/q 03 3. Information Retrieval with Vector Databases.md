## 3. Information Retrieval with Vector Databases

## Questions

#### 1. What is the primary advantage of Approximate Nearest Neighbors (ANN) over K-Nearest Neighbors (KNN) in vector search?  
A) Guarantees finding the absolute closest vectors  
B) Significantly faster search time at scale  
C) Uses graph-based data structures for navigation  
D) Requires no pre-computation or indexing  


#### 2. Which of the following statements about Hierarchical Navigable Small World (HNSW) graphs are true?  
A) HNSW uses multiple layers with exponentially fewer vectors in higher layers  
B) The search starts at the bottom layer and moves upward  
C) HNSW reduces search complexity from linear to approximately logarithmic  
D) Each layer contains a complete proximity graph of all vectors  


#### 3. Why is chunking important when vectorizing large documents for retrieval?  
A) It compresses the entire document into a single vector for faster search  
B) It improves search relevance by preserving local context  
C) It reduces the number of vectors stored in the database  
D) It helps balance between too much and too little context in each chunk  


#### 4. Which chunking strategies help minimize cutting off important context at chunk boundaries?  
A) Fixed-size chunking without overlap  
B) Overlapping chunking with 10% overlap  
C) Recursive character splitting based on document structure  
D) Semantic chunking based on vector similarity  


#### 5. What are the main challenges of using KNN for vector search in large datasets?  
A) Linear growth in distance calculations with dataset size  
B) Inability to find any relevant documents  
C) High computational cost for billions of documents  
D) Requires complex graph structures for navigation  


#### 6. In query rewriting for information retrieval, which of the following techniques improve search effectiveness?  
A) Clarifying ambiguous phrases  
B) Adding synonyms to increase match chances  
C) Removing unnecessary information  
D) Randomly expanding the query with unrelated terms  


#### 7. Which of the following are true about vector databases compared to relational databases?  
A) Vector databases are optimized for high-dimensional vector storage  
B) Relational databases perform vector search efficiently using ANN  
C) Vector databases build HNSW indexes for fast approximate search  
D) Relational databases typically perform vector search as inefficient KNN  


#### 8. What is a key limitation of cross-encoders in large-scale information retrieval?  
A) They provide poor search quality compared to bi-encoders  
B) They cannot pre-compute document embeddings  
C) They scale well to billions of documents  
D) They are extremely slow for real-time search  


#### 9. How does ColBERT differ from bi-encoders and cross-encoders?  
A) It generates one vector per document like bi-encoders  
B) It generates vectors for each token in the query and document  
C) It balances speed and interaction richness between bi- and cross-encoders  
D) It requires less vector storage than bi-encoders  


#### 10. Which of the following are benefits of reranking in information retrieval?  
A) Improves the relevance of top search results  
B) Eliminates the need for initial fast retrieval  
C) Uses more computationally expensive models after filtering  
D) Always returns more documents than the initial retrieval  


#### 11. What is the main reason semantic chunking can be more effective than fixed-size chunking?  
A) It groups sentences based on similar meanings rather than arbitrary lengths  
B) It always produces smaller chunks than fixed-size chunking  
C) It requires no vector calculations  
D) It follows the author’s train of thought more closely  


#### 12. Which of the following statements about Hypothetical Document Embeddings (HyDE) are correct?  
A) HyDE generates a hypothetical document from the query to improve search  
B) It matches the query vector directly to document vectors  
C) It can improve retrieval quality but adds latency and cost  
D) It replaces the need for any other retrieval method  


#### 13. Why is overlapping chunking often preferred over non-overlapping chunking?  
A) It reduces the total number of chunks needed  
B) It ensures words at chunk edges have context in multiple chunks  
C) It eliminates the need for chunk metadata  
D) It increases search relevance by preserving context  


#### 14. Which of the following are true about the search process in Navigable Small World graphs?  
A) The algorithm always finds the absolute closest neighbors  
B) It traverses edges between neighboring nodes to approach the query vector  
C) It picks the best path globally at each step  
D) It may not find the optimal overall path but performs well in practice  


#### 15. What are the trade-offs when choosing between bi-encoders and cross-encoders?  
A) Bi-encoders are faster but less accurate  
B) Cross-encoders are slower but provide better relevance scores  
C) Bi-encoders require concatenating query and document for scoring  
D) Cross-encoders can pre-compute document embeddings  


#### 16. In the context of vector search, what does the term "proximity graph" refer to?  
A) A graph connecting documents based on keyword similarity  
B) A graph where nodes represent vectors connected to their nearest neighbors  
C) A hierarchical structure used only in relational databases  
D) A graph used to store metadata filters  


#### 17. Which of the following are typical operations when working with a vector database?  
A) Creating collections and specifying vectorizers  
B) Batch adding documents and creating HNSW indexes  
C) Running keyword-only searches without vectors  
D) Performing vector searches with “near_text” queries  


#### 18. What is a key disadvantage of semantic chunking compared to fixed-size chunking?  
A) It is computationally expensive due to repeated vector calculations  
B) It always produces less relevant chunks  
C) It ignores document structure and meaning  
D) It cannot be combined with LLM-based chunking  


#### 19. How does context-aware chunking improve retrieval quality?  
A) By adding additional context metadata to chunks  
B) By ignoring the original document structure  
C) By reducing the number of chunks to speed up search  
D) By precomputing all possible queries  


#### 20. Which of the following statements about hybrid search in vector databases are true?  
A) It combines vector and keyword search results with weighted importance  
B) It only uses keyword search and ignores vector similarity  
C) It can improve search relevance by leveraging both semantic and lexical signals  
D) It requires separate databases for vector and keyword data



<br>

## Answers

#### 1. What is the primary advantage of Approximate Nearest Neighbors (ANN) over K-Nearest Neighbors (KNN) in vector search?  
A) ✗ Guarantees finding the absolute closest vectors — ANN trades accuracy for speed, so no guarantee.  
B) ✓ Significantly faster search time at scale — ANN is designed to speed up search dramatically.  
C) ✓ Uses graph-based data structures for navigation — Many ANN methods use graphs like HNSW.  
D) ✗ Requires no pre-computation or indexing — ANN requires building indexes like proximity graphs.  

**Correct:** B, C


#### 2. Which of the following statements about Hierarchical Navigable Small World (HNSW) graphs are true?  
A) ✓ HNSW uses multiple layers with exponentially fewer vectors in higher layers — This is core to HNSW’s speed.  
B) ✗ The search starts at the bottom layer and moves upward — Search starts at the top (smallest) layer and moves down.  
C) ✓ HNSW reduces search complexity from linear to approximately logarithmic — Hierarchy enables logarithmic search time.  
D) ✗ Each layer contains a complete proximity graph of all vectors — Only the bottom layer has all vectors; upper layers have subsets.  

**Correct:** A, C


#### 3. Why is chunking important when vectorizing large documents for retrieval?  
A) ✗ It compresses the entire document into a single vector for faster search — This is what chunking tries to avoid.  
B) ✓ It improves search relevance by preserving local context — Smaller chunks keep specific topics distinct.  
C) ✗ It reduces the number of vectors stored in the database — Chunking increases the number of vectors.  
D) ✓ It helps balance between too much and too little context in each chunk — Optimal chunk size balances detail and context.  

**Correct:** B, D


#### 4. Which chunking strategies help minimize cutting off important context at chunk boundaries?  
A) ✗ Fixed-size chunking without overlap — No overlap causes loss of context at edges.  
B) ✓ Overlapping chunking with 10% overlap — Overlap preserves context across chunk boundaries.  
C) ✓ Recursive character splitting based on document structure — Respects natural breaks, reducing context loss.  
D) ✓ Semantic chunking based on vector similarity — Groups related sentences, preserving meaning.  

**Correct:** B, C, D


#### 5. What are the main challenges of using KNN for vector search in large datasets?  
A) ✓ Linear growth in distance calculations with dataset size — KNN requires comparing to all vectors.  
B) ✗ Inability to find any relevant documents — KNN finds closest neighbors but is slow.  
C) ✓ High computational cost for billions of documents — Linear scaling makes it impractical at scale.  
D) ✗ Requires complex graph structures for navigation — KNN is brute force, no graphs needed.  

**Correct:** A, C


#### 6. In query rewriting for information retrieval, which of the following techniques improve search effectiveness?  
A) ✓ Clarifying ambiguous phrases — Makes queries more precise.  
B) ✓ Adding synonyms to increase match chances — Broadens search scope.  
C) ✓ Removing unnecessary information — Focuses on relevant terms.  
D) ✗ Randomly expanding the query with unrelated terms — Adds noise, reduces precision.  

**Correct:** A, B, C


#### 7. Which of the following are true about vector databases compared to relational databases?  
A) ✓ Vector databases are optimized for high-dimensional vector storage — Designed specifically for vectors.  
B) ✗ Relational databases perform vector search efficiently using ANN — They usually do brute-force KNN, inefficient.  
C) ✓ Vector databases build HNSW indexes for fast approximate search — Common ANN indexing method.  
D) ✓ Relational databases typically perform vector search as inefficient KNN — They lack specialized indexing.  

**Correct:** A, C, D


#### 8. What is a key limitation of cross-encoders in large-scale information retrieval?  
A) ✗ They provide poor search quality compared to bi-encoders — Cross-encoders provide better quality.  
B) ✓ They cannot pre-compute document embeddings — Must process query-document pairs together.  
C) ✗ They scale well to billions of documents — They scale poorly due to computation cost.  
D) ✓ They are extremely slow for real-time search — Computationally expensive for large corpora.  

**Correct:** B, D


#### 9. How does ColBERT differ from bi-encoders and cross-encoders?  
A) ✗ It generates one vector per document like bi-encoders — ColBERT generates vectors per token.  
B) ✓ It generates vectors for each token in the query and document — Token-level embeddings are core to ColBERT.  
C) ✓ It balances speed and interaction richness between bi- and cross-encoders — Combines benefits of both.  
D) ✗ It requires less vector storage than bi-encoders — Requires more storage due to token vectors.  

**Correct:** B, C


#### 10. Which of the following are benefits of reranking in information retrieval?  
A) ✓ Improves the relevance of top search results — Reranking refines initial results.  
B) ✗ Eliminates the need for initial fast retrieval — Initial retrieval is still needed for efficiency.  
C) ✓ Uses more computationally expensive models after filtering — Applies slower models on fewer candidates.  
D) ✗ Always returns more documents than the initial retrieval — Usually returns fewer, more relevant documents.  

**Correct:** A, C


#### 11. What is the main reason semantic chunking can be more effective than fixed-size chunking?  
A) ✓ It groups sentences based on similar meanings rather than arbitrary lengths — Preserves semantic coherence.  
B) ✗ It always produces smaller chunks than fixed-size chunking — Chunk size varies, not always smaller.  
C) ✗ It requires no vector calculations — Semantic chunking depends on vector similarity.  
D) ✓ It follows the author’s train of thought more closely — Groups related concepts together.  

**Correct:** A, D


#### 12. Which of the following statements about Hypothetical Document Embeddings (HyDE) are correct?  
A) ✓ HyDE generates a hypothetical document from the query to improve search — Creates an idealized target.  
B) ✗ It matches the query vector directly to document vectors — Matches hypothetical doc embeddings to docs.  
C) ✓ It can improve retrieval quality but adds latency and cost — Extra generation step adds overhead.  
D) ✗ It replaces the need for any other retrieval method — Used as an enhancement, not a replacement.  

**Correct:** A, C


#### 13. Why is overlapping chunking often preferred over non-overlapping chunking?  
A) ✗ It reduces the total number of chunks needed — Overlapping increases chunk count.  
B) ✓ It ensures words at chunk edges have context in multiple chunks — Preserves context at boundaries.  
C) ✗ It eliminates the need for chunk metadata — Metadata is still needed for tracking.  
D) ✓ It increases search relevance by preserving context — Better context improves retrieval.  

**Correct:** B, D


#### 14. Which of the following are true about the search process in Navigable Small World graphs?  
A) ✗ The algorithm always finds the absolute closest neighbors — It finds close but not guaranteed closest.  
B) ✓ It traverses edges between neighboring nodes to approach the query vector — Core graph traversal method.  
C) ✗ It picks the best path globally at each step — Only chooses best local step, not global path.  
D) ✓ It may not find the optimal overall path but performs well in practice — Approximate but effective.  

**Correct:** B, D


#### 15. What are the trade-offs when choosing between bi-encoders and cross-encoders?  
A) ✓ Bi-encoders are faster but less accurate — Speed vs. quality trade-off.  
B) ✓ Cross-encoders are slower but provide better relevance scores — More computation, better results.  
C) ✗ Bi-encoders require concatenating query and document for scoring — They embed separately.  
D) ✗ Cross-encoders can pre-compute document embeddings — They must process pairs together.  

**Correct:** A, B


#### 16. In the context of vector search, what does the term "proximity graph" refer to?  
A) ✗ A graph connecting documents based on keyword similarity — Proximity graphs connect vectors, not keywords.  
B) ✓ A graph where nodes represent vectors connected to their nearest neighbors — Defines neighborhood for ANN search.  
C) ✗ A hierarchical structure used only in relational databases — Used in vector databases, not relational.  
D) ✗ A graph used to store metadata filters — Metadata filters are separate from proximity graphs.  

**Correct:** B


#### 17. Which of the following are typical operations when working with a vector database?  
A) ✓ Creating collections and specifying vectorizers — Setup steps for organizing data.  
B) ✓ Batch adding documents and creating HNSW indexes — Common ingestion and indexing tasks.  
C) ✗ Running keyword-only searches without vectors — Vector DBs focus on vector search, though some support hybrid.  
D) ✓ Performing vector searches with “near_text” queries — Typical vector similarity search method.  

**Correct:** A, B, D


#### 18. What is a key disadvantage of semantic chunking compared to fixed-size chunking?  
A) ✓ It is computationally expensive due to repeated vector calculations — Requires many similarity checks.  
B) ✗ It always produces less relevant chunks — Usually more relevant due to semantic grouping.  
C) ✗ It ignores document structure and meaning — It explicitly uses meaning for chunking.  
D) ✗ It cannot be combined with LLM-based chunking — Can be combined for better results.  

**Correct:** A


#### 19. How does context-aware chunking improve retrieval quality?  
A) ✓ By adding additional context metadata to chunks — Helps LLMs generate better responses.  
B) ✗ By ignoring the original document structure — It respects and enhances structure.  
C) ✗ By reducing the number of chunks to speed up search — Usually adds processing, not reduces chunks.  
D) ✗ By precomputing all possible queries — Does not precompute queries, adds context to chunks.  

**Correct:** A


#### 20. Which of the following statements about hybrid search in vector databases are true?  
A) ✓ It combines vector and keyword search results with weighted importance — Hybrid search blends both.  
B) ✗ It only uses keyword search and ignores vector similarity — Vector similarity is integral.  
C) ✓ It can improve search relevance by leveraging both semantic and lexical signals — Combines strengths of both.  
D) ✗ It requires separate databases for vector and keyword data — Usually integrated in one system.  

**Correct:** A, C