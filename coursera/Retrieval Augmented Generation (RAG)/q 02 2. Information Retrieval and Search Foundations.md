## 2. Information Retrieval and Search Foundations

## Questions

#### 1. Which of the following best describe the primary difference between keyword search and semantic search?  
A) Keyword search matches exact words, semantic search matches meaning.  
B) Keyword search uses vector embeddings, semantic search uses bag-of-words.  
C) Semantic search can find synonyms, keyword search cannot.  
D) Keyword search is computationally more expensive than semantic search.  

#### 2. Metadata filtering in information retrieval is primarily used to:  
A) Rank documents by relevance to the query.  
B) Narrow down search results based on fixed document attributes.  
C) Perform semantic similarity matching.  
D) Exclude documents that do not meet specific criteria like author or date.  

#### 3. Which of the following statements about TF-IDF are true?  
A) TF-IDF rewards words that appear frequently in a document but rarely across the corpus.  
B) TF-IDF ignores document length, treating all documents equally.  
C) Common words like “the” have low IDF scores.  
D) TF-IDF is a scoring method used in semantic search.  

#### 4. BM25 improves upon TF-IDF by:  
A) Introducing term frequency saturation to avoid linear scaling with word count.  
B) Ignoring document length normalization.  
C) Penalizing longer documents to reduce bias.  
D) Using metadata filtering to improve ranking.  

#### 5. Which of the following are limitations of metadata filtering?  
A) It cannot rank documents by relevance.  
B) It can exclude relevant documents if filters are too strict.  
C) It performs retrieval based on document content.  
D) It is slow and computationally expensive.  

#### 6. In the bag-of-words model, which of the following is true?  
A) Word order is preserved to capture phrase meaning.  
B) Only word presence and frequency matter, not order.  
C) The vector representation is typically dense with many non-zero entries.  
D) The vector is usually sparse with many zero entries.  

#### 7. Which of the following are true about embedding models used in semantic search?  
A) They map words or documents to points in a high-dimensional vector space.  
B) The axes of the vector space have clear, interpretable meanings.  
C) Similar words cluster close together in the vector space.  
D) Embeddings are static and do not change during training.  

#### 8. Contrastive learning in embedding model training involves:  
A) Pulling positive pairs closer in vector space.  
B) Pushing negative pairs farther apart.  
C) Randomly assigning vectors to words without supervision.  
D) Iteratively updating model parameters based on pairwise similarity scores.  

#### 9. Which distance or similarity measures are commonly used to compare vectors in semantic search?  
A) Euclidean distance measures the straight-line distance between vectors.  
B) Cosine similarity measures the angle between vectors, ignoring magnitude.  
C) Dot product measures the length of the projection of one vector onto another.  
D) Jaccard similarity is the standard for vector comparison in semantic search.  

#### 10. Reciprocal Rank Fusion (RRF) is used to:  
A) Combine ranked lists from keyword and semantic search into a single ranking.  
B) Assign scores based on the sum of raw relevance scores from each list.  
C) Reward documents that rank highly in multiple lists.  
D) Penalize documents that appear only in one list.  

#### 11. Which of the following statements about hybrid search are correct?  
A) It combines keyword search, semantic search, and metadata filtering.  
B) It always returns more documents than either keyword or semantic search alone.  
C) It allows tuning the relative importance of keyword vs semantic ranking.  
D) It eliminates the need for metadata filtering.  

#### 12. When evaluating a retriever, which of the following metrics focus on the quality of the top-ranked documents?  
A) Precision@K  
B) Recall@K  
C) Mean Reciprocal Rank (MRR)  
D) Total number of documents retrieved  

#### 13. Which of the following are true about precision and recall?  
A) Precision penalizes returning irrelevant documents.  
B) Recall penalizes missing relevant documents.  
C) High precision always implies high recall.  
D) Precision and recall are independent and can vary inversely.  

#### 14. Mean Average Precision (MAP) differs from simple precision because it:  
A) Rewards ranking relevant documents higher in the list.  
B) Only considers the first relevant document retrieved.  
C) Averages precision scores at the ranks of all relevant documents.  
D) Ignores irrelevant documents in the calculation.  

#### 15. Which of the following are challenges or limitations of keyword search?  
A) It cannot handle synonyms or related meanings.  
B) It requires exact word matches to retrieve documents.  
C) It is computationally more expensive than semantic search.  
D) It ignores word order and context.  

#### 16. Which of the following best describe the role of document length normalization in BM25?  
A) It prevents longer documents from unfairly scoring higher just because they contain more words.  
B) It increases the score of longer documents to favor detailed content.  
C) It adjusts scores based on the average document length in the corpus.  
D) It is controlled by a tunable parameter that can be set between 0 and 1.  

#### 17. Which of the following statements about embedding vectors before and after training are true?  
A) Before training, vector locations are random and meaningless.  
B) After training, vectors cluster so that similar meanings are close together.  
C) Embeddings from different models can be directly compared.  
D) Training uses millions of positive and negative pairs to refine vector positions.  

#### 18. Which of the following are true about metadata filtering’s relationship to retrieval?  
A) Metadata filtering performs retrieval based on query content.  
B) Metadata filtering is applied after keyword or semantic search to narrow results.  
C) It is a rigid filter that excludes documents not matching exact metadata criteria.  
D) It can rank documents by relevance within the filtered set.  

#### 19. Which of the following are advantages of BM25 over TF-IDF?  
A) BM25 includes term frequency saturation to avoid linear scaling.  
B) BM25 incorporates document length normalization with tunable parameters.  
C) BM25 completely ignores inverse document frequency.  
D) BM25 generally performs better in production retrieval systems.  

#### 20. In semantic search, why is cosine similarity often preferred over Euclidean distance?  
A) Cosine similarity measures the angle between vectors, focusing on direction rather than magnitude.  
B) Euclidean distance can be biased by vector length differences.  
C) Cosine similarity is computationally more expensive than Euclidean distance.  
D) Cosine similarity can better capture semantic similarity when vector magnitudes vary.



<br>

## Answers

#### 1. Which of the following best describe the primary difference between keyword search and semantic search?  
A) ✓ Keyword search matches exact words, semantic search matches meaning.  
B) ✗ Keyword search uses vector embeddings, semantic search uses bag-of-words. (Opposite is true.)  
C) ✓ Semantic search can find synonyms, keyword search cannot.  
D) ✗ Keyword search is computationally more expensive than semantic search. (Semantic search is more expensive.)  

**Correct:** A, C


#### 2. Metadata filtering in information retrieval is primarily used to:  
A) ✗ Rank documents by relevance to the query. (Filtering narrows, does not rank.)  
B) ✓ Narrow down search results based on fixed document attributes.  
C) ✗ Perform semantic similarity matching. (Filtering is not content-based.)  
D) ✓ Exclude documents that do not meet specific criteria like author or date.  

**Correct:** B, D


#### 3. Which of the following statements about TF-IDF are true?  
A) ✓ TF-IDF rewards words that appear frequently in a document but rarely across the corpus.  
B) ✗ TF-IDF ignores document length, treating all documents equally. (Length normalization is needed.)  
C) ✓ Common words like “the” have low IDF scores.  
D) ✗ TF-IDF is a scoring method used in semantic search. (It’s used in keyword search.)  

**Correct:** A, C


#### 4. BM25 improves upon TF-IDF by:  
A) ✓ Introducing term frequency saturation to avoid linear scaling with word count.  
B) ✗ Ignoring document length normalization. (BM25 includes length normalization.)  
C) ✓ Penalizing longer documents to reduce bias.  
D) ✗ Using metadata filtering to improve ranking. (Filtering is separate.)  

**Correct:** A, C


#### 5. Which of the following are limitations of metadata filtering?  
A) ✓ It cannot rank documents by relevance.  
B) ✓ It can exclude relevant documents if filters are too strict.  
C) ✗ It performs retrieval based on document content. (It does not analyze content.)  
D) ✗ It is slow and computationally expensive. (It is fast and simple.)  

**Correct:** A, B


#### 6. In the bag-of-words model, which of the following is true?  
A) ✗ Word order is preserved to capture phrase meaning. (Order is ignored.)  
B) ✓ Only word presence and frequency matter, not order.  
C) ✗ The vector representation is typically dense with many non-zero entries. (Usually sparse.)  
D) ✓ The vector is usually sparse with many zero entries.  

**Correct:** B, D


#### 7. Which of the following are true about embedding models used in semantic search?  
A) ✓ They map words or documents to points in a high-dimensional vector space.  
B) ✗ The axes of the vector space have clear, interpretable meanings. (Axes are abstract.)  
C) ✓ Similar words cluster close together in the vector space.  
D) ✗ Embeddings are static and do not change during training. (They are learned and updated.)  

**Correct:** A, C


#### 8. Contrastive learning in embedding model training involves:  
A) ✓ Pulling positive pairs closer in vector space.  
B) ✓ Pushing negative pairs farther apart.  
C) ✗ Randomly assigning vectors to words without supervision. (Training is supervised.)  
D) ✓ Iteratively updating model parameters based on pairwise similarity scores.  

**Correct:** A, B, D


#### 9. Which distance or similarity measures are commonly used to compare vectors in semantic search?  
A) ✓ Euclidean distance measures the straight-line distance between vectors.  
B) ✓ Cosine similarity measures the angle between vectors, ignoring magnitude.  
C) ✓ Dot product measures the length of the projection of one vector onto another.  
D) ✗ Jaccard similarity is the standard for vector comparison in semantic search. (Used for sets, not vectors.)  

**Correct:** A, B, C


#### 10. Reciprocal Rank Fusion (RRF) is used to:  
A) ✓ Combine ranked lists from keyword and semantic search into a single ranking.  
B) ✗ Assign scores based on the sum of raw relevance scores from each list. (RRF uses ranks, not raw scores.)  
C) ✓ Reward documents that rank highly in multiple lists.  
D) ✗ Penalize documents that appear only in one list. (No explicit penalty.)  

**Correct:** A, C


#### 11. Which of the following statements about hybrid search are correct?  
A) ✓ It combines keyword search, semantic search, and metadata filtering.  
B) ✗ It always returns more documents than either keyword or semantic search alone. (It returns a combined ranked list, not necessarily more.)  
C) ✓ It allows tuning the relative importance of keyword vs semantic ranking.  
D) ✗ It eliminates the need for metadata filtering. (Filtering is still useful.)  

**Correct:** A, C


#### 12. When evaluating a retriever, which of the following metrics focus on the quality of the top-ranked documents?  
A) ✓ Precision@K  
B) ✓ Recall@K  
C) ✓ Mean Reciprocal Rank (MRR)  
D) ✗ Total number of documents retrieved (Does not measure quality.)  

**Correct:** A, B, C


#### 13. Which of the following are true about precision and recall?  
A) ✓ Precision penalizes returning irrelevant documents.  
B) ✓ Recall penalizes missing relevant documents.  
C) ✗ High precision always implies high recall. (They can vary inversely.)  
D) ✓ Precision and recall are independent and can vary inversely.  

**Correct:** A, B, D


#### 14. Mean Average Precision (MAP) differs from simple precision because it:  
A) ✓ Rewards ranking relevant documents higher in the list.  
B) ✗ Only considers the first relevant document retrieved. (Considers all relevant docs.)  
C) ✓ Averages precision scores at the ranks of all relevant documents.  
D) ✗ Ignores irrelevant documents in the calculation. (Irrelevant docs affect precision.)  

**Correct:** A, C


#### 15. Which of the following are challenges or limitations of keyword search?  
A) ✓ It cannot handle synonyms or related meanings.  
B) ✓ It requires exact word matches to retrieve documents.  
C) ✗ It is computationally more expensive than semantic search. (It is cheaper.)  
D) ✓ It ignores word order and context.  

**Correct:** A, B, D


#### 16. Which of the following best describe the role of document length normalization in BM25?  
A) ✓ It prevents longer documents from unfairly scoring higher just because they contain more words.  
B) ✗ It increases the score of longer documents to favor detailed content. (It penalizes longer docs.)  
C) ✓ It adjusts scores based on the average document length in the corpus.  
D) ✓ It is controlled by a tunable parameter that can be set between 0 and 1.  

**Correct:** A, C, D


#### 17. Which of the following statements about embedding vectors before and after training are true?  
A) ✓ Before training, vector locations are random and meaningless.  
B) ✓ After training, vectors cluster so that similar meanings are close together.  
C) ✗ Embeddings from different models can be directly compared. (Only embeddings from the same model are comparable.)  
D) ✓ Training uses millions of positive and negative pairs to refine vector positions.  

**Correct:** A, B, D


#### 18. Which of the following are true about metadata filtering’s relationship to retrieval?  
A) ✗ Metadata filtering performs retrieval based on query content. (It does not analyze content.)  
B) ✓ Metadata filtering is applied after keyword or semantic search to narrow results.  
C) ✓ It is a rigid filter that excludes documents not matching exact metadata criteria.  
D) ✗ It can rank documents by relevance within the filtered set. (Filtering only excludes, does not rank.)  

**Correct:** B, C


#### 19. Which of the following are advantages of BM25 over TF-IDF?  
A) ✓ BM25 includes term frequency saturation to avoid linear scaling.  
B) ✓ BM25 incorporates document length normalization with tunable parameters.  
C) ✗ BM25 completely ignores inverse document frequency. (It still uses IDF.)  
D) ✓ BM25 generally performs better in production retrieval systems.  

**Correct:** A, B, D


#### 20. In semantic search, why is cosine similarity often preferred over Euclidean distance?  
A) ✓ Cosine similarity measures the angle between vectors, focusing on direction rather than magnitude.  
B) ✓ Euclidean distance can be biased by vector length differences.  
C) ✗ Cosine similarity is computationally more expensive than Euclidean distance. (It is usually cheaper or comparable.)  
D) ✓ Cosine similarity can better capture semantic similarity when vector magnitudes vary.  

**Correct:** A, B, D