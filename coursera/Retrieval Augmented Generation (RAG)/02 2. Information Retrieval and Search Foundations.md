## 2. Information Retrieval and Search Foundations

[Study Notes](#study-notes)

[Questions](#questions)



### Key Points

#### 1. üîç Search Approaches  
- Keyword search looks for documents containing the exact words in the prompt.  
- Semantic search looks for documents with similar meaning to the prompt, even without matching words.  
- High-performing retrievers balance keyword search, semantic search, and metadata filtering based on project needs.

#### 2. üóÇÔ∏è Metadata Filtering  
- Metadata filtering narrows down documents based on rigid criteria like title, author, publication date, and access privileges.  
- Metadata filtering does not perform retrieval based on content but excludes or includes documents based on attributes.  
- Metadata filtering is fast, simple, reliable, but cannot rank or find documents by relevance.

#### 3. üìù Keyword Search - TF-IDF  
- TF (Term Frequency) counts how often a word appears in a document.  
- IDF (Inverse Document Frequency) weights words inversely proportional to their frequency across all documents, rewarding rare words.  
- TF-IDF score = TF(word, doc) √ó log(Total docs / Docs containing word).  
- Documents with rare keywords score higher than those with common words.

#### 4. ‚öñÔ∏è BM25 Scoring  
- BM25 is a refined keyword search scoring algorithm that improves on TF-IDF.  
- BM25 includes term frequency saturation, where increasing word count has diminishing returns.  
- BM25 normalizes scores by document length to penalize longer documents.  
- BM25 has tunable parameters: k‚ÇÅ (term frequency saturation) and b (length normalization).  
- BM25 is the standard keyword search algorithm in production systems.

#### 5. ü§ñ Semantic Search and Embeddings  
- Semantic search uses embedding models to convert text into vectors in high-dimensional space.  
- Similar meanings cluster close together in vector space, regardless of exact words.  
- Vector similarity is measured by cosine similarity, Euclidean distance, or dot product.  
- Embedding models are trained using contrastive learning with positive (similar) and negative (dissimilar) text pairs.  
- After training, embedding vectors meaningfully cluster similar texts together.

#### 6. üîÄ Hybrid Search  
- Hybrid search combines keyword search, semantic search, and metadata filtering.  
- Each search method returns a ranked list of documents, which are combined using Reciprocal Rank Fusion (RRF).  
- RRF scores documents based on their rank positions across multiple lists, rewarding documents ranked highly in multiple searches.  
- Hybrid search allows tuning the weight between keyword and semantic search results.

#### 7. üìä Retrieval Evaluation Metrics  
- Precision = Relevant Retrieved / Total Retrieved; measures accuracy of returned documents.  
- Recall = Relevant Retrieved / Total Relevant; measures coverage of relevant documents.  
- Precision@K and Recall@K measure precision and recall within the top K retrieved documents.  
- Mean Average Precision (MAP) averages precision scores at ranks of relevant documents, rewarding higher ranking of relevant docs.  
- Reciprocal Rank = 1 / rank of first relevant document; Mean Reciprocal Rank (MRR) averages this over many queries.  
- Recall and MRR are most cited metrics for retriever performance evaluation.



<br>

## Study Notes

### 1. üìö Introduction to Information Retrieval and Search

Information retrieval (IR) is all about finding relevant information quickly and accurately from a large collection of documents. Imagine you have a huge library with documents in many formats‚Äîarticles, reports, emails, web pages‚Äîand you want to find the best matches to your question or prompt. This is challenging because documents are designed for humans to read, not machines, and the queries can be unstructured or conversational.

The goal of IR systems is to **retrieve relevant documents rapidly despite the messiness and variety of data**. This module covers the foundational techniques used in retrieval, their theoretical strengths and weaknesses, how they can be combined, and how to evaluate their effectiveness.


### 2. üîç Two Main Search Approaches: Keyword Search vs Semantic Search

#### Keyword Search

Keyword search looks for documents that contain the **exact words** found in the user‚Äôs prompt. It‚Äôs like a direct word match: if your query contains the word ‚Äúpizza,‚Äù the system looks for documents that also contain ‚Äúpizza.‚Äù This method is simple, fast, and guarantees that the retrieved documents mention the exact terms you used.

#### Semantic Search

Semantic search goes beyond exact words. It tries to find documents that have **similar meaning** to the prompt, even if they don‚Äôt share the same words. For example, if you search for ‚Äúhappy,‚Äù semantic search might also find documents containing ‚Äúglad‚Äù or ‚Äújoyful.‚Äù This approach uses advanced models to understand the meaning behind words and phrases.

#### Summary

- **Keyword Search**: Matches exact words, fast, simple, but limited to literal matches.
- **Semantic Search**: Matches meaning, flexible, handles synonyms and related concepts, but computationally more expensive.


### 3. üß© Metadata Filtering: Narrowing Down Results by Document Attributes

Metadata filtering is a way to **exclude or include documents based on fixed attributes** like author, publication date, region, or subscription status. For example, you might want only articles published on a certain date or only those available to free subscribers.

- Metadata filters do **not** perform retrieval based on content.
- They act as a **rigid yes/no filter** applied after keyword or semantic search.
- This makes filtering **fast, reliable, and easy to debug**.
- However, metadata filtering **cannot rank or find documents by relevance**; it only narrows down the pool.

Example filters:
- Only articles from June to July 2024.
- Exclude all paid subscription articles.
- Include only articles from North America.


### 4. üìù Keyword Search Deep Dive: TF-IDF and BM25

#### Bag of Words Model

Keyword search often uses a **bag of words** approach, where the order of words is ignored, and only the presence and frequency of words matter. For example, the phrase ‚Äúmaking pizza without a pizza oven‚Äù is treated as a collection of words: making, pizza, without, oven, etc.

#### Term Frequency (TF)

Term frequency counts how many times a word appears in a document. More occurrences usually mean the document is more relevant to that word.

#### Inverse Document Frequency (IDF)

IDF adjusts for how common a word is across all documents. Common words like ‚Äúthe‚Äù or ‚Äúand‚Äù appear everywhere and are less useful for distinguishing documents. Rare words get higher IDF scores because they are more informative.

#### TF-IDF Score

The TF-IDF score combines term frequency and inverse document frequency:

\[
\text{Score} = \text{TF(word, doc)} \times \log\left(\frac{\text{Total documents}}{\text{Documents containing word}}\right)
\]

This means documents with rare but frequent keywords score higher.

#### BM25: The Modern Standard

BM25 is an improved keyword scoring algorithm widely used in production systems. It refines TF-IDF by:

- **Term Frequency Saturation**: The score increases with term frequency but with diminishing returns (e.g., doubling the word count doesn‚Äôt double the score).
- **Document Length Normalization**: Longer documents naturally have more words, so BM25 penalizes longer documents to avoid bias.

BM25 has tunable parameters:
- **k‚ÇÅ** controls how much term frequency affects the score.
- **b** controls how much document length normalization is applied.

BM25 is more flexible and performs better than classic TF-IDF.


### 5. ü§ñ Semantic Search and Embedding Models

#### Why Semantic Search?

Keyword search can‚Äôt handle synonyms or different word forms well. For example, ‚Äúhappy‚Äù and ‚Äúglad‚Äù mean similar things but are different words. Semantic search solves this by representing text as **vectors** in a high-dimensional space, where similar meanings are close together.

#### Embedding Models

An embedding model converts words, sentences, or documents into vectors‚Äîlists of numbers that represent their meaning. These vectors live in a multi-dimensional space (often hundreds or thousands of dimensions).

- Words with similar meanings cluster together.
- The exact axes of this space don‚Äôt have simple interpretations; it‚Äôs the relative distances that matter.

#### Measuring Similarity

To find how close two vectors are, we use:

- **Euclidean Distance**: The straight-line distance between two points.
- **Cosine Similarity**: Measures how similar the direction of two vectors is, ignoring their length. Values range from -1 (opposite) to 1 (same direction).
- **Dot Product**: Related to cosine similarity, measures projection length.

Semantic search ranks documents by how close their vectors are to the query vector.

#### Training Embedding Models: Contrastive Learning

Embedding models are trained using **positive and negative pairs**:

- Positive pairs: Texts with similar meaning (e.g., ‚ÄúHello‚Äù and ‚ÄúGood morning‚Äù).
- Negative pairs: Texts with different meaning (e.g., ‚ÄúGood morning‚Äù and ‚ÄúNoisy trombone‚Äù).

The model learns to **pull positive pairs closer** and **push negative pairs apart** in vector space. This training is repeated many times with millions of pairs, gradually improving the model‚Äôs ability to cluster similar meanings.


### 6. üîÄ Hybrid Search: Combining Keyword, Semantic, and Metadata Filtering

No single search method is perfect. Hybrid search combines:

- **Keyword search** for exact matches.
- **Semantic search** for meaning-based matches.
- **Metadata filtering** to narrow down results by fixed criteria.

Each method returns a list of documents (usually 20-50). These lists are combined and re-ranked using techniques like **Reciprocal Rank Fusion (RRF)**, which rewards documents that rank highly in multiple lists.

#### Reciprocal Rank Fusion (RRF)

RRF assigns points based on the rank position of a document in each list:

\[
\text{Score} = \sum \frac{1}{k + \text{rank}}
\]

where \(k\) is a tuning parameter. This method balances the influence of keyword and semantic rankings.

Hybrid search allows tuning the weight between keyword and semantic search depending on the project needs.


### 7. üìä Evaluating Retrieval Quality: Metrics and Their Meaning

To know if a retriever works well, we need to measure how good its results are. This requires:

- A **prompt** (query).
- A **ranked list** of returned documents.
- A **ground truth** set of documents labeled relevant or irrelevant.

#### Precision and Recall

- **Precision**: Of the documents retrieved, how many are relevant?

\[
\text{Precision} = \frac{\text{Relevant Retrieved}}{\text{Total Retrieved}}
\]

- **Recall**: Of all relevant documents, how many did we retrieve?

\[
\text{Recall} = \frac{\text{Relevant Retrieved}}{\text{Total Relevant}}
\]

Precision penalizes irrelevant documents; recall penalizes missing relevant ones.

#### Top-K Metrics

Metrics are often measured at the top K documents returned (e.g., top 5, top 10).

- **Precision@K**: Precision calculated on the top K results.
- **Recall@K**: Recall calculated on the top K results.

#### Mean Average Precision (MAP)

MAP averages the precision scores at the ranks where relevant documents appear, rewarding systems that rank relevant documents higher.

#### Reciprocal Rank and Mean Reciprocal Rank (MRR)

- **Reciprocal Rank**: The inverse of the rank of the first relevant document.
- **MRR**: Average reciprocal rank over many queries.

MRR measures how quickly the system finds the first relevant document.


### 8. üèÅ Summary and Key Takeaways

- **Keyword Search**: Fast, simple, exact word matching using TF-IDF or BM25 scoring.
- **Semantic Search**: Flexible, meaning-based search using vector embeddings and similarity measures.
- **Metadata Filtering**: Fast, rigid filtering based on document attributes, used to narrow results.
- **Hybrid Search**: Combines all three methods for best performance, using ranking fusion techniques.
- **Evaluation Metrics**: Precision, recall, MAP, and MRR help measure and improve retriever quality.

Understanding these foundations equips you to build and evaluate effective information retrieval systems that can handle diverse queries and document collections.



<br>

## Questions

#### 1. Which of the following best describe the primary difference between keyword search and semantic search?  
A) Keyword search matches exact words, semantic search matches meaning.  
B) Keyword search uses vector embeddings, semantic search uses bag-of-words.  
C) Semantic search can find synonyms, keyword search cannot.  
D) Keyword search is computationally more expensive than semantic search.  

#### 2. Metadata filtering in information retrieval is primarily used to:  
A) Rank documents by relevance to the query.  
B) Narrow down search results based on fixed document attributes.  
C) Perform semantic similarity matching.  
D) Exclude documents that do not meet specific criteria like author or date.  

#### 3. Which of the following statements about TF-IDF are true?  
A) TF-IDF rewards words that appear frequently in a document but rarely across the corpus.  
B) TF-IDF ignores document length, treating all documents equally.  
C) Common words like ‚Äúthe‚Äù have low IDF scores.  
D) TF-IDF is a scoring method used in semantic search.  

#### 4. BM25 improves upon TF-IDF by:  
A) Introducing term frequency saturation to avoid linear scaling with word count.  
B) Ignoring document length normalization.  
C) Penalizing longer documents to reduce bias.  
D) Using metadata filtering to improve ranking.  

#### 5. Which of the following are limitations of metadata filtering?  
A) It cannot rank documents by relevance.  
B) It can exclude relevant documents if filters are too strict.  
C) It performs retrieval based on document content.  
D) It is slow and computationally expensive.  

#### 6. In the bag-of-words model, which of the following is true?  
A) Word order is preserved to capture phrase meaning.  
B) Only word presence and frequency matter, not order.  
C) The vector representation is typically dense with many non-zero entries.  
D) The vector is usually sparse with many zero entries.  

#### 7. Which of the following are true about embedding models used in semantic search?  
A) They map words or documents to points in a high-dimensional vector space.  
B) The axes of the vector space have clear, interpretable meanings.  
C) Similar words cluster close together in the vector space.  
D) Embeddings are static and do not change during training.  

#### 8. Contrastive learning in embedding model training involves:  
A) Pulling positive pairs closer in vector space.  
B) Pushing negative pairs farther apart.  
C) Randomly assigning vectors to words without supervision.  
D) Iteratively updating model parameters based on pairwise similarity scores.  

#### 9. Which distance or similarity measures are commonly used to compare vectors in semantic search?  
A) Euclidean distance measures the straight-line distance between vectors.  
B) Cosine similarity measures the angle between vectors, ignoring magnitude.  
C) Dot product measures the length of the projection of one vector onto another.  
D) Jaccard similarity is the standard for vector comparison in semantic search.  

#### 10. Reciprocal Rank Fusion (RRF) is used to:  
A) Combine ranked lists from keyword and semantic search into a single ranking.  
B) Assign scores based on the sum of raw relevance scores from each list.  
C) Reward documents that rank highly in multiple lists.  
D) Penalize documents that appear only in one list.  

#### 11. Which of the following statements about hybrid search are correct?  
A) It combines keyword search, semantic search, and metadata filtering.  
B) It always returns more documents than either keyword or semantic search alone.  
C) It allows tuning the relative importance of keyword vs semantic ranking.  
D) It eliminates the need for metadata filtering.  

#### 12. When evaluating a retriever, which of the following metrics focus on the quality of the top-ranked documents?  
A) Precision@K  
B) Recall@K  
C) Mean Reciprocal Rank (MRR)  
D) Total number of documents retrieved  

#### 13. Which of the following are true about precision and recall?  
A) Precision penalizes returning irrelevant documents.  
B) Recall penalizes missing relevant documents.  
C) High precision always implies high recall.  
D) Precision and recall are independent and can vary inversely.  

#### 14. Mean Average Precision (MAP) differs from simple precision because it:  
A) Rewards ranking relevant documents higher in the list.  
B) Only considers the first relevant document retrieved.  
C) Averages precision scores at the ranks of all relevant documents.  
D) Ignores irrelevant documents in the calculation.  

#### 15. Which of the following are challenges or limitations of keyword search?  
A) It cannot handle synonyms or related meanings.  
B) It requires exact word matches to retrieve documents.  
C) It is computationally more expensive than semantic search.  
D) It ignores word order and context.  

#### 16. Which of the following best describe the role of document length normalization in BM25?  
A) It prevents longer documents from unfairly scoring higher just because they contain more words.  
B) It increases the score of longer documents to favor detailed content.  
C) It adjusts scores based on the average document length in the corpus.  
D) It is controlled by a tunable parameter that can be set between 0 and 1.  

#### 17. Which of the following statements about embedding vectors before and after training are true?  
A) Before training, vector locations are random and meaningless.  
B) After training, vectors cluster so that similar meanings are close together.  
C) Embeddings from different models can be directly compared.  
D) Training uses millions of positive and negative pairs to refine vector positions.  

#### 18. Which of the following are true about metadata filtering‚Äôs relationship to retrieval?  
A) Metadata filtering performs retrieval based on query content.  
B) Metadata filtering is applied after keyword or semantic search to narrow results.  
C) It is a rigid filter that excludes documents not matching exact metadata criteria.  
D) It can rank documents by relevance within the filtered set.  

#### 19. Which of the following are advantages of BM25 over TF-IDF?  
A) BM25 includes term frequency saturation to avoid linear scaling.  
B) BM25 incorporates document length normalization with tunable parameters.  
C) BM25 completely ignores inverse document frequency.  
D) BM25 generally performs better in production retrieval systems.  

#### 20. In semantic search, why is cosine similarity often preferred over Euclidean distance?  
A) Cosine similarity measures the angle between vectors, focusing on direction rather than magnitude.  
B) Euclidean distance can be biased by vector length differences.  
C) Cosine similarity is computationally more expensive than Euclidean distance.  
D) Cosine similarity can better capture semantic similarity when vector magnitudes vary.



<br>

## Answers

#### 1. Which of the following best describe the primary difference between keyword search and semantic search?  
A) ‚úì Keyword search matches exact words, semantic search matches meaning.  
B) ‚úó Keyword search uses vector embeddings, semantic search uses bag-of-words. (Opposite is true.)  
C) ‚úì Semantic search can find synonyms, keyword search cannot.  
D) ‚úó Keyword search is computationally more expensive than semantic search. (Semantic search is more expensive.)  

**Correct:** A, C


#### 2. Metadata filtering in information retrieval is primarily used to:  
A) ‚úó Rank documents by relevance to the query. (Filtering narrows, does not rank.)  
B) ‚úì Narrow down search results based on fixed document attributes.  
C) ‚úó Perform semantic similarity matching. (Filtering is not content-based.)  
D) ‚úì Exclude documents that do not meet specific criteria like author or date.  

**Correct:** B, D


#### 3. Which of the following statements about TF-IDF are true?  
A) ‚úì TF-IDF rewards words that appear frequently in a document but rarely across the corpus.  
B) ‚úó TF-IDF ignores document length, treating all documents equally. (Length normalization is needed.)  
C) ‚úì Common words like ‚Äúthe‚Äù have low IDF scores.  
D) ‚úó TF-IDF is a scoring method used in semantic search. (It‚Äôs used in keyword search.)  

**Correct:** A, C


#### 4. BM25 improves upon TF-IDF by:  
A) ‚úì Introducing term frequency saturation to avoid linear scaling with word count.  
B) ‚úó Ignoring document length normalization. (BM25 includes length normalization.)  
C) ‚úì Penalizing longer documents to reduce bias.  
D) ‚úó Using metadata filtering to improve ranking. (Filtering is separate.)  

**Correct:** A, C


#### 5. Which of the following are limitations of metadata filtering?  
A) ‚úì It cannot rank documents by relevance.  
B) ‚úì It can exclude relevant documents if filters are too strict.  
C) ‚úó It performs retrieval based on document content. (It does not analyze content.)  
D) ‚úó It is slow and computationally expensive. (It is fast and simple.)  

**Correct:** A, B


#### 6. In the bag-of-words model, which of the following is true?  
A) ‚úó Word order is preserved to capture phrase meaning. (Order is ignored.)  
B) ‚úì Only word presence and frequency matter, not order.  
C) ‚úó The vector representation is typically dense with many non-zero entries. (Usually sparse.)  
D) ‚úì The vector is usually sparse with many zero entries.  

**Correct:** B, D


#### 7. Which of the following are true about embedding models used in semantic search?  
A) ‚úì They map words or documents to points in a high-dimensional vector space.  
B) ‚úó The axes of the vector space have clear, interpretable meanings. (Axes are abstract.)  
C) ‚úì Similar words cluster close together in the vector space.  
D) ‚úó Embeddings are static and do not change during training. (They are learned and updated.)  

**Correct:** A, C


#### 8. Contrastive learning in embedding model training involves:  
A) ‚úì Pulling positive pairs closer in vector space.  
B) ‚úì Pushing negative pairs farther apart.  
C) ‚úó Randomly assigning vectors to words without supervision. (Training is supervised.)  
D) ‚úì Iteratively updating model parameters based on pairwise similarity scores.  

**Correct:** A, B, D


#### 9. Which distance or similarity measures are commonly used to compare vectors in semantic search?  
A) ‚úì Euclidean distance measures the straight-line distance between vectors.  
B) ‚úì Cosine similarity measures the angle between vectors, ignoring magnitude.  
C) ‚úì Dot product measures the length of the projection of one vector onto another.  
D) ‚úó Jaccard similarity is the standard for vector comparison in semantic search. (Used for sets, not vectors.)  

**Correct:** A, B, C


#### 10. Reciprocal Rank Fusion (RRF) is used to:  
A) ‚úì Combine ranked lists from keyword and semantic search into a single ranking.  
B) ‚úó Assign scores based on the sum of raw relevance scores from each list. (RRF uses ranks, not raw scores.)  
C) ‚úì Reward documents that rank highly in multiple lists.  
D) ‚úó Penalize documents that appear only in one list. (No explicit penalty.)  

**Correct:** A, C


#### 11. Which of the following statements about hybrid search are correct?  
A) ‚úì It combines keyword search, semantic search, and metadata filtering.  
B) ‚úó It always returns more documents than either keyword or semantic search alone. (It returns a combined ranked list, not necessarily more.)  
C) ‚úì It allows tuning the relative importance of keyword vs semantic ranking.  
D) ‚úó It eliminates the need for metadata filtering. (Filtering is still useful.)  

**Correct:** A, C


#### 12. When evaluating a retriever, which of the following metrics focus on the quality of the top-ranked documents?  
A) ‚úì Precision@K  
B) ‚úì Recall@K  
C) ‚úì Mean Reciprocal Rank (MRR)  
D) ‚úó Total number of documents retrieved (Does not measure quality.)  

**Correct:** A, B, C


#### 13. Which of the following are true about precision and recall?  
A) ‚úì Precision penalizes returning irrelevant documents.  
B) ‚úì Recall penalizes missing relevant documents.  
C) ‚úó High precision always implies high recall. (They can vary inversely.)  
D) ‚úì Precision and recall are independent and can vary inversely.  

**Correct:** A, B, D


#### 14. Mean Average Precision (MAP) differs from simple precision because it:  
A) ‚úì Rewards ranking relevant documents higher in the list.  
B) ‚úó Only considers the first relevant document retrieved. (Considers all relevant docs.)  
C) ‚úì Averages precision scores at the ranks of all relevant documents.  
D) ‚úó Ignores irrelevant documents in the calculation. (Irrelevant docs affect precision.)  

**Correct:** A, C


#### 15. Which of the following are challenges or limitations of keyword search?  
A) ‚úì It cannot handle synonyms or related meanings.  
B) ‚úì It requires exact word matches to retrieve documents.  
C) ‚úó It is computationally more expensive than semantic search. (It is cheaper.)  
D) ‚úì It ignores word order and context.  

**Correct:** A, B, D


#### 16. Which of the following best describe the role of document length normalization in BM25?  
A) ‚úì It prevents longer documents from unfairly scoring higher just because they contain more words.  
B) ‚úó It increases the score of longer documents to favor detailed content. (It penalizes longer docs.)  
C) ‚úì It adjusts scores based on the average document length in the corpus.  
D) ‚úì It is controlled by a tunable parameter that can be set between 0 and 1.  

**Correct:** A, C, D


#### 17. Which of the following statements about embedding vectors before and after training are true?  
A) ‚úì Before training, vector locations are random and meaningless.  
B) ‚úì After training, vectors cluster so that similar meanings are close together.  
C) ‚úó Embeddings from different models can be directly compared. (Only embeddings from the same model are comparable.)  
D) ‚úì Training uses millions of positive and negative pairs to refine vector positions.  

**Correct:** A, B, D


#### 18. Which of the following are true about metadata filtering‚Äôs relationship to retrieval?  
A) ‚úó Metadata filtering performs retrieval based on query content. (It does not analyze content.)  
B) ‚úì Metadata filtering is applied after keyword or semantic search to narrow results.  
C) ‚úì It is a rigid filter that excludes documents not matching exact metadata criteria.  
D) ‚úó It can rank documents by relevance within the filtered set. (Filtering only excludes, does not rank.)  

**Correct:** B, C


#### 19. Which of the following are advantages of BM25 over TF-IDF?  
A) ‚úì BM25 includes term frequency saturation to avoid linear scaling.  
B) ‚úì BM25 incorporates document length normalization with tunable parameters.  
C) ‚úó BM25 completely ignores inverse document frequency. (It still uses IDF.)  
D) ‚úì BM25 generally performs better in production retrieval systems.  

**Correct:** A, B, D


#### 20. In semantic search, why is cosine similarity often preferred over Euclidean distance?  
A) ‚úì Cosine similarity measures the angle between vectors, focusing on direction rather than magnitude.  
B) ‚úì Euclidean distance can be biased by vector length differences.  
C) ‚úó Cosine similarity is computationally more expensive than Euclidean distance. (It is usually cheaper or comparable.)  
D) ‚úì Cosine similarity can better capture semantic similarity when vector magnitudes vary.  

**Correct:** A, B, D