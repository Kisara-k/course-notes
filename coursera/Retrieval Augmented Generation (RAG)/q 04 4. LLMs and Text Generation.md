## 4. LLMs and Text Generation

## Questions

#### 1. What is the primary role of the attention mechanism in transformer models?  
A) To sequentially process tokens one at a time  
B) To determine which tokens in the input should most influence the representation of each token  
C) To encode the position of tokens in the input sequence  
D) To allow each token to consider the meaning and position of every other token simultaneously  

#### 2. Which of the following statements about transformer encoder and decoder components are true?  
A) The encoder builds a deep contextual understanding of the input text  
B) Most LLMs use both encoder and decoder components equally  
C) The decoder generates new text based on the encoder’s output  
D) Many LLMs only use the decoder component because they focus on text generation  

#### 3. How do input embeddings in transformers represent tokens?  
A) As sparse one-hot vectors indicating token identity only  
B) As dense semantic vectors combined with position vectors  
C) As vectors encoding only the token’s position in the sequence  
D) As vectors that represent both the token’s meaning and its position  

#### 4. What is the function of multiple attention heads in a transformer layer?  
A) Each head learns different abstract patterns from the input data  
B) They split the input sequence into smaller chunks for parallel processing  
C) They apply human-defined linguistic rules to the tokens  
D) Larger models may use over 100 attention heads to capture complex relationships  

#### 5. Why is text generation by LLMs computationally expensive?  
A) Because each token generation requires reprocessing the entire input sequence with attention  
B) Because the model must generate all tokens simultaneously  
C) Because the model must examine all tokens for context at every step  
D) Because transformers use recurrent neural networks internally  

#### 6. Which of the following best describes greedy decoding?  
A) Selecting the token with the highest probability at each step  
B) Sampling tokens randomly from the entire vocabulary  
C) Producing highly creative and diverse text outputs  
D) Often resulting in repetitive or generic text loops  

#### 7. How does the temperature parameter affect the token probability distribution during sampling?  
A) Higher temperature sharpens the distribution, making the model more confident  
B) Lower temperature flattens the distribution, increasing randomness  
C) Lower temperature makes the distribution spikier, favoring high-probability tokens  
D) Higher temperature flattens the distribution, encouraging more creative outputs  

#### 8. What is the key difference between Top-K and Top-P (nucleus) sampling?  
A) Top-K always selects from a fixed number of tokens regardless of their cumulative probability  
B) Top-P selects tokens until their cumulative probability exceeds a threshold  
C) Top-K dynamically adjusts the number of tokens based on model confidence  
D) Top-P always selects exactly K tokens  

#### 9. Which strategies help reduce repetition and loops in generated text?  
A) Increasing temperature to encourage randomness  
B) Applying repetition penalties to reduce probabilities of already used tokens  
C) Using logit biases to adjust token probabilities directly  
D) Using greedy decoding exclusively  

#### 10. In prompt engineering, what is the role of the system message?  
A) To provide high-level instructions that influence the LLM’s overall behavior  
B) To record the user’s questions and inputs  
C) To store the assistant’s previous responses  
D) To define the tone, style, and constraints for the model’s replies  

#### 11. Why is Retrieval-Augmented Generation (RAG) effective in reducing hallucinations?  
A) It retrains the model’s internal parameters with new data  
B) It injects relevant external documents into the prompt for grounding  
C) It forces the model to generate text only from retrieved factual information  
D) It eliminates all randomness in token sampling  

#### 12. What are common causes and consequences of hallucinations in LLM outputs?  
A) LLMs generate probable text sequences, which may not always be factually accurate  
B) Hallucinations are easy to detect because they produce nonsensical text  
C) Hallucinations can erode user trust even if they occur occasionally  
D) Hallucinations never occur when using RAG systems  

#### 13. Which of the following are true about citation generation in LLM responses?  
A) LLMs can be instructed to cite sources after each factual claim  
B) Citations generated by LLMs are always accurate and reliable  
C) Citation generation improves human verification of facts  
D) LLMs may hallucinate citations, so human oversight is necessary  

#### 14. When evaluating LLM performance, what does the faithfulness metric measure?  
A) Whether the response is relevant to the user’s prompt  
B) Whether the response is consistent with retrieved factual information  
C) The fluency and clarity of the generated text  
D) The number of tokens generated per second  

#### 15. What are the advantages and limitations of fine-tuning LLMs?  
A) Fine-tuning updates the model’s internal parameters to improve domain-specific performance  
B) Fine-tuning teaches the model new factual knowledge effectively  
C) Fine-tuning can degrade performance outside the target domain  
D) Fine-tuning is best suited for injecting up-to-date external knowledge dynamically  

#### 16. How do agentic LLM systems typically operate?  
A) A single LLM handles all tasks from retrieval to response generation  
B) Different specialized LLMs perform distinct tasks such as routing, evaluation, and citation  
C) They can operate sequentially, conditionally, or in parallel workflows  
D) They eliminate the need for retrievers by generating all information internally  

#### 17. Which of the following are true about context window management in LLMs?  
A) The context window limits the total number of tokens processed in prompt and completion  
B) Including irrelevant conversation history improves model accuracy  
C) Pruning old or irrelevant messages helps maintain prompt efficiency  
D) Reasoning tokens always reduce the context window size without benefits  

#### 18. What is the main reason why many LLMs only use the decoder part of the transformer architecture?  
A) Because decoders are simpler to train than encoders  
B) Because text generation requires predicting the next token based on previous tokens, which decoders specialize in  
C) Because encoders are only useful for translation tasks  
D) Because decoders can generate embeddings for semantic understanding  

#### 19. Which of the following statements about sampling parameters are correct?  
A) Setting temperature to zero is equivalent to greedy decoding  
B) Top-P sampling is more dynamic than Top-K because it adapts to the model’s confidence  
C) Increasing temperature always improves factual accuracy  
D) Repetition penalties discourage the model from repeating the same tokens excessively  

#### 20. Why might reasoning models struggle with in-context learning and example mixing?  
A) They perform best with clear goals and strict prompt formats  
B) They rely heavily on large context windows and high-level guidance  
C) They are designed to ignore example prompts to avoid bias  
D) They add computational cost but improve accuracy by encouraging step-by-step reasoning



<br>

## Answers

#### 1. What is the primary role of the attention mechanism in transformer models?  
A) ✗ Attention does not process tokens sequentially; it considers all tokens simultaneously.  
B) ✓ Correct. Attention weighs which tokens most influence each token’s representation.  
C) ✗ Position encoding is separate from attention.  
D) ✓ Correct. Each token considers meaning and position of every other token via attention.  

**Correct:** B, D


#### 2. Which of the following statements about transformer encoder and decoder components are true?  
A) ✓ Encoder builds deep contextual understanding of input text.  
B) ✗ Most LLMs use only the decoder, not both equally.  
C) ✓ Decoder generates new text based on encoder output in full transformer models.  
D) ✓ Many LLMs only use decoder because they focus on text generation.  

**Correct:** A, C, D


#### 3. How do input embeddings in transformers represent tokens?  
A) ✗ Transformers use dense vectors, not sparse one-hot vectors.  
B) ✓ Correct. Embeddings combine semantic meaning and position vectors.  
C) ✗ Position vectors alone don’t represent token meaning.  
D) ✓ Correct. Both meaning and position are encoded together.  

**Correct:** B, D


#### 4. What is the function of multiple attention heads in a transformer layer?  
A) ✓ Each head learns different abstract patterns from data.  
B) ✗ Attention heads do not split input into chunks; they attend to all tokens.  
C) ✗ Heads learn patterns automatically, not human-defined rules.  
D) ✓ Larger models use many heads to capture complex relationships.  

**Correct:** A, D


#### 5. Why is text generation by LLMs computationally expensive?  
A) ✓ Each token generation requires reprocessing the entire sequence with attention.  
B) ✗ Tokens are generated sequentially, not simultaneously.  
C) ✓ The model must examine all tokens for context at every step.  
D) ✗ Transformers do not use recurrent neural networks internally.  

**Correct:** A, C


#### 6. Which of the following best describes greedy decoding?  
A) ✓ Always picks the highest probability token at each step.  
B) ✗ Greedy decoding is deterministic, not random sampling.  
C) ✗ Greedy decoding tends to produce generic, not creative, text.  
D) ✓ Can result in repetitive loops or generic text.  

**Correct:** A, D


#### 7. How does the temperature parameter affect the token probability distribution during sampling?  
A) ✗ Higher temperature flattens, not sharpens, the distribution.  
B) ✗ Lower temperature sharpens (spikier), not flattens, the distribution.  
C) ✓ Lower temperature makes distribution spikier, favoring high-probability tokens.  
D) ✓ Higher temperature flattens distribution, encouraging creativity.  

**Correct:** C, D


#### 8. What is the key difference between Top-K and Top-P (nucleus) sampling?  
A) ✓ Top-K selects from a fixed number of tokens regardless of cumulative probability.  
B) ✓ Top-P selects tokens until cumulative probability exceeds a threshold.  
C) ✗ Top-K does not dynamically adjust number of tokens based on confidence.  
D) ✗ Top-P does not select exactly K tokens; it varies.  

**Correct:** A, B


#### 9. Which strategies help reduce repetition and loops in generated text?  
A) ✗ Increasing temperature increases randomness but may not reduce repetition.  
B) ✓ Repetition penalties reduce probabilities of already used tokens.  
C) ✓ Logit biases can adjust token probabilities to discourage repetition.  
D) ✗ Greedy decoding often causes repetition rather than prevents it.  

**Correct:** B, C


#### 10. In prompt engineering, what is the role of the system message?  
A) ✓ Provides high-level instructions influencing model behavior.  
B) ✗ User messages record user inputs, not system messages.  
C) ✗ Assistant messages record model responses, not system messages.  
D) ✓ Defines tone, style, and constraints for replies.  

**Correct:** A, D


#### 11. Why is Retrieval-Augmented Generation (RAG) effective in reducing hallucinations?  
A) ✗ RAG does not retrain model parameters; it augments prompts.  
B) ✓ Injects relevant external documents into prompts for grounding.  
C) ✓ Forces model to base answers on retrieved factual information.  
D) ✗ RAG does not eliminate all randomness in sampling.  

**Correct:** B, C


#### 12. What are common causes and consequences of hallucinations in LLM outputs?  
A) ✓ LLMs generate probable text, which may not be factually accurate.  
B) ✗ Hallucinations can be subtle and plausible, not always nonsensical.  
C) ✓ Hallucinations erode user trust even if occasional.  
D) ✗ Hallucinations can still occur with RAG, though less frequently.  

**Correct:** A, C


#### 13. Which of the following are true about citation generation in LLM responses?  
A) ✓ LLMs can be instructed to cite sources after factual claims.  
B) ✗ Citations generated by LLMs are not always accurate or reliable.  
C) ✓ Citation generation aids human verification of facts.  
D) ✓ LLMs may hallucinate citations, so human oversight is needed.  

**Correct:** A, C, D


#### 14. When evaluating LLM performance, what does the faithfulness metric measure?  
A) ✗ Relevancy measures relation to prompt, not faithfulness.  
B) ✓ Faithfulness measures consistency with retrieved factual information.  
C) ✗ Fluency measures clarity, not faithfulness.  
D) ✗ Tokens per second measure speed, not faithfulness.  

**Correct:** B


#### 15. What are the advantages and limitations of fine-tuning LLMs?  
A) ✓ Fine-tuning improves domain-specific performance by updating parameters.  
B) ✗ Fine-tuning does not effectively teach new factual knowledge.  
C) ✓ Fine-tuning can degrade performance outside the target domain.  
D) ✗ Fine-tuning is not ideal for injecting up-to-date external knowledge dynamically.  

**Correct:** A, C


#### 16. How do agentic LLM systems typically operate?  
A) ✗ Agentic systems use multiple specialized LLMs, not a single one.  
B) ✓ Different LLMs perform distinct tasks like routing, evaluation, citation.  
C) ✓ They can operate sequentially, conditionally, or in parallel workflows.  
D) ✗ They rely on retrievers; do not eliminate them.  

**Correct:** B, C


#### 17. Which of the following are true about context window management in LLMs?  
A) ✓ Context window limits total tokens processed in prompt and completion.  
B) ✗ Including irrelevant history usually reduces accuracy and efficiency.  
C) ✓ Pruning old or irrelevant messages maintains prompt efficiency.  
D) ✗ Reasoning tokens add cost but improve accuracy; they are not always dropped.  

**Correct:** A, C


#### 18. What is the main reason why many LLMs only use the decoder part of the transformer architecture?  
A) ✗ Decoders are not necessarily simpler to train.  
B) ✓ Decoders specialize in predicting next tokens based on previous tokens, ideal for generation.  
C) ✗ Encoders are useful beyond translation, but decoders suffice for generation.  
D) ✗ Decoders do not generate embeddings; encoders do that.  

**Correct:** B


#### 19. Which of the following statements about sampling parameters are correct?  
A) ✓ Temperature zero equals greedy decoding (always pick highest probability).  
B) ✓ Top-P is more dynamic than Top-K, adapting to model confidence.  
C) ✗ Increasing temperature does not guarantee better factual accuracy; often reduces it.  
D) ✓ Repetition penalties discourage excessive token repetition.  

**Correct:** A, B, D


#### 20. Why might reasoning models struggle with in-context learning and example mixing?  
A) ✓ They perform best with clear goals and strict prompt formats.  
B) ✓ They rely on large context windows and high-level guidance.  
C) ✗ They do not ignore example prompts; they use them carefully.  
D) ✓ They add computational cost but improve accuracy by encouraging step-by-step reasoning.  

**Correct:** A, B, D