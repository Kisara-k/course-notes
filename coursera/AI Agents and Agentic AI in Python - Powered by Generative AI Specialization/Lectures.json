[
  {
    "index": 0,
    "title": "1 Agentic AI Concepts",
    "content": "1 Agentic AI Concepts. 1 Introduction So what is Agentic AI? Well, I want to take a look at this with you and help you give an intuitive sense of what this is all about and why it's sort of fascinating. Now, what Agentic AI is, is basically AI that can go and act with agency. You think of it as autonomy, it can go and try to problem solve and react to different situations. And you've probably already seen it in your interaction with generative AI, but you probably weren't thinking of it that way. Now, Agentic AI is really putting this into practice and creating generative AI or other systems that can go and interact with the real world, take actions, see the result of those actions, and then respond based on that. So we're going to go and simulate this whole process. Well, it's not fully simulating it because we're going to kind of build an agentic system that's going to have a manual component to it. Meaning that as a human being, I'm going to help the agent accomplish actions and respond to situations. And so we're going to build an agentic system that has a human in the loop where the human is helping create that agency that it needs. It's helping carry out the actions. And that's one of the key things that we start seeing when we have these agentic systems as they do things for us sort of autonomously. So here's my prompt to get this started. You're going to help me cook whatever I need. Since you can't directly lift pots or pans or get to the fridge, you will tell me the steps and I will perform them. We will go one step at a time, ask me for what I want to cook. And this is a prompt that's essentially kicking off an agentic system where I'm going to follow the instructions that it tells me to follow. And then whenever it tells me there's some action that needs to take place, I'm going to take that action and then I'm going to tell it what happened and it says, okay, great, let's get started. What would you like to cook today? And I'm going to start off with something fairly simple. I'd like to cook a dish that combines the flavors of Ethiopia, Uzbekistan, while being keto friendly and up and under to 1000 calories. And it says, that sounds like a fun and unique challenge. How about we make a keto-friendly Ethiopian-Uzbek fusion dish featuring a flavorful marinated meat served with a side of spiced vegetables. Here's a plan. And it's starting to build out. Here's how I'm going to go and accomplish this. And it says, okay, here are the ingredients you will need. Lists all the ingredients. And then I want to draw your attention down to. This is instructions at the bottom. This is where it's telling me what to do. Here is the task to go and perform. And it says, marinating the meat. Mix the berbere spice, ground coriander, cumin, seeds, turmeric, fenugreek, paprika, etc. Coat the meat with the spice, ensure it's covered. Let me know when you have the ingredients ready and if you have any questions. So it's telling me, go and do this and tell me when it's done. And so I come back to it and I say, okay, meat is marinated. I did that. And now what does it do? It responds and it says, okay, great, let's move on to the next step, cooking the meat. Heat one tablespoon of olive oil or ghee in a skillet over medium heat. Once the oil is hot, add the marinated meat to the skillet. Cook for about five to seven minutes. Now it's giving me instructions, it's really telling me a task to go and perform. I'm going to go cook the meat is the task, but it's breaking that task down into sub steps that I'm going to go and perform. But you notice what's happening. It's telling me what to go and do. It's giving me the instructions to perform. It has the choice and what gets done and when, and I'm carrying out its instructions. That is it having agency, it's choice over what's happening. And then it says, let me know when the meat is cooked and resting and we can proceed. And so I come back and I want to point out the agentic aspect of this, the fact that it can react and have agency and choose what to do and respond to all kinds of things, including something like, no, it's starting to burn. So I tell it like, hey, here's what's happening with the meat, it's starting to burn. And I don't tell it what to do. And it says, if the meat is starting to burn, reduce the heat to medium or medium low immediately. You can also add a splash of water or broth to the pan to deglaze it and prevent further burning. So it is responding and adapting. Now, I'm simulating all of this. We're not automating it yet. What we're doing is we're seeing and building up intuition about what is about, and this is it happening. Except that rather than fully automating it as a human being, I'm going and performing its actions. So it's a collaboration between myself and the generative AI to get this dish cooked. So it tells me what to do, how to handle the potential burning meat, and then it says, let me know when the meat is done and resting, and we'll move on. And say, I go back and I say, okay, the meat is done. So I'm giving it feedback on its actions. I've completed the action you requested, it's ready. It comes back and says, okay, let's move on to preparing the vegetables. And again, let me know when the vegetables are ready. So it's giving me a task, I'm going to performing the task, and then I'm coming back and saying, okay, task is done. And then it comes up with the next task. And it says, now, in this case, it asked me if I forgot about this, I gave it a little curve ball. It said, let me know when the vegetables are ready. And I said, no vegetables, but I have durian fruit. So this is a little bit of a curve ball for it. How will it respond to, no, there's no vegetables? You asked me for vegetables. I don't have any, but I do have durian fruit. You have to deal with it. How does it respond? How does the agent handle that? And it comes back and it says, okay, well, it's quite different from vegetables, but I'm going to adapt. And so now it tells me how to prepare the durian fruit because it says, well, I want a path forward. I want to try to reach this goal of having created the dish. I don't have access to the resources that I thought. So I'll have to adapt and come up with some other way of solving this. So it adapts, says, okay, now here's how you're going to prepare the durian fruit. I have no idea if this works. This is not a dish I'm cooking in my house. And I tell it, it's done, it comes back, and it says, great, now here's how you go and plate the dish. Now, what happened here? Well, we flipped the script, right? We, instead of going into the generative AI and telling it, do this, do this, plan this, do this, I said, here's what I want you to accomplish. Here is the process you're going to use to accomplish this. The process is you're going to go one step at a time you're going to tell me what to do. I'm going to carry out the action for you and then I'm going to tell you when it's done, basically. And then you can go and respond and do the next thing. And what we see is, is that as we go and change our responses, for example, when we threw at this curveball over the durian fruit, it responds and adapts. And that's a critical thing with the Agentic AI, is it's nothing fixed. We think of all our computer systems that we've built for years and they're very rigid and fixed. Like if you take a computer system and you say, no vegetables, durian fruit, it falls apart. It has no idea what to do. And this is where the magic of Agentic AI starts to come in, is it sees, okay, I don't have vegetables, I've got durian fruit, I will do my best. I'm going to try to figure this out. And that's what we're looking for, is that ability to see things and respond to them, and then choose the next action based on what happened. Or like when the meat was getting ready to burn, adapting the plan and say no, okay, here's what you're going to do instead. You're going to turn down the heat and that's okay. Now tell me when you're got, you're done with that step. And so it's dynamically altering the steps based on the feedback that it's receiving me. And that is the key to agentic AI that we're going to be working with is its ability to ask for an action to be performed, because it's never going to be directly performing it. It's going to be specifying what it wants done. And then either a human being is going to go and do it, or an existing software or computer system that we know how to build is going to go and take that specification and carry it out. That's the key. Now, does it matter if it's a human being or computer system that's doing this? Not fully, the key difference that we're gonnasee is that if it's a human being, we are much more error tolerant. If it goes and it gives us the tasks that it once performed in many different formats, we can handle that and adapt because we're human beings. But computer systems aren't so good at doing that. So when it begins interfacing, when magenta begins interfacing with all the tools and computer systems that we use on a daily basis, if it decides to say, hey, yo, go create this recipe. And it says that to a spreadsheet. The spreadsheet is going to say, I have no idea what you're talking about because it doesn't work with yo create this recipe. It works in columns and numbers and values, and so it's in a much more rigid format. So part of the magic of Agentic AI is figuring out how do we bridge this gap between how it sort of intuitively and fluidly specifies what it wants done, and this rigid world of computer systems that we've built where everything has to be exact and perfect. Boy, we know that generative AI loves to explain itself and gives us all kinds of remarks and details and comments and all those types of things are really problematic when we start working with real computer systems. Now, it's fine in this example like I have, where I'm acting as the automation, I'm going in and listening to what it's saying, and then responding to it because I'm a human being and I can adapt as it can adapt. I can adapt to it giving comments, I can adapt to it explaining itself. I can adapt when it's short and sweet about what it wants. And a lot of what we're going to have to do is the prompt engineering around how do we interface our existing computer systems with these things that are very talkative and adaptive and all of these other things they do and aren't as rigid as we're used to. So I look forward to exploring this more with. 2 Flipped Interaction Pattern Let's look at the basis of this ISO gentic systems. How do we kick off and get generative AI to go and work this way? There is a prompt engineering pattern, and I've talked about it in my first class, Prompt Engineering for Chat GPT, and a paper I did. But I think of this as the basis for all this stuff, which is a flipped interaction, where rather than us telling the system what to do, rather than us asking it questions, it goes and tells us what to do, it asks us questions, or really it goes and ask existing computer systems questions. It runs queries and databases to collect information it needs, or it goes and tells those systems to run actions that it needs run in order to accomplish some goal that we give it. When I want to take a quick look at the flipped interaction pattern and talk about how this works. The basic structure of this pattern is, we go and tell it that you're going to ask me questions at one at a time in order to get enough information to suggest a restaurant for me to eat at in Nashville, Tennessee, tonight. You can think of this in this basic pattern as, ask me questions one at a time. One at a time is often the best for gentic systems. It doesn't have to be one at a time, but we'll talk about this in a minute. Go and ask me questions, or tell me what to do one step at a time. The goal of this is we want to create a situation where it goes and creates things that need to be done, tasks that need to be performed, or actions that need to be taken. Then the system will go and execute those actions on behalf of the agent and tell it what happened. This rough pattern of prompting, ask me questions one at a time, tell me what to do one step at a time, creates this environment where it's being told, you're in the driver's seat, go and do it. Then at the bottom I say ask the first question. This is a trick that works really well in lots of systems to make sure that we start off with one question only, or we say, \"Tell me the first step,\" so that we start off with one step only. We want to create a powerful pattern of behavior where it goes off and it knows what to do. It was going to do this one step at a time. Let's look at our prompt. Ask me questions one at a time in order to gather enough information to suggest restaurants for me to eat at in Nashville, Tennessee, tonight. Ask the first question. It comes back, what type of cuisine are you in the mood for tonight? I'm going to come back and I'm going to say Tex-Mex. Now I've given it an answer, and it has to adapt, and part of the adaptation is choosing the next question to ask me. That is wildly powerful. That simple thing of looking at what information it was given, and then choosing the next question in response is critically important, because the same thing happens in a task. It has for a task to be run, it looks at what happened, and then it adapts what the next task is going to be based on what happened. Same thing with questions. I say Tex-Mex, then it says, \"Do you have any dietary restrictions or preferences?\" I say no. It adapts the next question then, because if I had said yes, it probably would have gone off and then asked me questions about them. But since I said no, it said, do you prefer a casual dining experience or something more upscale? I say, kid friendly. I just give it some feedback. Notice I'm not constraining my responses to any type of format. I'm giving it very free-flowing. It's having to adapt, not only to the information I'm giving it, but the format that I'm giving it the information. It's very fluid in what it's doing. Would you like a restaurant with outdoor seating or indoor seating, or does it not matter? I say, \"Well, it doesn't matter, it's raining, so I'm probably not going to eat outside.\" Then it goes further and it says, \"Do you have a specific preference for a specific area?\" I say near Vanderbilt. Great, how far are you willing to travel? I say, \"Well, a short drive, 5-10 minutes. Then it says, \"Got it. Do you have any preference for a restaurant that takes reservations? Are you okay with walk-in?\" I say walk-in's fine, but I don't want to have an insane wait. You notice this is totally fluid. Each time I respond to one of its questions, it's adapting. That is the agentic part beginning to happen. That is the agentic magic. It's also adapting to all the different formats and ways that I can specify my input. This is one of the important parts of large language models, is that they can handle input in so many different formats, so many different ways of expressing the information and the idea without us having to go and build some complex thing to be able to handle all these crazy ways that I'm responding. Then after I give it the walk-ins thing, it says, \"Based on your preferences, I recommend Rosepepper Cantina.\" I go and Google Map this, and I'm like, well, right now from my house that's actually about 13 minutes with traffic, slightly farther than I want to drive. I just go back and I say, \"Hey, Google Map says it's 13 minutes away, feeling a little lazy for that trek.\" It comes back and it says, \"In that case, go to SATCo.\" SATCo happens to be right across the street from Vanderbilt. Now that's something really powerful and valuable that I can go and employ to help me solve a problem. Now I can solve a problem like this. Like where should I go? Help make a decision. Help guide me as a human being, and it's collecting information from me. There's no difference between it collecting information from me and it collecting information from a database. It's just the interface to collect that interface changes, or interface to collect that information changes. Asking the human being is much easier, because the human being can adapt to whatever it says. The difference is when it goes and collects information from a real computer system like a database, it's going to have to speak the language of that database. It's going to have to speak the language of that computer system, so it's going to have to translate the goal to whatever the target language is. if you think about something that these models and large language models are supremely good at, it is translation. Absolutely good at translation. Excellent experts at language. That piece is again one of the huge benefits, is that we go and express what we want in our human language, it decomposes it into a plan to go and accomplish it. It interactively begins executing its plan, and when it's a human on one side, it'll ask it in human language. But when it needs to go and it needs to talk to a database, it can go and begin speaking that database language. Just like in Star Wars. They had protocol droids that spoke all these different languages and could communicate with all these different systems. They acted as the interface to those systems. That is what generative AI is doing. Agentic AI is working very similarly, but rather than just asking it a question and us really directing everything one step at a time, agentic AI, we're giving it the ability to go and perform many different steps and adapt. It chooses the step, we perform it. It gets information back about what happened as a result of the step, and then it adapts, and that continues on and on. 3 The Agent Loop How do we go from a conversation where we're manually entering things in into an AI agent? Well, the key thing that we do is we build an agent loop. And an agent loop is a really simple construct that I'm going to walk you through, but we have to pay attention to the details of this loop. We have to understand how it works. And so we're going to take a look at how do we go from a conversation to an AI agent by creating a loop programmatically. So we're going to walk through what this architecture looks like and why it's important. And what are some of the key things we have to think about? So let's go back to the conversation that we had. And we were asking it to guide us through some activity, like cooking something. So we might have sent a prompt in, like, help me cook this and tell it to me one step at a time. And the AI then would come back and say, go get these ingredients. And then me, the human, I would go back and say, I'm out of beans. And then it would do what an agent does, which is it adapts, right? It is showing agency in that it is telling me what to do. It is deciding what to do. And it's adaptive in that when something goes awry, it can go and adapt and handle it in a way that, like, our existing computer systems just don't do this very well, right? We write them. They're very brittle. They can't adapt. So this is one of the key things is that when I go and say, I'm out of beans, it can come back and say, well, just skip the beans and move on. Or use this instead. Like, that's the thing that makes agents so incredible and so valuable when we build a really powerful agent, is that it can go and adapt and handle situations more robustly sometimes if we do a good job than if we just wrote code. Now, what happens when we have an agent that is working with a computer system, right? We don't want to go and have to listen to what the agent says and then go and type a bunch of stuff into a computer and then copy and paste what the agent says back in or what the computer says is the response back into the agent. Like, that's not very efficient. That's not what we want to do. So how do we move away from that? Well, what we want to do is we want to have the human or some human-driven process kick off the agent by giving it some type of task to perform. Like, help me add this travel expense. And then what we want the agent to do is to go and use the computer system, use a series of APIs, use a series of functions or whatever resources it's been given to go and complete that task. So basically to take our goal that we're stating in natural language, our hopes and dreams for the computation, and translate it into computation. We're telling it what we want, and it's translating it into computation. Now, if you think about the transformer model, one of the first things it was tested on was doing translation. It just turns out that it's really good at translating human language into computation. And it can do that in different ways. So if we were going to go and add an expense for travel, it might need to go and list the rows in a spreadsheet and then get some result back from the spreadsheet, like the rows, that it could go and look at and see, does that expense already exist? And then if it decides it doesn't, it goes and says, add a row for this. And what we want to do is basically have this sort of two-stage. The human goes and kicks off the computation or some human-driven process goes and kicks off this agent. And then it translates that goal or set of goals into a series of calls or interactions with a computer. Now, the question then becomes, how do we make it interact with the computer autonomously? We want this AI agent to be able to go and work on its own. Now, one way we could do this, obviously, is we could say, do this, and tell me how to do it, what API calls to make. And then it could output code, and I could go and copy and paste that code into my editor and run it and accomplish that. But that's not what we want. Then it's human in the loop fully, and that's not really an AI agent so much. Now, it is. It's just a human-in-the-loop AI agent. And I think what we want to look at is how do we do this where once the human has provided the task that the agent runs autonomously. Well, the way that we do that is by creating a loop. I call this an agent loop. And it's a really fairly simple thing once we wrap our heads around how it works. Now, the details of it are important, and we'll go through all the different details and ways that you can adapt the loop and do things that are really fascinating to empower your agent with all kinds of capabilities. But let's drill in on the basic loop to get started. So what happens is after we give the task to the AI, we want it to start deciding a series of actions to take, and we want to run it in a loop. So basically, we're going to run it in a loop, and what our loop is going to be is we're going to give it a prompt. It's going to generate a response which contains an action that it would like to be taken on the computer. We're going to then figure out what that action is. We're going to execute it on the computer, and we're going to give it feedback on what happened. And then we are going to keep doing this in a loop until either the agent or we decide that it should terminate. Now, we are in control of the loop. It gets to decide what the action is at each iteration of the loop. But this is the core idea. So, for example, we get a task, add travel expense. And so the first prompt is decide the first action to add the travel expense. Here are the options of things that you can do. It chooses an option. We identify what that option is, and then we programmatically, based on the response we receive, go and take that action. And then we, really importantly, add that back into the prompt for the next iteration and say, basically, what are you going to do next? Now, we don't actually have to say what are you going to do next, which is kind of fascinating. But that's the core of it. This thing right here is where we take a conversation. We take just sending a bunch of prompts, and we turn it into something autonomous that has agency that can go and do things without us. This agent loop is the key thing. We're going to prompt. We're going to get a response. We're going to turn that response into an action. We're going to execute that action in the computer. And then we're going to go and get feedback on that action. Now, in order to do that, let's look at it in a little bit more detail. What we're going to do is we're going to construct our prompt. And this is what you're going to be doing whenever you're building an agent. You're going to be building a system that does essentially these six things. Now, you're going to have maybe more stuff that's in between. If you go and use some framework or library that's already out there, underneath the hood, it's going to be doing something like this. So, we construct the prompt. This is critically important. Getting a great prompt is super important for the agent. Most of our work is going to be in creating great prompts and thinking about how do we guide our agent through prompting. Second, we're going to send that prompt off to the large language model to get a response. Then we're going to parse the response. So, this means that we have to go and think about how do we get it to produce a response in a format that we can parse and understand what it's asking us to do. Now, the key thing is that we have to be able to parse and understand it with basically our traditional software infrastructure, which is fairly rigid and brittle. And so, we have to think about building our parsing infrastructure, prompting it in a way that we can go and figure out what it wants to do and do it reliably. We're then going to, once we've gotten a programmatic representation of the action it wants to take, like an API call, we go and make that API call or whatever that action is, copy a file, whatever. And then we go and we get the result back. Now, the result could be the feedback from the API, the result, the message that comes out, the HTTP code in the body of that HTTP response. It could be an error message or an error code from the API, whatever it is, but we're going to take some feedback. We're going to turn it into a string. We're going to decide if we're going to continue the loop. And if we are, we're going to take the feedback and we're going to now construct a new prompt, typically where we take in the feedback and add it to the original prompt. And so, over time, it's basically building up a bigger and bigger prompt, and it's going and taking action. This is the most sort of basic approach, and it will take you very, very, very far. In fact, almost all the time, this is the architecture that you need. You don't need a ton of other fancy stuff. You just need this simple loop. So, we're going to go and look at and dive into this loop in more detail. 4 Adding Structure to AI Agent Outputs One of the most fundamental things that we have to do is figure out how do we get the model to output an action that we can understand. If you've ever worked with generative AI models, they are very chatty. They come up with all kinds of descriptions of what they're doing and every time you call them, they do something different. One of the most fundamental things is thinking about how do we prompt one of these models in order to get an action back that we can programmatically determine what the action is and then execute it on behalf of the agent. Now the key thing is the agent, we say it's going to decide what to do and do it. But in reality, it decides what to do, and then we have some traditional software that parses its response and goes and executes the action that it specifies. The agent itself can't reach into our computer. It can only output a bunch of texts that we can then parse and do something with. We have to figure out how do we structure our prompts so that we can extract actions from them. This is one of the core things that we have to do. Now the reason that we're doing this is because we're going to generate a prompt. We're going to put in a prompt. We're going to response. We're going to get an action, then we have to execute it. We have to do this fully autonomously in a loop. Now, if you walk in and go into some generative AI tool and you prompt it, for example, if you went in and you said, \"Give me a Bash shell command to check my IP address on Linux.\" You might get this response the first time where if I had to parse this and this was the only format that I was ever going to get, I could probably write some code that would look for that first Bash markdown block and extract the code that was in it, and then automatically turned it into a string and exec it somehow. Now, I could do that. But if you go and you prompt the things multiple times, you'll see what happens and every time it's slightly different. Now, it's pretty close on these two, but there's also some slight variations. If you look, the first one had this SQL markdown block to represent part of it. This second one does not. This one has a slightly different version, it has a different number of blocks. What happens is that every time we execute generative AI, we're going to get a different response. It's not deterministic. It has this randomness to it. One of the big challenges is we want it to output everything in a really consistent form and typically want to want to minimize the amount of chatty interactions that we get where it's saying, and by the way, look at this fluffy command you could run, or maybe you want to run this alternative command and get all this other stuff. We want to narrow it down. The reason we're doing this is because whenever we're building these AI agents, one of the critical things that we're going to build is this AI environment interface. The environment is the computer that it's going to be executing things or the set of computers that's going to be executing actions on. The environment interface, that interface between the AI and what it can go and do, the actions it can take is something where we're going to have to write code to do it to parse the response. We're going to have to get a fairly limited range of responses to be able to write something where we can parse and understand the range of outputs that it does. There are at least two ways to do this. There may be more, but there's two that are the most common approaches and we're going to talk about both of them in this course. I'm going to start off with the first one, which is doing some prompt engineering and then parsing. And the reason I'm starting off with the first one is the second one requires that the large language model you're working with have some special capabilities to it, namely something called function calling, which we'll talk about later. But we have to be able to go and get an action out. Now, the first way will work with any LLM, and there are certain times when you may want to do it this way because you want to do something really unique and different. This is the bare metal full control over how you go about doing this. We're going to learn this way first. We're going to talk about this way first, and then we'll later go back and look at the other way, which will save us some and in some cases, will be better, but not always. There may be times where you want to use one versus the other. It could be because of the LLM you're working with doesn't support function calling or because there's some unique capability through the language you're creating or something else that the other approach is just not going to be able to do. Here's what we're going to do. We're going to construct this response or construct the prompt. We're going to generate a response and then we're going to parse it. Parsing requires that it be in a very rigorous strict format. This is what we want to do is we want to figure out how do we prompt engineer so that our parser will almost always get something that works. Now, it's not ever going to be guaranteed that it will get something out that it works, and we're going to have things like retry and stuff like that. But we're going to try to prompt engineer our way to get as consistent as possible a format for the response that goes into the parser. Let's take a look at a first example of how we can do this. Then we'll start to build up some intuition about what we're going to do. Here's what we're going to do. We're going to build a simple agentic prompt. That is a prompt where we tell it a series of actions it can take, and we ask it to output its response in a very specific format. We're going to say, \"Whenever I ask you to solve a problem, you can take these actions. Pick up, use, or discard, like you're playing in a game, you will always produce your output in this exact format. Then I've got, basically, insert action, colon, insert object.\" Now this is using the template pattern for a prompt, and I'll link to this in the course. But the idea behind this is we're giving the LLM a template for its output that we want it to follow. When you have placeholders like this in your prompt, LLMs are really good at following your instructions in terms of placeholders and filling them in correctly. I'm telling it, I want you to output an action, a colon, and then an object. Some action you're going to take on some object. I give it the list of objects like pan, butter, green bean, salt, garlic, and spatula and then I tell it you can output one action at a time. Now, you can do multiple actions at a time. That's more advanced. It's a little trickier. But for now, we're just going to do one action at a time. Now I tell it now go ask me the problem to solve. What I'm trying to do is I'm trying to create a prompt that kicks off something that looks like that loop. Now, it goes and asked me for what problem I'd like to solve, I say, \"Cook savory green beans.\" Now, what is its output? Normally, if you did this, you just said, tell me how to cook savory green beans, you're going to get this long recipe instead of steps. What do I get with this prompt? Pickup:pan. You notice what it's done. It's followed my instructions, and it's filling in the template according to my instructions. If we go back and we look at the original template and the prompt, it said insert action and then insert object. Now, couple of interesting things here. One is, you notice that it's decided to capitalize the action and my original actions were not capitalized. Now, this is okay. That's something I could parse and deal with, but it also just begins to show you there's so much variation. Next time I run this prompt, I may get pick up in title case, like I originally had it, and not all caps. But it does follow the fundamental template, which is pickup:pan. I then tell it the result of its action. I'm telling it in the prompt, you can output one action at a time. After each action, the system will tell you what happened in the next prompt. This is simulating the loop that we're going to do. When it says, pickup:pan, I then say, \"handle breaks off pan.\" So I'm telling it and giving it feedback on what happened. I then comes back and it says, \"Okay, discard the pan. We can't use that pan anymore.\" Now, if you think about a loop, and we are going through, we're taking some action. We're getting a result, and then we're giving it feedback. We're doing the same thing in a conversation. In fact, when we go and model the interaction with the agent, I said that we're going to go and take action. We're going to get feedback, and then we're going to bring that back into the prompt for the next iteration of the loop. That's what we're doing here. This is building up a conversation. Every time we add a message to the conversation, it sees the entire conversation. We're basically building a bigger and bigger prompt by building a bigger a longer and longer conversation. This is really critical. The LOM has no memory. It does not remember anything. Every time we add messages to that conversation, it's adding to its memory of what has happened. When I say handle breaks off pan, that gets sent back to it as a user prompt. It says, \"Okay, now I have to decide on my next output, which just discard the pan. Basically, we're running a loop, essentially. But I'm just manually triggering the loop by giving it the feedback on the action. Let's run another example of this because we can get more complicated, as well. Whenever I ask you to solve a problem, you can take these actions. Fetch web page text, and then I have in parentheses the URL, base64_encode, and some value. Now, these look like Python functions, and they're taking parameters. Now we're going to move it closer to a output format that we can directly parse and then call some function that we want to work with. Whatever your programming language is, you can go and output something that tells you a function to run in your program or an API to call. What we're doing is trying to construct a prompt that does that. Now I'm giving it a different template. You will always produce your output in this exact format, and I say, \"Insert your reasoning\". Part of the reason for this is it's nice to give it a placeholder for where it should be chatty. If you are trying to do this bare metal prompting, and all you're doing is giving it the part for the action, but it keeps giving you all this chatty stuff around it, one of the simplest ways to solve it is just to give it a placeholder and say, \"This is where you're going to put your reasoning. This is where you're going to be verbose and chatty.\" Then you typically want that at the start so that it can reason effectively about what it's supposed to do and tell you what it's going to do, and then output the action. I'm saying insert your reasoning and then action, and then I'm saying var equals action. I have some pseudo code here saying, \"You can assign a variable to the result of an action.\" Then I tell it again, one action at a time, and after each time you run an action, I'm going to give you the result of that action. Now, ask for me for the problem to solve. It says, \"Okay, share the problem you want to solve.\" I say, \"Give me a base 64 encoding of the Vanderbilt home page.\" Comes back and it says, \"Understood, and you notice it's put all its chattiness in one place,\" and then it gives me page text, equals fetch web page text, and it has the URL. Then I come back and I say, \"Okay, page text is assigned,\" and then it comes back and it says, \"Okay, now the encoded text is equal to base 64 encoded page text.\" But you notice what we've done. We've created a language for it to describe the actions it wants to take. We're getting consistent parsable output. I could go and I could look for these action markdown blocks, and you can't see it in the slides, but the original actual raw prompt has markdown formatting for that action block. I can now go parse for that. I could then go look at the text inside of that, and I could interpret it as some type of programmatic action. So I've created a structure now where it can tell me how to accomplish the computation, but output it in a format that's not as raw as the original Bash that we saw, but now is something that we can go and parse and execute. Now if I've got a fetch web page text function, I can go and call it regardless of what programming language you're working. Now we can also go and do things that look like what we did with Bash and so just to go full circle, imagine we really did want it to write Bash code or execute things autonomously. We could do that. We say, whenever I ask you problem, you can take these actions. You can execute output Bash or Python. Then I tell you insert your reasoning again, it puts Bash or Python and the code. One at a time, I'll give you the feedback from standard out. I'll tell you you write a program. I'll run it. I'll see what it prints out to the terminal or to standard out. I'll copy and paste that, put it back in as a response to the system. Well, in this case, is what I'm telling it, and then you can decide the next action. I go back and I say, \"Okay, an unknown device has shown up on my network at this IP address, and it comes back and it outputs a Bash block.\" I can run that. I can programmatically parse and identify what's in there. I can take it exec it. Now, you probably don't want to do this. There's a real risk of doing this, but this is to just go full circle on the example. Then I could run it. I ran the actual command. I gave it the raw output to standard out, comes back, and it tells me the next command, which is to run map. Basically, I'm going through and I'm prompting in a way that allows my output to be consistent, and I'm getting to tell it what it can do, how to output it, and specifying the format for all of that. Now, I said that there are two ways to do this. We've been through the first one, which is the bare metal way, prompt engineering and parsing. There is a second way that we will talk about later, which is to use a large language model that supports function calling. The short version of this, which we'll talk about later, is that some LLMs allow you to prompt it and give it a lift of functions, and it will return to you typically in JSON. A description of which function to call and with what arguments. Basically, it skips the part where you have to go and do the prompt engineering to design and get it to output a consistent response. This is really short circuiting and skipping one step, which is to get a consistent output format. You still have to have a great prompt to help the reasoning along. You still have to have all kinds of other things in place, but it simplifies one pieces, which it will go and output in a consistent format. There can still be errors in that format, so it doesn't alleviate you of all the issues, but it helps get you farther along the path and there can be times when you want to do this, times when you don't want to do it. We'll talk about those in more detail."
  },
  {
    "index": 1,
    "title": "2 AI Agent Framework",
    "content": "2 AI Agent Framework. 1 GAIL - Goals, Actions, Information, Language I want you to imagine you've just hired a brand new intern at your company. What is the guaranteed way to make sure that they fail at a task? The basic answer is you give them terrible instructions on what to do. And the prompt is the instructions for our agent. So if we don't give our agent a great prompt to run that loop, we're going to end up setting it up to fail. So we have to think carefully about what goes into that prompt, how do we structure it? And there's so much, and often it gets approached as just like, it's a big wall of text. Just go add more text to it until it works. But I want you to kind of break this down and sort of think about it in a slightly more structured way. How do we go about and construct this prompt? So I want to give you a basic mental framework to think about. You can think of it as GAIL, basically goals or instructions, what you want your intern to do and how, the actions that your intern can take, things that they can do in the office like send an email or go down to a particular department and get some accounting code, whatever it is, the actions that that intern can take, information that the intern needs to go and complete this particular task. So you're going to have things that are independent of task, right? Things that you always want the intern to do, guidelines to follow, rules for your organization. You know, there's a bounded set of actions maybe that that intern is allowed to take. And then you're going to also tell them about language, how to communicate with you, how to describe the results. Like the output should always be a TPS report. The output should always be in this format so that I can, you know, you know, respond to you appropriately. You're going to give them some rules about how to communicate with you. Now I want to break this down a little bit and talk about this. I know this is overwhelming to look at, but once you start getting into the details of it, you'll see that this framework can help you to think through it. What are the goals and instructions? Things that you want them to do, be helpful, take on this particular persona. You want the agent to always respond in sort of a customer service centric manner, rules, things that can and cannot do, process. This is going to be a really important one. First, you should always go and check the list of expenses that are already entered into the system. Then you should determine if the expense has already been entered. And if it has, you should ask the user if they want to add it anyways. Process is things that are sort of process steps that should always be followed. Just like you might teach an intern, you'd say, you know, you're not just going to go in and say, go and enter this expense. And then you see, oh man, they're entering duplicates. You'd go back and say, well, here's the process. First, we always check if there's already one there. So goals, instructions, this is where we get to define its behavior. We get to tell it things that it should always do and in what order, that type of stuff. Sort of the road rules, actions. These are things that the intern can do or an agent can do. These are the ways that it can interact with the computer system or its environment. The actions it can take. Information, so this could be the feedback from the actions that it takes. This could be documents that are needed to complete the task. This could be whatever it is, but this is something that is typically ephemeral. It's something that's related to the current task that it's working on. It may be transit, sort of temporary, like we're going to capture the feedback from executing a particular API call and we're going to provide that back as information. That's all the information. The things that it gets as input at the beginning and then sort of our ongoing sort of session state as we're going and completing the action. This is information that it needs to decide the next action. Initially, when it's deciding the next action to take, it's just getting whatever information we input in. And then over time, as it executes actions, the information is going to accumulate things like the result of other actions that have taken place or updates to the system or whatever it is. And then finally, the language, how we want it to communicate with us. Now we've talked about this, about how to structure outputs, and that's where the language comes in. As we're trying to tell it how to communicate with us. Now we can do some really amazing things with language later, and we're going to talk about those, but language can be a really important piece as well. So let's take a look at this in an agent prompt. So here is our agent prompt. I'm going to take one second, read it to you. You are action agent, a helpful AI assistant. This is our persona. We're telling it who it is. Your goal is to accomplish the task given by the user. So this is what it's supposed to do. If you have enough information to directly respond to the user's request, you should do so. If you need to complete tasks, you can use the provided tools to help you. Whenever you're completely done with the task, you should tell the user the result and terminate the conversation. The available tools are, and you would have a list of tools, but I'm leaving it out here and just putting in dots so that I can get this prompt under one slide. Always respond in the following format. And then I have stop and think step-by-step, insert a rich description of your thoughts there, and then some action format. And this action format is similar to what we did in the past where we were looking at sort of the raw prompt engineering structure that we could use to get it to output actions. Now if we think about in terms of this GAIL framework, this is the G, the A, and the L. There is no I necessarily in here. We have goals slash instructions. We're telling it its persona. We're telling it the goal. We're telling it process that it should follow. We're giving it a set of tools or actions that it can take. And then we're telling it the language to communicate with us in. And so we're building up a structure for an agent prompt. Now when we go and we start thinking about building a prompt, a prompt is going to be broken down into a series of messages, and messages have different roles. And typically what we're going to do is we're going to tell it the G, A, and L part as system messages. These are like ground rules, things you're always supposed to do. And we're going to give it information like feedback on what's happening or inputs to the system as user messages. So those are going to be the key things. Now inside the prompt, we're also going to have typically the assistance decisions, what it chose to do, but I'm not representing that in here. So when we go off and we begin building these prompts, these are the core things we're going to be doing. We're going to be telling it goals and instructions on how to accomplish tasks. We'll be giving it a list of actions that it can take or tools it can use. We'll give it information that it needs in order to select which action or tool to use. And then we're going to have language that defines how to communicate back with us. And we can use function calling to make this language sometimes simpler, but we're still going to have language in this whole thing that helps us decide what it's asking us to do and how, and we can do all kinds of interesting tricks there. So these are sort of the core things that we're going to use and build upon. 2 Giving Agents Tools How do these agentic systems go and interact with the world? Well, we typically have them interact with the world through tools or actions that they can take through those tools. What we're going to talk about now is this concept of a tool or an action that an agent can go and use to try to accomplish a task because most of the time we don't want it to just dream up whatever it wants to do in order to accomplish the task. For example, if we're going to plan a trip, and it tells us that, well, you need to go and plan a trip that involves flying, and we don't want to fly, that doesn't make any sense, so we want to tell it, what are the allowed methods of transportation? Or if we tell it that it can go and collect information from us about what's happening as we're trying to figure out that Internet connection on our machine, if we tell it it's allowed to use a screenshot as a way of collecting information, that's one method that it could do it. If we don't want to take screenshots, we might say, well, your only tool is to ask me. Your only action is to ask me what happened for me to report back. We can think about giving them different tools to go and accomplish tasks. Now, I'm going to go back to this cooking example, and we're going to equip our agentic AI with some tools. Here's what we're going to say. These are the only tools available to you. Pan, you have a one quart sautee pan, a skillet, a large cast iron skillet, a wood fire, and I say a fire burning using oak. You're going to help me cook whatever I need. Since you can't directly lift pots and pans or get food out, you will tell me the steps, and I will perform them. We will go one step at a time, ask me for what I want to cook. Now what I'm doing is I'm giving it, here are the tools that you have available at your disposal to cook this. It's not the universe of cooking utensils and devices. You can't go and say, well, I want an Instant Pot, and I want a microwave, no, you've got a wood fire, a skillet, and a pan, that's it. Now, it has to figure out how does it solve the problem with the tools available to it. These are the tools in the prompt, and you very commonly see something like this done more sophisticated ways, more programmatic ways. Often these tools are described using languages that are more innate to computers, but this is basically a rough approximation of this, and what we'll see is it can clue in on this and do it. This is where we're telling it, these are the tools available to you. We could also say, these are the actions available to you, like, put pan on fire, things like that. It's a different way of approaching it, but we're going to take a look with the tool approach. It says, what do you want to cook today? I go back and I say, I want to cook pizza on the wood fire with a skillet or a sautee pan. It says, it's a fantastic choice , gather your ingredients. I've got my first task to go and perform, so I go and grab all the ingredients, and I say, I'm ready. Then it comes back, and it says, prepare the dough. It tells me what to do to prepare the dough, and again, asking me when I'm done. I say, I'm done. Then it comes back and it says, prepare the wood fire, and tells me how to get the fire prepared. Notice it's using a tool that I have access to that I've told it it can use, and then it tells me how to put the cast iron skillet on it. Tell me once the skillet's preheated. I say, it's done. Comes back, and it says, now let's continue. Cook the dough. Carefully brush a thin layer of olive oil on the preheated skillet. Again, it's solving the problem with the tools I gave it. I have constrained its universe of options and actions and things it can do down to this set of tools that I need to accomplish the problem. Because what's common in the real world is we don't get to just do whatever we want, we have limited resources, limited tools or things that we have to solve the problem with. An agentic AI has to be able to do the same thing. It can't go and say, you want to do X with a computer, therefore, I will reach into the world's fastest supercomputer and do that. No, you don't have access to it. You don't get to do that. We have to be able to put constraints on what tools it can use to solve the problem, and that's how we're doing it here in the prompt. Then I say, I'm done with that step, comes back and it says, now add the toppings. Again, we're still using, roughly, the tools that we've got. Keeps going, place the skillet on the wood fire, gives me some different information. It's roughly constrained to all the tools that I've basically told it to use, and it's solving the problem. This is how we go about, begin to think about getting our agent to solve the problem, but not to just solve it arbitrarily, but to solve it in a way that is bounded in terms of, when we think about our computer systems, they have limited actions that they can perform. They are constrained. They are rigid. We have to start taking agentic AI, and we have to start putting bounds on how it interacts with those systems by giving it a concept of a tool or an action that could be performed with those systems, and we have to bound those tools and actions to what those systems can actually perform. It can do all kinds of things. It can dream up all kinds of ways of solving the problem. It could go and tell a human all kinds of crazy ways of solving the problem, and a human could go and probably perform many of those ways of solving the problem without being told explicitly to be bounded in some way. But the moment we begin putting these agentic AI systems in contact with something like our customer relationship management system, our CRM system, or in control over our email, or in control over whatever other system it is, those systems are going to be very limited in what they can do and the actions they can take, and so we're going to have to express those tools or actions that bound the capabilities of those systems to the agentic AI. Now, do we call this a tool or an action? There's no hard and fast rule to this, I would say. Many times action is the better way to think about it. I'm going to go and say, these are the explicit actions that you can take, particularly when we start talking about interfacing with computer systems. Computer systems that often makes sense to talk in terms of actions because there's such finite number of things that you can ask the system to do, and so it's easier to spell it out as a series of actions. If you're talking about interfacing with something as open ended as a human being, thinking about expressing it in terms of tools often makes more sense because human beings can perform many different actions with a tool, and they can flexibly adapt, and so if the agentic AI says, go and perform this action with this tool, a human will know how to do that. Or if it says, use this tool to do this, a human can go and fill in the blanks. But when we're interfacing with our rigid computer systems, they can't do that, and so often we have to describe each individual action, or operation, or instruction. Think about like CPUs, and now they have a limited number of instructions they can perform, and we have to start thinking about defining it more explicitly in terms of actions. Which one do you use? It depends on the context, depends on what you're trying to do, but you can think about expressing it both ways. There's also other ways that you could do it as well, but these are two of the most common ways to do it. 3 Tool Descriptions and Naming So when we start describing tools and actions to the system, sometimes it's really intuitive, right? If you ask generative AI to use a pot or a skillet, it understands what a pot and a skillet are. But if we start to ask it to interface with our computer systems where we've built all this crazy stuff, we have to start thinking about how do we describe the capabilities of the tools or the actions within the system. That is the moment we move outside of concepts and tools that are well known to like all of these custom computer software tools that we built that may have similar names but do and mean completely different things depending on which computer you're working with. Well, now we have to start thinking about how do we deal with that problem with agentic AI? How do we describe to it the capabilities of the tools that we're giving it access to. So let me give you an example of this. So we built a flipped interaction prompt again, and we're going to describe the tools at the disposal of the agentic AI. Except the agentic AI in this case is going to help us escape when we're trapped in an alien spaceship. So we say, tools, the X 155 tool, and we have a dash, and we say it prepares alien pizza, the Q 63 tool, and we have a dash, and we say it opens a dimensional portal to configurable destination. And then we have the l199 tool, and it causes ship to play Beatles music on a loop. And so we now have a set of tools that we can use to potentially escape the alien spaceship. Now, you notice the names for these tools don't make any sense anymore, right? These aren't tools that we know about. We've never seen these tools before because they're on this alien spaceship. But we want to get the agentic AI to help us, so we tell it what we know about the tools. We don't rely on the name alone to explain the concept. And then we say, okay, I'm trapped in an alien ship, you're going to help me escape. Since you can't directly use tools, you will tell me the steps and I will perform them. Each step must use a tool. We will go one step at a time. Tell me the first step. And so now I'm giving it, here's the list of these alien tools at our disposal, here's what they do. Now. Help me one step at a time. Use the tools to escape the alien ship. How does it do? Step one, prepare an alien pizza. Use tool x155 to prepare an alien pizza. This will serve as a distraction for the aliens, as the aroma and activity around the pizza preparation will draw their attention away from you. I'm not sure if this was my first step that I would use, but I like its thinking here, it's going to distract the aliens. Now, I didn't specify if they're friendly or not. That may be learning culturally that we tend to view aliens as something scary. I guess if I say I'm trapped in an alien spaceship, maybe there's some context there that it may not be a good thing. Then we come back and let's look at this again. So we just gave it x155, but the dash told it, this thing prepares pizza. And that's how it knew on this step one that he could go and prepare pizza. The description of the tool was critical. And in our computer systems, we've built the equivalent of alien tools. All these different custom little software things. And every time we want to work with one, we have to give a really clear and accurate description of what that tool's capabilities are and how you work with it and interact with it. Very, very important. Then goes on and it says, step two, open a dimensional portal. Use tool Q 63 to open a dimensional portal. This portal will be our escape route, leading to a configurable destination. And then it goes on and it says, and use tool l199 to cause the ship to play Beatles music on a loop. This will create an additional layer of distraction, making the aliens focus on the music and further reducing their attention on your escape. So what we've done is we've created new tools that it knew nothing about. We've taught it what those tools do in our prompt. And just to go back and review, all we had to do was we had to go and give it up front. Here's the tools available to you, but not only the tools that are available to you and their names, but what they do. So naming is important, but also the description of what the tool does can be really, really important. Now, how much does tool naming play a role in this? Well, tool naming is a critically important thing. And in fact, the name can be make or break when we're trying to build agentic systems and see how well they use tools. It really, really has to understand what that tool is. And if we can think of a great name for the tool, a very rich and descriptive name, often it'll know how to use it more effectively. We can create crazy tool names like I just did, and then tell it what those do. But often we want both really great names and really good descriptions, but I'm going to go back and I'm just going to drive this home. Now, originally when I did these tools, I had to go and give it descriptions of what they are because the names were bad because they'd never heard these names before. Now what I'm going to do this time is I'm going to go back and I'm going to rename these tools and I'm going to give them names that are descriptive and make sense for what the tool does. So I've renamed my tools to makeAlienPizza, openDimensionalPortal and playBeatlesMusic. And now I tell it I'm trapped in the alien spaceship, what do I need to do? And it goes through and it says, you know, make alien pizza. Same first step, solves the problem the same way, says, go make alien pizza to distract. I don't know why making alien pizza to distract is an intuitive thing, but apparently it thinks it is. Now, let's look at what happens if I change the names of these tools and I don't give a description. So I've got mkpz, sort of like make pizza. I've got odprtl, open dimensional portal, and I've got pbm, which now I've kind of forgotten what it was. Play Beatles music. So I've got these tool names, not very intuitive, they're kind of abbreviated, short. They also look like a lot of things that we've created in computer systems as short abbreviations where we, as the users of those systems, have institutional knowledge that that short little abbreviated name means this. And I can expand it out of my head because I know what it is, but it will not have that institutional knowledge. It will not know what those things are. Now, sometimes it may if they're common things or it's really clear, but often it will not. And so you have to be careful when you start thinking about tool naming, that it's going to be able to understand it. So when I change the tool names and I've now taken away the descriptions like I did before, now it says, okay, first step is to use the mkpz tool to create a detailed map of the alien ship's layout. So it's lost its understanding of the mkpz tool being the make alien pizza tool. And so this kind of helps you understand that it's really, really important that we go about thinking about tool naming, and it's also sort of lost the odprtl. It says, okay, open a portal that will help you move to another part of the ship. So it doesn't have as much context that this is an interdimensional portal, it can take me to any location. And so it's just saying, well, maybe this is just to move around the ship. And then if I go back and I think about, you know, creating tools and building things up, naming becomes really, really important. But also the mixture of tools becomes important. Because, for example, when I go and I say here's a bunch of tools, and I have sort of distracting things and environment, it may not get it right. So in this case I renamed the tools. They all make sense, but I didn't really give description about how you might use them. I didn't really say, here's some tools that you might want to use, but you may not need all of them. And so even how you present the tools, the ordering, the language, you give it around it, like does it have to use all of them or none of them? Are they optional? And as much context as we can give is really important. So in this case, I go in and give it the tools, pliers, scooter, and escape hatch. And I would expect that the first thing it would say would be escape hatch, but instead it says, you know, we need to locate and access the escape hatch, which is great, but then it assumes that the pliers are there for a reason. And it says, okay, use the pliers to loosen the bolts or screws. And I never said there was bolts or screws. So there's missing contextual information, there's missing description and what the tool is used for or why I might want to use it, or that I have to use all of them or none of them. And so we really, really have to think carefully about all of these issues when we begin building agentic AI. Something as simple as the name can make or break these systems, the name of a tool, the description of what a tool does. Many people go and they build these systems and they're like, it doesn't work. The generative AI is so bad when reality is, all you need to go is tweak the name of the tool, the surrounding contextual information you give it about the tools, how you might put the tools together, how many of them have to be used, do they all have to be used or none of them? All these types of things become really, really important details when we start building agentic AI systems. 4 Tool Results and Agent Feedback We've got our Agentic AI going and step by step, trying to solve problems or build out plans in advance. We've got it so that it knows how to be bounded by tools, and we're figuring out how to give it the right information. Let's talk about one of the critical steps. As it goes and uses tools, we're going to have to tell it the outcome of using that tool. That is, it cannot directly observe the result, and so we need to give it feedback on what the result was. Now, the way that we've been doing this, and we've shown it over and over, but I haven't explicitly talked about it as much is we have been giving it a prompt. It goes and tells us what to do. We go and perform that action, and the next prompt that we're giving it is the result of the action. This is critical. The result of applying that tool or performing that action is the follow up prompt that we give it. At the beginning, what we're doing is we're giving it the goal. Go do this, and I want you to do it one step at a time and adapt. It then goes and tells us the first step to perform, and from that point on, as the human being or the system or whatever it is, what we're feeding back to it is not necessarily human questions, but it is the result of actions from tools. Just like we would have in a conversation, what we can imagine is a conversation where a human maybe kicks it off. Then it goes and decides an action that needs to be run. But then the rest of the messages in the conversation where a human would normally be responding, where we were responding and telling it, hey, that cooking action is done. Instead, what we're going to do is we're going to insert the result of the action that it told us to run. Now, we did this implicitly along the way in many cases, but I want to make this really explicit. It's going to say, go and run this action, and then we're going to take it, and we're going to give it a prompt as a follow up that is the result of the action, so we can see what happened. In response, it will tell the next action to run, and then we will go and take the result of that action and put it in as the next prompt, which will be the result of running the action. What we're doing is, we are thinking about, like, where a human being went in and said, help me figure out what restaurant to go to for dinner. Then it kicked off this process where the human being was giving it information, and then it was asking another question. The human being was giving it information, and then it was asking another question. We're going to do something very similar except that it's going to specify an action rather than a question. Then a computer system is going to go and run that action, and we're going to take the result of that action, and we're going to put it in as the next prompt. Then it's going to side the next action based on the result. Then we're going to go and run that action in the computer system, and then we're going to put it in as a result of the next prompt. I'm going to give you a simulation of this in the context of cooking again, because we love steps and planning. With cooking, it's all somewhat intuitive, and we're going to talk about it in terms of a microwave. What we're going to do is we're going to take the microwave, and we're going to break it down. I've called it tools. It probably would have been better to name this actions. But you bear with me on my subtle inaccuracy here in the prompting because it'll still work out just fine. I've told it, here are your tools. Microwave get current time, microwave reset time, microwave increase time, microwave stop, microwave start, microwave open door, microwave close door, insert food in microwave. These are names that start to look like what computer systems speak. These are the names that the programmers types of names that we go and create. If you were creating a Python program, you might create variable names that look like this or function names that look like this, could possibly be. You're probably not going to prefix them with microwave like this, but they might look something like this. What we're doing is we're starting to move the tools and action descriptions closer to the interfaces that our real computing systems work with so that it can start thinking about breaking down and solving problems by specifying actions that look like what it could actually go and run. Now, in this case, we're imagining we've got this biker wave that's computerized, and it can go and run some actions. I say notes. You must set the time on the microwave to start it. You must work independently without input from me. I'm giving it some additional context about how these actions work and some baselines that it needs to know. You have to set the microwave time in order to apply start. If you want to use this action, it depends on another action. That type of information critically important, and it may not be able to infer it. You're going to help me cook food in the microwave. You can only use the provided tools and refer to them by their name. You must complete one step at a time and wait for a description of the outcome. Tell me the first step to reheat a quesadilla? It comes back, and it says, you need to insert the quesadilla in the microwave. Use the insert food and microwave tool to place the quesadilla inside. Then I tell it, here's the result. Food and microwave. This interaction is starting to look much closer to what we could have it do to interface with the real computer system. It then comes back, says, microwave increase time. Then I tell it result, time increased by five seconds. Now, notice the first time, I didn't tell it how much time that we will get an increase. If we go back and we look at the actions, it's just microwave increase time, but there's no specification of how much time. Now, this is where the outcome of applying the tool is becoming critically important because I'm telling I did that. I pressed that button, or I ran that action or I applied that tool, however you want to talk about it. The result is that the time increased by five seconds. This is the first instance where it now knows how much time it's going to get an increase by. Now it has to see that, 1-2 minutes, that's not going to cut it, so now it has to adapt. It comes back and it says, continue to use microwave increase time tool to set the total reheating time. If you want to reheat for one minute, you'll need to increase the time for 60 seconds, use the tool incrementally until you reach the desired time. It's still interfacing on a human level. I go back and I say, result, time increased to 60 seconds. It says, perfect. Now, it says, use the microwave start tool to start the microwave, and I'm just going to throw a little curveball in there for it. I say error door open. It comes back and it says, it looks like the door's open. Use the microwave closed door action. Now I go back and I say result, the door is closed. Notice, every time I'm giving a result, I'm giving it more up to date information about the state of the world around it so that it can make an appropriate decision. When I threw it this curveball and I said, error door open, I'm telling it, you asked me to perform this action. I couldn't do it. But critically, I'm telling it information about why I couldn't do it. I'm giving it information that needs to respond appropriately. When we start building agentic systems, the error messages are critically important that these are very rich and thoughtful error messages. We need to think about, I want to make it so darn clear with my error message, like, what is going on and why? A lot of times we neglect these things in computer systems. We go and we just rely on incomprehensible error messages, and we're just like, programmers will figure it out, IT folks will figure it out. It actually really matters. We want to think carefully about it. Now, sometimes these systems can get really smart, and they can just dissect all of those messages without us going and doing it for them. But sometimes we really need to be thoughtful in the feedback that we give. Error door open, it says, result door closed. Then just one more little note on this. If I had said error 32, and it doesn't know what error 32 is , it's totally hosed. No way it's going to be able to figure out what to do. It's really critical that if it doesn't know an error code, that I translate that error code into something that it will understand the meaning of. If it knows the error codes, who cares? Give it to it in numerical representation, it'll be just fine. Tell it the door is closed now. It says, now, start the microwave, and I tell it, the result the microwave is running. Then it basically comes back and it says, when it's done, use the microwave open door tool to open the door and remove your quesadilla. You see how we've brought it closer to our computer systems. We started to name things in a way that it can figure out and apply these actions or tools. But also, what we're doing is we're now starting to replace human feedback with the result of the tool. Now, the result of the tool right now is me faking it. But what will happen long term is that rather than me faking it, the actual computer system will provide the message back to it. You asked to run this tool, this is what happened. It will then respond, decide the next thing for the computer system to do and go through a chain of these things. The feedback will be critically important that we give it. We must tell it the result of its action so that it understands and can interpret the state of the world around it or the state of the system that it's interacting with, those feedback messages are absolutely critical. The quality of them, the quality of the error messages can make or break these systems. If you go and you create a system and you're like, it's making mistake and mistake, and it's just compounding these mistakes, you probably want to go and look and think about how do you give a really intelligent error message back? Because often if you tweak the error message, suddenly, it'll be able to get out of its loop. There's probably some missing information that it needs to know that if you just gave it in the error message, would help resolve everything. 5 GAME - Goals, Actions, Memory, Environment When we're thinking about agents, it's helpful to take a step back and think about what are the fundamental things that are happening in the agent loop and can we abstract this out into something that we can use to help us think about and design agents and also to design our code around. So I wanna go back to this agent loop. So we have basically a loop that is executing until the task is solved. So we go and we execute our agent and then when we're done, we get back some output out here. Now, what are the key things that we are doing in this loop? Well, at each step, we're deciding what we're going to do. So we're letting the agent go and decide the next action. Basically, we're constructing a prompt, we're having to decide the next action. How does it decide the next action? Well, we give it some goals or instructions describing what it's trying to achieve. So it looks at its goals and then it tries to decide the next action to take. We then go and execute that action and we get some result back and what do we do with that action? Well, we put it into a memory and then we go and loop back around. Now, when we execute the action, where are we executing this action? Well, we're executing it in some type of environment. This environment could be just the process that we're running in. It could be some Python code that we're running. It could be some action we're taking in a cloud. It could be some API call in some third-party system. And so there's different environments where it could be executing actions and then we're giving it the result of actions and storing it as a memory. But the core idea of this loop stays the same. So if we start thinking about what we're talking about, we can begin building a framework. When we build an agent, we're gonna have a set of goals or instructions that it's following. We're gonna have a set of actions that it can take to achieve those goals. So in general, we're gonna go and configure an agent by giving it a set of goals or instructions. We're gonna give it a set of actions that it can take in order to try to achieve those goals. We're gonna give it memory so that it can see the results of past actions and know what happened and remember what it did. And then the last thing we're gonna do is we're gonna give it an environment to execute those actions in. So if we think about this, these things basically compose an agent. So if we have some AI agent, we have a set of goals that it's trying to achieve. And in with the goals, I sort of loosely use the term goals. It's gonna have instructions about process and other things. It's gonna have the actions that it can take to try to achieve those goals. The next thing it's going to have is memory so that as we execute the loop, it remembers what happened. And it's gonna have some environment that it can execute actions in and get feedback on them. So we're not gonna typically be, we're gonna have many different sort of environments that we may be able to plug this in. We may even be able to build the agent in a reusable fashion. So we have the different goals, actions, and memory. And the actions are things like get my availability on my calendar, create a meeting invite, create a draft email. And then we may have different environments where we plug that agent in. For example, it might be running in a Google environment where it's a Google calendar in Gmail. Or we may then take that same agent and plug it into an environment where it's Office 365. And now it's an Outlook calendar. And we're dealing with Outlook email. And so the goals and actions and memory, the set of sort of tools that we're given it are the same things like get my availability, create event, and create a draft email. With those don't change, but how we implement them and execute them in the environment changes. And so we can change the environment. Similarly, we can have some set of actions and memory that we're doing like we want to be able to do things with a calendar. So we have like check availability, draft email, but we change our goals. And so for one agent, we say, if you get a request to schedule a meeting, you just immediately create the invite and you directly send out the email. Another one we have may have a different goal. We want to say you check the availability and then you email the user. The goal is to inform the user of their options, not to automatically schedule it for them. The goals change. So these sort of four things, if we start using them as basic abstractions for thinking about the design of agents, but also thinking about a framework for implementing in code to design agents, it can be really helpful. The other thing I want to encourage you about when you're thinking about agents, just think about prototyping quickly in conversations. Go and try things out. See what the effect of different tool namings and things are by playing around in an interactive conversation with some tool like ChatGBT or Cloud. 6 Simulating Agents with ChatGPT When we're building agents, one of the hardest parts is getting the design of the agent, the instructions for the agent, the set of tools that it uses, the format of the information that comes back to it. That is really the hard part. The actual implementation in code is much, much less difficult than actually thinking through what are the instructions, the prompts, the tools, how do we design all of this so it's robust and resilient? Now, one of the best things that we can do is we can try to rapidly iterate. One of the things that we can do to iterate faster is by simulating agents through a conversation. Because at the end of the day, the agent loop is essentially a big automated conversation. We can do exactly what's going on in the agent loop. Well, not totally exactly, but really close simply by going into something like ChatGPT and having a conversation if we set it up in the right way, so it looks very similar to the prompts that the actual agent will be receiving in our agent loop. Now, this is a way of doing rapid prototyping, and let's take an example and go through it and see how we go about doing this. What I'm going to do is I'm going to set up a simulation, and this is the prompt that I'm going to use to build that simulation. I'd like to simulate an AI agent that I'm designing. The agent will be built using the game framework with these components. Goals, document all the code of the project. Actions, list files, and I have fully qualified path, read file, write file. At each step, your output must be an action to take, stop and wait, and I will type in the result of the action as my next message. Ask me for the first task to perform. The goals are specified here, the actions are here, the environment is me. I am the execution environment. When it's going to happen, is this going to output an action? I'm going to go and tell it, here's what the result of your action is. I am becoming the manual human execution environment, which allows me to dream up any tool under the sun and how it might behave, inject errors arbitrarily. Then finally, the memory is essentially just the conversation. I input this prompt and it says, great, what's the first action you'd like to perform? I come back and I say, document the project. Then it goes and basically is running the first iteration of the loop. Our agent loop has been kicked off and it says, list files, fully qualified path. Now, there's many different things I could do at this point. I could allow it to put something like fully qualified path in, or I could be more strict and I can say, you know what? That's not really the behavior I want because I wanted to fill in the path and it didn't. I go back and I say, well, error, no path provided. It just filled in some random variable name and it says, please provide the fully qualified path of the project directory. What this has shown me is, I've got a flaw in my design of my agent. Now, if we go back and we look at the agent design, it had a tool to list the files, read a file, and write a file, but they all defend on a fully qualified path, and we haven't given it a starting path. Now, this could be something we could inject into the memory as a fully qualified starting path. That would be one way of fixing this design flaw. Another way would be to give it some tool to actually go and look at the underlying directory that we're running in, and so it can query that. I'm going to go and do that. I'm going to update the prompt. Basically, all I've done is I've taken my original prompt, I've just adapted it, and I've given it a new tool. Now, notice this tool is really fast to implement because all I have to do is update my prompt and add it as a new action that this thing can take. I dream up why I want it to look like. I'm prototyping in a conversation. I've added a new tool, which is GetProjectDirectory. It goes through, says what task, I say document the project, and now what it does is it runs the GetProjectDirectory task, and then I simulate the output of that tool. I haven't even built it. There's no code. I'm just simulating in a conversation, and then that's all it needs. It can go and then say, okay, list files, and now it gives me the actual fully qualified path of the directory based on the output of my prior tool that I simulated. I'm going to simulate that these are the files that are in the directory. It then comes back and it says, okay, read file. Our agent loop is running, it's running actions each time when it runs an action, we are acting as the environment, we are simulating the output back, and our conversation is the memory that it has. When I say, okay, well, print hello world is all that's in that file. It then comes back, it runs the next action, which is write file. Now, things get interesting, because I've discovered a potential design issue that I might want to fix. I didn't really specify very clearly what it meant to document the project, and it's deciding to overwrite the existing file and add comments to it. So what I've got is an ambiguity in my goals. Now I could go back and I could refactor my goals to make it more explicit about what it's supposed to do, or I could change my tools and my tool design to make it really clear through my tool design, or to prevent this type of thing, for example, I could throw an error if you try to write over an existing file, or I could go and make it so it's create doc, and it passes in the name of the file it's documenting for, and I could take that decision making out of the agent's hands. But you see, this conversation is allowing me to discover those issues and rapidly iterate and change the design of the agent much faster than we can do if we're gonna do this in code and execute and compile or whatever we're dealing with. And then I can go back and continue simulating, see if it handles different languages okay. So now I've got a Java file that's being read, and it goes through and it documents it. I could go and test and see how long can the conversation get, like how many files can I put in that original listing, and give it text back before it breaks down. It becomes much easier to do that initial rapid experimentation. Now I'm still gonna go and do more experimentation after I implement it, but before I even get to implementing it, I wanna go and do this rapid, really cheap experimentation to make sure that I have designed my agent appropriately, that I've got the right goals and instructions in place, I've got the right actions in place, I've got the right format for the information that's coming back from my environment, in this case me, my simulated environment. And I wanna look at how is a conversation gonna work, how long can it get, will it break down, will it be able to get what it needs out of the conversation. And so it gives me that ability to go and control what the agent sees, to inject errors, to identify potential ambiguities, and to make better designs, and improve the decision making of my agent by simulating a conversation, giving it all kinds of interesting situations to handle, to see how it reasons. And then to quickly update the agent as quickly as editing in a prompt."
  },
  {
    "index": 2,
    "title": "3 Rethinking How Software is Made",
    "content": "3 Rethinking How Software is Made. 1 Building the Imposssible with AI Agents One of the most important things to do when you're building AI agents is to think more innovatively about what types of problems you're solving and how you're solving them. You shouldn't be rushing out to try to build something that you can already build today without generative AI. In most cases, if we can already solve the problem well without generative AI, generative AI isn't really going to bring something amazing to the table. We want to think about the problems that we can't solve any other way. The problems that just a few years ago would have seemed absolutely impossible and intractable. And it turns out that there's many of those problems. We just have to think more creatively about how we build our agents and how we use generative AI and what a tool is. A tool for an agent can be as simple as a prompt, a really specialized prompt that can do something amazing that's never been possible before. We're going to have to rethink what we use as input to our systems. You're so used to input to systems that is so rigid and fixed. And when we start breaking those boundaries and start thinking about richer inputs, photographs, rich conversations, audio, all kinds of other things that LLMs can work with, we can build AI agents that are much more extraordinary. So let's take a look now and explore the future of computing and how computing is going to be transformed by generative AI, get a perspective on what's new and different, and then start thinking about how do we use this to build really amazing AI agents. 2 Rethinking How We Teach Innovation So we think of building applications as we go and hire a software developer, right? You go and find a software developer and you tell them this is what I'd like you to build me. And then the software developer goes and spins their wheels for weeks or months or years and delivers something to you that you realize doesn't actually do what you want it to do and you go back and forth with them. And hopefully you eventually arrive at some software that you want. Or more likely you're just given some software application that was licensed for your organization and you're told to use it even if it was built for somebody else and what they really want it to do. The future of software is going to look very different because the way we can build applications is so much more user friendly and the innovation is going to happen at the edge, the people who are actually using the applications. So let me give you an example of what the future of application building looks like. This is the Vanderbilt travel and business expense policy. And it's like a 17-page PDF outlining all the rules for travel expenses. So what does the travel expense auditing software of the future look like and how do we build it? So I took this and I uploaded it to ChatGPT and I said, please read the provided document so that you can answer questions about it. And then what I did is I uploaded a rental car receipt. And I said, please analyze the attached receipt and let me know if it complies. And ChatGPT comes back and I want to highlight this part at the bottom. It says collision damage waiver. For domestic travel, the CDW should be declined as rental vehicles are fully insured through Vanderbilt's insurance portfolio. The receipt shows a charge for CDW, which may not comply if this is domestic travel. So it read the travel policy. It read the receipt and it applied the travel policy to the receipt. Now, I could build a system like this as a software engineer, but it's not just me. It's me plus a team of people. And it would probably take us years to get to something that's as good as we got right there simply by saying, take this travel policy, read it, and apply it to this receipt. And that's a task that anybody can do, right? People are writing policies all the time. Well, now if we want to become and build policy auditing software, it's simply giving it the policy and the thing to apply the policy to. And suddenly we have software. That's a transformation in how we build software. Or I want to look at this example that I did the other day with my son. So my son, he's always been collecting things since he was little. You know, he started off, it was like rocks and sticks that he wanted to bring into the car and bring home with us. And then as he got older, it was like Pokemon cards. And now he's older, and so it's shoes. And the other day I looked at him and I said, buddy, I know what you need. He said, what, dad? I said, you need inventory management. And he's like, oh my gosh, here dad goes again with chat GPT. I know what's coming. And so we've typed this prompt together. And this is the future of inventory management. Whenever I give you a photo, create an inventory of what I gave you with approximate price. Keep the inventory up to date as I add pictures. And then he knew what to do with it. He went and took a picture of his Air Jordans. And it comes back and it says inventory item. Air Jordan basketball shoes. Description, red colorway with iconic Jumpman logo. Condition, used with visible wear and slight discoloration. And then he went and took a picture of his old van, skate shoes. And it comes back and it says, here's the updated inventory. Item, Air Jordan basketball shoes. The number two in the inventory now is van skate shoes. Condition, heavily worn and it has approximate prices. And he has just built an inventory management system that is more sophisticated than what teams of software engineers could build. In fact, I would challenge you to take a team of software engineers and say, without generative AI, build a vision-based inventory management system that works with nearly any object in the world and do it in five minutes. It's not going to happen. It's not going to happen in a week. It's not going to happen in a month, a year, decades at best without generative AI. And now my son can build this in two sentences. So that transforms who can build the applications of the future. My son can build the future of inventory management. And that means we have to retrain people how to think about how they solve problems with computation and empower them to start thinking about building software. 3 Hallucination is a New Form of Computing I want to start with talking about translation, and this may seem a little crazy, but these models, the reason they were built originally is people were thinking about language translation. They were testing them in the original papers, they were testing them on translation tasks, translating from one language to another. It turns out they're supremely good at translation, but good at translation in a way that we have to rethink what translation means. So let's look at an example of this. I'm going to go into chat GBT, and I'm going to have it do a translation task. I'm going to say, imagine a language based on the stacking of rocks. Translate teachers drive the world forward into this language. Now, think about that. This language doesn't exist. I mean, asking it to imagine and build its own language based on the stacking of rocks, and then to translate a phrase in English into this imaginary language that's based on the stacking of rocks. That is a translation task that is really amazing, but it's not one that a computer has ever been able to approach. A human could, but not a computer. It comes back and it says, in a language based on the stacking of rocks, each concept or word would be represented by a specific arrangement of rocks using their size, shape, and positioning. Teachers, a stack of rocks arranged to symbolize stability, foundation, and guidance. Three large flat rocks forming a stable base of a triangle. Middle layer, smaller rounded rocks symbolizing individual teachers, and top layer, a single flat rock representing unity and guidance. Drive, a dynamic arrangement suggesting movement and propulsion. It goes on and it describes how we could stack rocks to represent this idea of teachers drive the world forward. It goes on to even draw a little ASCII. art diagrams. If you think about the complexity of that translation task, it's not only imagining a new language, but it's then translating into that new language. It is doing something that computers could not do before. It's these capabilities that are going to reshape what we can do in computing, and also reshape who is an innovator in computing. Let's start taking a look at how we use translation and some of these capabilities to do new things. One of the things that you always hear about with generative AI is everybody complains about hallucination. Hallucination, how will we ever trust these systems because of hallucination? Hallucination is being talked about as if it's this problem, as if it's this flaw in generative AI. No, hallucination is a new form of computation that we can now use. It's an incredibly important form of computation. In fact, I think if we didn't have hallucination, we wouldn't be interested in generative AI. I want to take you through what this new form of computation can do and how that reshapes computing. Like you come home at the end of the day, and you realize that you haven't paid attention to what was in your pantry, and now you need to cook something. You assemble the items on the counter and you say, this is all I have in my pantry, give me a dinner recipe. It comes back and it says, black bean and avocado salad with yogurt lime dressing. It goes on then to give you the complete instructions to cook it. Now, think about that. We just applied this new operation, this new type of computation, hallucination to solve a real-world problem that would have been really hard to solve with computing before. Now, we used a lot of other things to make this work, but it dreamed up something and it was easy. It was easy to apply this new form of computation. When we start thinking about how we can apply it, we can use it not only to come up with games or what we might eat for tonight, but also to inform us. One of the biggest problems that I have with how people use generative AI is they go off and they try to use it to seek answers. This is the wrong way of thinking about using it. Not only because it can make mistakes in the answer, because it can hallucinate at the wrong time when you don't want it to, or when you don't recognize that it's hallucinating, which is the bigger problem. We have to think about using it in ways that are more thoughtful and more creative that allows to tap into this hallucination computation. One of the ways we can do that is we can never ask for answers. Instead, we can ask for perspectives and always ask for more than one of something. Because the moment we ask for more than one of something, we begin learning. Let's look at an example of this. I'm camping with my family in the woods. We have tinfoil, a skillet, and no other cooking tools other than fire. Give me three ways to bake a cake and compare and contrast the difficult. Now, notice what I'm doing. I'm asking it to hallucinate three different ways. Not one, not an answer, not how to bake the cake, but many different ways. It comes back with, I can do a tinfoil oven, I can do skillet baking, I can do a Dutch oven substitute, and then it compares and contrasts them across a number of dimensions, like heat control and outcome and ease of use. But you notice what's happened. Now that it's given me more than one, now that I've applied hallucination as an operation, I get lots of different ways, but I'm learning. I'm learning ways that I may not have known, and I'm also learning when I might use one versus the other, but I then still have the ability to decide how I'm going to do it and why. I might come up with a fourth way that combines these in some way, or I might follow up with questions, but it stops being about give me an answer, and it starts using hallucination as a new computation that can help inform my perspective, discover new ways of solving problems, apply things in unique and unconventional ways to all kinds of interesting things. Now, one of the things that we can do with it is we can help solve problems in human bias. And one of the problems that we have in human bias is confirmation bias. We look at data and we say, this is what it means. And we don't look at other perspectives on data. And we with AI before of the past only ended up suffering from more bias because it couldn't give us alternative perspectives. It was trained and gave us one perspective. But now we can say, please look at the data and generate three conflicting interpretations of it, and we can force ourselves to look at many different perspectives on the data. And then it forces us to think more deeply about it and weigh the different perspectives and decide what is appropriate. The AI doesn't become the source of the bias. It becomes the source of helping us to break out of our bias and our perspectives. Or maybe we go and we have humans that are collaborating on a project and they're meeting and we take the meeting transcript. And I don't know why people think summarizing meetings is the end all be all of generative AI. I think it's the worst use case. I think it's a waste of generative AI because we can do something much more valuable. We can simply look at the transcript and say, please review the transcript and identify three potential ambiguities for the team to discuss. And it can read the transcript and look at the project that the team is discussing and try to find problems where maybe there's a gap and we need to have the team members communicate further. And it doesn't matter if it hallucinates because if it hallucinates, the worst case is that people talked more about those things and came to agreement that there wasn't ambiguity there. And it furthered human communication. 4 New Ways to Access and Extract Information The other thing about this new form of computing is it's going to have to force us to rethink how we approach data. Because in the past, we really thought of data as something that we feed into our traditional computational systems to get results. And so we've had this enormous amount of focus on the structure and format of the data. And it turns out a lot of that effort is probably going to be wasted in the future. A lot of the ways that we are managing data, collecting data, working with data are probably not the right approach for the future. Because generative AI is much more flexible in not only its definition of what data is, but what it can get data out of and how it can fuse data together and the formats that it can accept. And it's not stuck in this rigid world of the way that we did computing before. So let's take a look at a good example of this. Now the other day, somebody dropped off this catering menu on my door. And I thought, ah, I wonder if generative AI knows how to build lunch orders. So I snapped a picture of this catering menu, and I said, I need a lunch order for 15 people. And ChadTPD came back, and it gave me a lunch order. House salad, garlic knots, and it's built a really good lunch order. It's got a variety of foods. But the interesting thing, right, is what was the data? It was a picture of the menu. It wasn't a database where somebody had meticulously mapped all of this information into standard well-known forms and fields and carefully curated it. No. I just gave it a picture, and it extracted the information it needed in order to solve the problem on demand. And suddenly, we can begin solving problems in new ways, because we can get data out of things that we couldn't access before. For example, what if you're in a room, and you need to figure out how to use the coffee maker? So I was in this hotel room, and the coffee maker was giving me trouble. And so I snapped a picture of the coffee maker, because it kept flashing this light, and I couldn't figure it out. And I said, explain to me step by step how to use this. And it came back, and it gave me complete instructions on how to use this cured coffee maker. Now before, to do this, I would have to have some way of querying a database of user manuals of different products in order to discover the instructions and then try to look through an index to find the right page. And it would have required all this structuring and managing of data. And now my querying, the way that I access it is I simply take a picture and say, tell me how to use it step by step. And it not only transforms what can become data, but it completely transforms how we query and access data. So suddenly data can be accessed at any time in any place, contextually relevant to where we are in the world in a new way. It's kind of like in The Matrix, when Trinity needs to be able to fly this helicopter to save Neo and everybody else, she doesn't know how to fly the helicopter. And so this operator downloads all the instructions into her brain, and then she flies the helicopter and saves everyone. Except this is getting close to that simply by snapping a picture and saying, tell me how to use it. Because suddenly I can query and access the information, the learning, the instructions on demand in a new way."
  }
]
